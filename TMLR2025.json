{
  "https://openreview.net/forum?id=gwUOzI4DuV": {
    "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment",
    "volume": "main",
    "abstract": "Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges because interactions with an environment are prohibited. In this paper, we propose a new framework, namely \\textbf{M}ulti-\\textbf{A}gent \\textbf{C}ausal \\textbf{C}redit \\textbf{A}ssignment (\\textbf{MACCA}), to address credit assignment in the offline MARL setting. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to integrate with various offline MARL methods seamlessly. Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. In our experiments, we demonstrate that MACCA not only outperforms state-of-the-art methods but also enhances performance when integrated with other backbones",
    "checked": true,
    "id": "80e2f19b543685bbcfc4137216ebe688060adf00",
    "semantic_title": "macca: offline multi-agent reinforcement learning with causal credit assignment",
    "citation_count": 3,
    "authors": [
      "Ziyan Wang",
      "Yali Du",
      "Yudi Zhang",
      "Meng Fang",
      "Biwei Huang"
    ]
  },
  "https://openreview.net/forum?id=9oToxYVOSW": {
    "title": "Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation",
    "volume": "main",
    "abstract": "Gradient-based optimization has been a cornerstone of machine learning that enabled the vast ad- vances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can be- come limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications — neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) — we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function",
    "checked": true,
    "id": "e007954131351392042d9176e862f66dbdc43696",
    "semantic_title": "efficient and flexible neural network training through layer-wise feedback propagation",
    "citation_count": 2,
    "authors": [
      "Leander Weber",
      "Jim Berend",
      "Moritz Weckbecker",
      "Alexander Binder",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ]
  },
  "https://openreview.net/forum?id=Tnwci2kLna": {
    "title": "CXAD: Contrastive Explanations for Anomaly Detection: Algorithms, Complexity Results and Experiments",
    "volume": "main",
    "abstract": "Anomaly/Outlier detection (AD/OD) is often used in controversial applications to detect unusual behavior which is then further investigated or policed. This means an explanation of why something was predicted as an anomaly is desirable not only for individuals but also for the general population and policy-makers. However, existing explainable AI (XAI) methods are not well suited for Explainable Anomaly detection (XAD). In particular, most XAI methods provide instance-level explanations, whereas a model/global-level explanation is desirable for a complete understanding of the definition of normality or abnormality used by an AD algorithm. Further, existing XAI methods try to explain an algorithm's behavior by finding an explanation of why an instance belongs to a category. However, by definition, anomalies/outliers are chosen because they are different from the normal instances. We propose a new style of model agnostic explanation, called contrastive explanation, that is designed specifically for AD algorithms. It addresses the novel challenge of providing a model-agnostic and global-level explanation by finding contrasts between the outlier group of instances and the normal group. We propose three formulations: (i) Contrastive Explanation, (ii) Strongly Contrastive Explanation, and (iii) Multiple Strong Contrastive Explanations. The last formulation is specifically for the case where a given dataset is believed to have many types of anomalies. For the first two formulations, we show the underlying problem is in the computational class P by presenting linear and polynomial time exact algorithms. We show that the last formulation is computationally intractable, and we use an integer linear program for that version to generate experimental results. We demonstrate our work on several data sets such as the CelebA image data set, the HateXplain language data set, and the COMPAS dataset on fairness. These data sets are chosen as their ground truth explanations are clear or well-known",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Davidson",
      "Nicolás Kennedy",
      "S. S. Ravi"
    ]
  },
  "https://openreview.net/forum?id=FPJKZDzdsW": {
    "title": "Fairness with respect to Stereotype Predictors: Impossibilities and Best Practices",
    "volume": "main",
    "abstract": "As AI systems increasingly influence decision-making from consumer recommendations to educational opportunities, their accountability becomes paramount. This need for oversight has driven extensive research into algorithmic fairness, a body of work that has examined both allocative and representational harms. However, numerous works examining representational harms such as stereotypes encompass many different concepts measured by different criteria, yielding many, potentially conflicting, characterizations of harm. The abundance of measurement approaches makes the mitigation of stereotypes in downstream machine learning models highly challenging. Our work introduces and unifies a broad class of auditors through the framework of \\textit{stereotype predictors}. We map notions of fairness with respect to these predictors to existing notions of group fairness. We give guidance, with theoretical foundations, for selecting one or a set of stereotype predictors and provide algorithms for achieving fairness with respect to stereotype predictors under various fairness notions. We demonstrate the effectiveness of our algorithms with different stereotype predictors in two empirical case studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inbal Rachel Livni Navon",
      "Omer Reingold",
      "Judy Hanwen Shen"
    ]
  },
  "https://openreview.net/forum?id=6Aj0aNXfRy": {
    "title": "Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) often suffer from performance degradation as the network depth increases. This paper addresses this issue by introducing initialization methods that enhance signal propagation (SP) within GNNs. We propose three key metrics for effective SP in GNNs: forward propagation, backward propagation, and graph embedding variation (GEV). While the first two metrics derive from classical SP theory, the third is specifically designed for GNNs. We theoretically demonstrate that a broad range of commonly used initialization methods for GNNs, which exhibit performance degradation with increasing depth, fail to control these three metrics simultaneously. To deal with this limitation, a direct exploitation of the SP analysis--searching for weight initialization variances that optimize the three metrics--is shown to significantly enhance the SP in deep GCNs. This approach is called \\textit{\\textbf{S}ignal \\textbf{P}ropagation \\textbf{o}n \\textbf{G}raph-guided \\textbf{Init}ialization (\\textbf{SPoGInit})}. Our experiments demonstrate that SPoGInit outperforms commonly used initialization methods on various tasks and architectures. Notably, SPoGInit enables performance improvements as GNNs deepen, which represents a significant advancement in addressing depth-related challenges and highlights the validity and effectiveness of the SP analysis framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senmiao Wang",
      "Yupeng Chen",
      "Yushun Zhang",
      "Ruoyu Sun",
      "Tian Ding"
    ]
  },
  "https://openreview.net/forum?id=p0KTYl2B9T": {
    "title": "Spaced Scheduling for Large Language Model Training",
    "volume": "main",
    "abstract": "Recent breakthroughs in deep learning have accelerated progress toward increasingly capable large language models (LLMs), even sparking discussions about the path to Artificial General Intelligence (AGI). Yet, current LLM training pipelines continue to depend on heuristics and human-driven empirical analysis to curate data. In practice, more sophisticated data selection methods often incur high costs, exhibit limited adaptability, or do not consistently surpass simple random baselines across various models and datasets. In this work, we propose Spaced Scheduled Training (Sst), a novel adaptive data selection strategy that prioritizes training examples based solely on per-example perplexity computed from the model's own evolving parameters. By obviating the need for external reference models, Sst customizes data selection to the model's unique characteristics, including its pre-training data composition, and eliminates biases commonly introduced by these external models. Extensive experiments on seven LLMs (0.5B to 32B parameters) in the instruction-finetuning (IFT) setting show that Sst consistently outperforms representative state-of-the-art selection approaches like Deita and InsTag on the Open LLM Leaderboard. For instance, with Qwen2.5-32B and a 30k examples data budget, Sst achieved a 42.75% Open LLM Leaderboard score, exceeding a leading data-selection baseline (38.56%) and the full-100k dataset baseline (39.58%). We further present a theoretical framework to assess computational overhead of model-based selection methods, showing that Sst remains efficient in practical scenarios, and propose strategies to mitigate the overhead in worst-case scenarios. Our findings underscore the potential of model-informed dynamic data selection, offering an efficient, adaptable, and cost-effective approach. We release our training code, trained models, and data mixes in our public repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine El hattami",
      "Nicolas Chapados",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=PMO30TLI4l": {
    "title": "Selective Concept Bottleneck Models Without Predefined Concepts",
    "volume": "main",
    "abstract": "Concept-based models like Concept Bottleneck Models (CBMs) have garnered significant interest for improving model interpretability by first predicting human-understandable concepts before mapping them to the output classes. Early approaches required costly concept annotations. To alleviate this, recent methods utilized large language models to automatically generate class-specific concept descriptions and learned mappings from a pretrained black-box model's raw features to these concepts using vision-language models. However, these approaches assume prior knowledge of which concepts the black-box model has learned. In this work, we discover the concepts encoded by the model through unsupervised concept discovery techniques instead. We further leverage a simple input-dependent concept selection mechanism that dynamically retains a sparse set of relevant concepts of each input, enhancing both sparsity and interpretability. Our approach not only improves downstream performance, but also needs significantly fewer concepts for accurate classification. Lastly, we show how large vision-language models can guide the editing of our models' weights to correct model errors",
    "checked": false,
    "id": "984be2ce918e3d495fc0174519f34d7b472445ab",
    "semantic_title": "concept bottleneck models without predefined concepts",
    "citation_count": 12,
    "authors": [
      "Simon Schrodi",
      "Julian Schur",
      "Max Argus",
      "Thomas Brox"
    ]
  },
  "https://openreview.net/forum?id=T49vPTkIt5": {
    "title": "Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments. However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which always results in algorithmic instability, difficulty in convergence, or entrapment in local optima. While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques. In this paper, we introduce \\textbf{E}volutionary action \\textbf{SPA}ce \\textbf{R}eduction with \\textbf{K}nowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback. In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively. Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents. The code can be found in https://github.com/LiuZhihao2022/eSpark",
    "checked": true,
    "id": "d7a505964defe667f9c7aa8cb7b79f1caef89614",
    "semantic_title": "knowing what not to do: leverage language model insights for action space pruning in multi-agent reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Zhihao Liu",
      "Xianliang Yang",
      "Zichuan Liu",
      "Yifan Xia",
      "Wei Jiang",
      "Yuanyu Zhang",
      "Lijuan Li",
      "Guoliang Fan",
      "Lei Song",
      "Jiang Bian"
    ]
  },
  "https://openreview.net/forum?id=kEUvWFHEsn": {
    "title": "[RE] GNNBoundary: Finding Boundaries and Going Beyond Them",
    "volume": "main",
    "abstract": "Graph classification models are becoming increasingly popular, while explainability methods face challenges due to the discrete nature of graphs and other factors. However, investigating model decision-making, such as through decision-boundary regions, helps prevent misclassification and improve model robustness. This study aims to reproduce the findings of GNNBoundary: Towards Explaining Graph Neural Networks Through the Lens of Decision Boundaries (Wang & Shen, 2024). Their work supports 3 main claims: (1) their proposed algorithm can identify adjacent class pairs reliably, (2) their GNNBoundary can effectively and consistently generate near-boundary graphs outperforming the cross entropy baseline and (3) the generated near-boundary graphs can be used to accurately assess key properties of the decision boundary; margin, thickness, and complexity. We reproduce the experiments on the same datasets and extended them to two additional real-world datasets. Beyond that, we test different boundary probability ranges and their effect on decision boundary metrics, develop an additional baseline, and conduct hyperparameter tuning. We confirm the first claim regarding the adjacency discovery as well as the second claim that GNNBoundary outperforms the cross-entropy baseline under the limitation that it requires intensive hyperparameter tuning for convergence. The third claim is partially accepted as we observe a high variance between reported and obtained results, disproving the reliability and precision of the boundary statistics. Code and instructions are available at: https://github.com/jhb300/re_gnnboundary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Henrik Bertrand",
      "Lukas Bierling",
      "Ina Klaric",
      "Aron Wezenberg"
    ]
  },
  "https://openreview.net/forum?id=lTt2cTW8h1": {
    "title": "Return-Aligned Decision Transformer",
    "volume": "main",
    "abstract": "Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. It is increasingly important to adjust the performance of AI agents to meet human requirements, for example, in applications like video games and education tools. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and includes a mechanism to control the agent's performance using the target return. However, the action generation is hardly influenced by the target return because DT's self-attention allocates scarce attention scores to the return tokens. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to more effectively align the actual return with the target return. RADT leverages features extracted by paying attention solely to the return, enabling action generation to consistently depend on the target return. Extensive experiments show that RADT significantly reduces the discrepancies between the actual return and the target return compared to DT-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsunehiko Tanaka",
      "Kenshi Abe",
      "Kaito Ariu",
      "Tetsuro Morimura",
      "Edgar Simo-Serra"
    ]
  },
  "https://openreview.net/forum?id=R7QFlwvnne": {
    "title": "Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier",
    "volume": "main",
    "abstract": "For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to easily tune language models to maximize auxiliary, non-preferential objectives according to the LLM designer's preferences (e.g., tuning lexical style or minimizing specific kinds of harmful content). Critically, these designer objectives may not be amply human-labeled or represented in available data, align with user preferences, or even be able to be captured tractably by binary preference pairs. To leverage the simplicity and performance of DPO with the generality of RL, we propose a unified approach. Based on a simple decomposition of preference and auxiliary objectives, we allow for tuning LLMs to optimize user and designer preferences without any additional specialized or preference data, computational cost, stability \"tweaks\", hyperparameter tuning, or training instability. The proposed method, Unified Preference Optimization, shows the ability to effectively generalize to user preferences and auxiliary objectives, while preserving or surpassing alignment performance on challenging benchmarks across a range of model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudhan Badrinath",
      "Prabhat Agarwal",
      "Jiajing Xu"
    ]
  },
  "https://openreview.net/forum?id=Z0DhgU8fBt": {
    "title": "[Re] Improving Interpretation Faithfulness for Vision Transformers",
    "volume": "main",
    "abstract": "This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by Hu et al. (2024) alongside interpretability methods for Vision Transformers from Chefer et al. (2021) and Xu et al. (2022). We investigate claims made by Hu et al. (2024), namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Izabela Kurek",
      "Wojciech Trejter",
      "Stipe Frkovic",
      "Andro Erdelez"
    ]
  },
  "https://openreview.net/forum?id=y8VXikiIU0": {
    "title": "Enhancing Sample Generation of Diffusion Models using Noise Level Correction",
    "volume": "main",
    "abstract": "The denoising process of diffusion models can be interpreted as an approximate projection of noisy samples onto the data manifold. Moreover, the noise level in these samples approximates their distance to the underlying manifold. Building on this insight, we propose a novel method to enhance sample generation by aligning the estimated noise level with the true distance of noisy samples to the manifold. Specifically, we introduce a noise level correction network, leveraging a pre-trained denoising network, to refine noise level estimates during the denoising process. Additionally, we extend this approach to various image restoration tasks by integrating task-specific constraints, including inpainting, deblurring, super-resolution, colorization, and compressed sensing. Experimental results demonstrate that our method significantly improves sample quality in both unconstrained and constrained generation scenarios. Notably, the proposed noise level correction framework is compatible with existing denoising schedulers (e.g., DDIM), offering additional performance improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abulikemu Abuduweili",
      "Chenyang Yuan",
      "Changliu Liu",
      "Frank Permenter"
    ]
  },
  "https://openreview.net/forum?id=YCBVcGSZeR": {
    "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
    "volume": "main",
    "abstract": "Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using Bayesian optimization, our parametric Markov-copula model yields more favorable error-cost trade-offs, improving the area under the error-cost curve by 4.3% on average for cascades with $k\\geq 3$ models. In the low-sample regime with $n \\leq 30$ training examples, the performance improvement widens to 10.2%, suggesting that our framework's inductive assumptions about the interactions between the error rates of different LLMs enhance sample efficiency. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing systems of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael J. Zellinger",
      "Matt Thomson"
    ]
  },
  "https://openreview.net/forum?id=WfVXe88oMh": {
    "title": "Proximal Policy Distillation",
    "volume": "main",
    "abstract": "We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that integrates student-driven distillation and Proximal Policy Optimization (PPO) to increase sample efficiency and to leverage the additional rewards that the student policy collects during distillation. To assess the efficacy of our method, we compare PPD with two common alternatives, student-distill and teacher-distill, over a wide range of reinforcement learning environments that include discrete actions and continuous control (ATARI, Mujoco, and Procgen). For each environment and method, we perform distillation to a set of target student neural networks that are smaller, identical (self-distillation), or larger than the teacher network. Our findings indicate that PPD improves sample efficiency and produces better student policies compared to typical policy distillation approaches. Moreover, PPD demonstrates greater robustness than alternative methods when distilling policies from imperfect demonstrations. The code for the paper is released as part of a new Python library built on top of stable-baselines3 to facilitate policy distillation: <Anonymized GitHub Repository>",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Spigler"
    ]
  },
  "https://openreview.net/forum?id=6RCs2tLsHq": {
    "title": "Metamorphic Forward Adaptation Network: Dynamically Adaptive and Modular Multi-layer Learning",
    "volume": "main",
    "abstract": "Back-propagation is a widely used algorithm for training neural networks by adjusting weights based on error gradients. However, back-propagation is biologically implausible with global derivative computation and lacks robustness in long-term dynamic learning. A previously proposed alternative to back-propagation is the Forward-Forward algorithm, which bypasses global gradient dependency and localises computations, making it a more biologically plausible approach. However, Forward-Forward has been evaluated in limited environments, does not yet match back-propagation's performance, and only supports classification, not regression. This research introduces the Metamorphic Forward Adaptation Network (MFAN), using a contrastive learning property as its core, and retaining the layer-wise architecture of the Forward-Forward algorithm. Compared to the Forward-Forward model being limited to discrete classification, MFAN can process discrete and continuous data, showing stability, adaptability, and the ability to handle evolving data. MFAN performs well in continuous data stream scenarios, demonstrating superior adaptability and robustness compared to back-propagation, particularly in tasks requiring dynamic, long-term learning",
    "checked": false,
    "id": "8fad565cd99d91f115afb29e958ec6cdedac70cc",
    "semantic_title": "context-aware feature extraction network for high-precision uav-based vehicle detection in urban environments",
    "citation_count": 0,
    "authors": [
      "Yu Sun",
      "Vijja Wichitwechkarn",
      "Ronald Clark",
      "Mirko Kovac",
      "Basaran Bahadir Kocer"
    ]
  },
  "https://openreview.net/forum?id=rkfop9GyxB": {
    "title": "Lie Symmetry Net: Preserving Conservation Laws in Modelling Financial Market Dynamics via Differential Equations",
    "volume": "main",
    "abstract": "This paper employs a novel Lie symmetries-based framework to model the intrinsic symmetries within financial market. Specifically, we introduce Lie symmetry net (LSN), which characterises the Lie symmetries of the differential equations (DE) estimating financial market dynamics, such as the Black-Scholes equation. To simulate these differential equations in a symmetry-aware manner, LSN incorporates a Lie symmetry risk derived from the conservation laws associated with the Lie symmetry operators of the target differential equations. This risk measures how well the Lie symmetries are realised and guides the training of LSN under the structural risk minimisation framework. Extensive numerical experiments demonstrate that LSN effectively realises the Lie symmetries and achieves an error reduction of more than one order of magnitude compared to state-of-the-art methods. The code is available at https://github.com/Jxl163/LSN_code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelian Jiang",
      "Tongtian Zhu",
      "Yingxiang Xu",
      "Can Wang",
      "Yeyu Zhang",
      "Fengxiang He"
    ]
  },
  "https://openreview.net/forum?id=NbRybPuWCv": {
    "title": "A Framework for Finding Local Saddle Points in Two-Player Zero-Sum Black-Box Games",
    "volume": "main",
    "abstract": "Saddle point optimization is a critical problem employed in numerous real-world applications, including portfolio optimization, generative adversarial networks, and robotics. It has been extensively studied in cases where the objective function is known and differentiable. Existing work in black-box settings with unknown objectives that can only be sampled either assumes convexity-concavity in the objective to simplify the problem or operates with noisy gradient estimators. In contrast, we introduce a framework inspired by Bayesian optimization which utilizes Gaussian processes to model the unknown (potentially nonconvex-nonconcave) objective and requires only zeroth-order samples. Our approach frames the saddle point optimization problem as a two-level process which can flexibly leverage existing general-sum Nash game solvers to solve for saddle points of zero-sum games. The upper level of our framework produces a model of the objective function by sampling in promising locations, and the lower level of our framework uses the existing model to frame and solve a general-sum game to identify locations to sample. This lower level procedure can be designed in complementary ways, and we demonstrate the flexibility of our approach by introducing variants which appropriately trade off between factors like runtime, the cost of function evaluations, and the number of available initial samples. We experimentally demonstrate these algorithms on synthetic and realistic datasets in black-box nonconvex-nonconcave settings, showcasing their ability to efficiently locate local saddle points in these contexts",
    "checked": true,
    "id": "f1496bf0f1182376c322c6effae9961db1a17954",
    "semantic_title": "a framework for finding local saddle points in two-player zero-sum black-box games",
    "citation_count": 0,
    "authors": [
      "Shubhankar Agarwal",
      "Hamzah I Khan",
      "Sandeep P. Chinchali",
      "David Fridovich-Keil"
    ]
  },
  "https://openreview.net/forum?id=kK0WrBZAli": {
    "title": "Scalable Multi-Output Gaussian Processes with Stochastic Variational Inference",
    "volume": "main",
    "abstract": "The Multi-Output Gaussian Process (MOGP) is a popular tool for modelling data from multiple sources. A typical choice to build a covariance function for a MOGP is the Linear Model of Coregionalisation (LMC) which parametrically models the covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises this idea by modelling the covariance between outputs using a kernel applied to latent variables, one per output, leading to a flexible MOGP model that allows efficient generalisation to new outputs with few data points. The computational complexity in LV-MOGP grows linearly with the number of outputs, which makes it unsuitable for problems with a large number of outputs. In this paper, we propose a stochastic variational inference approach for the LV-MOGP that allows mini-batches for both inputs and outputs, making computational complexity per training iteration independent of the number of outputs. We demonstrate the performance of the model by benchmarking against some other MOGP models in several real-world datasets, including spatial-temporal climate modelling and spatial transcriptomics",
    "checked": true,
    "id": "3094b1bf913ab6af02a9d15235eb1658a73e6ba0",
    "semantic_title": "scalable multi-output gaussian processes with stochastic variational inference",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Jiang",
      "Sokratia Georgaka",
      "Magnus Rattray",
      "Mauricio A Álvarez"
    ]
  },
  "https://openreview.net/forum?id=IGsEgWM4to": {
    "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized code generation but are require significant resources and tend to over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs is a cost-effective alternative, yet standard supervised approaches rely solely on correct examples, overlooking valuable insights from failures. We introduce CodeLutra, a new framework that leverages both correct and incorrect code attempts. Instead of purely instructing with correct solutions, CodeLutra uses iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This process narrows the performance gap with state-of-the-art, larger models, without requiring massive datasets or auxiliary models. For example, on a challenging data science coding task, using only 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's level. By capitalizing on both successes and mistakes, \\textsc{CodeLutra} offers a scalable, efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leitian Tao",
      "Xiang Chen",
      "Tong Yu",
      "Tung Mai",
      "Ryan A. Rossi",
      "Yixuan Li",
      "Saayan Mitra"
    ]
  },
  "https://openreview.net/forum?id=bpaLYaf6Dp": {
    "title": "Disappearance of Timestep Embedding: A Case Study on Neural ODE and Diffusion Models",
    "volume": "main",
    "abstract": "Dynamical systems are often time-varying, whose modeling requires a function that evolves with respect to time. Recent studies such as the neural ordinary differential equation proposed a time-dependent neural network, which provides a neural network varying with respect to time. However, we claim that the architectural choice to build a time-dependent neural network significantly affects its time-awareness but still lacks sufficient validation in its current states. In this study, we conduct an in-depth analysis of the architecture of neural ordinary differential equations. Here, we report a vulnerability of vanishing timestep embedding, which disables the time-awareness of a time-dependent neural network. Specifically, we find that the ConcatConv operation, which is widely used in neural ordinary differential equations, causes an additive effect of timestep embedding, which is readily canceled out by the subsequent batch normalization. This vanishing timestep embedding also arises for group normalization and is analyzed thoroughly with respect to the number of channels, groups, and relative variance. Furthermore, we find that this vulnerability can also be observed in diffusion models because they employ a similar architecture that incorporates timestep embedding to discriminate between different timesteps during a diffusion process. Our analysis provides a detailed description of this phenomenon as well as several solutions to address the root cause. Through experiments on neural ordinary differential equations and diffusion models, we observed that ensuring alive time-awareness via proposed solutions boosted their performance, such as classification accuracy, FID, and inception score, which implies that their current implementations lack sufficient time-dependency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bum Jun Kim",
      "Yoshinobu Kawahara",
      "Sang Woo Kim"
    ]
  },
  "https://openreview.net/forum?id=GtXSN52nIW": {
    "title": "Sparser, Better, Faster, Stronger: Sparsity Detection for Efficient Automatic Differentiation",
    "volume": "main",
    "abstract": "From implicit differentiation to probabilistic modeling, Jacobian and Hessian matrices have many potential use cases in Machine Learning (ML), but they are viewed as computationally prohibitive. Fortunately, these matrices often exhibit sparsity, which can be leveraged to speed up the process of Automatic Differentiation (AD). This paper presents advances in sparsity detection, previously the performance bottleneck of Automatic Sparse Differentiation (ASD). Our implementation of sparsity detection is based on operator overloading, able to detect both local and global sparsity patterns, and supports flexible index set representations. It is fully automatic and requires no modification of user code, making it compatible with existing ML codebases. Most importantly, it is highly performant, unlocking Jacobians and Hessians at scales where they were considered too expensive to compute. On real-world problems from scientific ML, graph neural networks and optimization, we show significant speed-ups of up to three orders of magnitude. Notably, using our sparsity detection system, ASD outperforms standard AD for one-off computations, without amortization of either sparsity detection or matrix coloring",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Hill",
      "Guillaume Dalle"
    ]
  },
  "https://openreview.net/forum?id=IcOBCufqFO": {
    "title": "Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations",
    "volume": "main",
    "abstract": "Vision-language contrastive learning frameworks such as CLIP enable learning representations from natural language supervision and provide strong zero-shot classification capabilities. However, due to the nature of the supervisory signal in these paradigms, they lack the ability to learn localized features, leading to degraded performance on dense prediction tasks such as segmentation and detection. On the other hand, self-supervised learning methods have shown the ability to learn granular representations, complementing the high-level features in vision-language training. In this work, we present Harmony, a framework that combines vision-language training with discriminative and generative self-supervision to learn visual features that can be generalized across different downstream vision tasks. Our framework is specifically designed to work on web-scraped data by not relying on negative examples in the self-supervised learning path and addressing the one-to-one correspondence issue using soft CLIP targets generated by an EMA model. Moreover, Harmony optimizes for five different objectives simultaneously, efficiently utilizing the supervision in each data example, making it even more suited in data-constrained settings. We comprehensively evaluate Harmony across various vision downstream tasks and find that it significantly outperforms the baseline CLIP and outperforms the previously leading joint self- and weakly supervised methods, SLIP, MaskCLIP, and DetailCLIP. Specifically, when compared against these methods, Harmony shows superior performance in linear-probing, fine-tuning, and zero-shot classification on ImageNet-1k, semantic segmentation on ADE20K, and both object detection and instance segmentation on MS-COCO, when pre-training a ViT-B on CC3M. We also show that Harmony outperforms SILC on detection, linear and fine-tuning classification, and outperforms other self-supervised learning methods like iBOT and MAE across all tasks evaluated. Our code is publicly available at https://github.com/MohammedSB/Harmony",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Baharoon",
      "Jonathan Klein",
      "Dominik Michels"
    ]
  },
  "https://openreview.net/forum?id=3ECbEZg2If": {
    "title": "Full-Rank Unsupervised Node Embeddings for Directed Graphs via Message Aggregation",
    "volume": "main",
    "abstract": "Linear message-passing models have emerged as compelling alternatives to non-linear graph neural networks for unsupervised node embedding learning, due to their scalability and competitive performance on downstream tasks. However, we identify a fundamental flaw in recently proposed linear models that combine embedding aggregation with concatenation during each message-passing iteration: rank deficiency. A rank-deficient embedding matrix contains column vectors which take arbitrary values, leading to ill-conditioning that degrades downstream task accuracy, particularly in unsupervised tasks such as graph alignment. We deduce that repeated embedding aggregation and concatenation introduces linearly dependent features, causing rank deficiency. To address this, we propose ACC (Aggregate, Compress, Concatenate), a novel model that avoids redundant feature computation by applying aggregation to the messages from the previous iteration, rather than the embeddings. Consequently, ACC generates full-rank embeddings, significantly improving graph alignment accuracy from 10% to 60% compared to rank-deficient embeddings, while also being faster to compute. Additionally, ACC employs directed message-passing and achieves node classification accuracies comparable to state-of-the-art self-supervised graph neural networks on directed graph benchmarks, while also being over 70 times faster on graphs with over 1 million edges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ciwan Ceylan",
      "Kambiz Ghoorchian",
      "Danica Kragic"
    ]
  },
  "https://openreview.net/forum?id=u4YDVFodYX": {
    "title": "Prior Learning in Introspective VAEs",
    "volume": "main",
    "abstract": "Variational Autoencoders (VAEs) are a popular framework for unsupervised learning and data generation. A plethora of methods have been proposed focusing on improving VAEs, with the incorporation of adversarial objectives and the integration of prior learning mechanisms being prominent directions. When it comes to the former, an indicative instance is the recently introduced family of Introspective VAEs aiming at ensuring that a low likelihood is assigned to unrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE), one of only two members of the Introspective VAE family, the other being the original IntroVAE. We select S-IntroVAE for its state-of-the-art status and its training stability. In particular, we investigate the implication of incorporating a multimodal and trainable prior into this S-IntroVAE. Namely, we formulate the prior as a third player and show that when trained in cooperation with the decoder constitutes an effective way for prior learning, which shares the Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a modified formulation of the optimal ELBO in S-IntroVAE, we develop theoretically motivated regularizations, namely (i) adaptive variance clipping to stabilize training when learning the prior and (ii) responsibility regularization to discourage the formation of inactive prior modes. Finally, we perform a series of targeted experiments on a 2D density estimation benchmark and in an image generation setting comprised of the (F)-MNIST and CIFAR-10 datasets demonstrating the effect of prior learning in S-IntroVAE in generation and representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Athanasiadis",
      "Fredrik Lindsten",
      "Michael Felsberg"
    ]
  },
  "https://openreview.net/forum?id=EDQ8QOGqjr": {
    "title": "Learning Using a Single Forward Pass",
    "volume": "main",
    "abstract": "We propose a learning algorithm to overcome the limitations of traditional backpropagation in resource-constrained environments: Solo Pass Embedded Learning Algorithm (SPELA). SPELA operates with local loss functions to update weights, significantly saving on resources allocated to the propagation of gradients and storing computational graphs while being sufficiently accurate. Consequently, SPELA can closely match backpropagation using less memory. Moreover, SPELA can effectively fine-tune pre-trained image recognition models for new tasks. Further, SPELA is extended with significant modifications to train CNN networks, which we evaluate on CIFAR-10, CIFAR-100, and SVHN 10 datasets, showing equivalent performance compared to backpropagation. Our results indicate that SPELA, with its features such as local learning and early exit, is a potential candidate for learning in resource-constrained edge AI applications",
    "checked": true,
    "id": "52f29792f2901985bd44c78de09e16dcbd258836",
    "semantic_title": "learning using a single forward pass",
    "citation_count": 0,
    "authors": [
      "Aditya Somasundaram",
      "Pushkal Mishra",
      "Ayon Borthakur"
    ]
  },
  "https://openreview.net/forum?id=hQjwDqfSzj": {
    "title": "Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making",
    "volume": "main",
    "abstract": "Statistical models are often defined by a generative process for simulating synthetic data, but this can lead to intractable likelihoods. Likelihood free inference (LFI) methods enable Bayesian inference to be performed in this case. Extending a popular approach to simulation-efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood Free Inference (MOBOLFI) to perform LFI using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a separate discrepancy for each data source. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling detection of conflicting information and deeper understanding of the importance of different data sources in estimating individual parameters. The adaptive choice of simulation parameters using multi-objective Bayesian optimization ensures simulation efficient approximation of likelihood components for all data sources. We illustrate our approach in sequential sampling models (SSMs), which are widely used in psychology and consumer-behavior modeling. SSMs are often fitted using multi-source data, such as choice and response time. The advantages of our approach are illustrated in comparison with a single discrepancy for an SSM fitted to data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles",
    "checked": true,
    "id": "ede1395863e3c9e8a41812b9589eff46b248dc27",
    "semantic_title": "multi-objective bayesian optimization for likelihood-free inference in sequential sampling models of decision making",
    "citation_count": 0,
    "authors": [
      "David Chen",
      "Xinwei Li",
      "Eui-Jin Kim",
      "Prateek Bansal",
      "David J Nott"
    ]
  },
  "https://openreview.net/forum?id=FNRdaHz3qN": {
    "title": "Change Point Detection in the Frequency Domain with Statistical Reliability",
    "volume": "main",
    "abstract": "Effective condition monitoring in complex systems requires identifying change points (CPs) in the frequency domain, as the structural changes often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akifumi Yamada",
      "Tomohiro Shiraishi",
      "Shuichi Nishino",
      "Teruyuki Katsuoka",
      "Kouichi Taji",
      "Ichiro Takeuchi"
    ]
  },
  "https://openreview.net/forum?id=HBZoXjUAqV": {
    "title": "Recall and Refine: A Simple but Effective Source-free Open- set Domain Adaptation Framework",
    "volume": "main",
    "abstract": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where novel classes — also referred to as target-private unknown classes — are present. Source-free Open-set Domain Adaptation (SF-OSDA) methods address OSDA without accessing labeled source data, making them particularly relevant under privacy constraints. However, SF-OSDA presents significant challenges due to distribution shifts and the introduction of novel classes. Existing SF-OSDA methods typically rely on thresholding the prediction entropy of a sample to identify it as either a known or unknown class, but fail to explicitly learn discriminative features for the target-private unknown classes. We propose Recall and Refine (RRDA), a novel SF-OSDA framework designed to address these limitations by explicitly learning features for target-private unknown classes. RRDA employs a two-stage process. First, we enhance the model's capacity to recognize unknown classes by training a target classifier with an additional decision boundary, guided by synthetic samples generated from target domain features. This enables the classifier to effectively separate known and unknown classes. Second, we adapt the entire model to the target domain, addressing both domain shifts and distinguishability to unknown classes. Any off-the-shelf source-free domain adaptation method (e.g.\\ SHOT, AaD) can be seamlessly integrated into our framework at this stage. Extensive experiments on three benchmark datasets demonstrate that RRDA significantly outperforms existing SF-OSDA and OSDA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ismail Nejjar",
      "Hao Dong",
      "Olga Fink"
    ]
  },
  "https://openreview.net/forum?id=ylUVRikhTL": {
    "title": "Mixed-View Panorama Synthesis using Geospatially Guided Diffusion",
    "volume": "main",
    "abstract": "We introduce the task of mixed-view panorama synthesis, where the goal is to synthesize a novel panorama given a small set of input panoramas and a satellite image of the area. This contrasts with previous work which only uses input panoramas (same-view synthesis), or an input satellite image (cross-view synthesis). We argue that the mixed-view setting is the most natural to support panorama synthesis for arbitrary locations worldwide. A critical challenge is that the spatial coverage of panoramas is uneven, with few panoramas available in many regions of the world. We introduce an approach that utilizes diffusion-based modeling and an attention-based architecture for extracting information from all available input imagery. Experimental results demonstrate the effectiveness of our proposed method. In particular, our model can handle scenarios when the available panoramas are sparse or far from the location of the panorama we are attempting to synthesize",
    "checked": true,
    "id": "9f52715bd5bf9328e0565e9fcbbed323278d596a",
    "semantic_title": "mixed-view panorama synthesis using geospatially guided diffusion",
    "citation_count": 1,
    "authors": [
      "Zhexiao Xiong",
      "Xin Xing",
      "Scott Workman",
      "Subash Khanal",
      "Nathan Jacobs"
    ]
  },
  "https://openreview.net/forum?id=S6fe4aH6YA": {
    "title": "Link Prediction with Relational Hypergraphs",
    "volume": "main",
    "abstract": "Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to inductive link prediction with relational hypergraphs, where the task is over $k$-ary relations, substantially harder than link prediction on knowledge graphs with binary relations only. In this paper, we propose a framework for link prediction with relational hypergraphs, empowering applications of graph neural networks on fully relational structures. Theoretically, we conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms and also via logical expressiveness. Empirically, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction and also lead to competitive results for transductive link prediction",
    "checked": true,
    "id": "d5591f0ffa407eae914b45e5329768f8ce553862",
    "semantic_title": "link prediction with relational hypergraphs",
    "citation_count": 4,
    "authors": [
      "Xingyue Huang",
      "Miguel Romero Orth",
      "Pablo Barcelo",
      "Michael M. Bronstein",
      "Ismail Ilkan Ceylan"
    ]
  },
  "https://openreview.net/forum?id=DDUsc1lD27": {
    "title": "Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), agents have no incentive to exhibit predictable trajectories, and are often pushed (through e.g. policy entropy regularisation) to randomise their actions in favor of exploration. This lack of predictability awareness often makes it challenging for other agents and humans to predict an agent's trajectories, possibly triggering unsafe scenarios (e.g. in human-robot interaction). We propose a novel method to induce predictable trajectories in RL agents, termed Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate to quantify predictability. Our method maximizes a linear combination of a standard discounted reward and the negative entropy rate, thus trading off optimality with predictability. We show how the entropy rate can be formally cast as an average reward, how entropy-rate value functions can be estimated from a learned model and incorporate this in policy-gradient algorithms, and demonstrate how this approach produces predictable (near-optimal) policies in tasks inspired by human-robot use-cases",
    "checked": true,
    "id": "8aaca0d0fff646acf187628d679844b4bbfeee70",
    "semantic_title": "predictable reinforcement learning dynamics through entropy rate minimization",
    "citation_count": 2,
    "authors": [
      "Daniel Jarne Ornia",
      "Giannis Delimpaltadakis",
      "Jens Kober",
      "Javier Alonso-Mora"
    ]
  },
  "https://openreview.net/forum?id=FVFqrxeF8e": {
    "title": "Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients",
    "volume": "main",
    "abstract": "We propose Mirror Descent Optimal Transport (MDOT), a novel method for solving discrete optimal transport (OT) problems with high precision, by unifying temperature annealing in entropic-regularized OT (EOT) with mirror descent techniques. In this framework, temperature annealing produces a sequence of EOT dual problems, whose solution gradually gets closer to the solution of the original OT problem. We solve each problem efficiently using a GPU-parallel nonlinear conjugate gradients algorithm (PNCG) that outperforms traditional Sinkhorn iterations under weak regularization. Moreover, our investigation also reveals that the theoretical convergence rate of Sinkhorn iterations can exceed existing non-asymptotic bounds when its stopping criterion is tuned in a manner analogous to MDOT. Our comprehensive ablation studies of MDOT-PNCG affirm its robustness across a wide range of algorithmic parameters. Benchmarking on 24 problem sets of size $n=4096$ in a GPU environment demonstrate that our method attains high-precision, feasible solutions significantly faster than a representative set of existing OT solvers—including accelerated gradient methods and advanced Sinkhorn variants—in both wall-clock time and number of operations. Empirical convergence rates range between $O(n^2 \\varepsilon^{-1/4})$ and $O(n^2 \\varepsilon^{-1})$, where $\\varepsilon$ is the optimality gap. For problem sizes up to $n=16\\,384$, the empirical runtime scales as $\\widetilde{O}(n^2)$ for moderate precision and as $\\widetilde{O}(n^{5/2})$ at worst for high precision. These findings establish MDOT-PNCG as a compelling alternative to current OT solvers, particularly in challenging weak-regularization regimes",
    "checked": true,
    "id": "aeae241d9dbd868d00c1e982f6c8e9664cf31c38",
    "semantic_title": "efficient and accurate optimal transport with mirror descent and conjugate gradients",
    "citation_count": 3,
    "authors": [
      "Mete Kemertas",
      "Allan Douglas Jepson",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=p7jQEf3wlh": {
    "title": "Efficient Hardware Scaling and Diminishing Returns in Large-Scale Training of Language Models",
    "volume": "main",
    "abstract": "To train the exceedingly large neural networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model training. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies with current best practices. In experiments with model sizes up to 70B parameters and utilizing up to 2048 H100 GPUs, we demonstrate that: (1) Naive scale out with Fully Sharded Data Parallelism (FSDP) incurs communication overhead which leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jared Fernandez",
      "Luca Wehrstedt",
      "Leonid Shamis",
      "Mostafa Elhoushi",
      "Kalyan Saladi",
      "Yonatan Bisk",
      "Emma Strubell",
      "Jacob Kahn"
    ]
  },
  "https://openreview.net/forum?id=cqDH0e6ak2": {
    "title": "Flow map matching with stochastic interpolants: A mathematical framework for consistency models",
    "volume": "main",
    "abstract": "Generative models based on dynamical equations such as flows and diffusions offer exceptional sample quality, but require computationally expensive numerical integration during inference. The advent of consistency models has enabled efficient one-step or few-step generation, yet despite their practical success, a systematic understanding of their design has been hindered by the lack of a comprehensive theoretical framework. Here we introduce Flow Map Matching (FMM), a principled framework for learning the two-time flow map of an underlying dynamical generative model, thereby providing this missing mathematical foundation. Leveraging stochastic interpolants, we propose training objectives both for distillation from a pre-trained velocity field and for direct training of a flow map over an interpolant or a forward diffusion process. Theoretically, we show that FMM unifies and extends a broad class of existing approaches for fast sampling, including consistency models, consistency trajectory models, and progressive distillation. Experiments on CIFAR-10 and ImageNet-32 highlight that our approach can achieve sample quality comparable to flow matching while reducing generation time by a factor of 10-20",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Matthew Boffi",
      "Michael Samuel Albergo",
      "Eric Vanden-Eijnden"
    ]
  },
  "https://openreview.net/forum?id=qVUEuhlaEa": {
    "title": "Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning is an evolving machine learning paradigm, in which multiple clients perform computations based on their individual private data, interspersed by communication with a remote server. A common strategy to curtail communication costs is Local Training, which consists in performing multiple local stochastic gradient descent steps between successive communication rounds. However, the conventional approach to local training overlooks the practical necessity for client-specific personalization, a technique to tailor local models to individual needs. We introduce Scafflix, a novel algorithm that efficiently integrates explicit personalization with local training. This innovative approach benefits from these two techniques, thereby achieving doubly accelerated communication, as we demonstrate both in theory and practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yi",
      "Laurent Condat",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=z3RIiidJgD": {
    "title": "MemBench: Memorized Image Trigger Prompt Dataset for Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in Text-to-Image generation tasks, leading to the development of many commercial models. However, recent studies have reported that diffusion models often repeatedly generate memorized images in train data when triggered by specific prompts, potentially raising social issues ranging from copyright to privacy concerns. To sidestep the memorization, recent studies have been conducted to develop memorization mitigation methods for diffusion models. Nevertheless, the lack of benchmarks hinders the assessment of the true effectiveness of these methods. In this work, we present MemBench, the first benchmark for evaluating image memorization mitigation methods. Our benchmark includes a large number of memorized image trigger prompts in various Text-to-Image diffusion models. Furthermore, in contrast to the prior work evaluating mitigation performance only on trigger prompts, we present metrics evaluating on both trigger prompts and general prompts, so that we can see whether mitigation methods address the memorization issue while maintaining performance for general prompts. Through our MemBench evaluation, we revealed that existing memorization mitigation methods notably degrade the overall performance of diffusion models and need to be further developed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunsan Hong",
      "Tae-Hyun Oh",
      "Minhyuk Sung"
    ]
  },
  "https://openreview.net/forum?id=3HKNwejEEq": {
    "title": "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA",
    "volume": "main",
    "abstract": "The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future",
    "checked": true,
    "id": "66aba2df8c6994aefe8136d98d93eba63f36f385",
    "semantic_title": "neurips 2023 competition: privacy preserving federated learning document vqa",
    "citation_count": 0,
    "authors": [
      "Marlon Tobaben",
      "Mohamed Ali Souibgui",
      "Rubèn Tito",
      "Khanh Nguyen",
      "Raouf Kerkouche",
      "Kangsoo Jung",
      "Joonas Jälkö",
      "Lei Kang",
      "Andrey Barsky",
      "Vincent Poulain d'Andecy",
      "Aurélie JOSEPH",
      "Aashiq Muhamed",
      "Kevin Kuo",
      "Virginia Smith",
      "Yusuke Yamasaki",
      "Takumi Fukami",
      "Kenta Niwa",
      "Iifan Tyou",
      "Hiro Ishii",
      "Rio Yokota",
      "Ragul N",
      "Rintu Kutum",
      "Josep Llados",
      "Ernest Valveny",
      "Antti Honkela",
      "Mario Fritz",
      "Dimosthenis Karatzas"
    ]
  },
  "https://openreview.net/forum?id=R2rasAEPVi": {
    "title": "Leopard: A Vision Language Model for Text-Rich Multi- Image Tasks",
    "volume": "main",
    "abstract": "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose Leopard, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we proposed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of images. Experiments on a diverse set of benchmarks reveal that our model consistently outperforms state-of-the-art systems, such as Llama-3.2 and Qwen2-VL, in challenging text-rich, multi-image evaluations. Remarkably, our approach achieves outstanding performance using only 1.2M fully open-sourced training instances, outperforming models that rely on large-scale in-house data, highlighting its efficiency and effectiveness. Our code and data are available at https://anonymous.4open.science/r/Leopard-908F",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Jia",
      "Wenhao Yu",
      "Kaixin Ma",
      "Tianqing Fang",
      "Zhihan Zhang",
      "Siru Ouyang",
      "Hongming Zhang",
      "Dong Yu",
      "Meng Jiang"
    ]
  },
  "https://openreview.net/forum?id=bAM8y3Hm0p": {
    "title": "Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity Resolution",
    "volume": "main",
    "abstract": "The entity resolution problem requires finding pairs across datasets that belong to different owners but refer to the same entity in the real world. To train and evaluate solutions (either rule-based or machine-learning-based) to the entity resolution problem, generating a ground truth dataset with entity pairs or clusters is needed. However, such a data annotation process involves humans as domain oracles to review the plaintext data for all candidate record pairs from different parties, which inevitably infringes the privacy of data owners, especially in privacy-sensitive cases like medical records. To the best of our knowledge, there is no prior work on privacy-preserving ground truth labeling in the context of entity resolution. We propose a novel blind annotation protocol based on homomorphic encryption that allows domain oracles to collaboratively label ground truth without sharing data in plaintext with other parties. In addition, we design a domain-specific, user-friendly language that conceals the complex underlying homomorphic encryption circuits, making it more accessible and easier for users to adopt this technique. The empirical experiments indicate the feasibility of our privacy-preserving protocol (f-measure on average achieves more than 90\\% compared with the real ground truth)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiang Yao",
      "Weizhao Jin",
      "Srivatsan Ravi"
    ]
  },
  "https://openreview.net/forum?id=B0E2yjrNb8": {
    "title": "Dynamic Schwartz-Fourier Neural Operator for Enhanced Expressive Power",
    "volume": "main",
    "abstract": "Recently, neural operators have emerged as a prevailing approach for learning discretization-invariant mappings between function spaces. A particular example is the Fourier Neural Operator (FNO), which constrains integral kernels to be convolutions and learns the kernel directly in the frequency domain. Due to the capacity of Fourier transforms to effectively reduce the dimensionality and preserve information, FNOs demonstrate superior performance in terms of both efficiency and accuracy. In FNOs, the convolution kernel is fixed as a point-wise multiplication in the frequency domain; however, these translation-invariant kernels might limit the expressiveness of FNOs. For instance, if the underlying system lacks translational symmetries, the kernels learned by the FNO will still exhibit translational invariance, thereby limiting the model's expressive power. We propose a dynamic Schwartz operator that induces interactions between modes to enhance the expressiveness of FNOs. In this work, we introduce a novel approach that equips FNOs with Schwartz operators to learn dynamic kernels, termed Dynamic Kernel Fourier Neural Operators (DSFNOs). By incorporating this dynamic mechanism, our model gains the ability to capture relevant frequency information patterns, facilitating a better understanding and representation of complex physical phenomena. Through experiments, we demonstrate that DSFNOs can improve FNOs on a range of tasks, highlighting the effectiveness of our proposed approach. The code is available at https://github.com/wenhangao21/TMLR25_DSFNO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Gao",
      "Jian Luo",
      "Ruichen Xu",
      "Yi Liu"
    ]
  },
  "https://openreview.net/forum?id=z27hb0rmLT": {
    "title": "Normality-Guided Distributional Reinforcement Learning for Continuous Control",
    "volume": "main",
    "abstract": "Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirically quite close to normal. We design a method that exploits this property, employing variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically significant improvements in 10 out of 16 continuous task settings, while utilizing a reduced number of weights and achieving faster training time compared to an ensemble-based method for quantifying value distribution uncertainty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Seung Byun",
      "Andrew Perrault"
    ]
  },
  "https://openreview.net/forum?id=VdW9SkALSd": {
    "title": "Mathematical Characterization of Better-than-Random Multiclass Models",
    "volume": "main",
    "abstract": "A binary supervised model outperforms chance if and only if the determinant of the confusion matrix is positive. This is equivalent to saying that the associated point in the ROC space is above the random guessing line. This also means that Youden's J, Cohen's $\\kappa$ and Matthews' correlation coefficient are positive. We extend these results to any number of classes: for a target variable with $m \\geq 2$ classes, we show that a model does better than chance if and only if the entries of the confusion matrix verify $m(m-1)$ homogeneous polynomial inequalities of degree 2, which can be expressed using generalized likelihood ratios. We also obtain a more theoretical formulation: a model does better than chance if and only if it is a maximum likelihood estimator of the target variable. When this is the case, we find that the multiclass versions of the previous metrics remain positive. If $m>2$, we notice that no-skill classifiers are only a small part of the topological boundary between better-than-random models and bad models. For $m=3$, we show that bad models occupy exactly 90\\% of the ROC space, far more than the 50\\% of the two-class problems. Finally, we propose to define weak multiclass classifiers by conditions on these generalized likelihood ratios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sébastien Foulle"
    ]
  },
  "https://openreview.net/forum?id=3qmnxysNbi": {
    "title": "To Be Greedy, or Not to Be – That Is the Question for Population Based Training Variants",
    "volume": "main",
    "abstract": "Achieving excellent results with neural networks requires careful hyperparameter tuning, which can be automated via hyperparameter optimization algorithms such as Population Based Training (PBT). PBT stands out for its capability to efficiently optimize hyperparameter schedules in parallel and within the wall-clock time of training a single network. Several PBT variants have been proposed that improve performance in the experimental settings considered in the associated publications. However, the experimental settings and tasks vary across publications, while the best previous PBT variant is not always included in the comparisons, thus making the relative performance of PBT variants unclear. In this work, we empirically evaluate five single-objective PBT variants on a set of image classification and reinforcement learning tasks with different setups (such as increasingly large search spaces). We find that the Bayesian Optimization (BO) variants of PBT tend to behave greedier than the non-BO ones, which is beneficial when aggressively pursuing short-term gains improves long-term performance and harmful otherwise. This is a previously overlooked caveat to the reported improvements of the BO PBT variants. Examining their theoretical properties, we find that the returns of BO PBT variants are guaranteed to asymptotically approach the returns of the greedy hyperparameter schedule (rather than the optimal one, as claimed in prior work). Together with our empirical results, this leads us to conclude that there is currently no single best PBT variant capable of outperforming others both when pursuing short-term gains is helpful in the long term, and when it is harmful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Chebykin",
      "Tanja Alderliesten",
      "Peter Bosman"
    ]
  },
  "https://openreview.net/forum?id=XPEEsKneKs": {
    "title": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems",
    "volume": "main",
    "abstract": "When solving inverse problems, one increasingly popular approach is to use pre-trained diffusion models as plug-and-play priors. This framework can accommodate different forward models without re-training while preserving the generative capability of diffusion models. Despite their success in many imaging inverse problems, most existing methods rely on privileged information such as derivative, pseudo-inverse, or full knowledge about the forward model. This reliance poses a substantial limitation that restricts their use in a wide range of problems where such information is unavailable, such as in many scientific applications. We propose Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free approach that can solve inverse problems by only accessing forward model evaluations and a pre-trained diffusion model prior. We study the empirical effectiveness of EnKG across various inverse problems, including scientific settings such as inferring fluid flows and astronomical objects, which are highly non-linear inverse problems that often only permit black-box access to the forward model. We open-source our code at https://github.com/devzhk/enkg-pytorch",
    "checked": true,
    "id": "ffde2508da9e9e1dc7e571c57c4f84005813265f",
    "semantic_title": "ensemble kalman diffusion guidance: a derivative-free method for inverse problems",
    "citation_count": 5,
    "authors": [
      "Hongkai Zheng",
      "Wenda Chu",
      "Austin Wang",
      "Nikola Borislavov Kovachki",
      "Ricardo Baptista",
      "Yisong Yue"
    ]
  },
  "https://openreview.net/forum?id=5EXrH2h3I5": {
    "title": "A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models",
    "volume": "main",
    "abstract": "Few-shot semantic segmentation (FSS) is a crucial challenge in computer vision, driving extensive research into a diverse range of methods, from advanced meta-learning techniques to simple transfer learning baselines. With the emergence of vision foundation models (VFM) serving as generalist feature extractors, we seek to explore the adaptation of these models for FSS. While current FSS benchmarks focus on adapting pre-trained models to new tasks with few images, they emphasize in-domain generalization, making them less suitable for VFM trained on large-scale web datasets. To address this, we propose a novel realistic benchmark with a simple and straightforward adaptation process tailored for this task. Using this benchmark, we conduct a comprehensive comparative analysis of prominent VFM and semantic segmentation models. To evaluate their effectiveness, we leverage various adaption methods, ranging from linear probing to parameter efficient fine-tuning (PEFT) and full fine-tuning. Our findings show that models designed for segmentation can be outperformed by self-supervised (SSL) models. On the other hand, while PEFT methods yields competitive performance, they provide little discrepancy in the obtained results compared to other methods, highlighting the critical role of the feature extractor in determining results. To our knowledge, this is the first study on the adaptation of VFM for FSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reda Bensaid",
      "Vincent Gripon",
      "François Leduc-Primeau",
      "Lukas Mauch",
      "Ghouthi BOUKLI HACENE",
      "Fabien Cardinaux"
    ]
  },
  "https://openreview.net/forum?id=7aJxaPg30d": {
    "title": "Extending Graph Condensation to Multi-Label Datasets: A Benchmark Study",
    "volume": "main",
    "abstract": "As graph data grows increasingly complicated, training graph neural networks (GNNs) on large-scale datasets presents significant challenges, including computational resource constraints, data redundancy, and transmission inefficiencies. While existing graph condensation techniques have shown promise in addressing these issues, they are predominantly designed for single-label datasets, where each node is associated with a single class label. However, many real-world applications, such as social network analysis and bioinformatics, involve multi-label graph datasets, where one node can have various related labels. To deal with this problem, we extend traditional graph condensation approaches to accommodate multi-label datasets by introducing modifications to synthetic dataset initialization and condensing optimization. Through experiments on eight real-world multi-label graph datasets, we prove the effectiveness of our method. In the experiment, the GCond framework, combined with K-Center initialization and binary cross-entropy loss (BCELoss), generally achieves the best performance. This benchmark for multi-label graph condensation not only enhances the scalability and efficiency of GNNs for multi-label graph data but also offers substantial benefits for diverse real-world applications",
    "checked": true,
    "id": "cd9a56e56f037bc962043448b3fe007f2f72a4ce",
    "semantic_title": "extending graph condensation to multi-label datasets: a benchmark study",
    "citation_count": 1,
    "authors": [
      "Liangliang Zhang",
      "Haoran Bao",
      "Yao Ma"
    ]
  },
  "https://openreview.net/forum?id=pvtgffHtJm": {
    "title": "Diffusion Model Predictive Control",
    "volume": "main",
    "abstract": "We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach that learns a multi-step action proposal and a multi-step dynamics model, both using diffusion models, and combines them for use in online MPC. On the popular D4RL benchmark, we show performance that is significantly better than existing model-based offline planning methods using MPC (e.g. MBOP) and competitive with state-of-the-art (SOTA) model-based and model-free reinforcement learning methods. We additionally illustrate D-MPC's ability to optimize novel reward functions at run time and adapt to novel dynamics, and highlight its advantages compared to existing diffusion-based planning baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyao Zhou",
      "Sivaramakrishnan Swaminathan",
      "Rajkumar Vasudeva Raju",
      "J Swaroop Guntupalli",
      "Wolfgang Lehrach",
      "Joseph Ortiz",
      "Antoine Dedieu",
      "Miguel Lazaro-Gredilla",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=aFAMPSmNHR": {
    "title": "Do Think Tags Really Help LLMs Plan? A Critical Evaluation of ReAct-Style Prompting",
    "volume": "main",
    "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of considerable interest and debate. Among the original papers arguing for emergent reasoning abilities of LLMs, ReAct became particularly popular by claiming to tease out LLM reasoning abilities with special prompting involving \"interleaving reasoning trace with action execution\". In this paper, we critically examine the claims of ReAct style prompting for planning and sequential decision-making problems. By introducing systematic variations to the input prompt, we perform a sensitivity analysis along the original claims of ReAct. Our experiments in AlfWorld and WebShop, domains that were used in the original ReAct work, show that the performance is minimally influenced by the interleaved reasoning trace or by the content of these generated reasoning traces. Instead, the performance of LLMs is primarily driven by the unreasonably high degree of similarity between input example tasks and queries, with shockingly little ability to generalize. In addition to raising questions on claims about reasoning abilities, this lack of generalization also implicitly forces the prompt designer to provide instance-specific examples, significantly increasing the cognitive burden on the human. Our empirical results show that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities, thereby leading to severe lack of generalization beyond the few-shot examples given in the prompts. Our code and prompt settings can be found here on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Bhambri",
      "Mudit Verma",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=3q1bUIHTJK": {
    "title": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting",
    "volume": "main",
    "abstract": "Obeying precise constraints on top of multiple external attributes is a common computational problem underlying seemingly different domains, from controlled text generation to protein engineering. Existing language model (LM) controllability methods for multi-attribute constraint satisfaction often rely on specialized architectures or gradient-based classifiers, limiting their flexibility to work with arbitrary black-box evaluators and pretrained models. Current general-purpose large language models, while capable, cannot achieve fine-grained multi-attribute control over external attributes. Thus, we create Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of finetuning language models on any sequential domain to satisfy user-specified constraints on multiple external real-value attributes. Our method trains LMs as editors by sampling diverse multi-attribute edit pairs from an initial set of paraphrased outputs. During inference, LM iteratively improves upon its previous solution to satisfy constraints for all attributes by leveraging our designed constraint satisfaction reward. We additionally experiment with reward-weighted behavior cloning to further improve the constraint satisfaction rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text Style Transfer, where the goal is to simultaneously modify the sentiment and complexity of reviews, and (2) Protein Design, focusing on modulating fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical results show that MACS achieves the highest threshold satisfaction in both FineCS tasks, outperforming strong domain-specific baselines. Our work opens new avenues for generalized and real-value multi-attribute control, with implications for diverse applications spanning natural language processing and bioinformatics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Baheti",
      "Debanjana Chakraborty",
      "Faeze Brahman",
      "Ronan Le Bras",
      "Ximing Lu",
      "Nouha Dziri",
      "Yejin Choi",
      "Mark Riedl",
      "Maarten Sap"
    ]
  },
  "https://openreview.net/forum?id=0jhoriH9yA": {
    "title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development",
    "volume": "main",
    "abstract": "Transformer architectures have revolutionized a broad spectrum of AI applications by leveraging attention mechanisms for parallelized and long-range sequence processing. Despite their remarkable success, building and customizing transformers remains prohibitively complex for many domain experts who lack deep knowledge of low-level implementations. We introduce AttentionSmithy, a modular software package that lowers the barrier to transformer innovation by decomposing key components---attention modules, feed-forward networks, normalization layers, and positional encodings---into reusable building blocks. By disentangling architectural elements into well-defined interfaces, users can rapidly prototype, adapt, and evaluate transformer variants without extensive coding overhead. Our framework currently supports four distinct positional encoding strategies (sinusoidal, learned, rotary, and ALiBi), offers modular integration of multiple attention methods (including standard attention, Longformer, and Linformer), and integrates seamlessly with neural architecture search (NAS) for automated design exploration. The system is designed to support future extensions with minimal overhead. We validate AttentionSmithy by replicating the original ``Attention Is All You Need'' transformer under resource constraints, demonstrating robust performance on a machine translation task. Leveraging the package's integrated NAS capability, we identified an optimized model configuration that outperformed our baseline, demonstrating the framework's effectiveness for automated architecture search and model improvement. We further illustrate AttentionSmithy's adaptability through gene-specific modeling, where a variant of a BERT-style architecture achieves over 95\\% accuracy on downstream cell type classification tasks using ranked transcriptomic data. These case studies underscore AttentionSmithy's core advantage: enabling specialized experimentation across diverse application domains---from natural language processing to genomic analysis---by obviating the need for labor-intensive, low-level framework manipulation. We anticipate that AttentionSmithy will serve as a foundation for creative transformer-based solutions, expediting research and development in numerous scientific and industrial fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caleb Cranney",
      "Jesse G Meyer"
    ]
  },
  "https://openreview.net/forum?id=JzmXo0rfry": {
    "title": "Evaluating explainability techniques on discrete-time graph neural networks",
    "volume": "main",
    "abstract": "Discrete-time temporal Graph Neural Networks (GNNs) are powerful tools for modeling evolving graph-structured data and are widely used in decision-making processes across domains such as social network analysis, financial systems, and collaboration networks. Explaining the predictions of these models is an important research area due to the critical role their decisions play in building trust in social or financial systems. However, the explainability of Temporal Graph Neural Networks remains a challenging and relatively unexplored field. Hence, in this work, we propose a novel framework to evaluate explainability techniques tailored for discrete-time temporal GNNs. Our framework introduces new training and evaluation settings that capture the evolving nature of temporal data, defines metrics to assess the temporal aspects of explanations, and establishes baselines and models specific to discrete-time temporal networks. Through extensive experiments, we outline the best explainability techniques for discrete-time GNNs in terms of fidelity, efficiency, and human-readability trade-offs. By addressing the unique challenges of temporal graph data, our framework sets the stage for future advancements in explaining discrete-time GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Dileo",
      "Matteo Zignani",
      "Sabrina Tiziana Gaito"
    ]
  },
  "https://openreview.net/forum?id=Q70C1HQ0VO": {
    "title": "Alternators For Sequence Modeling",
    "volume": "main",
    "abstract": "This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and often outperform strong baselines such as Mambas, neural ODEs, and diffusion models in the domains we studied",
    "checked": true,
    "id": "b82554a8e084b487ded937ee84d493d4e6271977",
    "semantic_title": "alternators for sequence modeling",
    "citation_count": 1,
    "authors": [
      "Mohammad Reza Rezaei",
      "Adji Bousso Dieng"
    ]
  },
  "https://openreview.net/forum?id=yzACI2vFaX": {
    "title": "Evaluating Long Range Dependency Handling in Code Generation LLMs",
    "volume": "main",
    "abstract": "As language models support larger and larger context sizes, evaluating their ability to make effective use of that context becomes increasingly important. We analyze the ability of several code generation models to handle long range dependencies using a suite of multi-step key retrieval tasks in context windows up to 8k tokens in length. The tasks progressively increase in difficulty and allow more nuanced evaluation of model capabilities than tests like the popular needle-in-the-haystack test. We find that performance degrades significantly for many models (up to 2x) when a function references another function that is defined later in the prompt. We also observe that models that use sliding window attention mechanisms have difficulty handling references further than the size of a single window. We perform simple prompt modifications using call graph information to improve multi-step retrieval performance up to 3x. Our analysis highlights ways that long-context performance needs deeper consideration beyond retrieval of single facts within a document",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Assogba",
      "Donghao Ren"
    ]
  },
  "https://openreview.net/forum?id=FHkWY4aGsN": {
    "title": "CLImage: Human-Annotated Datasets for Complementary-Label Learning",
    "volume": "main",
    "abstract": "Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical applicability remains unverified for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels, and it is not clear how far the assumptions are from reality. Secondly, their evaluation has been limited to synthetically labeled datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels from human annotators. Our efforts resulted in the creation of four datasets: CLCIFAR10, CLCIFAR20, CLMicroImageNet10, and CLMicroImageNet20, derived from well-known classification datasets CIFAR10, CIFAR100, and TinyImageNet200. These datasets represent the very first real-world CLL datasets, namely CLImage, which are publicly available at: https://github.com/ntucllab/CLImage_Dataset. Through extensive benchmark experiments, we discovered a notable decrease in performance when transitioning from synthetically labeled datasets to real-world datasets. We investigated the key factors contributing to the decrease with a thorough dataset-level ablation study. Our analyses highlight annotation noise as the most influential factor in the real-world datasets. In addition, we discover that the biased-nature of human-annotated complementary labels and the difficulty to validate with only complementary labels are two outstanding barriers to practical CLL. These findings suggest that the community focus more research efforts on developing CLL algorithms and validation schemes that are robust to noisy and biased complementary-label distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiu-Hsuan Wang",
      "Mai Tan Ha",
      "Nai-Xuan Ye",
      "Wei-I Lin",
      "Hsuan-Tien Lin"
    ]
  },
  "https://openreview.net/forum?id=amUisgrmte": {
    "title": "ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis",
    "volume": "main",
    "abstract": "Deep learning is providing a wealth of new approaches to the problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with limitations in their applicability. This work introduces ViewFusion, an end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target view only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely underdetermined conditions (thanks to its generative nature)---all while generating views of quality on par or even better than comparable methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small Neural 3D Mesh Renderer dataset. Code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernard Spiegl",
      "Andrea Perin",
      "Stephane Deny",
      "Alexander Ilin"
    ]
  },
  "https://openreview.net/forum?id=tXnVRpRlR8": {
    "title": "Learning Actionable Counterfactual Explanations in Large State Spaces",
    "volume": "main",
    "abstract": "Recourse generators provide actionable insights, often through feature-based counterfactual explanations (CFEs), to help negatively classified individuals understand how to adjust their input features to achieve a positive classification. These feature-based CFEs, which we refer to as \\emph{low-level} CFEs, are overly specific (e.g., coding experience: \\(4 \\to 5+\\) years) and often recommended in a feature space that doesn't straightforwardly align with real-world actions. To bridge this gap, we introduce three novel recourse types grounded in real-world actions: high-level continuous (\\emph{hl-continuous}), high-level discrete (\\emph{hl-discrete}), and high-level ID (\\emph{hl-id}) CFEs. We formulate single-agent CFE generation methods for hl-discrete and hl-continuous CFEs. For the hl-discrete CFE, we cast the task as a weighted set cover problem that selects the least cost set of hl-discrete actions that satisfy the eligibility of features, and model the hl-continuous CFE as a solution to an integer linear program that identifies the least cost set of hl-continuous actions capable of favorably altering the prediction of a linear classifier. Since these methods require costly optimization per agent, we propose data-driven CFE generation approaches that, given instances of agents and their optimal CFEs, learn a CFE generator that quickly provides optimal CFEs for new agents. This approach, also viewed as one of learning an optimal policy in a family of large but deterministic MDPs, considers several problem formulations, including formulations in which the actions and their effects are unknown, and therefore addresses informational and computational challenges. We conduct extensive empirical evaluations using publicly available healthcare datasets (BRFSS, Foods, and NHANES) and fully-synthetic data. For negatively classified agents identified by linear and threshold-based binary classifiers, we compare the proposed forms of recourse to low-level CFEs, which suggest how the agent can transition from state \\(\\mathbf{x}\\) to a new state \\(\\mathbf{x}'\\) where the model prediction is desirable. We also extensively evaluate the effectiveness of our neural network-based, data-driven CFE generation approaches. Empirical results show that the proposed data-driven CFE generators are accurate and resource-efficient, and the proposed forms of recourse offer various advantages over the low-level CFEs",
    "checked": true,
    "id": "1942d245a3deb92ef63bd56251f2d1b6977f4699",
    "semantic_title": "learning actionable counterfactual explanations in large state spaces",
    "citation_count": 0,
    "authors": [
      "Keziah Naggita",
      "Matthew Walter",
      "Avrim Blum"
    ]
  },
  "https://openreview.net/forum?id=9M4NKMZOPu": {
    "title": "Learning distributed representations with efficient SoftMax normalization",
    "volume": "main",
    "abstract": "Learning distributed representations, or embeddings, that encode the relational similarity patterns among objects is a relevant task in machine learning. A popular method to learn the embedding matrices $X, Y$ is optimizing a loss function of the term ${\\rm SoftMax}(XY^T)$. The complexity required to calculate this term, however, runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time heuristic approximation to compute the normalization constants of ${\\rm SoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some pre-trained embedding datasets that the proposed estimation method achieves higher or comparable accuracy with competing methods. From this result, we design an efficient and task-agnostic algorithm that learns the embeddings by optimizing the cross entropy between the softmax and a set of probability distributions given as inputs. The proposed algorithm is interpretable and easily adapted to arbitrary embedding problems. We consider a few use cases and observe similar or higher performances and a lower computational time than similar ``2Vec'' algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Dall'Amico",
      "Enrico Maria Belliardo"
    ]
  },
  "https://openreview.net/forum?id=QQZ8uPxFb3": {
    "title": "Explaining Node Embeddings",
    "volume": "main",
    "abstract": "Node embedding algorithms produce low-dimensional latent representations of nodes in a graph. These embeddings are often used for downstream tasks, such as node classification and link prediction. In this paper, we investigate the following two questions: (Q1) Can we explain each embedding dimension with human-understandable graph features (e.g. degree, clustering coefficient and PageRank). (Q2) How can we modify existing node embedding algorithms to produce embeddings that can be easily explained by human-understandable graph features? We find that the answer to Q1 is yes and introduce a new framework called XM (short for eXplain eMbedding) to answer Q2. A key aspect of XM involves minimizing the nuclear norm of the generated explanations. We show that by minimizing the nuclear norm, we minimize the lower bound on the entropy of the generated explanations. We test XM on a variety of real-world graphs and show that XM not only preserves the performance of existing node embedding methods, but also enhances their explainability",
    "checked": true,
    "id": "29977715c0f556cbf2274a409a3beb879987306c",
    "semantic_title": "explaining node embeddings",
    "citation_count": 0,
    "authors": [
      "Zohair Shafi",
      "Ayan Chatterjee",
      "Tina Eliassi-Rad"
    ]
  },
  "https://openreview.net/forum?id=F42CRfcp3D": {
    "title": "Diversity-Driven View Subset Selection for Indoor Novel View Synthesis",
    "volume": "main",
    "abstract": "Novel view synthesis of indoor scenes can be achieved by capturing a monocular video sequence of the environment. However, redundant information caused by artificial movements in the input video data reduces the efficiency of scene modeling. To address this, we formulate the problem as a combinatorial optimization task for view subset selection. In this work, we propose a novel subset selection framework that integrates a comprehensive diversity-based measurement with well-designed utility functions. We provide a theoretical analysis of these utility functions and validate their effectiveness through extensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset designed for indoor novel view synthesis, featuring complex and extended trajectories that simulate intricate human behaviors. Experiments on IndoorTraj show that our framework consistently outperforms baseline strategies while using only 5–20% of the data, highlighting its remarkable efficiency and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Wang",
      "Han Zhou",
      "Matthew B. Blaschko",
      "Tinne Tuytelaars",
      "Minye Wu"
    ]
  },
  "https://openreview.net/forum?id=Q2M4yijKSo": {
    "title": "Flexible Infinite-Width Graph Convolutional Neural Networks",
    "volume": "main",
    "abstract": "A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed and tunable only through a small number of hyperparameters, thus eliminating the possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well because they are able to flexibly learn representations for the task at hand. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of node classification tasks on graphs. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a \"knob\" to control the amount of flexibility and hence representation learning. We found that representation learning gives noticeable performance improvements for heterophilous node classification tasks, but less so for homophilous node classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Anson",
      "Edward Milsom",
      "Laurence Aitchison"
    ]
  },
  "https://openreview.net/forum?id=QI0l842vSq": {
    "title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) learn to represent nodes by aggregating information from their neighbors. As GNNs increase in depth, their receptive field grows exponentially, leading to high memory costs. Several works in the literature proposed to address this shortcoming by sampling subgraphs, or by using historical embeddings. These methods have mostly focused on benchmarks of single-label node classification on homophilous graphs, where neighboring nodes often share the same label. However, most of these methods rely on static heuristics that may not generalize across different graphs or tasks. We argue that the sampling method should be adaptive, adjusting to the complex structural properties of each graph. To this end, we introduce GRAPES, an adaptive sampling method that learns to identify the set of nodes crucial for training a GNN. GRAPES trains a second GNN to predict node sampling probabilities by optimizing the downstream task objective. We evaluate GRAPES on various node classification benchmarks involving homophilous as well as heterophilous graphs. We demonstrate GRAPES' effectiveness in accuracy and scalability, particularly in multi-label heterophilous graphs. Additionally, GRAPES uses orders of magnitude less GPU memory than a strong baseline based on historical embeddings. Unlike other sampling methods, GRAPES maintains high accuracy even with smaller sample sizes and, therefore, can scale to massive graphs. Our implementation is publicly available online",
    "checked": true,
    "id": "0e98e6bf270463cd2b5841a669b34a90494441d5",
    "semantic_title": "grapes: learning to sample graphs for scalable graph neural networks",
    "citation_count": 5,
    "authors": [
      "Taraneh Younesian",
      "Daniel Daza",
      "Emile van Krieken",
      "Thiviyan Thanapalasingam",
      "Peter Bloem"
    ]
  },
  "https://openreview.net/forum?id=zo5b60AuAH": {
    "title": "Local Differential Privacy-Preserving Spectral Clustering for General Graphs",
    "volume": "main",
    "abstract": "Spectral clustering is a widely used algorithm to find clusters in networks. Several researchers have studied the stability of spectral clustering under local differential privacy with the additional assumption that the underlying networks are generated from the stochastic block model (SBM). However, we argue that this assumption is too restrictive since social networks do not originate from the SBM. Thus, we delve into an analysis for general graphs in this work. Our primary focus is the edge flipping method -- a common technique for protecting local differential privacy. We show that, when the edges of an $n$-vertex graph satisfying some reasonable well-clustering assumptions are flipped with a probability of $O(\\log n/n)$, the clustering outcomes are largely consistent. Empirical tests further corroborate these theoretical findings. Conversely, although clustering outcomes have been stable for non-sparse and well-clustered graphs produced from the SBM, we show that in general, spectral clustering may yield highly erratic results on certain graphs when the flipping probability is $\\omega(\\log n/n)$. This indicates that the best privacy budget obtainable for general graphs is $\\Theta(\\log n)$",
    "checked": true,
    "id": "50808d60fed0c31674cb813365533a8b57e00032",
    "semantic_title": "local differential privacy-preserving spectral clustering for general graphs",
    "citation_count": 1,
    "authors": [
      "Sayan Mukherjee",
      "Vorapong Suppakitpaisarn"
    ]
  },
  "https://openreview.net/forum?id=WzS33L1iPC": {
    "title": "Visually Descriptive Language Model for Vector Graphics Reasoning",
    "volume": "main",
    "abstract": "Despite significant advancements, current large multimodal models (LMMs) struggle to bridge the gap between low-level visual perception—focusing on shapes, sizes, and layouts—and high-level language reasoning involving semantics, events, and logic. This limitation becomes evident in tasks requiring precise visual perception, such as comparing geometric properties or solving visual algorithmic reasoning problems. To study this failure mode, we focus on an important visual domain: vector graphics —images composed purely of 2D objects and shapes, which are prevalent in Web, PC, and Mobile environments. Importantly, we consider rasterized vector graphics without assuming access to their underlying vector code. We identify two key research questions: how can we enable precise visual perception, and how can we facilitate high-level reasoning based on such low-level perceptions? To accurately capture low-level visual details, we explore using SVG for the precise encoding of visual scenes. However, SVGs are not readily interpretable by LLMs or LMMs in a zero-shot manner. To address this challenge, we propose the Visually Descriptive Language Model (VDLM) to build a bridge between low-level visual perception and high-level language reasoning. VDLM learns an intermediate symbolic representation called Primal Visual Description (PVD), which translates raw SVGs into a higher-level abstraction comprising primitive attributes. This abstraction allows for direct interpretation by foundation models for zero-shot generalization to different reasoning tasks. Without any human-annotated data, VDLM leads to significant improvements in state-of-the-art LMMs, such as GPT-4o, across various low-level multimodal perception and reasoning tasks on rasterized vector graphics. Additionally, we provide extensive analyses of VDLM's performance, showing that our framework offers improved interpretability due to its disentangled perception and reasoning processes. As the first attempt to construct a descriptive intermediate representation for low-level visual reasoning, we also conduct an in-depth error analysis, highlighting remaining limitations and suggesting directions for future research",
    "checked": true,
    "id": "6fde101147cf3b1980f1ac54a45026dddf110f3a",
    "semantic_title": "visually descriptive language model for vector graphics reasoning",
    "citation_count": 4,
    "authors": [
      "Zhenhailong Wang",
      "Joy Hsu",
      "Xingyao Wang",
      "Kuan-Hao Huang",
      "Manling Li",
      "Jiajun Wu",
      "Heng Ji"
    ]
  },
  "https://openreview.net/forum?id=PPGJ3EvENv": {
    "title": "Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport",
    "volume": "main",
    "abstract": "Detecting anomalies in datasets is a longstanding problem in machine learning. In this context, anomalies are defined as a sample that significantly deviates from the remaining data. Meanwhile, Optimal Transport (OT) is a field of mathematics concerned with the transportation, between two probability distribution, at least effort. In classical OT, the optimal transportation strategy of a distribution to itself is the identity, i.e., each sample keeps its mass. In this paper, we tackle anomaly detection by forcing samples to displace its mass, while keeping the least effort objective. We call this new transportation problem Mass Repulsing Optimal Transport (MROT). Naturally, samples lying in low density regions of space will be forced to displace mass very far, incurring a higher transportation cost. In contrast, samples on high density regions are able to send their mass just outside an \\emph{exclusion zone}. We use these concepts to design a new anomaly score. Through a series of experiments in existing benchmarks, and fault detection problems, we show that our algorithm improves over existing methods. Our code is publicly available at https://github.com/eddardd/MROT",
    "checked": true,
    "id": "90fda71788307ea96e198939231b24874ac935f2",
    "semantic_title": "unsupervised anomaly detection through mass repulsing optimal transport",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "EL HABAZI Adel",
      "Fred Maurice NGOLE MBOULA"
    ]
  },
  "https://openreview.net/forum?id=Xmk1or5eH8": {
    "title": "Algorithm Configuration for Structured Pfaffian Settings",
    "volume": "main",
    "abstract": "Data-driven algorithm design uses historical problem instances to automatically adjust and optimize algorithms to their application domain, typically by selecting algorithms from parameterized families. While the approach has been highly successful in practice, providing theoretical guarantees for several algorithmic families remains challenging. This is due to the intricate dependence of the algorithmic performance on the parameters, often exhibiting a piecewise discontinuous structure. In this work, we present new frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both statistical and online learning settings. For the statistical learning setting, we introduce the Pfaffian GJ framework, an extension of the classical Goldberg-Jerrum (GJ) framework (Bartlett et al., 2022; Goldberg & Jerrum, 1993), that is capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions (quotients of two polynomials), our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a refined piecewise structure, which automatically translates to learning guarantees using our proposed framework. For the online learning setting, we provide a new tool for verifying the dispersion property of a sequence of loss functions, a sufficient condition that allows no-regret learning for sequences of piecewise structured loss functions where the piecewise structure involves Pfaffian transition boundaries. We use our framework to provide novel learning guarantees for many challenging data-driven design problems of interest, including data-driven linkage-based clustering, graph-based semi-supervised learning, and regularized logistic regression",
    "checked": true,
    "id": "bd060781042b776f1ea0299155c4fb9c88d4d74b",
    "semantic_title": "algorithm configuration for structured pfaffian settings",
    "citation_count": 1,
    "authors": [
      "Maria Florina Balcan",
      "Anh Tuan Nguyen",
      "Dravyansh Sharma"
    ]
  },
  "https://openreview.net/forum?id=zF9IrMTjCC": {
    "title": "Generalizable Representation Learning for fMRI-based Neurological Disorder Identification",
    "volume": "main",
    "abstract": "Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhui Cui",
      "Haleh Akrami",
      "Anand Joshi",
      "Richard Leahy"
    ]
  },
  "https://openreview.net/forum?id=Uz9J77Riul": {
    "title": "Information Theoretic Guarantees For Policy Alignment In Large Language Models",
    "volume": "main",
    "abstract": "Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy based on an $f$-divergence like $\\mathsf{KL}$ divergence. The best of $n$ alignment policy selects the sample with the highest reward from $n$ independent samples. Recent work shows that the reward improvement of the aligned policy scales as $\\sqrt{\\mathsf{KL}}$, with an explicit bound on the $\\mathsf{KL}$ for best of $n$ policies. We show that this $\\sqrt{\\mathsf{KL}}$ bound holds if the reference policy's reward has sub-gaussian tails. For best of $n$ policies, the $\\mathsf{KL}$ bound applies to any $f$-divergence through a reduction to exponential order statistics using the Rényi representation. Tighter control can be achieved with Rényi divergence if additional tail information is known. Finally, we demonstrate how these bounds transfer to golden rewards, resulting in decreased golden reward improvement due to proxy reward overestimation and approximation errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Mroueh",
      "Apoorva Nitsure"
    ]
  },
  "https://openreview.net/forum?id=o6ukhJLzMQ": {
    "title": "Augmented Invertible Koopman Autoencoder for long-term time series forecasting",
    "volume": "main",
    "abstract": "Following the introduction of Dynamic Mode Decomposition and its numerous extensions, many neural autoencoder-based implementations of the Koopman operator have recently been proposed. This class of methods appears to be of interest for modeling dynamical systems, either through direct long-term prediction of the evolution of the state or as a powerful embedding for downstream methods. In particular, a recent line of work has developed invertible Koopman autoencoders (IKAEs), which provide an exact reconstruction of the input state thanks to their analytically invertible encoder, based on coupling layer normalizing flow models. We identify that the conservation of the dimension imposed by the normalizing flows is a limitation for the IKAE models, and thus we propose to augment the latent state with a second, non-invertible encoder network. This results in our new model: the Augmented Invertible Koopman AutoEncoder (AIKAE). We demonstrate the relevance of the AIKAE through a series of long-term time series forecasting experiments, on satellite image time series as well as on a benchmark involving predictions based on a large lookback window of observations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Frion",
      "Lucas Drumetz",
      "Mauro Dalla Mura",
      "Guillaume Tochon",
      "Abdeldjalil AISSA EL BEY"
    ]
  },
  "https://openreview.net/forum?id=wyOv4kGkbU": {
    "title": "Test-time Contrastive Concepts for Open-world Semantic Segmentation with Vision-Language Models",
    "volume": "main",
    "abstract": "Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts of image-text pairs to align both modalities with a simple contrastive objective, have paved the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image pixels are assigned the closest query in feature space. However, this works well when a user exhaustively lists all possible visual concepts in an image that contrast against each other for the assignment. This corresponds to the current evaluation setup in the literature, which relies on having access to a list of in-domain relevant concepts, typically classes of a benchmark dataset. Here, we consider the more challenging (and realistic) scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic \"background\" text, we propose two different approaches to automatically generate, at test time, query-specific textual contrastive concepts. We do so by leveraging the distribution of texts in the VLM's training set or crafted LLM prompts. We also propose a metric designed to evaluate this scenario and show the relevance of our approach on commonly used datasets",
    "checked": false,
    "id": "d1635967b7f44ce3714d78f2cf0eea4cd5636e8e",
    "semantic_title": "test-time contrastive concepts for open-world semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Monika Wysoczańska",
      "Antonin Vobecky",
      "Amaia Cardiel",
      "Tomasz Trzcinski",
      "Renaud Marlet",
      "Andrei Bursuc",
      "Oriane Siméoni"
    ]
  },
  "https://openreview.net/forum?id=qmHlTkLdbL": {
    "title": "Online Bandit Nonlinear Control with Dynamic Batch Length and Adaptive Learning Rate",
    "volume": "main",
    "abstract": "This paper is concerned with the online bandit nonlinear control, which aims to learn the best stabilizing controller from a pool of stabilizing and destabilizing controllers of unknown types for a given nonlinear dynamical system. We develop an algorithm, named Dynamic Batch length and Adaptive learning Rate (DBAR), and study its stability and regret. Unlike the existing Exp3 algorithm requiring an exponentially stabilizing controller, DBAR only needs a significantly weaker notion of controller stability, in which case substantial time may be required to certify the system stability. Dynamic batch length in DBAR effectively addresses this issue and enables the system to attain asymptotic stability, where the algorithm behaves as if there were no destabilizing controllers. Moreover, adaptive learning rate in DBAR only uses the state norm information to achieve a tight regret bound even when none of the stabilizing controllers in the pool are exponentially stabilizing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Kim",
      "Javad Lavaei"
    ]
  },
  "https://openreview.net/forum?id=L33DSu3zvq": {
    "title": "Tighter sparse variational Gaussian processes",
    "volume": "main",
    "abstract": "Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang D Bui",
      "Matthew Ashman",
      "Richard E. Turner"
    ]
  },
  "https://openreview.net/forum?id=BbwlJpNXgW": {
    "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
    "volume": "main",
    "abstract": "Large language models trained on web-scale corpora can memorize undesirable data containing misinformation, copyrighted material, or private or sensitive information. Recently, several machine unlearning algorithms have been proposed to eliminate the effect of such datapoints from trained models-- that is, to approximate *a model that had never been trained on these datapoints in the first place*. However, evaluating the effectiveness of unlearning algorithms remains an open challenge. Previous work has relied on heuristics-- such as verifying that the model can no longer reproduce the specific information targeted for removal while maintaining accuracy on unrelated test data. These approaches inadequately capture the complete effect of reversing the influence of datapoints on a trained model. In this work, we propose the RESTOR framework for machine unlearning evaluation, which assesses the ability of unlearning algorithms for targeted data erasure, by evaluating the ability of models to forget the knowledge introduced in these datapoints, while simultaneously recovering the model's knowledge state had it never encountered these datapoints. RESTOR helps uncover several novel insights about popular unlearning algorithms, and the mechanisms through which they operate-- for instance, identifying that some algorithms merely emphasize forgetting but not recovering knowledge, and that localizing unlearning targets can enhance unlearning performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keivan Rezaei",
      "Khyathi Chandu",
      "Soheil Feizi",
      "Yejin Choi",
      "Faeze Brahman",
      "Abhilasha Ravichander"
    ]
  },
  "https://openreview.net/forum?id=fuOHI59rUW": {
    "title": "MarDini: Masked Auto-regressive Diffusion for Video Generation at Scale",
    "volume": "main",
    "abstract": "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Shikun Liu",
      "Zijian Zhou",
      "Mengmeng Xu",
      "Yanping Xie",
      "Xiao Han",
      "Juan Camilo Perez",
      "Ding Liu",
      "Kumara Kahatapitiya",
      "Menglin Jia",
      "Jui-Chieh Wu",
      "Sen He",
      "Tao Xiang",
      "Jürgen Schmidhuber",
      "Juan-Manuel Perez-Rua"
    ]
  },
  "https://openreview.net/forum?id=LLWJkR6gaI": {
    "title": "Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control",
    "volume": "main",
    "abstract": "Imitation learning is an efficient method for teaching robots a variety of tasks. Diffusion Policy, which uses a conditional denoising diffusion process to generate actions, has demonstrated superior performance, particularly in learning from multi-modal demonstrates. However, it relies on executing multiple actions predicted from the same inference step to retain performance and prevent mode bouncing, which limits its responsiveness, as actions are not conditioned on the most recent observations. To address this, we introduce Responsive Noise-Relaying Diffusion Policy (RNR-DP), which maintains a noise-relaying buffer with progressively increasing noise levels and employs a sequential denoising mechanism that generates immediate, noise-free actions at the head of the sequence, while appending noisy actions at the tail. This ensures that actions are responsive and conditioned on the latest observations, while maintaining motion consistency through the noise-relaying buffer. This design enables the handling of tasks requiring responsive control, and accelerates action generation by reusing denoising steps. Experiments on response-sensitive tasks demonstrate that, compared to Diffusion Policy, ours achieves 18% improvement in success rate. Further evaluation on regular tasks demonstrates that RNR-DP also exceeds the best acceleration method (DDIM) by 6.9% in success rate, highlighting its computational efficiency advantage in scenarios where responsiveness is less critical",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoqun Chen",
      "Xiu Yuan",
      "Tongzhou Mu",
      "Hao Su"
    ]
  },
  "https://openreview.net/forum?id=Ed1DBB3sBQ": {
    "title": "Conformal Prediction: A Theoretical Note and Benchmarking Transductive Node Classification in Graphs",
    "volume": "main",
    "abstract": "Conformal prediction has become increasingly popular for quantifying the uncertainty associated with machine learning models. Recent work in graph uncertainty quantification has built upon this approach for conformal graph prediction. The nascent nature of these explorations has led to conflicting choices for implementations, baselines, and method evaluation. In this work, we analyze the design choices made in the literature and discuss the tradeoffs associated with existing methods. Building on the existing implementations for existing methods, we introduce techniques to scale existing methods to large-scale graph datasets without sacrificing performance. Our theoretical and empirical results justify our recommendations for future scholarship in graph conformal prediction",
    "checked": true,
    "id": "bee6768e7c5fde8e1e0385d2fb085c584f0ed61a",
    "semantic_title": "conformal prediction: a theoretical note and benchmarking transductive node classification in graphs",
    "citation_count": 0,
    "authors": [
      "Pranav Maneriker",
      "Aditya T. Vadlamani",
      "Anutam Srinivasan",
      "Yuntian He",
      "Ali Payani",
      "srinivasan parthasarathy"
    ]
  },
  "https://openreview.net/forum?id=A4RLpHPXCu": {
    "title": "Offset Unlearning for Large Language Models",
    "volume": "main",
    "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs",
    "checked": true,
    "id": "72010b7fe48301a38e8063109b8ef8fcfd573e05",
    "semantic_title": "offset unlearning for large language models",
    "citation_count": 14,
    "authors": [
      "James Y. Huang",
      "Wenxuan Zhou",
      "Fei Wang",
      "Fred Morstatter",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ]
  },
  "https://openreview.net/forum?id=oYmRiWCQ1W": {
    "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation",
    "volume": "main",
    "abstract": "Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praveen Srinivasa Varadhan",
      "amogh gulati",
      "Ashwin Sankar",
      "Srija Anand",
      "Anirudh Gupta",
      "Anirudh Mukherjee",
      "Shiva Kumar Marepally",
      "Ankur Bhatia",
      "Saloni Jaju",
      "Suvrat Bhooshan",
      "Mitesh M Khapra"
    ]
  },
  "https://openreview.net/forum?id=cCQKwd5MFP": {
    "title": "Part-aware Prompted Segment Anything Model for Adaptive Segmentation",
    "volume": "main",
    "abstract": "Precision medicine, such as patient-adaptive treatments assisted by medical image analysis, poses new challenges for image segmentation algorithms due to the large variability across different patients and the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}rompted}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, $\\text{P}^2\\text{SAM}$ enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the part-aware prompt mechanism, we propose a distribution-guided retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^2\\text{SAM}$ improves the performance by $\\texttt{+} 8.0\\%$ and $\\texttt{+} 2.0\\%$ mean Dice score for two different patient-adaptive segmentation applications, respectively. In addition, $\\text{P}^2\\text{SAM}$ also exhibits impressive generalizability in other adaptive segmentation tasks in the natural image domain, $\\textit{e.g.}$, $\\texttt{+} 6.4\\%$ mIoU within personalized object segmentation task. Code will be released upon acceptance",
    "checked": true,
    "id": "470cacf502daf31e7da621666eb012ffa49305c1",
    "semantic_title": "part-aware prompted segment anything model for adaptive segmentation",
    "citation_count": 3,
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ]
  },
  "https://openreview.net/forum?id=kY2fKLOGkI": {
    "title": "On the Utility of Existing Fine-Tuned Models on Data-Scarce Domains",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been observed to perform well on a wide range of downstream tasks when fine-tuned on domain-specific data. However, such data may not be readily available in many applications, motivating zero-shot or few-shot approaches using existing domain or task adjacent (fine-tuned) models, which we call DAFT. While several fine-tuned models for various tasks are available, finding one appropriate DAFT model for a given task is often not straight forward. In this paper, we explore different utilization techniques of these existing DAFT models for data-scarce problems, i.e., tasks for which data is not available or limited. We observe that for zero-shot problems, ensembling of DAFT models provides an accuracy performance close to that of the single best model. With few-shot problems (few data from target domain available), this performance can be improved further by picking or putting more weights to the DAFT models that are expected to perform better on the target task",
    "checked": true,
    "id": "2abf7bd98138a35eb377babfcf04c067e437f7af",
    "semantic_title": "on the utility of existing fine-tuned models on data-scarce domains",
    "citation_count": 0,
    "authors": [
      "Md Ibrahim Ibne Alam",
      "Parikshit Ram",
      "Soham Dan",
      "Horst Samulowitz",
      "Koushik Kar"
    ]
  },
  "https://openreview.net/forum?id=k8x44wVIs1": {
    "title": "Group Fair Federated Learning via Stochastic Kernel Regularization",
    "volume": "main",
    "abstract": "Ensuring \\textbf{group fairness} in federated learning (FL) presents unique challenges due to data heterogeneity and communication constraints. We propose Kernel Fair Federated Learning (\\texttt{KFFL}), a novel framework that incorporates group fairness into FL models using the Kernel Hilbert-Schmidt Independence Criterion (KHSIC) as a fairness regularizer. To address scalability, \\texttt{KFFL} approximates KHSIC with Random Feature Maps (RFMs), significantly reducing computational and communication overhead while achieving \\textit{group fairness}. To address the resulting non-convex optimization problem, we propose \\texttt{FedProxGrad}, a federated proximal gradient algorithm that guarantees convergence. Through experiments on standard benchmark datasets across both IID and Non-IID settings for regression and classification tasks, \\texttt{KFFL} demonstrates its ability to balance accuracy and fairness effectively, outperforming existing methods by comprehensively exploring the Pareto Frontier. Furthermore, we introduce \\texttt{KFFL-TD}, a time-delayed variant that further reduces communication rounds, enhancing efficiency in decentralized environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huzaifa Arif",
      "Pin-Yu Chen",
      "Keerthiram Murugesan",
      "Alex Gittens"
    ]
  },
  "https://openreview.net/forum?id=quE8gDDegf": {
    "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification",
    "volume": "main",
    "abstract": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, \\emph{class prototype learning} (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67\\% improvement over strong baseline methods",
    "checked": false,
    "id": "4a3cda4ee6bcb5d40604d93cb37c3a82b89c2bee",
    "semantic_title": "e xploring w eak - to -s trong g eneralization for clip-based c lassification",
    "citation_count": 0,
    "authors": [
      "Jinhao Li",
      "Sarah Monazam Erfani",
      "Lei Feng",
      "James Bailey",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=OjWB2671AR": {
    "title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning",
    "volume": "main",
    "abstract": "Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation---a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization",
    "checked": true,
    "id": "1869e4b3ec0a248c6ba360bf17706621207d310e",
    "semantic_title": "deep augmentation: dropout as augmentation for self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Rickard Brüel Gabrielsson",
      "Tongzhou Wang",
      "Manel Baradad",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=haf78jerSt": {
    "title": "Forecasting Company Fundamentals",
    "volume": "main",
    "abstract": "Company fundamentals are key to assessing companies' financial and overall success and stability. Forecasting them is important in multiple fields, including investing and econometrics. While statistical and contemporary machine learning methods have been applied to many time series tasks, there is a lack of comparison of these approaches on this particularly challenging data regime. To this end, we try to bridge this gap and thoroughly evaluate the theoretical properties and practical performance of 24 deterministic and probabilistic company fundamentals forecasting models on real company data. We observe that deep learning models provide superior forecasting performance to classical models, in particular when considering uncertainty estimation. To validate the findings, we compare them to human analyst expectations and find that their accuracy is comparable to the automatic forecasts. We further show how these high-quality forecasts can benefit automated stock allocation. We close by presenting possible ways of integrating domain experts to further improve performance and increase reliability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Divo",
      "Eric Endress",
      "Kevin Endler",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ]
  },
  "https://openreview.net/forum?id=6g1WJ55N51": {
    "title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control",
    "volume": "main",
    "abstract": "We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $\\epsilon t$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting algorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon t$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting",
    "checked": true,
    "id": "331586853f0b2f45d66fa8bde7bdc5dde07725cf",
    "semantic_title": "etgl-ddpg: a deep deterministic policy gradient algorithm for sparse reward continuous control",
    "citation_count": 1,
    "authors": [
      "Ehsan Futuhi",
      "Shayan Karimi",
      "Chao Gao",
      "Martin Müller"
    ]
  },
  "https://openreview.net/forum?id=kHl4JzyNzF": {
    "title": "Music Foundation Model as Generic Booster for Music Downstream Tasks",
    "volume": "main",
    "abstract": "We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions",
    "checked": true,
    "id": "fa689259129b052f5f97934128d476f7c82d4a23",
    "semantic_title": "music foundation model as generic booster for music downstream tasks",
    "citation_count": 1,
    "authors": [
      "Wei-Hsiang Liao",
      "Yuhta Takida",
      "Yukara Ikemiya",
      "Zhi Zhong",
      "Chieh-Hsin Lai",
      "Giorgio Fabbro",
      "Kazuki Shimada",
      "Keisuke Toyama",
      "Kin Wai Cheuk",
      "Marco A. Martínez-Ramírez",
      "Shusuke Takahashi",
      "Stefan Uhlich",
      "Taketo Akama",
      "Woosung Choi",
      "Yuichiro Koyama",
      "Yuki Mitsufuji"
    ]
  },
  "https://openreview.net/forum?id=pdC092Nn8N": {
    "title": "Studying Exploration in RL: An Optimal Transport Analysis of Occupancy Measure Trajectories",
    "volume": "main",
    "abstract": "The rising successes of RL are propelled by combining smart algorithmic strategies and deep architectures to optimize the distribution of returns and visitations over the state-action space. A quantitative framework to compare the learning processes of these eclectic RL algorithms is currently absent but desired in practice. We address this gap by representing the learning process of an RL algorithm as a sequence of policies generated during training, and then studying the policy trajectory induced in the manifold of state-action occupancy measures. Using an optimal transport-based metric, we measure the length of the paths induced by the policy sequence yielded by an RL algorithm between an initial policy and a final optimal policy. Hence, we first define the Effort of Sequential Learning (ESL). ESL quantifies the relative distance that an RL algorithm travels compared to the shortest path from the initial to the optimal policy. Furthermore, we connect the dynamics of policies in the occupancy measure space and regret (another metric to understand the suboptimality of an RL algorithm), by defining the Optimal Movement Ratio (OMR). OMR assesses the fraction of movements in the occupancy measure space that effectively reduce an analogue of regret. Finally, we derive approximation guarantees to estimate ESL and OMR with a finite number of samples and without access to an optimal policy. Through empirical analyses across various environments and algorithms, we demonstrate that ESL and OMR provide insights into the exploration processes of RL algorithms and the hardness of different tasks in discrete and continuous MDPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reabetswe M. Nkhumise",
      "Debabrota Basu",
      "Tony J. Prescott",
      "Aditya Gilra"
    ]
  },
  "https://openreview.net/forum?id=gG8sQUUtN7": {
    "title": "LASP: Linear Attention Sequence Parallelism",
    "volume": "main",
    "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: \\url{https://github.com/OpenNLPLab/LASP}",
    "checked": false,
    "id": "660d80773f55c5dc889de3ce93b71672133a91bc",
    "semantic_title": "linear attention sequence parallelism",
    "citation_count": 2,
    "authors": [
      "Weigao Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Yu Qiao",
      "Yiran Zhong"
    ]
  },
  "https://openreview.net/forum?id=PHsfZnF2FC": {
    "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design choices and added complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Importantly, MOORL addresses the key challenges of hybrid RL in terms of being design-free. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without relying on extensive design choices. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Chaudhary",
      "Washim Uddin Mondal",
      "Laxmidhar Behera"
    ]
  },
  "https://openreview.net/forum?id=2NSb3cJE03": {
    "title": "Time-Uniform Confidence Spheres for Means of Random Vectors",
    "volume": "main",
    "abstract": "We study sequential mean estimation in $\\mathbb{R}^d$. In particular, we derive time-uniform confidence spheres---\\emph{confidence sphere sequences} (CSSs)---which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\\psi$ random vectors (which includes sub-gamma, and sub-exponential distributions). Many of our results are optimal. For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Chugg",
      "Hongjian Wang",
      "Aaditya Ramdas"
    ]
  },
  "https://openreview.net/forum?id=6BlOCx5c5T": {
    "title": "How far away are truly hyperparameter-free learning algorithms?",
    "volume": "main",
    "abstract": "Despite major advances in methodology, hyperparameter tuning remains a crucial (and expensive) part of the development of machine learning systems. Even ignoring architectural choices, deep neural networks have a large number of optimization and regularization hyperparameters that need to be tuned carefully per workload in order to obtain the best results. In a perfect world, training algorithms would not require workload-specific hyperparameter tuning, but would instead have default settings that performed well across many workloads. Recently, there has been a growing literature on optimization methods which attempt to reduce the number of hyperparameters---particularly the learning rate and its accompanying schedule. Given these developments, how far away is the dream of neural network training algorithms that completely obviate the need for painful tuning? In this paper, we evaluate the potential of learning-rate-free methods as components of hyperparameter-free methods. We freeze their (non-learning rate) hyperparameters to default values, and score their performance using the recently-proposed AlgoPerf: Training Algorithms benchmark. We found that literature-supplied default settings performed poorly on the benchmark, so we performed a search for hyperparameter configurations that performed well across all workloads simultaneously. The best \"algoperf-calibrated\" learning-rate-free methods had much improved performance but still lagged slightly behind a similarly calibrated NadamW baseline in overall benchmark score. Our results suggest that there is still much room for improvement for learning-rate-free methods, and that testing against a strong, workload-agnostic baseline is important to improve hyperparameter reduction techniques",
    "checked": true,
    "id": "eb917217a143ceb9059bbf6659020741a1fba00e",
    "semantic_title": "how far away are truly hyperparameter-free learning algorithms?",
    "citation_count": 0,
    "authors": [
      "Priya Kasimbeg",
      "Vincent Roulet",
      "Naman Agarwal",
      "Sourabh Medapati",
      "Fabian Pedregosa",
      "Atish Agarwala",
      "George E. Dahl"
    ]
  },
  "https://openreview.net/forum?id=VTgixSbrJI": {
    "title": "Hitchhiker's guide on the relation of Energy-Based Models with other generative models, sampling and statistical physics: a comprehensive review",
    "volume": "main",
    "abstract": "Energy-Based Models (EBMs) have emerged as a powerful framework in the realm of generative modeling, offering a unique perspective that aligns closely with principles of statistical mechanics. This review aims to provide physicists with a comprehensive understanding of EBMs, delineating their connection to other generative models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Normalizing Flows. We explore the sampling techniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods, and draw parallels between EBM concepts and statistical mechanics, highlighting the significance of energy functions and partition functions. Furthermore, we delve into state-of-the-art training methodologies for EBMs, covering recent advancements and their implications for enhanced model performance and efficiency. This review is designed to clarify the often complex interconnections between these models, which can be challenging due to the diverse communities working on the topic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Carbone"
    ]
  },
  "https://openreview.net/forum?id=VPl3T43Hxb": {
    "title": "A Local Polyak-Łojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models",
    "volume": "main",
    "abstract": "Most prior work on the convergence of gradient descent (GD) for overparameterized neural networks relies on strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (large, spectral, balanced). Recent efforts to relax these assumptions focus on two-layer linear networks trained with the squared loss. In this work, we derive a linear convergence rate for training two-layer linear neural networks with GD for general losses and under relaxed assumptions on the step size, width, and initialization. A key challenge in deriving this result is that classical ingredients for deriving convergence rates for nonconvex problems, such as the Polyak-Łojasiewicz (PL) condition and Descent Lemma, do not hold globally for overparameterized neural networks. Here, we prove that these two conditions hold locally with local constants that depend on the weights. Then, we provide bounds on these local constants, which depend on the initialization of the weights, the current loss, and the global PL and smoothness constants of the non-overparameterized model. Based on these bounds, we derive a linear convergence rate for GD. Our convergence analysis not only improves upon prior results but also suggests a better choice for the step size, as verified through our numerical experiments",
    "checked": false,
    "id": "ad848a24708cf438340f97023c091908789c94db",
    "semantic_title": "a local polyak-lojasiewicz and descent lemma of gradient descent for overparametrized linear models",
    "citation_count": 0,
    "authors": [
      "Ziqing Xu",
      "Hancheng Min",
      "Salma Tarmoun",
      "Enrique Mallada",
      "Rene Vidal"
    ]
  },
  "https://openreview.net/forum?id=p499xXaclC": {
    "title": "Pruning Feature Extractor Stacking for Cross-domain Few-shot Learning",
    "volume": "main",
    "abstract": "Combining knowledge from source domains to learn efficiently from a few labelled instances in a target domain is a transfer learning problem known as cross-domain few-shot learning (CDFSL). Feature extractor stacking (FES) is a state-of-the-art CDFSL method that maintains a collection of source domain feature extractors instead of a single universal extractor. FES uses stacked generalisation to build an ensemble from extractor snapshots saved during target domain fine-tuning. It outperforms several contemporary universal model-based CDFSL methods in the Meta-Dataset benchmark. However, it incurs higher storage cost because it saves a snapshot for every fine-tuning iteration for every extractor. In this work, we propose a bidirectional snapshot selection strategy for FES, leveraging its cross-validation process and the ordered nature of its snapshots, and demonstrate that a 95% snapshot reduction can be achieved while retaining the same level of accuracy",
    "checked": true,
    "id": "6ed6f78ce0411a6f2bbf374741ffacc3d2e97f86",
    "semantic_title": "pruning feature extractor stacking for cross-domain few-shot learning",
    "citation_count": 0,
    "authors": [
      "Hongyu Wang",
      "Eibe Frank",
      "Bernhard Pfahringer",
      "Geoff Holmes"
    ]
  },
  "https://openreview.net/forum?id=nuN1mRrrjX": {
    "title": "Cometh: A continuous-time discrete-state graph diffusion model",
    "volume": "main",
    "abstract": "Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, tailored to the specificities of graph data. In addition, we also successfully replaced the set of structural encodings previously used in the discrete graph diffusion model with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a near-perfect performance of 99.5% on the planar graph dataset and outperforms DiGress by 12.6% on the large GuacaMol dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Siraudin",
      "Fragkiskos D. Malliaros",
      "Christopher Morris"
    ]
  },
  "https://openreview.net/forum?id=LDBjgS5Ez7": {
    "title": "Uniform Noise Distribution and Compact Clusters: Unveiling the Success of Self-Supervised Learning in Label Noise",
    "volume": "main",
    "abstract": "Label noise is ubiquitous in real-world datasets, posing significant challenges to machine learning models. While self-supervised learning (SSL) algorithms have empirically demonstrated effectiveness in learning noisy labels, the theoretical understanding of their effectiveness remains underexplored. In this paper, we present a theoretical framework to understand how SSL methods enhance learning with noisy labels, especially for the instance-dependent label noise. We reveal that the uniform and compact cluster structures induced by contrastive SSL play a crucial role in mitigating the adverse effects of label noise. Specifically, we theoretically show that a classifier trained on SSL-learned representations significantly outperforms one trained using traditional supervised learning methods. This results from two key merits of SSL representations over label noise: 1. Uniform Noise Distribution: Label noise becomes uniformly distributed over SSL representations with respect to the true class labels, rather than the noisy ones, leading to an easier learning task. 2. Enhanced Cluster Structure: SSL enhances the formation of well-separated and compact categorical clusters, increasing inter-class distances while tightening intra-class clusters. We further theoretically justify the benefits of training a classifier on such structured representations, demonstrating that it encourages the classifier trained on noisy data to be aligned with the optimal classifier. Extensive experiments validate the robustness of SSL representations in combating label noise, confirming the practical values of our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Xu",
      "Li Yi",
      "Gezheng Xu",
      "Xi Chen",
      "Ian McLeod",
      "Charles Ling",
      "Boyu Wang"
    ]
  },
  "https://openreview.net/forum?id=S6JpSsYBDZ": {
    "title": "RefinedFields: Radiance Fields Refinement for Planar Scene Representations",
    "volume": "main",
    "abstract": "Planar scene representations have recently witnessed increased interests for modeling scenes from images, as their lightweight planar structure enables compatibility with image-based models. Notably, K-Planes have gained particular attention as they extend planar scene representations to support in-the-wild scenes, in addition to object-level scenes. However, their visual quality has recently lagged behind that of state-of-the-art techniques. To reduce this gap, we propose RefinedFields, a method that leverages pre-trained networks to refine K-Planes scene representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and improves upon its base representation on the task of novel view synthesis. Our project page can be found at https://refinedfields.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Kassab",
      "Antoine Schnepf",
      "Jean-Yves Franceschi",
      "Laurent Caraffa",
      "Jeremie Mary",
      "Valerie Gouet-Brunet"
    ]
  },
  "https://openreview.net/forum?id=NeQYi56MFj": {
    "title": "M3CoL: Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification",
    "volume": "main",
    "abstract": "Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive Learning approach to capture nuanced shared relations inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with their corresponding samples from other modalities thereby capturing shared relations between them. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research",
    "checked": false,
    "id": "baa3f5de929e76427312ae2c13d7d42444c355ca",
    "semantic_title": "harnessing shared relations via multimodal mixup contrastive learning for multimodal classification",
    "citation_count": 0,
    "authors": [
      "Raja Kumar",
      "Raghav Singhal",
      "Pranamya Prashant Kulkarni",
      "Deval Mehta",
      "Kshitij Sharad Jadhav"
    ]
  },
  "https://openreview.net/forum?id=N2rWhTgits": {
    "title": "Guided Discrete Diffusion for Electronic Health Record Generation",
    "volume": "main",
    "abstract": "Electronic health records (EHRs) are a pivotal data source that enables numerous applications in computational medicine, e.g., disease progression prediction, clinical trial design, and health economics and outcomes research. Despite wide usability, their sensitive nature raises privacy and confidentially concerns, which limit potential use cases. To tackle these challenges, we explore the use of generative models to synthesize artificial, yet realistic EHRs. While diffusion-based methods have recently demonstrated state-of-the-art performance in generating other data modalities and overcome the training instability and mode collapse issues that plague previous GAN-based approaches, their applications in EHR generation remain underexplored. The discrete nature of tabular medical code data in EHRs poses challenges for high-quality data generation, especially for continuous diffusion models. To this end, we introduce a novel tabular EHR generation method, EHR-D3PM, which enables both unconditional and conditional generation using the discrete diffusion model. Our experiments demonstrate that EHR-D3PM significantly outperforms existing generative baselines on comprehensive fidelity and utility metrics while maintaining less attribute and membership vulnerability risks. Furthermore, we show EHR-D3PM is effective as a data augmentation method and enhances performance on downstream tasks when combined with real data",
    "checked": true,
    "id": "3643812e72325a7d92e57b21ad2cb24faf563f30",
    "semantic_title": "guided discrete diffusion for electronic health record generation",
    "citation_count": 7,
    "authors": [
      "Jun Han",
      "Zixiang Chen",
      "Yongqian Li",
      "Yiwen Kou",
      "Eran Halperin",
      "Robert E. Tillman",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=sq5AJvVuha": {
    "title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models",
    "volume": "main",
    "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget",
    "checked": true,
    "id": "aeb6fbef9a2f455a71773488e09f1c4f6686ac55",
    "semantic_title": "dygmamba: efficiently modeling long-term temporal dependency on continuous-time dynamic graphs with state space models",
    "citation_count": 3,
    "authors": [
      "Zifeng Ding",
      "Yifeng Li",
      "Yuan He",
      "Antonio Norelli",
      "Jingcheng Wu",
      "Volker Tresp",
      "Michael M. Bronstein",
      "Yunpu Ma"
    ]
  },
  "https://openreview.net/forum?id=5f7YlSKG1l": {
    "title": "Towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion",
    "volume": "main",
    "abstract": "Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine whether these effects can be identified from observational data. Identifying total effects in fully specified causal graphs has received considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of cycles and latent confounding, and when no variable set is sufficient for adjustment",
    "checked": true,
    "id": "3c5e09eeb9360cbeb8999f5ddde3f9bf0a9e57cd",
    "semantic_title": "towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion",
    "citation_count": 2,
    "authors": [
      "Charles K. Assaad"
    ]
  },
  "https://openreview.net/forum?id=ZrqLpXbXvA": {
    "title": "Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning",
    "volume": "main",
    "abstract": "Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the \"explanatory units\" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allow for arbitrary unmeasured confounding among features. We demonstrate the resulting graph can differentiate between interpretable features that causally influence model predictions versus those that are merely associated with model predictions due to confounding. Our approach is motivated by a counterfactual theory of causal explanation wherein good explanations point to factors that are \"difference-makers\" in an interventionist sense",
    "checked": false,
    "id": "6e2836fc572ef2f9d2965e64ef5ba17d7eb48d03",
    "semantic_title": "petrophysical prediction of oil reservoirs using explainable artificial intelligence",
    "citation_count": 0,
    "authors": [
      "Numair Sani",
      "Daniel Malinsky",
      "Ilya Shpitser"
    ]
  },
  "https://openreview.net/forum?id=z3JZzu9EA3": {
    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications",
    "checked": true,
    "id": "6bcd708d2e49b34f34f157daa6bf1c3e062f57c5",
    "semantic_title": "a survey on large language model acceleration based on kv cache management",
    "citation_count": 18,
    "authors": [
      "Haoyang LI",
      "Yiming Li",
      "Anxin Tian",
      "Tianhao Tang",
      "Zhanchao Xu",
      "Xuejia Chen",
      "Nicole HU",
      "Wei Dong",
      "Li Qing",
      "Lei Chen"
    ]
  },
  "https://openreview.net/forum?id=CkVlt2Qgdb": {
    "title": "Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity",
    "volume": "main",
    "abstract": "Machine learning (ML) algorithms can often exhibit discriminatory behavior, negatively affecting certain populations across protected groups. To address this, numerous debiasing methods, and consequently evaluation measures, have been proposed. Current evaluation measures for debiasing methods suffer from two main limitations: (1) they primarily provide a global estimate of unfairness, failing to provide a more fine-grained analysis, and (2) they predominantly analyze the model output on a specific task, failing to generalize the findings to other tasks. In this work, we introduce Pointwise Normalized Kernel Alignment (PNKA), a pointwise representational similarity measure that addresses these limitations by measuring how debiasing measures affect the intermediate representations of individuals. On tabular data, the use of PNKA reveals previously unknown insights: while group fairness predominantly influences a small subset of the population, maintaining high representational similarity for the majority, individual fairness constraints uniformly impact representations across the entire population, altering nearly every data point. We show that by evaluating representations using PNKA, we can reliably predict the behavior of ML models trained on these representations. Moreover, applying PNKA to language embeddings shows that existing debiasing methods may not perform as intended, failing to remove biases from stereotypical words and sentences. Our findings suggest that current evaluation measures for debiasing methods are insufficient, highlighting the need for a deeper understanding of the effects of debiasing methods, and show how pointwise representational similarity metrics can help with fairness audits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camila Kolling",
      "Till Speicher",
      "Vedant Nanda",
      "Mariya Toneva",
      "Krishna P. Gummadi"
    ]
  },
  "https://openreview.net/forum?id=ZgjhykPSdU": {
    "title": "Statistical Error Bounds for GANs with Nonlinear Objective Functionals",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) are unsupervised learning methods for training a generator distribution to produce samples that approximate those drawn from a target distribution. Many such methods can be formulated as minimization of a metric or divergence between probability distributions. Recent works have derived statistical error bounds for GANs that are based on integral probability metrics (IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. In general, IPMs are defined by optimizing a linear functional (difference of expectations) over a space of discriminators. A much larger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed using $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences) together with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz functions). These GANs have nonlinear objective functions, depending on the choice of $f$, and have been shown to exhibit improved performance in a number of applications. In this work we derive statistical error bounds for $(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of finite-sample concentration inequalities. These results prove the statistical consistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs in the appropriate limit. Our results use novel Rademacher complexity bounds which provide new insight into the performance of IPM-GANs for distributions with unbounded support and have application to statistical learning tasks beyond GANs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremiah Birrell"
    ]
  },
  "https://openreview.net/forum?id=JkMifr17wc": {
    "title": "Closed-Form Diffusion Models",
    "volume": "main",
    "abstract": "Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves competitive sampling times while running on consumer-grade CPUs",
    "checked": true,
    "id": "1242e11e676fe238157127ef2825b89c50d99d10",
    "semantic_title": "closed-form diffusion models",
    "citation_count": 11,
    "authors": [
      "Christopher Scarvelis",
      "Haitz Sáez de Ocáriz Borde",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=Oyueig10Ed": {
    "title": "Policy Optimization via Adv2: Adversarial Learning on Advantage Functions",
    "volume": "main",
    "abstract": "We revisit the reduction of learning in adversarial Markov decision processes [MDPs] to adversarial learning based on $Q$--values; this reduction has been considered in a number of recent articles as one building block to perform policy optimization. Namely, we first consider and extend this reduction in an ideal setting where an oracle provides value functions: it may involve any adversarial learning strategy (not just exponential weights) and it may be based indifferently on $Q$--values or on advantage functions. We then present two extensions: on the one hand, convergence of the last iterate for a vast class of adversarial learning strategies (again, not just exponential weights), satisfying a property called monotonicity of weights; on the other hand, stronger regret criteria for learning in MDPs, inherited from the stronger regret criteria of adversarial learning called strongly adaptive regret and tracking regret. Third, we demonstrate how adversarial learning, also referred to as aggregation of experts, relates to aggregation (orchestration) of expert policies: we obtain stronger forms of performance guarantees in this setting than existing ones, via yet another, simple reduction. Finally, we discuss the impact of the reduction of learning in adversarial MDPs to adversarial learning in the practical scenarios where transition kernels are unknown and value functions must be learned. In particular, we review the literature and note that many strategies for policy optimization feature a policy-improvement step based on exponential weights with estimated $Q$--values. Our main message is that this step may be replaced by the application of any adversarial learning strategy on estimated $Q$--values or on estimated advantage functions. We leave the empirical evaluation of these twists for future research",
    "checked": true,
    "id": "2c860df4131cb10fb94821cec993699fd2d46f50",
    "semantic_title": "policy optimization via adv2: adversarial learning on advantage functions",
    "citation_count": 2,
    "authors": [
      "Matthieu Jonckheere",
      "Chiara Mignacco",
      "Gilles Stoltz"
    ]
  },
  "https://openreview.net/forum?id=4xXJDO8Bvu": {
    "title": "Node Classification With Reject Option",
    "volume": "main",
    "abstract": "One of the key tasks in graph learning is node classification. While Graph neural networks have been used for various applications, their adaptivity to reject option settings has not been previously explored. In this paper, we propose NCwR, a novel approach to node classification in Graph Neural Networks (GNNs) with an integrated reject option. This allows the model to abstain from making predictions for samples with high uncertainty. We propose cost-based and coverage-based methods for classification with abstention in node classification settings using GNNs. We perform experiments using our method on standard citation network datasets Cora, CiteSeer, PubMed and ogbn-arxiv. We also model the Legal judgment prediction problem on the ILDC dataset as a node classification problem, where nodes represent legal cases and edges represent citations. We further interpret the model by analyzing the cases in which it abstains from predicting and visualizing which part of the input features influenced this decision",
    "checked": true,
    "id": "7be8668a9dfc33278b868b3fa4176f01e11185ae",
    "semantic_title": "node classification with reject option",
    "citation_count": 0,
    "authors": [
      "Uday Bhaskar Kuchipudi",
      "Jayadratha Gayen",
      "Charu Sharma",
      "Naresh Manwani"
    ]
  },
  "https://openreview.net/forum?id=xT8BEgXmVc": {
    "title": "Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models",
    "volume": "main",
    "abstract": "Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue across different number of agents in a centralized architecture, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Extensive results on Starcraft Multi-Agent Challenge (SMAC) and MAMujoco demonstrate superior sample efficiency and overall performance compared to strong model-free approaches and existing model-based methods",
    "checked": true,
    "id": "ba5420997282c19fa04db9cbe8d9e877ef4f73c4",
    "semantic_title": "decentralized transformers with centralized aggregation are sample-efficient multi-agent world models",
    "citation_count": 2,
    "authors": [
      "Yang Zhang",
      "Chenjia Bai",
      "Bin Zhao",
      "Junchi Yan",
      "Xiu Li",
      "Xuelong Li"
    ]
  },
  "https://openreview.net/forum?id=3cnpZ5SIjU": {
    "title": "Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse",
    "volume": "main",
    "abstract": "For a widely-studied data model and general loss and sample-hardening functions we prove that the losses of Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by representations that exhibit Neural-Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and UCL losses. In contrast to existing literature, our theoretical results for SCL do not require class-conditional independence of augmented views and work for a general loss function class that includes the widely used InfoNCE loss function. Moreover, our proofs are simpler, compact, and transparent. Similar to existing literature, our theoretical claims also hold for the practical scenario where batching is used for optimization. We empirically demonstrate, for the first time, that Adam optimization (with batching) of HSCL and HUCL losses with random initialization and suitable hardness levels can indeed converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard-negatives or feature normalization, however, the representations learned via Adam suffer from Dimensional-Collapse (DC) and fail to attain the NC-geometry. These results exemplify the role of hard-negative sampling in contrastive representation learning and we conclude with several open theoretical problems for future work. The code can be found at https://github.com/rjiang03/HCL/tree/main",
    "checked": true,
    "id": "d97fda96d01250760a192547e2dd1357f1c40d3a",
    "semantic_title": "hard-negative sampling for contrastive learning: optimal representation geometry and neural- vs dimensional-collapse",
    "citation_count": 2,
    "authors": [
      "Ruijie Jiang",
      "Thuan Nguyen",
      "Shuchin Aeron",
      "Prakash Ishwar"
    ]
  },
  "https://openreview.net/forum?id=Ckh17xN2R2": {
    "title": "Infrastructure for AI Agents",
    "volume": "main",
    "abstract": "\\textbf{AI agents} plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. To facilitate beneficial interactions and mitigate harmful ones, much research focuses on directly modifying agent behaviour. For example, developers can train agents to follow user instructions. This focus on direct modifications is useful, but insufficient. We will also need external protocols and systems that shape how agents interact with institutions and other actors. For instance, agents will need more efficient protocols to communicate with each other and form agreements. In addition, attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of \\textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents",
    "checked": true,
    "id": "9504e2f28fd2316124d45bdb216f58781e1b81b6",
    "semantic_title": "infrastructure for ai agents",
    "citation_count": 3,
    "authors": [
      "Alan Chan",
      "Kevin Wei",
      "Sihao Huang",
      "Nitarshan Rajkumar",
      "Elija Perrier",
      "Seth Lazar",
      "Gillian K Hadfield",
      "Markus Anderljung"
    ]
  },
  "https://openreview.net/forum?id=WADLPccB6o": {
    "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems",
    "volume": "main",
    "abstract": "In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems",
    "checked": true,
    "id": "15c5ada80de939768177c5d13614f2de8f4abe5b",
    "semantic_title": "conformal bounds on full-reference image quality for imaging inverse problems",
    "citation_count": 0,
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ]
  },
  "https://openreview.net/forum?id=k1eYngOvf0": {
    "title": "G-RepsNet: A Lightweight Construction of Equivariant Networks for Arbitrary Matrix Groups",
    "volume": "main",
    "abstract": "Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key insight in our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations leads to scalable equivariant networks. Further, these networks are universal approximators of functions equivariant to orthogonal groups. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as $O(5)$, $O(1, 3)$, and $O(3)$ with scalars, vectors, and second-order tensors as data types. On image classification tasks, we find that G-RepsNet using second-order representations is competitive and often even outperforms sophisticated state-of-the-art equivariant models such as GCNNs and $E(2)$-CNNs. To further illustrate the generality of our approach, we show that G-RepsNet is competitive to G-FNO and EGNN on N-body predictions and solving PDEs respectively, while being efficient",
    "checked": true,
    "id": "9f24687521c534aaa822970176d6ea7e3cd2b164",
    "semantic_title": "g-repsnet: a lightweight construction of equivariant networks for arbitrary matrix groups",
    "citation_count": 0,
    "authors": [
      "Sourya Basu",
      "Suhas Lohit",
      "Matthew Brand"
    ]
  },
  "https://openreview.net/forum?id=4uPJN6yfY1": {
    "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes",
    "volume": "main",
    "abstract": "Machine-learning from a disparate set of tables, a data lake, requires assembling features by merging and aggregating tables. Data discovery can extend autoML to data tables by automating these steps. We present an in-depth analysis of such automated table augmentation for machine learning tasks, analyzing different methods for the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. We use two data lakes: Open Data US, a well-referenced real data lake, and a novel semi-synthetic dataset, YADL (Yet Another Data Lake), which we developed as a tool for benchmarking this data discovery task. Systematic exploration on both lakes outlines 1) the importance of accurately retrieving join candidates, 2) the efficiency of simple merging methods, and 3) the resilience of tree-based learners to noisy conditions. Our experimental environment is easily reproducible and based on open data, to foster more research on feature engineering, autoML, and learning in data lakes",
    "checked": true,
    "id": "33c4cea9159564a7bc4239f183c7b0bf6ecc4fff",
    "semantic_title": "retrieve, merge, predict: augmenting tables with data lakes",
    "citation_count": 4,
    "authors": [
      "Riccardo Cappuzzo",
      "Aimee Coelho",
      "Félix Lefebvre",
      "Paolo Papotti",
      "Gaël Varoquaux"
    ]
  },
  "https://openreview.net/forum?id=55593xywWG": {
    "title": "Foundation Models Meet Federated Learning: A One-shot Feature-sharing Method with Privacy and Performance Guarantees",
    "volume": "main",
    "abstract": "Adapting foundation models for downstream tasks via Federated Learning (FL) is a promising strategy for protecting privacy while leveraging the capability of foundation models. However, FL's iterative training and model transmission result in high communication costs and GPU memory demands, making large foundation models impractical for FL. This paper introduces a one-shot FL method with a server-side performance bound to enable foundation models by reducing communication costs and GPU memory requirements. Our approach, FedPFT (FL with Parametric Feature Transfer), involves clients learning and transferring parametric models for features extracted from frozen foundation models in a single round. Parametric models are then used to generate synthetic features at the server to train a classifier head. We evaluate FedPFT across eight vision datasets using three vision foundation models. Our findings demonstrate that FedPFT is agnostic to data heterogeneity and network topology and it enhances the communication-accuracy frontier up to 7.8\\%. Finally, we show FedPFT's compatibility with differential privacy and its resilience against reconstruction attacks. Our work highlights the capability of private, feature-sharing methods for one-shot knowledge transfer using foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Beitollahi",
      "Alex Bie",
      "Sobhan Hemati",
      "Leo Maxime Brunswic",
      "Xu Li",
      "Xi Chen",
      "Guojun Zhang"
    ]
  },
  "https://openreview.net/forum?id=XHXAvACdgv": {
    "title": "NITO: Neural Implicit Fields for Resolution-free and Domain-Adaptable Topology Optimization",
    "volume": "main",
    "abstract": "Structural topology optimization plays a crucial role in engineering by determining the optimal material layout within a design space to maximize performance under given constraints. We introduce Neural Implicit Topology Optimization (NITO), a deep learning regression approach to accelerate topology optimization tasks. We demonstrate that, compared to state-of-the-art diffusion models, NITO generates structures that are under 15% as structurally sub-optimal and does so ten times faster. Furthermore, we show that NITO is entirely resolution-free and domain-agnostic, offering a more scalable solution than the current fixed-resolution and domain-specific diffusion models. To achieve this state-of-the-art performance, NITO combines three key innovations. First, we introduce the Boundary Point Order-Invariant MLP (BPOM), which represents loads and supports in a sparse and domain-agnostic manner, allowing NITO to train on variable conditioning, domain shapes, and mesh resolutions. Second, we adopt a neural implicit field representation, which allows NITO to synthesize topologies of any shape or resolution. Finally, we propose an inference-time refinement step using a few steps of gradient-based optimization to enable NITO to achieve results comparable to direct optimization methods. These three innovations empower NITO with a precision and versatility that is currently unparalleled among competing deep learning approaches for topology optimization. Code & Data: https://github.com/ahnobari/NITO_Public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Heyrani Nobari",
      "Lyle Regenwetter",
      "Giorgio Giannone",
      "Faez Ahmed"
    ]
  },
  "https://openreview.net/forum?id=tYjoHjShxF": {
    "title": "An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration",
    "volume": "main",
    "abstract": "In out-of-distribution (OOD) generalization tasks, fine-tuning pre-trained models has become a prevalent strategy. Different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training dataset size, and training strategies impact generalization and uncertainty calibration on downstream tasks. We evaluated 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 120,000 GPU hours. Our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving OOD accuracy over algorithm improvement alone. Additionally, we find that larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence, contrary to some prior studies that found modern deep networks to calibrate worse than classical shallow models. Our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration",
    "checked": true,
    "id": "37f729f59495f001cee0eff1637afdb92f1ab3e7",
    "semantic_title": "an empirical study of pre-trained model selection for out-of-distribution generalization and calibration",
    "citation_count": 3,
    "authors": [
      "Hiroki Naganuma",
      "Ryuichiro Hataya",
      "Kotaro Yoshida",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=Hy2KAldqAo": {
    "title": "Robust Offline Imitation Learning from Diverse Auxiliary Data",
    "volume": "main",
    "abstract": "Offline imitation learning enables learning a policy solely from a set of expert demonstrations, without any environment interaction. To alleviate the issue of distribution shift arising due to the small amount of expert data, recent works incorporate large numbers of auxiliary demonstrations alongside the expert data. However, the performance of these approaches rely on assumptions about the quality and composition of the auxiliary data, and they are rarely successful when those assumptions do not hold. To address this limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data (ROIDA). ROIDA first identifies high-quality transitions from the entire auxiliary dataset using a learned reward function. These high-reward samples are combined with the expert demonstrations for weighted behavioral cloning. For lower-quality samples, ROIDA applies temporal difference learning to steer the policy towards high-reward states, improving long-term returns. This two-pronged approach enables our framework to effectively leverage both high and low-quality data without any assumptions. Extensive experiments validate that ROIDA achieves robust and consistent performance across multiple auxiliary datasets with diverse ratios of expert and non-expert demonstrations. ROIDA effectively leverages unlabeled auxiliary data, outperforming prior methods reliant on specific data assumptions",
    "checked": true,
    "id": "a2fcaf0e11b80e431e2e8bbac2916c29702a4402",
    "semantic_title": "robust offline imitation learning from diverse auxiliary data",
    "citation_count": 1,
    "authors": [
      "Udita Ghosh",
      "Dripta S. Raychaudhuri",
      "Jiachen Li",
      "Konstantinos Karydis",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openreview.net/forum?id=7KkytYYhMv": {
    "title": "Rethinking the Value of Training-Free Structured Pruning of LLMs",
    "volume": "main",
    "abstract": "This paper investigates the effectiveness of training-free structured pruning techniques for Large Language Models (LLMs), with a particular focus on depth and width pruning strategies. Through an extensive empirical evaluation across a diverse range of tasks, datasets and modalities, we reveal critical limitations in current pruning methods. While some tasks exhibit minimal performance degradation, others face significant deterioration, even at low pruning rates, contradicting prior findings that often rely on selective benchmarks. Our analysis also finds that depth pruning, despite its simplicity, usually outperforms the more granular width pruning approaches in maintaining downstream task performance. Our findings highlight that existing evaluations of pruned LLMs often overstate their effectiveness due to incomplete or limited evaluation tasks, necessitating a critical reassessment of the true value of pruning and emphasizing the need to explore more robust pruning algorithms",
    "checked": true,
    "id": "b25aa26caaaf5b1a048dc12309bf1e0085e80e4e",
    "semantic_title": "rethinking the value of training-free structured pruning of llms",
    "citation_count": 0,
    "authors": [
      "Nahush Lele",
      "Arnav Chavan",
      "Aryamaan Thakur",
      "Deepak Gupta"
    ]
  },
  "https://openreview.net/forum?id=Qhfw5CUVd7": {
    "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through \\textbf{F}ine-\\textbf{G}rained \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{F}eedback (\\textbf{\\ours}), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters",
    "checked": true,
    "id": "fa84ef486184eb1c3d63949b700342bbcaf7b0c7",
    "semantic_title": "fgaif: aligning large vision-language models with fine-grained ai feedback",
    "citation_count": 17,
    "authors": [
      "Liqiang Jing",
      "Xinya Du"
    ]
  },
  "https://openreview.net/forum?id=OE4P1tW8iQ": {
    "title": "Noise-free Loss Gradients: A Surprisingly Effective Baseline for Coreset Selection",
    "volume": "main",
    "abstract": "The exponential rise in size and complexity of deep learning models and datasets have resulted in a considerable demand for computational resources. Coreset selection is one of the methods to alleviate this rising demand. The goal is to select a subset from a large dataset to train a model that performs almost at par with the one trained on the large dataset while reducing computational time and resource requirements. Existing approaches either attempt to identify remarkable samples (e.g., Forgetting, Adversarial Deepfool, EL2N, etc.) that stand out from the rest or solve complex optimization (e.g., submodular maximization, OMP) problems to compose the coresets. This paper proposes a novel and intuitive approach to efficiently select a coreset based on the similarity of loss gradients. Our method works on the hypothesis that gradients of samples belonging to a given class will point in similar directions during the early training phase. Samples with most neighbours that produce similar gradient directions, in other words, that produce noise-free gradients, will represent that class. Through extensive experimentation, we have demonstrated the effectiveness of our approach in out-performing state-of-the-art coreset selection algorithms on a range of benchmark datasets from CIFAR-10 to ImageNet with architectures of varied complexity (ResNet-18, ResNet-50, VGG-16, ViT).We have also demonstrated the effectiveness of our approach in Generative Modelling by implementing coreset selection to reduce training time for various GAN models (DCGAN, MSGAN, SAGAN, SNGAN) for different datasets (CIFAR-10, CIFAR-100, Tiny ImageNet) while not impacting the performance metrics significantly. Source code is provided at URL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saumyaranjan Mohanty",
      "Chimata Anudeep",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=BDPvuD5FTg": {
    "title": "Graph-based Confidence Calibration for Large Language Models",
    "volume": "main",
    "abstract": "Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases",
    "checked": true,
    "id": "e1536547084406d9f9864cc2dc08ca46add4a30b",
    "semantic_title": "graph-based confidence calibration for large language models",
    "citation_count": 2,
    "authors": [
      "Yukun Li",
      "Sijia Wang",
      "Lifu Huang",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=mjsoESaWDH": {
    "title": "Preferential Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "Preferential Bayesian optimization (PBO) is a framework for optimizing a decision-maker's latent preferences over available design choices. While real-world problems often involve multiple conflicting objectives, existing PBO methods assume that preferences can be encoded by a single objective function. For instance, in the customization of robotic assistive devices, technicians aim to maximize user comfort while minimizing energy consumption to extend battery life. Likewise, in autonomous driving policy design, stakeholders must evaluate safety and performance trade-offs before committing to a policy. To bridge this gap, we introduce the first framework for PBO with multiple objectives. Within this framework, we propose dueling scalarized Thompson sampling (DSTS), a multi-objective generalization of the popular dueling Thompson sampling algorithm, which may also be of independent interest beyond our setting. We evaluate DSTS across four synthetic test functions and two simulated tasks—exoskeleton personalization and driving policy design—demonstrating that it outperforms several benchmarks. Finally, we prove that DSTS is asymptotically consistent. Along the way, we provide, to our knowledge, the first convergence guarantee for dueling Thompson sampling in single-objective PBO",
    "checked": true,
    "id": "e6280a37ab1adb45fadd03e38bc00d9f0e3d0536",
    "semantic_title": "preferential multi-objective bayesian optimization",
    "citation_count": 2,
    "authors": [
      "Raul Astudillo",
      "Kejun Li",
      "Maegan Tucker",
      "Chu Xin Cheng",
      "Aaron Ames",
      "Yisong Yue"
    ]
  },
  "https://openreview.net/forum?id=xVEHiAZ7uR": {
    "title": "Adam-family Methods with Decoupled Weight Decay in Deep Learning",
    "volume": "main",
    "abstract": "In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by AdamW, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, compared to the existing results on the choices of the parameters for the moment terms in Adam, we show that our proposed framework provides more flexibility for these parameters. As a practical application of our proposed framework, we propose a novel Adam-family method named Adam with Decoupled Weight Decay (AdamD), and establish its convergence properties under mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and is comparable to AdamW, in the aspects of both generalization performance and efficiency",
    "checked": true,
    "id": "b4e7f304bfc61a4679961de80e37d1c6f53be90d",
    "semantic_title": "adam-family methods with decoupled weight decay in deep learning",
    "citation_count": 3,
    "authors": [
      "Kuangyu Ding",
      "Nachuan Xiao",
      "Kim-chuan Toh"
    ]
  },
  "https://openreview.net/forum?id=lmHh4FmPWZ": {
    "title": "Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements",
    "checked": true,
    "id": "7ebf1eb74d4dae3e9bf91e5a34e1bef1f68c0645",
    "semantic_title": "generalized compressed sensing for image reconstruction with diffusion probabilistic models",
    "citation_count": 0,
    "authors": [
      "Ling-Qi Zhang",
      "Zahra Kadkhodaie",
      "Eero P Simoncelli",
      "David H. Brainard"
    ]
  },
  "https://openreview.net/forum?id=0c6iG28rRl": {
    "title": "Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification",
    "volume": "main",
    "abstract": "Predicting simple function classes has been widely used as a testbed for developing theory and understanding of the trained Transformer's in-context learning (ICL) ability. In this paper, we revisit the training of Transformers on linear regression tasks, and different from all the existing literature, we consider a bi-objective prediction task of predicting both the conditional expectation $\\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This additional uncertainty quantification objective provides a handle to (i) better design out-of-distribution experiments to distinguish ICL from in-weight learning (IWL) and (ii) make a better separation between the algorithms with and without using the prior information of the training distribution. Theoretically, we show that the trained Transformer reaches near Bayes optimum, suggesting the usage of the information of the training distribution. Our method can be extended to other cases. Specifically, with the Transformer's context window $S$, we prove a generalization bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\min\\{S, T\\}/(n T)})$ on $n$ tasks with sequences of length $T$, providing sharper analysis compared to previous results of $\\tilde{\\mathcal{O}}(\\sqrt{1/n})$. Empirically, we illustrate that while the trained Transformer behaves as the Bayes-optimal solution as a natural consequence of supervised training in distribution, it does not necessarily perform a Bayesian inference when facing task shifts, in contrast to the \\textit{equivalence} between these two proposed in many existing literature. We also demonstrate the trained Transformer's ICL ability over covariate shift and prompt-length shift and interpret them as a generalization over a meta distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Liu",
      "Zhongze Cai",
      "Guanting Chen",
      "Xiaocheng Li"
    ]
  },
  "https://openreview.net/forum?id=qRAjZuf48S": {
    "title": "A Theoretical Study of Neural Network Expressive Power via Manifold Topology",
    "volume": "main",
    "abstract": "A prevalent assumption regarding real-world data is that it lies on or close to a low-dimensional manifold. When deploying a neural network on data manifolds, the required size, i.e., the number of neurons of the network, heavily depends on the intricacy of the underlying latent manifold. While significant advancements have been made in understanding the geometric attributes of manifolds, it's essential to recognize that topology, too, is a fundamental characteristic of manifolds. In this study, we investigate network expressive power in terms of the latent data manifold. Integrating both topological and geometric facets of the data manifold, we present a size upper bound of ReLU neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Yao",
      "Lingjie Yi",
      "Mayank Goswami",
      "Chao Chen"
    ]
  },
  "https://openreview.net/forum?id=Gl6dF9soQo": {
    "title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
    "volume": "main",
    "abstract": "Learning predictive world models is crucial for enhancing the planning capabilities of reinforcement learning (RL) agents. Recently, MuZero-style algorithms, leveraging the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains. However, these methods struggle to scale in heterogeneous scenarios with diverse dependencies and task variability. To overcome these limitations, we introduce UniZero, a novel approach that employs a transformer-based world model to effectively learn a shared latent space. By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in the latent space. We show that UniZero significantly outperforms existing baselines in benchmarks that require long-term memory. Additionally, UniZero demonstrates superior scalability in multitask learning experiments conducted on Atari benchmarks. In standard single-task RL settings, such as Atari and DMControl, UniZero matches or even surpasses the performance of current state-of-the-art methods. Finally, extensive ablation studies and visual analyses validate the effectiveness and scalability of UniZero's design choices. Our code is available at \\textcolor{magenta}{https://github.com/opendilab/LightZero}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Pu",
      "Yazhe Niu",
      "Zhenjie Yang",
      "Jiyuan Ren",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://openreview.net/forum?id=WvgoxpGpuU": {
    "title": "T2L: Efficient Zero-Shot Action Recognition with Temporal Token Learning",
    "volume": "main",
    "abstract": "Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the temporal aspects inherent to the video domain. In this study, we present Efficient Zero-Shot Action Recognition with Temporal Token Learning(T2L), a simple and efficient adaptation of CLIP that addresses these challenges. T2L leverages Temporal Token Learning (TTL) for seamless temporal adaptation, requiring no fundamental changes to the core CLIP architecture while preserving its remarkable generalization abilities. TTL relies on temporal feature diversity (TFD), a novel learning objective, which guides TTL to focus on capturing motion, thereby enhancing its learning capabilities from videos. We perform extensive experiments on nine different benchmark datasets, thoroughly evaluating T2L for zero-shot learning and base-to-novel video action recognition, and also demonstrating its potential for few-shot generalization. Impressively, with merely 5.2 million learnable parameters, T2L can be efficiently trained on a single GPU (with 25x less learnable parameters, 3x reduction in GFLOPs, and 4x improvement in throughput when compared with prior best model), outperforming existing approaches in several evaluations",
    "checked": true,
    "id": "14193486de7c29741863c2936d2b51846c3dad35",
    "semantic_title": "t2l: efficient zero-shot action recognition with temporal token learning",
    "citation_count": 0,
    "authors": [
      "Shahzad Ahmad",
      "Sukalpa Chanda",
      "Yogesh S Rawat"
    ]
  },
  "https://openreview.net/forum?id=BMGikHBjlx": {
    "title": "Ctrl-V: Higher Fidelity Autonomous Vehicle Video Generation with Bounding-Box Controlled Object Motion",
    "volume": "main",
    "abstract": "Controllable video generation has attracted significant attention, largely due to advances in video diffusion models. In domains such as autonomous driving, developing highly accurate predictions for object motions is essential. This paper addresses the key challenge of enabling fine-grained control over object motion in the context of driving video synthesis. To accomplish this, we 1) employ a distinct, specialized model to forecast the trajectories of object bounding boxes, 2) adapt and enhance a separate video diffusion network to create video content conditioned on these high-quality trajectory forecasts, and 3) we are able to exert precise control over object position/movements using bounding boxes in both 2D and 3D spaces. Our method, Ctrl-V, leverages modified and fine-tuned Stable Video Diffusion (SVD) models to solve both trajectory and video generation. Extensive experiments conducted on the KITTI, Virtual-KITTI 2, BDD100k, and nuScenes datasets validate the effectiveness of our approach in producing realistic and controllable video generation. Project page: \\url{https://oooolga.github.io/ctrl-v.github.io/}",
    "checked": true,
    "id": "b756d04e6e410aff8506677084636652ad4648fc",
    "semantic_title": "ctrl-v: higher fidelity autonomous vehicle video generation with bounding-box controlled object motion",
    "citation_count": 0,
    "authors": [
      "Ge Ya Luo",
      "ZhiHao Luo",
      "Anthony Gosselin",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=Teu1Blr2YJ": {
    "title": "Node Feature Forecasting in Temporal Graphs: an Interpretable Online Algorithm",
    "volume": "main",
    "abstract": "In this paper, we propose an online algorithm mspace for forecasting node features in temporal graphs, which captures spatial cross-correlation among different nodes as well as the temporal auto-correlation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Evaluations against various baselines, including temporal graph neural network (TGNN) models and classical Kalman filters, demonstrate that mspace performs comparably to the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent performance across datasets with varying training sizes, a notable advantage over TGNN models that require abundant training samples to effectively learn the spatiotemporal trends in the data. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoretical bounds on multi-step forecasting error of mspace and show that it scales linearly with the number of forecast steps $q$ as $\\mathcal{O}(q)$. For an asymptotically large number of nodes $n$, and timesteps $T$, the computational complexity of mspace grows linearly with both \\$n\\$ and \\$T\\$, i.e., $\\mathcal{O}(nT)$, while its space complexity remains constant $\\mathcal{O}(1)$. We compare the performance of various mspace variants against ten recent TGNN baselines and two classical baselines, ARIMA and the Kalman filter, across ten real-world datasets. Lastly, we have investigated the interpretability of different mspace variants by analyzing model parameters alongside dataset characteristics to jointly derive model-centric and data-centric insights",
    "checked": true,
    "id": "bc273c281f1df1aa246c3359d2656c2c0922b826",
    "semantic_title": "node feature forecasting in temporal graphs: an interpretable online algorithm",
    "citation_count": 0,
    "authors": [
      "Aniq Ur Rahman",
      "Justin Coon"
    ]
  },
  "https://openreview.net/forum?id=Reh1S8rxfh": {
    "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach",
    "volume": "main",
    "abstract": "In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for reasonable causal models reflecting the broad knowledge of domain experts, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge-based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. The experiments in this work have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. These experiments have also revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: https://github.com/mas-takayama/LLM-and-SCD",
    "checked": true,
    "id": "c213737923f58ad6c4cd18a8c17bca6522d7f4c6",
    "semantic_title": "integrating large language models in causal discovery: a statistical causal approach",
    "citation_count": 20,
    "authors": [
      "MASAYUKI TAKAYAMA",
      "Tadahisa OKUDA",
      "Thong Pham",
      "Tatsuyoshi Ikenoue",
      "Shingo Fukuma",
      "Shohei Shimizu",
      "Akiyoshi Sannai"
    ]
  },
  "https://openreview.net/forum?id=72YVabBErN": {
    "title": "Efficient Open Set Single Image Test Time Adaptation of Vision Language Models",
    "volume": "main",
    "abstract": "Adapting models to dynamic, real-world environments characterized by shifting data distributions and unseen test scenarios is a critical challenge in deep learning. In this paper, we consider a realistic and challenging Test-Time Adaptation setting, where a model must continuously adapt to test samples that arrive sequentially, one at a time, while distinguishing between known and unknown classes. Current Test-Time Adaptation methods operate under closed-set assumptions or batch processing, differing from the real-world open-set scenarios. We address this limitation by establishing a comprehensive benchmark for Open-set Single-image Test-Time Adaptation using Vision-Language Models. Furthermore, we propose ROSITA, a novel framework that leverages dynamically updated feature banks to identify reliable test samples and employs a contrastive learning objective to improve the separation between known and unknown classes. Our approach effectively adapts models to domain shifts for known classes while rejecting unfamiliar samples. Extensive experiments across diverse real-world benchmarks demonstrate that ROSITA sets a new state-of-the-art in open-set TTA, achieving both strong performance and computational efficiency for real-time deployment. The code is released at https://github.com/manogna-s/ROSITA.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=eQeYyup1tm": {
    "title": "Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks",
    "volume": "main",
    "abstract": "Grokking is an intriguing phenomenon of delayed generalization, where neural networks initially memorize training data with perfect accuracy but exhibit poor generalization, subsequently transitioning to a generalizing solution with continued training. While factors such as weight norms and sparsity have been proposed to explain this delayed generalization, the influence of network structure remains underexplored. In this work, we link the grokking phenomenon to the lottery ticket hypothesis to investigate the impact of internal network structures. We demonstrate that utilizing lottery tickets obtained during the generalizing phase (termed grokked tickets) significantly reduces delayed generalization across various tasks, including multiple modular arithmetic operations, polynomial regression, sparse parity, and MNIST classification. Through controlled experiments, we show that the mitigation of delayed generalization is not due solely to reduced weight norms or increased sparsity, but rather to the discovery of good subnetworks. Furthermore, we find that grokked tickets exhibit periodic weight patterns and undergo rapid structural changes that coincide with improvements in generalization. Additionally, pruning techniques like the edge-popup algorithm can identify these effective structures without modifying the weights, thereby transforming memorizing networks into generalizing ones. These results underscore the novel insight that structural exploration plays a pivotal role in understanding grokking",
    "checked": true,
    "id": "325e649380a0d6bc10f51dda6679e39a26851e68",
    "semantic_title": "bridging lottery ticket and grokking: understanding grokking from inner structure of networks",
    "citation_count": 1,
    "authors": [
      "Gouki Minegishi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://openreview.net/forum?id=6o3vVBWYis": {
    "title": "Language Models Are Good Tabular Learners",
    "volume": "main",
    "abstract": "Transformer-based language models have become the de facto standard in natural language processing. However, they underperform in the tabular data domain compared to traditional tree-based methods. We posit that current models fail to achieve the full potential of language models due to (i) heterogeneity of tabular data; and (ii) challenges faced by the model in interpreting numerical values. Based on this hypothesis, we propose the Tabular Domain Transformer (TDTransformer) framework. TDTransformer has distinct embedding processes for different types of columns. The alignment layers for different column-types transform these embeddings to a common space. Besides, TDTransformer adapts piece-wise linear encoding for numerical values for better performance. We test the proposed method on 76 real-world tabular classification datasets from the OpenML benchmark. Extensive experiments indicate that TDTransformer significantly improves the state-of-the-art methods",
    "checked": true,
    "id": "2517c035aeffdc8f5039a23f29322e890dec426c",
    "semantic_title": "language models are good tabular learners",
    "citation_count": 1,
    "authors": [
      "Zhenhan Huang",
      "Kavitha Srinivas",
      "Horst Samulowitz",
      "Niharika S. D'Souza",
      "Charu C. Aggarwal",
      "Pin-Yu Chen",
      "Jianxi Gao"
    ]
  },
  "https://openreview.net/forum?id=vc7poEYOFK": {
    "title": "Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching",
    "volume": "main",
    "abstract": "Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential (VAPO) Flow Bayes, a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VAPO learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VAPO, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks",
    "checked": true,
    "id": "ab8f30257748b2830dca3930d6a613c054de7ad9",
    "semantic_title": "learning energy-based generative models via potential flow: a variational principle approach to probability density homotopy matching",
    "citation_count": 0,
    "authors": [
      "Junn Yong Loo",
      "Leong Fang Yu",
      "Michelle Adeline",
      "Julia K. Lau",
      "Hwa Hui Tew",
      "Arghya Pal",
      "VISHNU MONN BASKARAN",
      "Chee-Ming Ting",
      "Raphael CW Phan"
    ]
  },
  "https://openreview.net/forum?id=lyxRBPmmnV": {
    "title": "Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations",
    "volume": "main",
    "abstract": "Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks",
    "checked": true,
    "id": "934152abcb942dadffb0c34aea1e55d947efc91d",
    "semantic_title": "neural slot interpreters: grounding object semantics in emergent slot representations",
    "citation_count": 1,
    "authors": [
      "Bhishma Dedhia",
      "Niraj Jha"
    ]
  },
  "https://openreview.net/forum?id=FEo55EIvGI": {
    "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective",
    "volume": "main",
    "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse compared to those trained with cross-entropy loss. Furthermore, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the impact of label smoothing on model performance and calibration. Then, leveraging the unconstrained feature model, we derive closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses. We show that models trained with label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs",
    "checked": true,
    "id": "08901f2f5b363f28660f401afafccc2e6c9373d5",
    "semantic_title": "cross entropy versus label smoothing: a neural collapse perspective",
    "citation_count": 9,
    "authors": [
      "Li Guo",
      "George Andriopoulos",
      "Zifan Zhao",
      "Zixuan Dong",
      "Shuyang Ling",
      "Keith W. Ross"
    ]
  },
  "https://openreview.net/forum?id=3Jm4dbrKGZ": {
    "title": "Lurie Networks with Robust Convergent Dynamics",
    "volume": "main",
    "abstract": "The Lurie network is a novel and unifying time-invariant neural ODE. Many existing continuous-time models, including recurrent neural networks and neural oscillators, are special cases of the Lurie network in this context. Mild constraints on the weights and biases of the Lurie network are derived to ensure a generalised concept of stability is guaranteed. This generalised stability measure is that of k-contraction which permits global convergence to a point, line or plane in the neural state-space. This includes global convergence to one of multiple equilibrium points or limit cycles as observed in many dynamical systems including associative and working memory. Weights and biases of the Lurie network, which satisfy the k-contraction constraints, are encoded through unconstrained parametrisations. The novel stability results and parametrisations provide a toolset for training over the space of k-contracting Lurie network's using standard optimisation algorithms. These results are also leveraged to construct and train a graph Lurie network satisfying the same convergence properties. Empirical results show the improvement in prediction accuracy, generalisation and robustness on a range of simulated dynamical systems, when the graph structure and k-contraction conditions are introduced. These results also compare favourably against other well known stability-constrained models and an unconstrained neural ODE",
    "checked": true,
    "id": "d7714d5e003d84b8fef1dc9734b0724c5ad7eac4",
    "semantic_title": "lurie networks with robust convergent dynamics",
    "citation_count": 0,
    "authors": [
      "Carl R Richardson",
      "Matthew C. Turner",
      "Steve R. Gunn"
    ]
  },
  "https://openreview.net/forum?id=hMPzJ3qKpf": {
    "title": "LocalFormer: Mitigating Over-Globalising in Transformers on Graphs with Localised Training",
    "volume": "main",
    "abstract": "As Transformers become more popular for graph machine learning, a significant issue has recently been observed. Their global attention mechanisms tend to overemphasize distant vertices, leading to the phenomenon of ``over-globalising.'' This phenomenon often results in the dilution of essential local information, particularly in graphs where local neighbourhoods carry significant predictive power. Existing methods often struggle with rigidity in their local processing, where tightly coupled operations limit flexibility and adaptability in diverse graph structures. Additionally, these methods can overlook critical structural nuances, resulting in an incomplete integration of local and global contexts. This paper addresses these issues by proposing LocalFormer, a novel framework, to effectively localise a transformer model by integrating a distinct local module and a complementary module that integrates global information. The local module focuses on capturing and preserving fine-grained, neighbourhood-specific patterns, ensuring that the model maintains sensitivity to critical local structures. In contrast, the complementary module dynamically integrates broader context without overshadowing the localised information, offering a balanced approach to feature aggregation across different scales of the graph. Through collaborative and warm-up training strategies, these modules work synergistically to mitigate the adverse effects of over-globalising, leading to improved empirical performance. Our experimental results demonstrate the effectiveness of LocalFormer compared to state-of-the-art baselines on vertex-classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naganand Yadati"
    ]
  },
  "https://openreview.net/forum?id=UcrVnXBdZI": {
    "title": "On the effectiveness of Rotation-Equivariance in U-Net: A Benchmark for Image Segmentation",
    "volume": "main",
    "abstract": "Numerous studies have recently focused on incorporating different variations of equivariance in Convolutional Neural Networks (CNNs). In particular, rotation-equivariance has gathered significant attention due to its relevance in many applications related to medical imaging, microscopic imaging, satellite imaging, industrial tasks, etc. While prior research has primarily focused on enhancing classification tasks with rotation equivariant CNNs, their impact on more complex architectures, such as U-Net for image segmentation, remains scarcely explored. Indeed, previous work interested in integrating rotation-equivariance into U-Net architecture have focused on solving specific applications with a limited scope. In contrast, this paper aims to provide a more exhaustive evaluation of rotation equivariant U-Net for image segmentation across a broader range of tasks. We benchmark their effectiveness against standard U-Net architectures, assessing improvements in terms of performance and sustainability (i.e., computational cost). Our evaluation focuses on datasets whose orientation of objects of interest is arbitrary in the image (e.g., Kvasir-SEG), but also on more standard segmentation datasets (such as COCO-Stuff) as to explore the wider applicability of rotation equivariance beyond tasks undoubtedly concerned by rotation equivariance. The main contribution of this work is to provide insights into the trade-offs and advantages of integrating rotation equivariance for segmentation tasks",
    "checked": true,
    "id": "898ad18bf8e4f858502177ee4465f0c6a9476720",
    "semantic_title": "on the effectiveness of rotation-equivariance in u-net: a benchmark for image segmentation",
    "citation_count": 0,
    "authors": [
      "Robin Ghyselinck",
      "Valentin Delchevalerie",
      "Bruno Dumas",
      "Benoit Frenay"
    ]
  },
  "https://openreview.net/forum?id=J6oxTJPOyN": {
    "title": "LEGO-Learn: Label-Efficient Graph Open-Set Learning",
    "volume": "main",
    "abstract": "How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to a large number of labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that addresses open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then selects highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the $C$ known ID classes and an additional class representing OOD nodes (hence, a $C+1$ classifier). This classifier utilizes a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, achieving up to a $6.62\\%$ improvement in ID classification accuracy and a $7.49\\%$ increase in AUROC for OOD detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyan Xu",
      "Kay Liu",
      "Zhengtao Yao",
      "Philip S. Yu",
      "Mengyuan Li",
      "Kaize Ding",
      "Yue Zhao"
    ]
  },
  "https://openreview.net/forum?id=VlwqIz41Hp": {
    "title": "Generalized Prediction Set with Bandit Feedback",
    "volume": "main",
    "abstract": "In high-stakes environments where uncertainties abound, set-valued prediction offers a cautious and robust mechanism by presenting multiple potential labels as the prediction for each test instance to mitigate the potential risk associated with prediction errors. Yet, integrating this paradigm with out-of-distribution (OOD) detection remains scarcely explored in such settings as online learning with bandit feedback. The bandit feedback mechanism informs the learner about the correctness of the pulled arm/action instead of the explicit ground truth label, leaving the true class label unknown when an incorrect action is taken. To address this challenge, we introduce BanditGPS which conducts set-valued prediction with OOD detection in the bandit feedback setting, using an estimation to the ground truth of class labels. BanditGPS achieves three objectives: render small/informative prediction sets, enhance the OOD detection performance, and control the recall for all normal classes to meet prescribed requirements. Our approach is characterized by the loss function, which trades off between high OOD detection and small prediction sets. Theoretically, we prove that the convergence rate of the regret is $\\tilde{\\mathcal{O}}(T^{-1/2})$. The empirical results further show that BanditGPS effectively controls the recalls with promising performances on OOD detection and informative prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://openreview.net/forum?id=mk1YIkVvTQ": {
    "title": "Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose \\emph{Concept2Concept}, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo. Warning: This paper contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salma Abdel Magid",
      "Weiwei Pan",
      "Simon Warchol",
      "Grace Guo",
      "Junsik Kim",
      "Mahia Rahman",
      "Hanspeter Pfister"
    ]
  },
  "https://openreview.net/forum?id=VuLEOyTiPO": {
    "title": "GeNIe: Generative Hard Negative Images Through Diffusion",
    "volume": "main",
    "abstract": "Data augmentation is crucial in training deep models, preventing them from overfitting to limited data. Recent advances in generative AI, e.g., diffusion models, have enabled more sophisticated augmentation techniques that produce data resembling natural images. We introduce $\\texttt{GeNIe}$ a novel augmentation method which leverages a latent diffusion model conditioned on a text prompt to combine two contrasting data points (an image from the source category and a text prompt from the target category) to generate challenging augmentations. To achieve this, we adjust the noise level (equivalently, number of diffusion iterations) to ensure the generated image retains low-level and background features from the source image while representing the target category, resulting in a hard negative sample for the source category. We further automate and enhance $\\texttt{GeNIe}$ by adaptively adjusting the noise level selection on a per image basis (coined as $\\texttt{GeNIe-Ada}$), leading to further performance improvements. Our extensive experiments, in both few-shot and long-tail distribution settings, demonstrate the effectiveness of our novel augmentation method and its superior performance over the prior art. Our code is available at https://github.com/UCDvision/GeNIe",
    "checked": true,
    "id": "7f0e374f1920d962359d7e9ccb7a7d7e2d18e6f4",
    "semantic_title": "genie: generative hard negative images through diffusion",
    "citation_count": 4,
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Anuj Singh",
      "Navaneet K L",
      "Hamed Pirsiavash",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=FFnRLvWefK": {
    "title": "System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of optimizing initial conditions and termination time in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and the state's value can not be measured in real-time but only with a delay while the measuring device processes the sample. To identify the optimal conditions in limited trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information. At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block. We further develop a two-stage BO framework to effectively incorporate search space constraints, enabling efficient optimization of both initial conditions and observation timings. We conduct extensive experiments showcasing SANODEP's potential for few-shot BO within dynamical systems. We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy",
    "checked": true,
    "id": "fcc3aee3d63369b46495f52c938226e7f6a37977",
    "semantic_title": "system-aware neural ode processes for few-shot bayesian optimization",
    "citation_count": 3,
    "authors": [
      "Jixiang Qing",
      "Rebecca D. Langdon",
      "Robert Matthew Lee",
      "Behrang Shafei",
      "Mark van der Wilk",
      "Calvin Tsay",
      "Ruth Misener"
    ]
  },
  "https://openreview.net/forum?id=x9VQFjtOPS": {
    "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks",
    "volume": "main",
    "abstract": "LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects—task definition, LLM profiling, and search procedures—making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. This survey aims to provide a comprehensive but integrated technical review on existing LIS frameworks. Specifically, we unify task definitions under Markov Decision Process (MDP) and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For ongoing paper updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Search",
    "checked": true,
    "id": "c52045988ba4b6cedabeea93ac4b03c5d7a79a85",
    "semantic_title": "a survey on llm test-time compute via search: tasks, llm profiling, search algorithms, and relevant frameworks",
    "citation_count": 6,
    "authors": [
      "Xinzhe Li"
    ]
  },
  "https://openreview.net/forum?id=bCmEP1Ltwq": {
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "volume": "main",
    "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted operations research techniques. In this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach matches or surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes",
    "checked": true,
    "id": "f0a50c531fff4a7fdd7715fe20d2ade10f438fb4",
    "semantic_title": "neural deconstruction search for vehicle routing problems",
    "citation_count": 1,
    "authors": [
      "André Hottung",
      "Paula Wong-Chung",
      "Kevin Tierney"
    ]
  },
  "https://openreview.net/forum?id=IbQTE24aZw": {
    "title": "Deflated Dynamics Value Iteration",
    "volume": "main",
    "abstract": "The Value Iteration (VI) algorithm is an iterative procedure to compute the value function of a Markov decision process, and is the basis of many reinforcement learning (RL) algorithms as well. As the error convergence rate of VI as a function of iteration $k$ is $O(\\gamma^k)$, it is slow when the discount factor $\\gamma$ is close to $1$. To accelerate the computation of the value function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses matrix splitting and matrix deflation techniques to effectively remove (deflate) the top $s$ dominant eigen-structure of the transition matrix $\\mathcal{P}^\\pi$. We prove that this leads to a $\\tilde{O}(\\gamma^k |\\lambda_{s+1}|^k)$ convergence rate, where $\\lambda_{s+1}$ is the $(s+1)$-th largest eigenvalue of the dynamics matrix. We also extend DDVI to the RL setting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We empirically show the effectiveness of the proposed algorithms",
    "checked": true,
    "id": "345355d4fa7424f81b30e10129f50de73216cdbb",
    "semantic_title": "deflated dynamics value iteration",
    "citation_count": 2,
    "authors": [
      "Jongmin Lee",
      "Amin Rakhsha",
      "Ernest K. Ryu",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=zVo6PfBa0K": {
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "volume": "main",
    "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality—a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domains, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform the current state-of-the-art methods on almost all competition-level planning tasks",
    "checked": true,
    "id": "4435bc3de88b489a7ad20374db6af7f05926d371",
    "semantic_title": "generating symbolic world models via test-time scaling of large language models",
    "citation_count": 3,
    "authors": [
      "Zhouliang Yu",
      "Yuhuan Yuan",
      "Tim Z. Xiao",
      "Fuxiang Frank Xia",
      "Jie Fu",
      "Ge Zhang",
      "Ge lin",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=JyjTJAG9yZ": {
    "title": "Personalized Layer Selection for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) combine node attributes over a fixed granularity of the local graph structure around a node to predict its label. However, different nodes may relate to a node-level property with a different granularity of its local neighborhood, and using the same level of smoothing for all nodes can be detrimental to their classification. In this work, we challenge the common fact that a single GNN layer can classify all nodes of a graph by training GNNs with a distinct personalized layer for each node. Inspired by metric learning, we propose a novel algorithm, MetSelect, to select the optimal representation layer to classify each node. In particular, we identify a prototype representation of each class in a transformed GNN layer and then, classify using the layer where the distance is smallest to a class prototype after normalizing with that layer's variance. Results on 10 datasets and 3 different GNNs show that we significantly improve the node classification accuracy of GNNs in a plug-and-play manner. We also find that using variable layers for prediction enables GNNs to be deeper and more robust to poisoning attacks. We hope this work can inspire future works to learn more adaptive and personalized graph representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Sharma",
      "Vineeth Rakesh",
      "Yingtong Dou",
      "Srijan Kumar",
      "Mahashweta Das"
    ]
  },
  "https://openreview.net/forum?id=WgJgIULL9Q": {
    "title": "HyperVQ: MLR-based Vector Quantization in Hyperbolic Space",
    "volume": "main",
    "abstract": "The success of models operating on tokenized data has heightened the need for effective tokenization methods, particularly in vision and auditory tasks where inputs are naturally continuous. A common solution is to employ Vector Quantization (VQ) within VQ Variational Autoencoders (VQVAEs), transforming inputs into discrete tokens by clustering embeddings in Euclidean space. However, Euclidean embeddings not only suffer from inefficient packing and limited separation—due to their polynomial volume growth—but are also prone to codebook collapse, where only a small subset of codebook vectors are effectively utilized. To address these limitations, we introduce HyperVQ, a novel approach that formulates VQ as a hyperbolic Multinomial Logistic Regression (MLR) problem, leveraging the exponential volume growth in hyperbolic space to mitigate collapse and improve cluster separability. Additionally, HyperVQ represents codebook vectors as geometric representatives of hyperbolic decision hyperplanes, encouraging disentangled and robust latent representations. Our experiments demonstrate that HyperVQ matches traditional VQ in generative and reconstruction tasks, while surpassing it in discriminative performance and yielding a more efficient and disentangled codebook",
    "checked": true,
    "id": "61613785fe09423e8df112ea726bb1195bc2575c",
    "semantic_title": "hypervq: mlr-based vector quantization in hyperbolic space",
    "citation_count": 4,
    "authors": [
      "Nabarun Goswami",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=1weZ9Wsajk": {
    "title": "Optimizing Cycle Life Prediction of Lithium-ion Batteries via a Physics-Informed Model",
    "volume": "main",
    "abstract": "Accurately measuring the cycle lifetime of commercial lithium-ion batteries is crucial for performance and technology development. We introduce a novel hybrid approach combining a physics-based equation with a self-attention model to predict the cycle lifetimes of commercial lithium iron phosphate graphite cells via early-cycle data. After fitting capacity loss curves to this physics-based equation, we then use a self-attention layer to reconstruct entire battery capacity loss curves. Our model exhibits comparable performances to existing models while predicting more information: the entire capacity loss curve instead of cycle life. This provides more robustness and interpretability: our model does not need to be retrained for a different notion of end-of-life and is backed by physical intuition",
    "checked": true,
    "id": "2ea1a0f132357e236e9ab3c199280eac5ba9caed",
    "semantic_title": "optimizing cycle life prediction of lithium-ion batteries via a physics-informed model",
    "citation_count": 0,
    "authors": [
      "Nathan Sun",
      "Daniel Nicolae",
      "Sara Sameer",
      "Karena Yan"
    ]
  },
  "https://openreview.net/forum?id=WOwQKguWT0": {
    "title": "When SNN meets ANN: Error-Free ANN-to-SNN Conversion for Extreme Edge Efficiency",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to convolutional neural networks (CNN), thanks to advanced ANN-to-SNN conversion techniques, all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. However, these conversion techniques incur a large number of time steps, and consequently, high spiking activity. In this paper, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the existing conversion approaches. Our framework modifies the standard integrate-and-fire (IF) neuron model used in SNNs with no change in computational complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To reduce spiking activity, we propose training the source ANN with a fine-grained $\\ell_1$ regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with low latency, low compute energy, thanks to the low time steps and high spike sparsity, and high test accuracy, for example, $75.12$% with only $4$ time steps on the ImageNet dataset. Codes will be made available. Code is available at https://github.com/godatta/SNN_meets_ANN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gourav Datta",
      "Zeyu Liu",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Peter Anthony Beerel"
    ]
  },
  "https://openreview.net/forum?id=fqSVqPcaVi": {
    "title": "ASTRA: A Scene-aware Transformer-based Model for Trajectory Prediction",
    "volume": "main",
    "abstract": "We present ASTRA (A Scene-aware Transformer-based model for trajectory prediction), a light-weight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Izzeddin Teeti",
      "Aniket Thomas",
      "Munish Monga",
      "Sachin Kumar Giroh",
      "Uddeshya Singh",
      "Andrew Bradley",
      "Biplab Banerjee",
      "Fabio Cuzzolin"
    ]
  },
  "https://openreview.net/forum?id=78N9tCL6Ly": {
    "title": "Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances",
    "checked": true,
    "id": "c7fe09b6c8f067e0ed97401fbac56242d77371ef",
    "semantic_title": "leveraging unlabeled data sharing through kernel function approximation in offline reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Yen Ru Lai",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ]
  },
  "https://openreview.net/forum?id=xu4ATNjcdy": {
    "title": "Variational Stochastic Gradient Descent for Deep Neural Networks",
    "volume": "main",
    "abstract": "Optimizing deep neural networks is one of the main tasks in successful deep learning. Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better modeling the uncertainty of the gradients. Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer. We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Kuzina",
      "Haotian Chen",
      "Babak Esmaeili",
      "Jakub M. Tomczak"
    ]
  },
  "https://openreview.net/forum?id=2e1aZZd88C": {
    "title": "Non-Myopic Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO",
    "checked": true,
    "id": "561b1857598dec8935322c0fde2da06bd240bf27",
    "semantic_title": "non-myopic multi-objective bayesian optimization",
    "citation_count": 0,
    "authors": [
      "Syrine Belakaria",
      "Alaleh Ahmadian",
      "Barbara E Engelhardt",
      "Stefano Ermon",
      "Jana Doppa"
    ]
  },
  "https://openreview.net/forum?id=aKjJoEVKgO": {
    "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
    "volume": "main",
    "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain that focuses on developing efficient and sustainable training strategies to adapt models to emerging knowledge and achieve robustness in dynamic environments. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge. Since existing works concentrate mostly on continual fine-tuning for a limited selection of downstream tasks or training domains, we introduce a new benchmark designed to measure the adaptability of LLMs to changing pretraining data landscapes. We further examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) continual pretraining consistently improves <1.5B models studied in this work and is also superior to domain adaptation, (ii) larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus, (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting, (iv) continual pretraining boosts downstream task performance of GPT-2 family, (v) continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity while randomizing training domains leads to better transfer and final performance otherwise. We posit that our research establishes a new benchmark for CL in LLMs, providing a more realistic evaluation of knowledge retention and transfer across diverse domains",
    "checked": true,
    "id": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6",
    "semantic_title": "investigating continual pretraining in large language models: insights and implications",
    "citation_count": 29,
    "authors": [
      "Çağatay Yıldız",
      "Nishaanth Kanna Ravichandran",
      "Nitin Sharma",
      "Matthias Bethge",
      "Beyza Ermis"
    ]
  },
  "https://openreview.net/forum?id=XofMHO5yVY": {
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
    "volume": "main",
    "abstract": "Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score'' — a numerical estimate of the expertise of a reviewer in reviewing a paper — and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms currently employed in computer science conferences and come up with recommendations for stakeholders. Our four main findings are: - All algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, thereby highlighting the vital need for more research on the similarity-computation problem. - Most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter2 algorithm performs best. - The classical TF-IDF algorithm which can use full texts of papers is on par with Specter2 that uses only titles and abstracts. - The performance of off-the-shelf LLMs is worse than the specialized algorithms. We encourage researchers to participate in our survey and contribute their data to the dataset here: https://forms.gle/SP1Rh8eivGz54xR37",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Stelmakh",
      "John Frederick Wieting",
      "Yang Xi",
      "Graham Neubig",
      "Nihar B Shah"
    ]
  },
  "https://openreview.net/forum?id=NUV8THrLZC": {
    "title": "Efficient Exploration in Multi-Agent Reinforcement Learning via Farsighted Self-Direction",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning faces greater challenges with efficient exploration compared to single-agent counterparts, primarily due to the exponential growth in state and action spaces. Methods based on intrinsic rewards have been proven to enhance exploration efficiency in multi-agent scenarios effectively. However, these methods are plagued by instability during training and biases in exploration direction. To address these challenges, we propose Farsighted Self-Direction (FSD), a novel model-free method that utilizes a long-term exploration bonus to achieve coordinated exploration. Since prediction error against individual Q-values indicates a potential bonus for committed exploration, it is taken into account in action selection to directly guide the coordinated exploration. Further, we also use clipped double Q-learning to reduce noise in prediction error. We validate the method on didactic examples and demonstrate the outperformance of our method on challenging StarCraft II micromanagement tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Lao",
      "Xudong Guo",
      "Mengge Liu",
      "Junjie Yu",
      "Yi Liu",
      "Wenhui Fan"
    ]
  },
  "https://openreview.net/forum?id=8otbGorZK2": {
    "title": "Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images",
    "volume": "main",
    "abstract": "Despite considerable progress in image classification tasks, classification models seem unaffected by the images that significantly deviate from those that appear natural to human eyes. Specifically, while human perception can easily identify abnormal appearances or compositions in images, classification models overlook any alterations in the arrangement of object parts as long as they are present in any order, even if unnatural. Hence, this work exposes the vulnerability of having semantic and syntactic discrepancy in images (SSDI) in the form of corruptions that remove or shuffle image patches or present images in the form of puzzles. To address this vulnerability, we propose the concept of \"image grammar\", comprising \"image semantics\" and \"image syntax\". Image semantics pertains to the interpretation of parts or patches within an image, whereas image syntax refers to the arrangement of these parts to form a coherent object. We present a semi-supervised two-stage method for learning the image grammar of visual elements and environments solely from natural images. While the first stage learns the semantic meaning of individual object parts, the second stage learns how their relative arrangement constitutes an entire object. The efficacy of the proposed approach is then demonstrated by achieving SSDI detection rates ranging from 70% to 90% on corruptions generated from CelebA and SUN-RGBD datasets. Code is publicly available at: https://github.com/ChunTao1999/SSDI/",
    "checked": true,
    "id": "e7d88f9a88db846629893e2606ca724f4847cd8d",
    "semantic_title": "semantic-syntactic discrepancy in images (ssdi): learning meaning and order of features from natural images",
    "citation_count": 1,
    "authors": [
      "Chun Tao",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=sSAp8ITBpC": {
    "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)",
    "volume": "main",
    "abstract": "Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems",
    "checked": true,
    "id": "9fa830e5c3a108f13cdb25c05a9e6107e365ad83",
    "semantic_title": "operationalizing a threat model for red-teaming large language models (llms)",
    "citation_count": 13,
    "authors": [
      "Apurv Verma",
      "Satyapriya Krishna",
      "Sebastian Gehrmann",
      "Madhavan Seshadri",
      "Anu Pradhan",
      "John A. Doucette",
      "David Rabinowitz",
      "Leslie Barrett",
      "Tom Ault",
      "Hai Phan"
    ]
  },
  "https://openreview.net/forum?id=bXUipBbZDA": {
    "title": "Reinforcement Learning from Bagged Reward",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), it is commonly assumed that an immediate reward signal is generated for each action taken by the agent, helping the agent maximize cumulative rewards to obtain the optimal policy. However, in many real-world scenarios, designing immediate reward signals is difficult; instead, agents receive a single reward that is contingent upon a partial sequence or a complete trajectory. In this work, we define this challenging problem as RL from Bagged Reward (RLBR), where sequences of data are treated as bags with non-Markovian bagged rewards, leading to the formulation of Bagged Reward Markov Decision Processes (BRMDPs). Theoretically, we demonstrate that RLBR can be addressed by solving a standard MDP with properly redistributed bagged rewards allocated to each instance within a bag. Empirically, we find that reward redistribution becomes more challenging as the bag length increases, due to reduced informational granularity. Existing reward redistribution methods are insufficient to address these challenges. Therefore, we propose a novel reward redistribution method equipped with a bidirectional attention mechanism, enabling the accurate interpretation of contextual nuances and temporal dependencies within each bag. We experimentally demonstrate that the proposed method consistently outperforms existing approaches",
    "checked": false,
    "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "semantic_title": "a survey of reinforcement learning from human feedback",
    "citation_count": 142,
    "authors": [
      "Yuting Tang",
      "Xin-Qiang Cai",
      "Yao-Xiang Ding",
      "Qiyu Wu",
      "Guoqing Liu",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=AcLlg4J52H": {
    "title": "RS-Reg: Probabilistic and Robust Certified Regression through Randomized Smoothing",
    "volume": "main",
    "abstract": "Randomized smoothing has shown promising certified robustness against adversaries in classification tasks. Despite such success with only zeroth-order access to base models, randomized smoothing has not been extended to a general form of regression. By defining robustness in regression tasks flexibly through probabilities, we demonstrate how to establish upper bounds on input data point perturbation (using the $\\ell_2$ norm) for a user-specified probability of observing valid outputs. Furthermore, we showcase the asymptotic property of a basic averaging function in scenarios where the regression model operates without any constraint. We then derive a certified upper bound of the input perturbations when dealing with a family of regression models where the outputs are bounded. Our simulations verify the validity of the theoretical results and reveal the advantages and limitations of simple smoothing functions, i.e., averaging, in regression tasks. The code is publicly available at \\url{https://github.com/arekavandi/Certified_Robust_Regression}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aref Miri Rekavandi",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ]
  },
  "https://openreview.net/forum?id=qahoztvThX": {
    "title": "A functional framework for nonsmooth autodiff with {\\it maxpooling} functions",
    "volume": "main",
    "abstract": "We make a comment on the recent work by Boustany, by showing that the Murat-TrombettiTheorem provides a simple and efficient mathematical framework for nonsmooth automatic differentiation of {\\it maxpooling} functions. In particular it gives a the chain rule formula which correctly defines the composition of Lipschitz-continuous functions which are piecewise $C^1$. The formalism is applied to four basic examples, with some tests in PyTorch. A self contained proof of an important Stampacchia formula is in the appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Després"
    ]
  },
  "https://openreview.net/forum?id=ubrOSWyTS8": {
    "title": "∇QDARTS: Quantization as an Elastic Dimension to Differentiable NAS",
    "volume": "main",
    "abstract": "Differentiable Neural Architecture Search methods efficiently find high-accuracy architectures using gradient-based optimization in a continuous domain, saving computational resources. Mixed-precision search helps optimize precision within a fixed architecture. However, applying it to a NAS-generated network does not assure optimal performance as the optimized quantized architecture may not emerge from a standalone NAS method. In light of these considerations, this paper introduces ∇QDARTS, a novel approach that combines differentiable NAS with mixed-precision search for both weight and activation. ∇QDARTS aims to identify the optimal mixed-precision neural architecture capable of achieving remarkable accuracy while operating with minimal computational requirements in a single-shot, end-to-end differentiable framework, obviating the need for pretraining and proxy methods. Compared to fp32, ∇QDARTS shows impressive performance on CIFAR10 with (2,4) bit precision, reducing bit operations by 160× with a slight 1.57% accuracy drop. Increasing the capacity enables ∇QDARTS to match fp32 accuracy while reducing bit operations by 18×. For the ImageNet dataset, with just (2,4) bit precision, ∇QDARTS outperforms state-of-the-art methods such as APQ, SPOS, OQA, and MNAS by 2.3%, 2.9%, 0.3%, and 2.7% in terms of accuracy. By incorporating (2,4,8) bit precision, ∇QDARTS further minimizes the accuracy drop to 1% compared to fp32, alongside a substantial reduction of 17× in required bit operations and 2.6× in memory footprint. In terms of bit-operation (memory footprint) ∇QDARTS excels over APQ, SPOS, OQA, and MNAS with similar accuracy by 2.3× (12×), 2.4× (3×), 13% (6.2×), 3.4× (37%), for bit-operation (memory footprint), respectively. ∇QDARTS enhances the overall search and training efficiency, achieving a 3.1× and 1.54× improvement over APQ and OQA, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Payman Behnam",
      "Uday Kamal",
      "Sanjana Vijay Ganesh",
      "Zhaoyi Li",
      "Michael Andrew Jurado",
      "Alind Khare",
      "Igor Fedorov",
      "Gaowen Liu",
      "Alexey Tumanov"
    ]
  },
  "https://openreview.net/forum?id=sTdVnDW0HX": {
    "title": "Piecewise Constant Spectral Graph Neural Network",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network(PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahan Martirosyan",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=KQzJYI6eo0": {
    "title": "Global Graph Counterfactual Explanation: A Subgraph Mapping Approach",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been widely deployed in various real-world applications. However, most GNNs are black-box models that lack explanations. One strategy to explain GNNs is through counterfactual explanation, which aims to find minimum perturbations on input graphs that change the GNN predictions. Existing works on GNN counterfactual explanations primarily concentrate on the local-level perspective (i.e., generating counterfactuals for each individual graph), which suffers from information overload and lacks insights into the broader cross-graph relationships. To address such issues, we propose GlobalGCE, a novel global-level graph counterfactual explanation method. GlobalGCE aims to identify a collection of subgraph mapping rules as counterfactual explanations for the target GNN. According to these rules, substituting certain significant subgraphs with their counterfactual subgraphs will change the GNN prediction to the desired class for most graphs (i.e., maximum coverage). Methodologically, we design a significant subgraph generator and a counterfactual subgraph autoencoder in our GlobalGCE, where the subgraphs and the rules can be effectively generated. Extensive experiments demonstrate the superiority of our GlobalGCE compared to existing baselines. Our code can be found at \\url{https://github.com/YinhanHe123/GlobalGCE}",
    "checked": true,
    "id": "745e233c29fedbf002b31a3987ce614178106eb8",
    "semantic_title": "global graph counterfactual explanation: a subgraph mapping approach",
    "citation_count": 0,
    "authors": [
      "Yinhan He",
      "Wendy Zheng",
      "Yaochen Zhu",
      "Jing Ma",
      "Saumitra Mishra",
      "Natraj Raman",
      "Ninghao Liu",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=F6l3BBPElY": {
    "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers",
    "volume": "main",
    "abstract": "This work proposes a novel setup where a neural network is trained to predict multiple steps of the reverse diffusion process in an unrolled manner, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively denoises the input during the reverse process until the final layer estimates the original input, $x_0$. Additionally, we introduce a new learning target by using latent variables, rather than the conventional approach of predicting the original input $x_0$ or source error $\\epsilon_0$. In speech synthesis, using $x_0$ or $\\epsilon_0$ often leads to large prediction errors in the early stages of the denoising process, causing distortion in the recovered speech. Our method mitigates this issue and, through extensive evaluation, demonstrates the generation of high-fidelity speech in competitive time, outperforming current state-of-the-art techniques. Moreover, the proposed approach generalizes well to unseen speech. Sample audio is available at \\url{https://onexpeters.github.io/UDPNet/}",
    "checked": true,
    "id": "47df6a7118baa441598e69e1cb5a3bd41cec97f6",
    "semantic_title": "speech synthesis by unrolling diffusion process using neural network layers",
    "citation_count": 0,
    "authors": [
      "Peter Ochieng"
    ]
  },
  "https://openreview.net/forum?id=goe6fv6iSh": {
    "title": "Gaussian Pre-Activations in Neural Networks: Myth or Reality?",
    "volume": "main",
    "abstract": "The study of feature propagation at initialization in neural networks lies at the root of numerous initialization designs. A very common assumption is that the pre-activations are Gaussian. Although this convenient *Gaussian hypothesis* can be justified when the number of neurons per layer tends to infinity, it is challenged by both theoretical and experimental work for finite-width neural networks. Our main contribution is to construct a family of pairs of activation functions and initialization distributions that ensure that the pre-activations remain Gaussian throughout the network depth, even in narrow neural networks, under the assumption that the pre-activations are independent. In the process, we discover a set of constraints that a neural network should satisfy to ensure Gaussian pre-activations. In addition, we provide a critical review of the claims of the Edge of Chaos line of work and construct a non-asymptotic Edge of Chaos analysis. We also propose a unified view on the propagation of pre-activations, encompassing the framework of several well-known initialization procedures. More generally, our work provides a principled framework for addressing the much-debated question: is it desirable to initialize the training of a neural network whose pre-activations are guaranteed to be Gaussian? Our code is available on GitHub: https://github.com/p-wol/gaussian-preact/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Wolinski",
      "Julyan Arbel"
    ]
  },
  "https://openreview.net/forum?id=akumIxQjNN": {
    "title": "ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs",
    "volume": "main",
    "abstract": "The expansion of neural network sizes and the enhanced resolution of modern image sensors result in heightened memory and power demands to process modern computer vision models. In order to deploy these models in extremely resource-constrained edge devices, it is crucial to reduce their peak memory, which is the maximum memory consumed during the execution of a model. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision, including image classification and diffusion-based image generation. For image classification, our method yields 4x-5x theoretical peak memory reduction with less degradation in accuracy for most CNN-based architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods when applied to the same student network. The code is available at https://github.com/mengtang-lab/ReDistill",
    "checked": false,
    "id": "7012a749533b2c30c3c633844008134fe73968cf",
    "semantic_title": "redistill: residual encoded distillation for peak memory reduction",
    "citation_count": 1,
    "authors": [
      "Fang Chen",
      "Gourav Datta",
      "Mujahid Al Rafi",
      "Hyeran Jeon",
      "Meng Tang"
    ]
  },
  "https://openreview.net/forum?id=9fPinz1iH2": {
    "title": "Heterophily-informed Message Passing",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learnt embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g. generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks",
    "checked": true,
    "id": "a642b5644af6305dc3c274efc7064d655c40ae84",
    "semantic_title": "heterophily-informed message passing",
    "citation_count": 0,
    "authors": [
      "Haishan Wang",
      "Arno Solin",
      "Vikas K Garg"
    ]
  },
  "https://openreview.net/forum?id=AIby9MQXbu": {
    "title": "Robust Model Selection of Gaussian Graphical Models",
    "volume": "main",
    "abstract": "In Gaussian graphical model selection, noise-corrupted samples present significant challenges. It is known that even minimal amounts of noise can obscure the underlying structure, leading to fundamental identifiability issues. A recent line of work addressing this \"robust model selection\" problem narrows its focus to tree-structured graphical models. Even within this specific class of models, exact structure recovery is shown to be impossible. However, several algorithms have been developed that are known to provably recover the underlying tree-structure up to an (unavoidable) equivalence class. In this paper, we extend these results beyond tree-structured graphs. We first characterize the equivalence class up to which general graphs can be recovered in the presence of noise. Despite the inherent ambiguity (which we prove is unavoidable), the structure that can be recovered reveals local clustering information and global connectivity patterns in the underlying model. Such information is useful in a range of real-world problems, including power grids, social networks, protein-protein interactions, and neural structures. We then propose an algorithm which provably recovers the underlying graph up to the identified ambiguity. We further provide finite sample guarantees in the high-dimensional regime for our algorithm and validate our results through numerical simulations",
    "checked": false,
    "id": "6447ffa996c21d26af441f33d06bc6773e4002c6",
    "semantic_title": "a simple method for estimating gaussian graphical models",
    "citation_count": 0,
    "authors": [
      "Abrar Zahin",
      "Rajasekhar Anguluri",
      "Lalitha Sankar",
      "Oliver Kosut",
      "Gautam Dasarathy"
    ]
  },
  "https://openreview.net/forum?id=h434zx5SX0": {
    "title": "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
    "volume": "main",
    "abstract": "Causal discovery, the task of inferring causal structure from data, has the potential to uncover mechanistic insights from biological experiments, especially those involving perturbations. However, causal discovery algorithms over larger sets of variables tend to be brittle against misspecification or when data are limited. For example, single-cell transcriptomics measures thousands of genes, but the nature of their relationships is not known, and there may be as few as tens of cells per intervention setting. To mitigate these challenges, we propose a foundation model-inspired approach: a supervised model trained on large-scale, synthetic data to predict causal graphs from summary statistics — like the outputs of classical causal discovery algorithms run over subsets of variables and other statistical hints like inverse covariance. Our approach is enabled by the observation that typical errors in the outputs of a discovery algorithm remain comparable across datasets. Theoretically, we show that the model architecture is well-specified, in the sense that it can recover a causal graph consistent with graphs over subsets. Empirically, we train the model to be robust to misspecification and distribution shift using diverse datasets. Experiments on biological and synthetic data confirm that this model generalizes well beyond its training set, runs on graphs with hundreds of variables in seconds, and can be easily adapted to different underlying data assumptions",
    "checked": true,
    "id": "a8232bcaaff260e8b61e51253bf5cedbd08cc89a",
    "semantic_title": "sample, estimate, aggregate: a recipe for causal discovery foundation models",
    "citation_count": 7,
    "authors": [
      "Menghua Wu",
      "Yujia Bao",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=IizmQoF86Y": {
    "title": "A Learning-Based Framework for Fair and Scalable Solution Generation in Kidney Exchange Problems",
    "volume": "main",
    "abstract": "Reinforcement learning and Generative Flow Networks, known as GFlowNets, present an exciting possibility for neural networks to model distributions across various data structures. In this paper, we broaden their applicability to data structures consisting of optimal solutions for a combinatorial problem. Concretely, we propose using Q-learning and various policy gradient methods, as well as GFlowNets to learn the distribution of optimal solutions for kidney exchange problems (KEPs). This could provide a useful tool for decision-making authorities, policymakers and clinicians, as it offers them multiple optimal or near-optimal solutions, and provides a complementary landscape to their traditional integer programming-based toolbox for promoting fairness and societal benefits. Our reinforcement learning-based framework trained on KEP instances provides an effective addition to computationally expensive exact approaches, notably mixed-integer programming. Our experiments thoroughly evaluate the quality of the solution sets sampled from the trained neural networks in terms of optimality, their scalability when dealing with real-sized KEP instances, and their capability to generate a diverse pool of solutions. We also cover the use of their efficient solution generation capabilities to improve fairness and simulate the evolution of the KEP pool in a dynamic setting. Our contribution is thus: 1) methodological, as it introduces a novel setting for reinforcement learning in addition to GFlowNets, 2) implementational, as it delves beyond the theory and details how to use conditional information, and 3) of practical significance, as it considers a specific combinatorial problem in the healthcare domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William St-Arnaud",
      "Margarida Carvalho",
      "Golnoosh Farnadi"
    ]
  },
  "https://openreview.net/forum?id=HRvHCd03HM": {
    "title": "Double Horizon Model-Based Policy Optimization",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long ``distribution rollout'' (DR) and a short ``training rollout'' (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akihiro Kubo",
      "Paavo Parmas",
      "Shin Ishii"
    ]
  },
  "https://openreview.net/forum?id=vPVqQmjCy8": {
    "title": "LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling",
    "volume": "main",
    "abstract": "Time series~(TS) modeling is essential in dynamic systems like weather prediction and anomaly detection. Recent studies utilize Large Language Models (LLMs) for TS modeling, leveraging their powerful pattern recognition capabilities. These methods primarily position LLMs as the predictive backbone, often omitting the mathematical modeling within traditional TS models, such as periodicity. However, disregarding the potential of LLMs also overlooks their pattern recognition capabilities. To address this gap, we introduce \\textit{LLM-TS Integrator}, a novel framework that effectively integrates the capabilities of LLMs into traditional TS modeling. Central to this integration is our \\textit{mutual information} module. The core of this \\textit{mutual information} module is a traditional TS model enhanced with LLM-derived insights for improved predictive abilities. This enhancement is achieved by maximizing the mutual information between traditional model's TS representations and LLM's textual representation counterparts, bridging the two modalities. Moreover, we recognize that samples vary in importance for two losses: traditional prediction and mutual information maximization. To address this variability, we introduce the \\textit{sample reweighting} module to improve information utilization. This module assigns dual weights to each sample: one for prediction loss and another for mutual information loss, dynamically optimizing these weights via bi-level optimization. Our method achieves state-of-the-art or comparable performance across five mainstream TS tasks, including short-term and long-term forecasting, imputation, classification, and anomaly detection. Our code is available at: \\url{https://anonymous.4open.science/r/llm_ts_anonymous-F07D/README.MD}",
    "checked": true,
    "id": "013fefa7c76d4f07cd5b40bb12553cb17be9e98c",
    "semantic_title": "llm-ts integrator: integrating llm for enhanced time series modeling",
    "citation_count": 1,
    "authors": [
      "Can Chen",
      "Gabriel L. Oliveira",
      "Hossein Sharifi-Noghabi",
      "Tristan Sylvain"
    ]
  },
  "https://openreview.net/forum?id=beqSqPgE33": {
    "title": "Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees",
    "volume": "main",
    "abstract": "Graphical models are widely used in diverse application domains to model the conditional dependencies amongst a collection of random variables. In this paper, we consider settings where the graph structure is covariate-dependent, and investigate a deep neural network-based approach to estimate it. The method allows for flexible functional dependency on the covariate, and fits the data reasonably well in the absence of a Gaussianity assumption. Theoretical results with PAC guarantees are established for the method, under assumptions commonly used in an Empirical Risk Minimization framework. The performance of the proposed method is evaluated on several synthetic data settings and benchmarked against existing approaches. The method is further illustrated on real datasets involving data from neuroscience and finance, respectively, and produces interpretable results",
    "checked": true,
    "id": "f94e7f3658b928895c03be91b72d7b0e0a24f87b",
    "semantic_title": "covariate-dependent graphical model estimation via neural networks with statistical guarantees",
    "citation_count": 0,
    "authors": [
      "Jiahe Lin",
      "Yikai Zhang",
      "George Michailidis"
    ]
  },
  "https://openreview.net/forum?id=X6IY04Akw1": {
    "title": "Generalizable and Robust Spectral Method for Multi-view Representation Learning",
    "volume": "main",
    "abstract": "Multi-view representation learning (MvRL) has garnered substantial attention in recent years, driven by the increasing demand for applications that can effectively process and analyze data from multiple sources. In this context, graph Laplacian-based MvRL methods have demonstrated remarkable success in representing multi-view data. However, these methods often struggle with generalization to new data and face challenges with scalability. Moreover, in many practical scenarios, multi-view data is contaminated by noise or outliers. In such cases, modern deep-learning-based MvRL approaches that rely on alignment or contrastive objectives present degraded performance in downstream tasks, as they may impose incorrect consistency between clear and corrupted data sources. We introduce *SpecRaGE*, a novel fusion-based framework that integrates the strengths of graph Laplacian methods with the power of deep learning to overcome these challenges. SpecRage uses neural networks to learn parametric mapping that approximates a joint diagonalization of graph Laplacians. This solution bypasses the need for alignment while enabling generalizable and scalable learning of informative and meaningful representations. Moreover, it incorporates a meta-learning fusion module that dynamically adapts to data quality, ensuring robustness against outliers and noisy views. Our extensive experiments demonstrate that SpecRaGE outperforms state-of-the-art methods, particularly in scenarios with data contamination, paving the way for more reliable and efficient multi-view learning. Our code will be made publicly available upon acceptance",
    "checked": true,
    "id": "1faa5623932d4ee1b87780d086f87aa42583e402",
    "semantic_title": "generalizable and robust spectral method for multi-view representation learning",
    "citation_count": 1,
    "authors": [
      "Amitai Yacobi",
      "Ofir Lindenbaum",
      "Uri Shaham"
    ]
  },
  "https://openreview.net/forum?id=5qo8MF3QU1": {
    "title": "Out-of-Distribution Learning with Human Feedback",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) learning often relies on strong statistical assumptions or predefined OOD data distributions, limiting its effectiveness in real-world deployment for both OOD generalization and detection, especially when human inspection is minimal. This paper introduces a novel framework for OOD learning that integrates human feedback to enhance model adaptation and reliability. Our approach leverages freely available unlabeled data in the wild, which naturally captures environmental test-time OOD distributions under both covariate and semantic shifts. To effectively utilize such data, we propose selectively acquiring human feedback to label a small subset of informative samples. These labeled samples are then used to train both a multi-class classifier and an OOD detector. By incorporating human feedback, our method significantly improves model robustness and precision in handling OOD scenarios. We provide theoretical insights by establishing generalization error bounds for our algorithm. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a significant margin. Code is publicly available at https://github.com/HaoyueBaiZJU/ood-hf",
    "checked": true,
    "id": "04bf3cb0a104edd715d7ba639822174a86af7c11",
    "semantic_title": "out-of-distribution learning with human feedback",
    "citation_count": 4,
    "authors": [
      "Haoyue Bai",
      "Xuefeng Du",
      "Katie Rainey",
      "Shibin Parameswaran",
      "Yixuan Li"
    ]
  },
  "https://openreview.net/forum?id=B9BHjTN4z6": {
    "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning",
    "volume": "main",
    "abstract": "Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. Our documentation, examples, and source code are available at [https://github.com/RLE-Foundation/RLeXplore](https://github.com/RLE-Foundation/RLeXplore)",
    "checked": true,
    "id": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
    "semantic_title": "rlexplore: accelerating research in intrinsically-motivated reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Mingqi Yuan",
      "Roger Creus Castanyer",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng",
      "Glen Berseth"
    ]
  },
  "https://openreview.net/forum?id=hiiRCXmbAz": {
    "title": "Hyperparameters in Continual Learning: A Reality Check",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungmin Cha",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=Ucpfdn66k2": {
    "title": "When Are Bias-Free ReLU Networks Effectively Linear Networks?",
    "volume": "main",
    "abstract": "We investigate the implications of removing bias in ReLU networks regarding their expressivity and learning dynamics. We first show that two-layer bias-free ReLU networks have limited expressivity: the only odd function two-layer bias-free ReLU networks can express is a linear one. We then show that, under symmetry conditions on the data, these networks have the same learning dynamics as linear networks. This enables us to give analytical time-course solutions to certain two-layer bias-free (leaky) ReLU networks outside the lazy learning regime. While deep bias-free ReLU networks are more expressive than their two-layer counterparts, they still share a number of similarities with deep linear networks. These similarities enable us to leverage insights from linear networks to understand certain ReLU networks. Overall, our results show that some properties previously established for bias-free ReLU networks arise due to equivalence to linear networks",
    "checked": true,
    "id": "929e9a89b82c95f52eb23c7b932c1969487a5238",
    "semantic_title": "when are bias-free relu networks effectively linear networks?",
    "citation_count": 0,
    "authors": [
      "Yedi Zhang",
      "Andrew M Saxe",
      "Peter E. Latham"
    ]
  },
  "https://openreview.net/forum?id=nannw4SGfS": {
    "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics",
    "volume": "main",
    "abstract": "As learned image compression (LIC) methods become increasingly computationally demanding, enhancing their training efficiency is crucial. This paper takes a step forward in accelerating the training of LIC methods by modeling the neural training dynamics. We first propose a Sensitivity-aware True and Dummy Embedding Training mechanism (STDET) that clusters LIC model parameters into few separate modes where parameters are expressed as affine transformations of reference parameters within the same mode. By further utilizing the stable intra-mode correlations throughout training and parameter sensitivities, we gradually embed non-reference parameters, reducing the number of trainable parameters. Additionally, we incorporate a Sampling-then-Moving Average (SMA) technique, interpolating sampled weights from stochastic gradient descent (SGD) training to obtain the moving average weights, ensuring smooth temporal behavior and minimizing training state variances. Overall, our method significantly reduces training space dimensions and the number of trainable parameters without sacrificing model performance, thus accelerating model convergence. We also provide a theoretical analysis on the Noisy quadratic model, showing that the proposed method achieves a lower training variance than standard SGD. Our approach offers valuable insights for further developing efficient training methods for LICs",
    "checked": true,
    "id": "6abeb9b5b6ad6c3f6ea2b5fb6a0f0b2ddff59f49",
    "semantic_title": "accelerating learned image compression through modeling neural training dynamics",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ]
  },
  "https://openreview.net/forum?id=BaRD2Nfj41": {
    "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models",
    "volume": "main",
    "abstract": "Pretraining and finetuning models has become increasingly popular in decision-making. But there are still serious impediments in Imitation Learning from Observation (ILfO) with pretrained models. This study identifies two primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB). The EKB emerges due to the pretrained models' limitations in handling novel observations, which leads to inaccurate action inference. Conversely, the DKB stems from the reliance on limited demonstration datasets, restricting the model's adaptability across diverse scenarios. We propose separate solutions to overcome each barrier and apply them to Action Inference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new algorithm, AIME-NoB, integrates online interactions and a data-driven regulariser to mitigate the EKB. Additionally, it uses a surrogate reward function to broaden the policy's supported states, addressing the DKB. Our experiments on vision-based control tasks from the DeepMind Control Suite and MetaWorld benchmarks show that AIME-NoB significantly improves sample efficiency and converged performance, presenting a robust framework for overcoming the challenges in ILfO with pretrained models. Code available at https://github.com/IcarusWizard/AIME-NoB",
    "checked": true,
    "id": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
    "semantic_title": "overcoming knowledge barriers: online imitation learning from visual observation with pretrained world models",
    "citation_count": 0,
    "authors": [
      "Xingyuan Zhang",
      "Philip Becker-Ehmck",
      "Patrick van der Smagt",
      "Maximilian Karl"
    ]
  },
  "https://openreview.net/forum?id=yGGoOVpBVP": {
    "title": "Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods",
    "volume": "main",
    "abstract": "Recently, it has been observed that when training a deep neural net with SGD, the majority of the loss landscape's curvature quickly concentrates in a tiny *top* eigenspace of the loss Hessian, which remains largely stable thereafter. Independently, it has been shown that successful magnitude pruning masks for deep neural nets emerge early in training and remain stable thereafter. In this work, we study these two phenomena jointly and show that they are connected: We develop a methodology to measure the similarity between arbitrary parameter masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap* as the most useful such metric due to its interpretability and stability. To compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M parameters --an unprecedented scale by several orders of magnitude. Our experiments reveal an *overlap* between magnitude parameter masks and top Hessian eigenspaces consistently higher than chance-level, and that this effect gets accentuated for larger network sizes. This result indicates that *top Hessian eigenvectors tend to be concentrated around larger parameters*, or equivalently, that *larger parameters tend to align with directions of larger loss curvature*. Our work provides a methodology to approximate and analyze deep learning Hessians at scale, as well as a novel insight on the structure of their eigenspace",
    "checked": true,
    "id": "9ad03b95335af5b64e86fbe666f0516711b99524",
    "semantic_title": "connecting parameter magnitudes and hessian eigenspaces at scale using sketched methods",
    "citation_count": 0,
    "authors": [
      "Andres Fernandez",
      "Frank Schneider",
      "Maren Mahsereci",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=mvbZBaqSXo": {
    "title": "Dimension reduction via score ratio matching",
    "volume": "main",
    "abstract": "Gradient-based dimension reduction decreases the cost of Bayesian inference and probabilistic modeling by identifying maximally informative (and informed) low-dimensional projections of the data and parameters, allowing high-dimensional problems to be reformulated as cheaper low-dimensional problems. A broad family of such techniques identify these projections and provide error bounds on the resulting posterior approximations, via eigendecompositions of certain diagnostic matrices. Yet these matrices require gradients or even Hessians of the log-likelihood, excluding the purely data-driven setting and many problems of simulation-based inference. We propose a framework, derived from score-matching, to extend gradient-based dimension reduction to problems where gradients are unavailable. Specifically, we formulate an objective function to directly learn the score ratio function needed to compute the diagnostic matrices, propose a tailored parameterization for the score ratio network, and introduce regularization methods that capitalize on the hypothesized low-dimensional structure. We also introduce a novel algorithm to iteratively identify the low-dimensional reduced basis vectors more accurately with limited data based on eigenvalue deflation methods. We show that our approach outperforms standard score-matching for problems with low-dimensional structure, and demonstrate its effectiveness for PDE-constrained Bayesian inverse problems and conditional generative modeling",
    "checked": true,
    "id": "b9018ae0afe7f92707f5cdcb2b95e1b831d343a7",
    "semantic_title": "dimension reduction via score ratio matching",
    "citation_count": 1,
    "authors": [
      "Ricardo Baptista",
      "Michael Brennan",
      "Youssef Marzouk"
    ]
  },
  "https://openreview.net/forum?id=eIPwJgadfZ": {
    "title": "Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic Space",
    "volume": "main",
    "abstract": "Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts. However, learning in hyperbolic spaces poses significant challenges. In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem. Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions. In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima. From extensive empirical experiments, these methods are shown to achieve better classification accuracies than the projected gradient descent approach in most of the synthetic and real two-dimensional hyperbolic embedding dataset under the one-vs-rest multiclass-classification scheme",
    "checked": true,
    "id": "56ad3d35be28e983c6fd632fd7ef92a346e7241d",
    "semantic_title": "convex relaxation for solving large-margin classifiers in hyperbolic space",
    "citation_count": 0,
    "authors": [
      "Sheng Yang",
      "Peihan Liu",
      "Cengiz Pehlevan"
    ]
  },
  "https://openreview.net/forum?id=MbF1gYfIlY": {
    "title": "Can Kernel Methods Explain How the Data Affects Neural Collapse?",
    "volume": "main",
    "abstract": "A vast amount of literature has recently focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU)",
    "checked": true,
    "id": "c45d749646b4312a58564209172192e2ff5f7a4f",
    "semantic_title": "can kernel methods explain how the data affects neural collapse?",
    "citation_count": 3,
    "authors": [
      "Vignesh Kothapalli",
      "Tom Tirer"
    ]
  },
  "https://openreview.net/forum?id=CovLQwu611": {
    "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables creation of specialized language models for diverse tasks, resulting in numerous expert modules. In many practical use cases, these expert PEFT modules are integrated into a single model that answers arbitrary queries by routing queries to different experts. However, only a few experts can be kept in GPU memory due to memory constraints. Consequently, expert modules are frequently loaded and offloaded between CPU/GPU memory or disk storage. This frequent swapping dramatically increases communication overhead, leading unacceptable latency and degrading user experience. The large size of modern PEFT modules further exacerbates this latency. For example, QLoRA experts for 65B LLaMA are 3.2GB, making swapping a major communication bottleneck, particularly in memory-constrained environments. To address these issues, we present ComPEFT (compressed PEFT), a novel method for compressing fine-tuning residuals (task vectors) of PEFT models. Reducing expert PEFT module size effectively addresses both memory and communication limitations, facilitating faster swapping and enabling a higher density of experts within a given memory footprint. ComPEFT employs sparsification and ternary quantization to reduce PEFT module size without any additional training while preserving or enhancing model performance. Extensive evaluation across T5, T0, and LLaMA-based models with 200M − 65B parameters, ComPEFT achieves compression ratios of 8x − 50x. Specifically, we show that ComPEFT improves with scale – stronger models exhibit higher compressibility and better performance. We show ComPEFT applied to LLaMA − 65B outperforms QLoRA by 4.16% on MMLU with a 26x storage size reduction. Additionally, compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare ComPEFT with other PEFT methods, and test its efficacy for compressing full finetuning residual",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Leshem Choshen",
      "Colin Raffel",
      "Mohit Bansal"
    ]
  },
  "https://openreview.net/forum?id=322PpCGAX8": {
    "title": "MaxCutBench: Revisiting and Benchmarking Graph Neural Networks for Maximum Cut",
    "volume": "main",
    "abstract": "Recently, there has been much work on designing general heuristics for graph-based, combinatorial optimization problems via the incorporation of Graph Neural Networks (GNNs) to learn distribution-specific solution structures. However, there is a lack of consistency in evaluating these heuristics in terms of the baselines and instances chosen, making it difficult to assess the relative performance of the algorithms. In this paper, we introduce \\textbf{MaxCutBench}—an open-source benchmark suite dedicated to the NP-hard Maximum Cut problem. The suite offers a unified interface for $16$ algorithms, both traditional and machine-learning-based. Using our benchmark, we conduct an in-depth analysis of the implemented algorithms on a carefully selected set of hard instances from diverse graph datasets. Our main finding is that classical local search heuristics can outperform several highly cited learning-based approaches, including S2V-DQN (Khalil et al., 2017), ECO-DQN (Barrett et al., 2020), among others, in terms of objective value, generalization, inference time, and scalability. Additionally, we find that the performance of ECO-DQN either remains the same or improves when the GNN is replaced by simple linear regression. We hope our benchmark will contribute to the efforts of the community to standardize the evaluation of learned heuristics for combinatorial optimization. Code, data, and pre-trained models are available at: \\url{https://github.com/ankurnath/MaxCut-Bench}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankur Nath",
      "Alan Kuhnle"
    ]
  },
  "https://openreview.net/forum?id=YBPbMKJbLd": {
    "title": "Future-aware Safe Active Learning of Time Varying Systems using Gaussian Processes",
    "volume": "main",
    "abstract": "Experimental exploration of high-cost systems with safety constraints, common in engineering applications, is a challenging endeavor. Data-driven models offer a promising solution, but acquiring the requisite data remains expensive and is potentially unsafe. Safe active learning techniques prove essential, enabling the learning of high-quality models with minimal expensive data points and high safety. This paper introduces a safe active learning framework tailored for time-varying systems, addressing drift, seasonal changes, and complexities due to dynamic behavior. The proposed Time-aware Integrated Mean Squared Prediction Error (T-IMSPE) method minimizes posterior variance over current and future states, optimizing information gathering also in the time domain. Empirical results highlight T-IMSPE's advantages in model quality through synthetic and real-world examples. State of the art Gaussian processes are compatible with T-IMSPE. Our theoretical contributions include a clear delineation which Gaussian process kernels, domains, and weighting measures are suitable for T-IMSPE and even beyond for its non-time aware predecessor IMSPE",
    "checked": false,
    "id": "fdc69c043851bacf57ed840d930609bdfdfe09f5",
    "semantic_title": "future aware safe active learning of time varying systems using gaussian processes",
    "citation_count": 0,
    "authors": [
      "Markus Lange-Hegermann",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=CgWkVb2lHB": {
    "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
    "volume": "main",
    "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, focusing on key aspects of visual recognition, ranging from basic color and shape to semantic understanding. We introduce a dataset, LENS, to guide VLMs to follow the examination and check its readiness. Once the model is ready, we conduct the examination. We quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have the potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Hyeon-Woo",
      "Moon Ye-Bin",
      "Wonseok Choi",
      "Lee Hyun",
      "Tae-Hyun Oh"
    ]
  },
  "https://openreview.net/forum?id=jdvnaki7ZY": {
    "title": "Jet: A Modern Transformer-Based Normalizing Flow",
    "volume": "main",
    "abstract": "In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation, and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve a much simpler architecture that matches existing normalizing flow models and improves over them when paired with pretraining. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing the research frontier by serving as building components of more powerful generative models",
    "checked": true,
    "id": "0dc3ce7fa1ad45e39367da7118ce0947e69c5a0d",
    "semantic_title": "jet: a modern transformer-based normalizing flow",
    "citation_count": 3,
    "authors": [
      "Alexander Kolesnikov",
      "André Susano Pinto",
      "Michael Tschannen"
    ]
  },
  "https://openreview.net/forum?id=j6Rm6T2lFU": {
    "title": "Deep Koopman Learning using Noisy Data",
    "volume": "main",
    "abstract": "This paper proposes a data-driven framework to learn a finite-dimensional approximation of a Koopman operator for approximating the state evolution of a dynamical system under noisy observations. To this end, our proposed solution has two main advantages. First, the proposed method only requires the measurement noise to be bounded. Second, the proposed method modifies the existing deep Koopman operator formulations by characterizing the effect of the measurement noise on the Koopman operator learning and then mitigating it by updating the tunable parameter of the observable functions of the Koopman operator, making it easy to implement. The performance of the proposed method is demonstrated on several standard benchmarks. We then compare the presented method with similar methods proposed in the latest literature on Koopman learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjian Hao",
      "Devesh Upadhyay",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=DqPCWMiMU0": {
    "title": "CoDe: Blockwise Control for Denoising Diffusion Models",
    "volume": "main",
    "abstract": "Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines}. Our code is available at https://github.com/anujinho/code",
    "checked": true,
    "id": "72a2e7333f432ba2dde838d33f249d8a251a002c",
    "semantic_title": "code: blockwise control for denoising diffusion models",
    "citation_count": 5,
    "authors": [
      "Anuj Singh",
      "Sayak Mukherjee",
      "Ahmad Beirami",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=7rqV7Cb67L": {
    "title": "Fairness-Aware Dense Subgraph Discovery",
    "volume": "main",
    "abstract": "Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions",
    "checked": true,
    "id": "9b431a2c06e2d8414def6bbb3819e3a9506f7194",
    "semantic_title": "fairness-aware dense subgraph discovery",
    "citation_count": 0,
    "authors": [
      "Emmanouil Kariotakis",
      "Nicholas D Sidiropoulos",
      "Aritra Konar"
    ]
  },
  "https://openreview.net/forum?id=jXcx2oAIbw": {
    "title": "LLM-Guided Self-Supervised Tabular Learning With Task-Specific Pre-text Tasks",
    "volume": "main",
    "abstract": "One of the most common approaches for self-supervised representation learning is defining pre-text tasks to learn data representations. Existing works determine pre-text tasks in a \"task-agnostic'' way, without considering the forthcoming downstream tasks. This offers an advantage of broad applicability across tasks, but can also lead to a mismatch between task objectives, potentially degrading performance on downstream tasks. In this paper, we introduce TST-LLM, a framework that effectively reduces this mismatch when the natural language-based description of the downstream task is given without any ground-truth labels. TST-LLM instructs the LLM to use the downstream task's description and meta-information of data to discover features relevant to the target task. These discovered features are then treated as ground-truth labels to define \"target-specific'' pre-text tasks. TST-LLM consistently outperforms contemporary baselines, such as STUNT and LFR, with win ratios of 95% and 81%, when applied to 22 benchmark tabular datasets, including binary and multi-class classification, and regression tasks",
    "checked": false,
    "id": "b39b47ea594fe2365a055b6f0ca0a5915e88c6de",
    "semantic_title": "self-influence guided data reweighting for language model pre-training",
    "citation_count": 25,
    "authors": [
      "Sungwon Han",
      "Seungeon Lee",
      "Meeyoung Cha",
      "Sercan O Arik",
      "Jinsung Yoon"
    ]
  },
  "https://openreview.net/forum?id=9aiuB3kIjd": {
    "title": "FragFormer: A Fragment-based Representation Learning Framework for Molecular Property Prediction",
    "volume": "main",
    "abstract": "Molecular representation learning is central to molecular property prediction, which is a vital component in drug discovery. Existing methods, which mainly focus on the atom-level molecular graphs, often find it challenging to directly model the relation between fragment (substructure) and function of molecules, largely due to insufficient fragment priors. In this work, we propose a molecular self-supervised learning framework \\textbf{FragFormer}, which aims to learn the representation of fragments and their contextual relationships. Given the prior that an atom can be part of multiple functional groups, we develop $k$-\\textbf{D}egree \\textbf{Ove}rlapping fragmentation (\\textbf{DOVE}), which generates overlapping fragment graph by employing the iterative line graph. Besides, DOVE can preserve the connection information during the fragmentation phase compared to non-overlapping fragmentation. In the pre-training stage, we design a \\textit{nested masked fragment prediction} objective, to capture the hierarchical nature of fragments, namely that larger fragments can encompass multiple smaller ones. Based on FragFormer, we introduce a simple yet efficient \\textit{fragment-level} interpretation method \\textbf{FragCAM} for the molecular property prediction results with greater accuracy. Moreover, thanks to the fragment modeling, our model is more capable of processing large molecule, such as peptides, and capturing the long-range interactions inside molecules. Our approach achieves state-of-the-art (SOTA) performance on eight out of eleven molecular property prediction datasets on PharmaBench. On long-range biological benchmark with peptide data, FragFormer can beat strong baselines by a clear margin, which shows the model's potential to generalize to larger molecules. Finally, we demonstrate that our model can effectively identify decisive fragments for prediction results on a real-world dataset\\footnote{Our code is available at \\url{https://github.com/wjxts/FragFormer/}}",
    "checked": true,
    "id": "67f3d24314df085802b78a62526afd6afe7332c9",
    "semantic_title": "fragformer: a fragment-based representation learning framework for molecular property prediction",
    "citation_count": 0,
    "authors": [
      "Jiaxi Wang",
      "Yaosen Min",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://openreview.net/forum?id=spqbyeGyLR": {
    "title": "When resampling/reweighting improves feature learning in imbalanced classification? A toy-model study",
    "volume": "main",
    "abstract": "A toy model of binary classification is studied with the aim of clarifying the class-wise resampling/reweighting effect on the feature learning performance under the presence of class imbalance. In the analysis, a high-dimensional limit of the input space is taken while keeping the ratio of the dataset size against the input dimension finite and the non-rigorous replica method from statistical mechanics is employed. The result shows that there exists a case in which the no resampling/reweighting situation gives the best feature learning performance irrespectively of the choice of losses or classifiers, supporting recent findings in~\\citet{kang2019decoupling,cao2019learning}. It is also revealed that the key of the result is the symmetry of the loss and the problem setting. Inspired by this, we propose a further simplified model exhibiting the same property in the multiclass setting. These clarify when the class-wise resampling/reweighting becomes effective in imbalanced classification",
    "checked": false,
    "id": "f7b392ec3a91d64f448f25a61f5d143008d5d79f",
    "semantic_title": "when resampling/reweighting improves feature learning in imbalanced classification?: a toy-model study",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Obuchi",
      "Toshiyuki Tanaka"
    ]
  },
  "https://openreview.net/forum?id=Gdf4P7sEzE": {
    "title": "HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network",
    "volume": "main",
    "abstract": "In data science, hypergraphs are natural models for data exhibiting multi-way or group relationships in contrast to graphs which only model pairwise relationships. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important multi-way or group information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix — the magnetic Laplacian — which serves as the input to our proposed hypergraph neural network. We study $\\textit{HyperMagNet}$ for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatyana Benko",
      "Martin Buck",
      "Ilya Amburg",
      "Stephen J. Young",
      "Sinan Guven Aksoy"
    ]
  },
  "https://openreview.net/forum?id=TWOTKhwU5n": {
    "title": "ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting",
    "volume": "main",
    "abstract": "Addressing the challenges of irregularity and concept drift in streaming time series is crucial for real-world predictive modelling. Previous studies in time series continual learning often propose models that require buffering long sequences, potentially restricting the responsiveness of the inference system. Moreover, these models are typically designed for regularly sampled data, an unrealistic assumption in real-world scenarios. This paper introduces ODEStream, a novel buffer-free continual learning framework that incorporates a temporal isolation layer to capture temporal dependencies within the data. Simultaneously, it leverages the capability of neural ordinary differential equations to process irregular sequences and generate a continuous data representation, enabling seamless adaptation to changing dynamics in a data streaming scenario. Our approach focuses on learning how the dynamics and distribution of historical data change over time, facilitating direct processing of streaming sequences. Evaluations on benchmark real-world datasets demonstrate that ODEStream outperforms the state-of-the-art online learning and streaming analysis baseline models, providing accurate predictions over extended periods while minimising performance degradation over time by learning how the sequence dynamics change. The implementation of ODEStream is available at: \\url{https://github.com/FtoonAbushaqra/ODEStream.git}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Futoon M. Abushaqra",
      "Hao Xue",
      "Yongli Ren",
      "Flora D. Salim"
    ]
  },
  "https://openreview.net/forum?id=n4AaKOBWbB": {
    "title": "Amphibian: A Meta-Learning Framework for Rehearsal-Free, Fast Online Continual Learning",
    "volume": "main",
    "abstract": "Online continual learning is challenging as it requires fast adaptation over a stream of data in a non-stationary environment without forgetting the knowledge acquired in the past. To address this challenge, in this paper, we introduce Amphibian - a gradient-based meta-learner that learns to scale the direction of gradient descent to achieve the desired balance between fast learning and continual learning. For this purpose, using only the current batch of data, Amphibian minimizes a meta-objective that encourages alignments of gradients among given data samples along selected basis directions in the gradient space. From this objective, it learns a diagonal scale matrix in each layer that accumulates the history of such gradient alignments. Using these scale matrices Amphibian updates the model online only in the directions having positive cumulative gradient alignments among the data observed so far. With evaluation on standard continual image classification benchmarks, we show that such meta-learned scaled gradient descent in Amphibian achieves better accuracy in online continual learning than relevant baselines while enabling fast learning with less data and few-shot knowledge transfer to new tasks. We also introduce Amphibian-$\\beta$ a unified and principled framework for analyzing and understanding the fast learning and continual learning dynamics. Additionally, with loss landscape visualizations, we show such gradient updates incur minimum loss to the old task enabling fast continual learning in Amphibian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=qvJraN50DT": {
    "title": "Sample-efficient decoding of visual stimuli from fMRI through inter-individual functional alignment",
    "volume": "main",
    "abstract": "Deep learning is leading to major advances in the realm of brain decoding from functional Magnetic Resonance Imaging (fMRI). However, the large inter-individual variability in brain characteristics has constrained most studies to train models on one participant at a time. This limitation hampers the training of deep learning models, which typically requires very large datasets. Here, we propose to boost brain decoding of videos and static images across participants by aligning brain responses of training and left-out participants. Evaluated on a retrieval task, compared to the anatomically-aligned baseline, our method halves the median rank in out-of-subject setups. It also outperforms classical within-subject approaches when fewer than 100 minutes of data is available for the tested participant. Furthermore, we show that our alignment framework handles multiple subjects, which improves accuracy upon classical single-subject approaches. Finally, we show that this method aligns neural representations in accordance with brain anatomy. Overall, this study lays the foundations for leveraging extensive neuroimaging datasets and enhancing the decoding of individual brains when a limited amount of brain-imaging data is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Thual",
      "Yohann Benchetrit",
      "Felix Geilert",
      "Jérémy Rapin",
      "Iurii Makarov",
      "Stanislas Dehaene",
      "Bertrand Thirion",
      "Hubert Banville",
      "Jean-Remi King"
    ]
  },
  "https://openreview.net/forum?id=16f7ea1N3p": {
    "title": "LLM-Select: Feature Selection with Large Language Models",
    "volume": "main",
    "abstract": "In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., ``blood pressure'') in predicting an outcome of interest (e.g., ``heart failure''), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training \\textit{but also for deciding which features to collect in the first place}. This could potentially benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel P Jeong",
      "Zachary Chase Lipton",
      "Pradeep Kumar Ravikumar"
    ]
  },
  "https://openreview.net/forum?id=MTrhFmkC45": {
    "title": "Reproducibility Study of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
    "volume": "main",
    "abstract": "This paper presents a reproducibility study and extension of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We validate the original findings using a range of open-weight models (1.5B-70B parameters), GPT-4, and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems",
    "checked": false,
    "id": "9cf6d9d629937ba5c1fc88fb1e0035b68870601a",
    "semantic_title": "reproducibility study of cooperation, competition, and maliciousness: llm-stakeholders interactive negotiation",
    "citation_count": 0,
    "authors": [
      "Jose L. Garcia",
      "Karolina Hajkova",
      "Maria Marchenko",
      "Carlos Miguel Patiño"
    ]
  },
  "https://openreview.net/forum?id=DVeFqV56Iz": {
    "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model",
    "volume": "main",
    "abstract": "This manuscript studies the unsupervised change point detection problem in time series of graphs using a decoder-only latent space model. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that bridges the observed graphs and latent representations. The prior distributions of the latent spaces are learned from the observed data as empirical Bayes to assist change point detection. Specifically, the model parameters are estimated via maximum approximate likelihood, with a Group Fused Lasso regularization imposed on the prior parameters. The augmented Lagrangian is solved via Alternating Direction Method of Multipliers, and Langevin Dynamics are recruited for posterior inference. Simulation studies show good performance of the latent space model in supporting change point detection and real data experiments yield change points that align with significant events",
    "checked": true,
    "id": "fc1b8630b6cfc1a81c3c9669015a79c9a6f331bf",
    "semantic_title": "change point detection in dynamic graphs with decoder-only latent space model",
    "citation_count": 0,
    "authors": [
      "Yik Lun Kei",
      "Jialiang Li",
      "Hangjian Li",
      "Yanzhen Chen",
      "OSCAR HERNAN MADRID PADILLA"
    ]
  },
  "https://openreview.net/forum?id=OPFnpl7KiF": {
    "title": "Design Editing for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization} (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparamters, DEMO's score is competitive with the best previously reported scores in the literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yuan",
      "Youyuan Zhang",
      "Can Chen",
      "Haolun Wu",
      "Melody Zixuan Li",
      "Jianmo Li",
      "James J. Clark",
      "Xue Liu"
    ]
  },
  "https://openreview.net/forum?id=8L3khbpUJL": {
    "title": "Referential communication in heterogeneous communities of pre-trained visual deep networks",
    "volume": "main",
    "abstract": "As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of referential communication in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evidence that it is capturing high-level semantic features of objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matéo Mahaut",
      "Roberto Dessi",
      "Francesca Franzon",
      "Marco Baroni"
    ]
  },
  "https://openreview.net/forum?id=dghM7sOudh": {
    "title": "MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory",
    "volume": "main",
    "abstract": "While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation. The project repository is publicly available at: https://github.com/amodaresi/MemLLM",
    "checked": false,
    "id": "47c8f0d7232f52f1a48e933e32309dc35ad85f49",
    "semantic_title": "memllm: finetuning llms to use an explicit read-write memory",
    "citation_count": 10,
    "authors": [
      "Ali Modarressi",
      "Abdullatif Köksal",
      "Ayyoob Imani",
      "Mohsen Fayyaz",
      "Hinrich Schuetze"
    ]
  },
  "https://openreview.net/forum?id=UdcF3JbSKb": {
    "title": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For Uncertainty",
    "volume": "main",
    "abstract": "Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in NCGPs is prohibitively expensive for large datasets, thus requiring approximations in practice. The approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. We introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for NCGPs. As we demonstrate on large-scale classification problems, our method significantly accelerates posterior inference compared to competitive baselines by trading off reduced computation for increased uncertainty",
    "checked": true,
    "id": "cbd66e7f8f2db171eb108328d1a7bd5667a8ff12",
    "semantic_title": "accelerating non-conjugate gaussian processes by trading off computation for uncertainty",
    "citation_count": 3,
    "authors": [
      "Lukas Tatzel",
      "Jonathan Wenger",
      "Frank Schneider",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=B4SyciDyIh": {
    "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction",
    "volume": "main",
    "abstract": "Knowledge graph embedding (KGE) models encode the structural information of knowledge graphs to predicting new links. Effective training of these models requires distinguishing between positive and negative samples with high precision. Although prior research has shown that improving the quality of negative samples can significantly enhance model accuracy, identifying high-quality negative samples remains a challenging problem. This paper theoretically investigates the condition under which negative samples lead to optimal KG embedding and identifies a sufficient condition for an effective negative sample distribution. Based on this theoretical foundation, we propose \\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that \\emph{generates} negative samples satisfying this condition, in contrast to conventional methods that focus on \\emph{identifying} challenging negative samples within the training data. Importantly, the simplicity of \\textsc{EMU} ensures seamless integration with existing KGE models and negative sampling methods. To evaluate its efficacy, we conducted comprehensive experiments across multiple datasets. The results consistently demonstrate significant improvements in link prediction performance across various KGE models and negative sampling methods. Notably, \\textsc{EMU} enables performance improvements comparable to those achieved by models with embedding dimension five times larger. An implementation of the method and experiments are available at \\url{https://github.com/nec-research/EMU-KG}",
    "checked": true,
    "id": "a18c375afa74642140215fc95ec9a3eeb0006668",
    "semantic_title": "optimal embedding guided negative sample generation for knowledge graph link prediction",
    "citation_count": 0,
    "authors": [
      "Makoto Takamoto",
      "Daniel Onoro Rubio",
      "Wiem Ben Rim",
      "Takashi Maruyama",
      "Bhushan Kotnis"
    ]
  },
  "https://openreview.net/forum?id=muWEt1TOyo": {
    "title": "SE3Set: Harnessing Equivariant Hypergraph Neural Networks for Molecular Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network architecture tailored for advanced molecular representation learning. Hypergraphs are not merely an extension of traditional graphs; they are pivotal for modeling high-order relationships, a capability that conventional equivariant graph-based methods lack due to their inherent limitations in representing intricate many-body interactions. To achieve this, we first construct hypergraphs by proposing a new fragmentation method that considers both chemical and three-dimensional spatial information of the molecular system. We then design SE3Set, which incorporates equivariance into the hypergraph neural network. This ensures that the learned molecular representations are invariant to spatial transformations, thereby providing robustness essential for the accurate prediction of molecular properties. SE3Set has shown performance on par with state-of-the-art (SOTA) models for small molecule datasets like QM9 and MD17. It demonstrates outstanding performance on the MD22 dataset, achieving a remarkable ~20\\% improvement in accuracy across all molecules. Furthermore, on the OE62 dataset, SE3Set outperforms all short-range models. We also conducted a detailed analysis of OE62, highlighting the prevalence of complex many-body interactions in large molecules. This exceptional performance of SE3Set across diverse molecular structures underscores its transformative potential in computational chemistry, offering a route to more accurate and physically nuanced modeling. The code of this work is available at https://github.com/Navantock/SE3Set",
    "checked": true,
    "id": "0befffb1194cc1d76e8fd93ebc32e412167282b1",
    "semantic_title": "se3set: harnessing equivariant hypergraph neural networks for molecular representation learning",
    "citation_count": 2,
    "authors": [
      "Hongfei Wu",
      "Lijun Wu",
      "Guoqing Liu",
      "Zhirong Liu",
      "Bin Shao",
      "Zun Wang"
    ]
  },
  "https://openreview.net/forum?id=l4Qnj4tHBx": {
    "title": "Oblique Bayesian Additive Regression Trees",
    "volume": "main",
    "abstract": "Current implementations of Bayesian Additive Regression Trees (BART) are based on axis-aligned decision rules that recursively partition the feature space using a single feature at a time. Several authors have demonstrated that oblique trees, whose decision rules are based on linear combinations of features, can sometimes yield better predictions than axis-aligned trees and exhibit excellent theoretical properties. We develop an oblique version of BART that leverages a data-adaptive decision rule prior that recursively partitions the feature space along random hyperplanes. Using several synthetic and real-world benchmark datasets, we systematically compared our oblique BART implementation to axis-aligned BART and other tree ensemble methods, finding that oblique BART was competitive with --- and sometimes much better than --- those methods",
    "checked": true,
    "id": "5f0bcec2e82bce511c432018dd160079608e599a",
    "semantic_title": "oblique bayesian additive regression trees",
    "citation_count": 0,
    "authors": [
      "Paul-Hieu V. Nguyen",
      "Ryan Yee",
      "Sameer Deshpande"
    ]
  },
  "https://openreview.net/forum?id=FIWHRSuoos": {
    "title": "Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift",
    "volume": "main",
    "abstract": "Estimating the test performance of a model, possibly under distribution shift, without having access to the ground-truth labels is a challenging, yet very important problem for the safe deployment of machine learning algorithms in the wild. Existing works mostly rely on information from either the outputs or the extracted features of neural networks to estimate a score that correlates with the ground-truth test accuracy. In this paper, we investigate -- both empirically and theoretically -- how the information provided by the gradients can be predictive of the ground-truth test accuracy even under distribution shifts. More specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our intuition is that these gradients should be of higher magnitude when the model generalizes poorly. We provide the theoretical insights behind our approach and the key ingredients that ensure its empirical success. Extensive experiments conducted with various architectures on diverse distribution shifts demonstrate that our method significantly outperforms current state-of-the-art approaches. The code is available at \\url{https://github.com/Renchunzi-Xie/GdScore}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RENCHUNZI XIE",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Ievgen Redko",
      "Jianfeng Zhang",
      "Bo An"
    ]
  },
  "https://openreview.net/forum?id=PJUbMDkQVY": {
    "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
    "volume": "main",
    "abstract": "Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach FLP consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical \"emergent phase\". In our experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. Further, we present FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpus with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins",
    "checked": true,
    "id": "fa2a637f6532562a9eff1f5e9fef4438aae3f28b",
    "semantic_title": "scaling laws for predicting downstream performance in llms",
    "citation_count": 9,
    "authors": [
      "Yangyi Chen",
      "Binxuan Huang",
      "Yifan Gao",
      "Zhengyang Wang",
      "Jingfeng Yang",
      "Heng Ji"
    ]
  },
  "https://openreview.net/forum?id=c7vkDg558Z": {
    "title": "EDM-TTS: Efficient Dual-Stage Masked Modeling for Alignment-Free Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Tokenized speech modeling has significantly advanced zero-shot text-to-speech (TTS) capabilities. The most de facto approach involves a dual-stage process: text-to-semantic (T2S) followed by semantic-to-acoustic (S2A) generation. Several auto-regressive (AR) and non-autoregressive (NAR) methods have been explored in literature for both the stages. While AR models achieve state-of-the-art performance, its token-by-token generation causes inference inefficiencies, while NAR methods while being more efficient, require explicit alignment for upsampling intermediate representations, which constrains the model's capability for more natural prosody. To overcome these issues, we propose an **E**fficient **D**ual-stage **M**asked **TTS** (EDM-TTS) model that employs an alignment-free masked generative approach for the T2S stage that overcomes the constrains of an explicit aligner, while retaining the efficiency of NAR methods. For the S2A stage, we introduce an innovative NAR approach using a novel Injection Conformer architecture, that effectively models the conditional dependence among different acoustic quantization levels, optimized by a masked language modeling objective, enabling zero-shot speech generation. Our evaluations demonstrated not only the superior inference efficiency of EDM-TTS, but also its state-of-the-art high-quality zero-shot speech quality, naturalness and speaker similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabarun Goswami",
      "Hanqin Wang",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=B6y12Ot0cP": {
    "title": "Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure",
    "volume": "main",
    "abstract": "Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets",
    "checked": true,
    "id": "f94c8d22f66a1306595c5aa5dfe94da89dbf65b3",
    "semantic_title": "formal verification of graph convolutional networks with uncertain node features and uncertain graph structure",
    "citation_count": 0,
    "authors": [
      "Tobias Ladner",
      "Michael Eichelbeck",
      "Matthias Althoff"
    ]
  },
  "https://openreview.net/forum?id=J5IRyTKZ9s": {
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "volume": "main",
    "abstract": "Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Łucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian Tramèr",
      "Javier Rando"
    ]
  },
  "https://openreview.net/forum?id=GaUtrgXMHe": {
    "title": "Bayesian Transferability Assessment for Spiking Neural Networks",
    "volume": "main",
    "abstract": "Brain-inspired spiking neural networks (SNNs) attract broad interest in neuromorphic computing but suffer the problem of being difficult to optimize. Concurrently, pre-trained models (PTMs) have become a foundation for developing and applying artificial intelligence. Therefore, it is expected that pre-trained SNNs can alleviate the optimization difficulty of training from scratch. However, with a lot of PTMs available in the model hubs, effectively selecting the most appropriate PTM for a given task remains a significant challenge, often necessitating exhaustive fine-tuning and grid-searching. While several solutions to this challenge have been proposed for the mainstream artificial neural network (ANNs), aimed at developing efficient methods to assess the transferability of PTMs on target tasks, the realm of SNNs remains unexplored. The currently most used transferability assessment method for ANNs predicts transferability in a Bayesian perspective. Feature maps extracted by the PTM backbone on the target task are used to calculate the maximum model evidence as the indicator of transferability. However, ANNs and SNNs differ in architecture, rendering the existing Bayesian method incompatible with SNNs. To solve this problem, this paper introduces a novel approach to using the feature maps averaged over the time domain to calculate maximum evidence. Our proposed $\\textbf{M}$aximum $\\textbf{E}$vidence method with $\\textbf{A}$veraged $\\textbf{F}$eatures (MEAF) demonstrates effectiveness for SNNs. Additionally, the current algorithm calculates maximum evidence in an iterative way. To accelerate the selection of PTMs, an approximation method is proposed to avoid iteration in the calculation of maximum evidence, significantly reducing time consumption. It is shown through experiment that the proposed MEAF method is effective for the transferability assessment of SNNs. MEAF outperforms information theory-based assessment methods such as LEEP and NCE, which can directly adapt to SNNs on neuromorphic datasets, underscoring its potential to streamline PTM selection and application in the realm of SNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqing Hao",
      "Wenhui Wang"
    ]
  },
  "https://openreview.net/forum?id=vttqWoSJIW": {
    "title": "Relative Phase Equivariant Deep Neural Systems for Physical Layer Communications",
    "volume": "main",
    "abstract": "In the era of telecommunications, the increasing demand for complex and specialized communication systems has led to a focus on improving physical layer communications. Artificial intelligence (AI) has emerged as a promising solution avenue for doing so. Deep neural receivers have already shown significant promise in improving the performance of communications systems. However, a major challenge lies in developing deep neural receivers that match the energy efficiency and speed of traditional receivers. This work investigates the incorporation of inductive biases in the physical layer using group-equivariant deep learning to improve the parameter efficiency of deep neural receivers. We do so by constructing a deep neural receiver that is equivariant with respect to the phase of arrival. We show that the inclusion of relative phase equivariance significantly reduces the error rate of deep neural receivers at similar model sizes. Thus, we show the potential of group-equivariant deep learning in the domain of physical layer communications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Mark Hoogendoorn",
      "Rob van der Mei"
    ]
  },
  "https://openreview.net/forum?id=D2PjEPGXgh": {
    "title": "Multi-Bellman operator for convergence of $Q$-learning with linear function approximation",
    "volume": "main",
    "abstract": "We investigate the convergence of $Q$-learning with linear function approximation and introduce the multi-Bellman operator, an extension of the traditional Bellman operator. By analyzing the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes a contraction, yielding stronger fixed-point guarantees compared to the original Bellman operator. Building on these insights, we propose the multi-$Q$-learning algorithm, which achieves convergence and approximates the optimal solution with arbitrary precision. This contrasts with traditional $Q$-learning, which lacks such convergence guarantees. Finally, we empirically validate our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo S. Carvalho",
      "Pedro A. Santos",
      "Francisco S. Melo"
    ]
  },
  "https://openreview.net/forum?id=laPAh2hRFC": {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
    "volume": "main",
    "abstract": "Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, an algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SmoothLLM offers improved robustness against the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Robey",
      "Eric Wong",
      "Hamed Hassani",
      "George J. Pappas"
    ]
  },
  "https://openreview.net/forum?id=haP586YomL": {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "volume": "main",
    "abstract": "Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ]
  },
  "https://openreview.net/forum?id=sNzBi8rZTy": {
    "title": "Reinforcement Learning for Causal Discovery without Acyclicity Constraints",
    "volume": "main",
    "abstract": "Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets. Our implementation is provided at https://github.com/baosws/ALIAS",
    "checked": true,
    "id": "70073cba96d7fc4073951eb8b2c0e1b1dac22bd0",
    "semantic_title": "reinforcement learning for causal discovery without acyclicity constraints",
    "citation_count": 1,
    "authors": [
      "Bao Duong",
      "Hung Le",
      "Biwei Huang",
      "Thin Nguyen"
    ]
  },
  "https://openreview.net/forum?id=TR6iUG8i6Z": {
    "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
    "volume": "main",
    "abstract": "Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishan Gurumurthy",
      "Himanshu Pal",
      "Charu Sharma"
    ]
  },
  "https://openreview.net/forum?id=z37LCgSIzI": {
    "title": "ResiDual Transformer Alignment with Spectral Decomposition",
    "volume": "main",
    "abstract": "When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performance on different data distributions while modelling an extremely interpretable and parameter-efficient transformation, as we extensively show on 70 pre-trained network-dataset combinations (7 models, 10 datasets)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Basile",
      "Valentino Maiorca",
      "Luca Bortolussi",
      "Emanuele Rodolà",
      "Francesco Locatello"
    ]
  },
  "https://openreview.net/forum?id=uKZ0R4IQaO": {
    "title": "Dynamic Pricing in the Linear Valuation Model using Shape Constraints",
    "volume": "main",
    "abstract": "We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model eliminating the need for tuning parameters commonly required by existing methods. Previous works have addressed the challenge of unknown market noise distribution $F_0$ using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the assumption that $F_0$ satisfies Lipschitz (or stronger) conditions. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\\alpha$-H\\\"older continuous for some $\\alpha \\in (0,1]$, for which we derive a regret upper bound. Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains lower empirical regret in comparison to several existing methods in the literature while offering the advantage of being tuning-parameter free",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Bracale",
      "Moulinath Banerjee",
      "Yuekai Sun",
      "Salam Turki",
      "Kevin Stoll"
    ]
  },
  "https://openreview.net/forum?id=9Xj5w4DX0t": {
    "title": "Rank Suggestion in Non-negative Matrix Factorization: Residual Sensitivity to Initial Conditions (RSIC)",
    "volume": "main",
    "abstract": "Determining the appropriate rank in Non-negative Matrix Factorization (NMF) is a critical challenge that often requires extensive parameter tuning and domain-specific knowledge. Traditional methods for rank determination focus on identifying a single optimal rank, which may not capture the complex structure inherent in real-world datasets. In this study, we introduce a novel approach called Residual Sensitivity to Intial Conditions (RSIC) that suggests potentially multiple ranks of interest by analyzing the sensitivity of the relative residuals (e.g., relative reconstruction error) to different initializations. By computing the Mean Coordinatewise Interquartile Range (MCI) of the residuals across multiple random initializations, our method identifies regions where the NMF solutions are less sensitive to initial conditions and potentially more meaningful. We evaluate RSIC on a diverse set of datasets, including single-cell gene expression data, image data, and text data, and compare it against current state-of-the-art rank determination methods. Our experiments demonstrate that RSIC effectively identifies relevant ranks consistent with the underlying structure of the data, outperforming traditional methods in scenarios where they are computationally infeasible or less accurate. This approach provides a more scalable and generalizable solution for rank determination in NMF that does not rely on domain-specific knowledge or assumptions",
    "checked": true,
    "id": "ef655348d40b0fc09f1b126af8d822476bb07f3c",
    "semantic_title": "rank suggestion in non-negative matrix factorization: residual sensitivity to initial conditions (rsic)",
    "citation_count": 0,
    "authors": [
      "Marc A. Tunnell",
      "Zachary DeBruine",
      "Erin Carrier"
    ]
  },
  "https://openreview.net/forum?id=cFmmaxkD5A": {
    "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
    "volume": "main",
    "abstract": "Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at https://github.com/Alexiland/MLO-MAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Ramtin Hosseini",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Ranak Roy Chowdhury",
      "Rajesh K. Gupta",
      "Pengtao Xie"
    ]
  },
  "https://openreview.net/forum?id=ntGPYNUF3t": {
    "title": "Latte: Latent Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, \\textit{i.e.}, FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Gengyun Jia",
      "Ziwei Liu",
      "Yuan-Fang Li",
      "Cunjian Chen",
      "Yu Qiao"
    ]
  },
  "https://openreview.net/forum?id=LJHVPWNnV6": {
    "title": "Graph Potential Field Neural Network for Massive Agents Group-wise Path Planning",
    "volume": "main",
    "abstract": "Multi-agent path planning is important in both multi-agent path finding and multi-agent reinforcement learning areas. However, continual group-wise multi-agent path planning that requires the agents to perform as a team to pursue high team scores instead of individually is less studied. To address this problem, we propose a novel graph potential field-based neural network (GPFNN), which models a valid potential field map for path planning. Our GPFNN unfolds the T-step iterative optimization of the potential field maps as a T-layer feedforward neural network. Thus, a deeper GPFNN leads to more precise potential field maps without the over-smoothing issue. A potential field map inherently provides a monotonic potential flow from any source node to the target nodes to construct the optimal path (w.r.t. the potential decay), equipping our GPFNN with an elegant planning ability. Moreover, we incorporate dynamically updated boundary conditions into our GPFNN to address group-wise multi-agent path planning that supports both static targets and dynamic moving targets. Empirically, experiments on three different-sized mazes (up to $1025 \\times 1025$ sized mazes) with up to 1,000 agents demonstrate the planning ability of our GPFNN to handle both static and dynamic moving targets. Experiments on extensive graph node classification tasks on six graph datasets (up to millions of nodes) demonstrate the learning ability of our GPFNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming Lyu",
      "Xiaowei Zhou",
      "Xingrui Yu",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=JT2KMuo2BV": {
    "title": "Rethinking Patch Dependence for Masked Autoencoders",
    "volume": "main",
    "abstract": "In this work, we examine the impact of inter-patch dependencies in the decoder of masked autoencoders (MAE) on representation learning. We decompose the decoding mechanism for masked reconstruction into self-attention between mask tokens and cross-attention between masked and visible tokens. Our findings reveal that MAE reconstructs coherent images from visible patches not through interactions between patches in the decoder but by learning a global representation within the encoder. This discovery leads us to propose a simple visual pretraining framework: cross-attention masked autoencoders (CrossMAE). This framework employs only cross-attention in the decoder to independently read out reconstructions for a small subset of masked patches from encoder outputs. This approach achieves comparable or superior performance to traditional MAE across models ranging from ViT-S to ViT-H and significantly reduces computational requirements. By its design, CrossMAE challenges the necessity of interaction between mask tokens for effective masked pretraining. Code and models are publicly available: https://crossmae.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Fu",
      "Long Lian",
      "Renhao Wang",
      "Baifeng Shi",
      "XuDong Wang",
      "Adam Yala",
      "Trevor Darrell",
      "Alexei A Efros",
      "Ken Goldberg"
    ]
  },
  "https://openreview.net/forum?id=FkKBxp0FhR": {
    "title": "A Systematic Evaluation of the Planning and Scheduling Abilities of the Reasoning Model o1",
    "volume": "main",
    "abstract": "OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive Large Language Models (LLMs)–making it a new kind of model: a Large Reasoning Model (LRM)–and be generally capable of tackling procedural reasoning tasks. We present the first comprehensive evaluation of these models on the fundamental tasks of planning and scheduling. Previous research attempted to use LLMs' expressive generation capabilities to solve these problems, but met with only limited success. We fill in the gaps in this literature by testing a larger suite of state-of-the-art LLMs on a set of large benchmarks, and then use this as a baseline to evaluate o1-preview and o1-mini. We see that while they can offer significant accuracy improvements over LLMs, this single metric is misleading and incomplete, as LRM queries demand large and unpredictable costs and take significant amounts of time to complete. We provide a case study demonstrating that, at those same price points, other methods of inference time scaling can do just as well. We also show that, contrary to OpenAI's injunctions, o1's performance can be improved further by embedding it in compound systems that separately, but complementarily, scale inference time further. Finally, while the paper is focused on o1, we provide similar evaluations of a more recent (and open-weight) LRM -- DeepSeek R1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Atharva Gundawar",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=7bIfe2I7bK": {
    "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models",
    "volume": "main",
    "abstract": "The visual world is fundamentally compositional. Visual scenes are defined by the composition of objects and their relations. Hence, it is essential for computer vision systems to reflect and exploit this compositionality to achieve robust and generalizable scene understanding. While major strides have been made toward the development of general-purpose, multimodal generative models, including both text-to-image models and multimodal vision-language models, it remains unclear whether these systems are capable of accurately generating and interpreting scenes involving the composition of multiple objects and relations. In this work, we present an evaluation of the compositional visual processing capabilities in the current generation of text-to-image (DALL-E 3) and multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5, QWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these systems to human participants. The results suggest that these systems display some ability to solve compositional and relational tasks, showing notable improvements over the previous generation of multimodal models, but with performance nevertheless well below the level of human participants, particularly for more complex scenes involving many (>5) objects and multiple relations. These results highlight the need for further progress toward compositional understanding of visual scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhao Fu",
      "Andrew Jun Lee",
      "Yixin Anna Wang",
      "Ida Momennejad",
      "Trevor Bihl",
      "Hongjing Lu",
      "Taylor Whittington Webb"
    ]
  },
  "https://openreview.net/forum?id=3jdI0aEW3k": {
    "title": "Distributed and Secure Kernel-Based Quantum Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "764cfd1d64a18a869b41942b7bc64cc518cc8799",
    "semantic_title": "distributed and secure kernel-based quantum machine learning",
    "citation_count": 0,
    "authors": [
      "Arjhun Swaminathan",
      "Mete Akgün"
    ]
  },
  "https://openreview.net/forum?id=X3gSvQjShh": {
    "title": "An Embedding is Worth a Thousand Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "614d89e5c8839291270eb8ff0ed2e9bbadf49957",
    "semantic_title": "an embedding is worth a thousand noisy labels",
    "citation_count": 0,
    "authors": [
      "Francesco Di Salvo",
      "Sebastian Doerrich",
      "Ines Rieger",
      "Christian Ledig"
    ]
  },
  "https://openreview.net/forum?id=gxUp2d4JTw": {
    "title": "LTL-Constrained Policy Optimization with Cycle Experience Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameesh Shah",
      "Cameron Voloshin",
      "Chenxi Yang",
      "Abhinav Verma",
      "Swarat Chaudhuri",
      "Sanjit A. Seshia"
    ]
  },
  "https://openreview.net/forum?id=I1gALvbRxj": {
    "title": "Bézier Flow: a Surface-wise Gradient Descent Method for Multi-objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Sannai",
      "Yasunari Hikima",
      "Ken Kobayashi",
      "Akinori Tanaka",
      "Naoki Hamada"
    ]
  },
  "https://openreview.net/forum?id=SBM9yeNZz5": {
    "title": "Maximising the Utility of Validation Sets for Imbalanced Noisy-label Meta-learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Anh Dung",
      "Cuong C. Nguyen",
      "Vasileios Belagiannis",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=sSOxuUjE2o": {
    "title": "Controlled Training Data Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teresa Yeo",
      "Andrei Atanov",
      "Harold Luc Benoit",
      "Aleksandr Alekseev",
      "Ruchira Ray",
      "Pooya Esmaeil Akhoondi",
      "Amir Zamir"
    ]
  },
  "https://openreview.net/forum?id=Okxp1W8If0": {
    "title": "(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Quang Dang",
      "Reza Babanezhad Harikandeh",
      "Sharan Vaswani"
    ]
  },
  "https://openreview.net/forum?id=nWk5OtZ7ze": {
    "title": "Quantile Activation: Correcting a failure mode of traditional ML models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Challa",
      "Sravan Danda",
      "Laurent Najman",
      "Snehanshu Saha"
    ]
  },
  "https://openreview.net/forum?id=hCyT4RsF27": {
    "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Hemant Shahane",
      "Prathosh AP",
      "Sandeep Kumar"
    ]
  },
  "https://openreview.net/forum?id=dNWaTuKV9M": {
    "title": "Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha L. Raichur",
      "Lucas Heublein",
      "Tobias Feigl",
      "Alexander Rügamer",
      "Christopher Mutschler",
      "Felix Ott"
    ]
  },
  "https://openreview.net/forum?id=s1zfBJysbI": {
    "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inkyu Shin",
      "Qihang Yu",
      "Xiaohui Shen",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ]
  },
  "https://openreview.net/forum?id=GXlsrvOGIK": {
    "title": "On Learning Representations for Tabular Data Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inwon Kang",
      "Parikshit Ram",
      "Yi Zhou",
      "Horst Samulowitz",
      "Oshani Seneviratne"
    ]
  },
  "https://openreview.net/forum?id=baZLwdphqw": {
    "title": "Stabilizing the Kumaraswamy Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Gonzalo Mateos"
    ]
  },
  "https://openreview.net/forum?id=AHTz2mTlKk": {
    "title": "Empirical Bayes Trend Filtering Through a Variational Inference Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Xie"
    ]
  },
  "https://openreview.net/forum?id=MJOKrHqiV1": {
    "title": "Multi-Output Distributional Fairness via Post-Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li",
      "Qihang Lin",
      "Ayush Ghosh",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=uJELgNGiMW": {
    "title": "Meta-Learning to Teach Semantic Prompts for Open Domain Generalization in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirsha Bose",
      "Mainak Singha",
      "Ankit Jha",
      "Souradeep Mukhopadhyay",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=nay3Kvw8BD": {
    "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Zhu",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili"
    ]
  },
  "https://openreview.net/forum?id=OTwnNBxZFB": {
    "title": "Almost Sure Convergence of Stochastic Gradient Methods under Gradient Domination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Weissmann",
      "Sara Klein",
      "Waïss Azizian",
      "Leif Döring"
    ]
  },
  "https://openreview.net/forum?id=WfAvMdwiE8": {
    "title": "Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot Class-Incremental Tuning of Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuvendu Roy",
      "Elham Dolatabadi",
      "Arash Afkanpour",
      "Ali Etemad"
    ]
  },
  "https://openreview.net/forum?id=heeJqQXKg7": {
    "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy Dj Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=M62P7iOT7d": {
    "title": "DeformTime: capturing variable dependencies with deformable attention for time series forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Shu",
      "Vasileios Lampos"
    ]
  },
  "https://openreview.net/forum?id=9kFlOyLwyf": {
    "title": "Latent Covariate Shift: Unlocking Partial Identifiability for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Anton van den Hengel",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=Wj8yFjIpom": {
    "title": "$f$-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=vQDKYYuqWA": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chen",
      "Oier Mees",
      "Aviral Kumar",
      "Sergey Levine"
    ]
  },
  "https://openreview.net/forum?id=pKilnjQsb0": {
    "title": "Implicit Bias and Fast Convergence Rates for Self-attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavya Vasudeva",
      "Puneesh Deora",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=sXq3Wb3vef": {
    "title": "Decomposing The Dark Matter of Sparse Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Engels",
      "Logan Riggs Smith",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=Mae23iEqPS": {
    "title": "Predicting sub-population specific viral evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxian Shi",
      "Menghua Wu",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=SB7JzhDG45": {
    "title": "Simulation-based Bayesian Inference from Privacy Protected Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xiong",
      "Nianqiao Ju",
      "Sanguo Zhang"
    ]
  },
  "https://openreview.net/forum?id=0AOUWC4ss8": {
    "title": "Illustrated Landmark Graphs for Long-horizon Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Watson",
      "Arjun Krishna",
      "Rajeev Alur",
      "Dinesh Jayaraman"
    ]
  },
  "https://openreview.net/forum?id=Rwf31BYTAU": {
    "title": "Adaptive Incentive Design for Markov Decision Processes with Unknown Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Ma",
      "Shuo Han",
      "Ahmed Hemida",
      "Charles A kamhoua",
      "Jie Fu"
    ]
  },
  "https://openreview.net/forum?id=tUnyInYbjK": {
    "title": "Influence Learning in Complex Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Congeduti",
      "Roberto Rocchetta",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=t1utIThKHD": {
    "title": "An Information Theoretic Approach to Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Foster",
      "Kyle Fogarty",
      "Stefan Schoepf",
      "Zack Dugue",
      "Cengiz Oztireli",
      "Alexandra Brintrup"
    ]
  },
  "https://openreview.net/forum?id=JhYbGiFn3Y": {
    "title": "Emergent representations in networks trained with the Forward-Forward algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niccolo Tosato",
      "Lorenzo Basile",
      "Emanuele Ballarin",
      "Giuseppe De Alteriis",
      "Alberto Cazzaniga",
      "Alessio ansuini"
    ]
  },
  "https://openreview.net/forum?id=ZfPbCFZQbx": {
    "title": "Robust Symbolic Regression for Dynamical System Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramzi Dakhmouche",
      "Ivan Lunati",
      "Hossein Gorji"
    ]
  },
  "https://openreview.net/forum?id=0yPWtbR3MC": {
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niccolò Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "A. Cristiano I. Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ]
  },
  "https://openreview.net/forum?id=gangoPXSRw": {
    "title": "Probabilistic neural operators for functional uncertainty quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bülte",
      "Philipp Scholl",
      "Gitta Kutyniok"
    ]
  },
  "https://openreview.net/forum?id=A6tOXkkE4Z": {
    "title": "Decision-Focused Surrogate Modeling for Mixed-Integer Linear Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivi Dixit",
      "Rishabh Gupta",
      "Qi Zhang"
    ]
  },
  "https://openreview.net/forum?id=4ZJjr9YbBw": {
    "title": "A Vector Bernstein Inequality for Self-Normalized Martingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingvar Ziemann"
    ]
  },
  "https://openreview.net/forum?id=Cw2xlg0e46": {
    "title": "Long-context LLMs Struggle with Long In-context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Li",
      "Ge Zhang",
      "Quy Duc Do",
      "Xiang Yue",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=d9htascfP8": {
    "title": "Meta-learning Population-based Methods for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Hog",
      "Raghu Rajan",
      "André Biedenkapp",
      "Noor Awad",
      "Frank Hutter",
      "Vu Nguyen"
    ]
  },
  "https://openreview.net/forum?id=kd6CfmdPfX": {
    "title": "Posterior Sampling for Reinforcement Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Robert",
      "Aldo A. Faisal",
      "Ciara Pike-Burke"
    ]
  },
  "https://openreview.net/forum?id=wPHVijYksq": {
    "title": "A limitation on black-box dynamics approaches to Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brieuc Pinon",
      "Raphael Jungers",
      "Jean-Charles Delvenne"
    ]
  },
  "https://openreview.net/forum?id=w4nd5695sq": {
    "title": "Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stevens",
      "Emily Wenger",
      "Cathy Yuanchen Li",
      "Niklas Nolte",
      "Eshika Saxena",
      "Francois Charton",
      "Kristin E. Lauter"
    ]
  },
  "https://openreview.net/forum?id=xBbj46Y2fN": {
    "title": "What's Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aida Mohammadshahi",
      "Yani Ioannou"
    ]
  },
  "https://openreview.net/forum?id=CeNNIQ8GJf": {
    "title": "Efficient Multi-Agent Cooperation Learning through Teammate Lookahead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Chen",
      "Xinwei Chen",
      "Rong-Jun Qin",
      "Cong Guan",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://openreview.net/forum?id=DcIW0idrg8": {
    "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Ahmet Iscen",
      "Eunchan Jo",
      "Sua Choi",
      "Minsu Cho",
      "Cordelia Schmid"
    ]
  },
  "https://openreview.net/forum?id=HJbcwRbMQQ": {
    "title": "Efficient Training of Multi-task Neural Solver for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenguang Wang",
      "Zhang-Hua Fu",
      "Pinyan Lu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=B1q9po4LPl": {
    "title": "Uncovering Strong Lottery Tickets in Graph Transformers: A Path to Memory Efficient and Robust Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Ito",
      "Jiale Yan",
      "Hikari Otsuka",
      "Kazushi Kawamura",
      "Masato Motomura",
      "Thiem Van Chu",
      "Daichi Fujiki"
    ]
  },
  "https://openreview.net/forum?id=MKCwO34oIq": {
    "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Jiang",
      "Negar Hassanpour",
      "Mohammad Salameh",
      "Mohan Sai Singamsetti",
      "Fengyu Sun",
      "Wei Lu",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=4zGPT0ZwnU": {
    "title": "Theoretical Insights into Overparameterized Models in Multi-Task and Replay-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadamin Banayeeanzade",
      "Mahdi Soltanolkotabi",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=a6WthNFhL2": {
    "title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongyoon Kim",
      "Minchan Jeong",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Sumyeong Ahn",
      "Se-Young Yun"
    ]
  },
  "https://openreview.net/forum?id=G1p0YwrX8X": {
    "title": "Sparsified State-Space Models are Efficient Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woomin Song",
      "Jihoon Tack",
      "Sangwoo Mo",
      "Seunghyuk Oh",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=BvKYsaOVEn": {
    "title": "Removing Structured Noise using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tristan Stevens",
      "Hans van Gorp",
      "Faik C Meral",
      "Junseob Shin",
      "Jason Yu",
      "Jean-luc Robert",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=UaaT2fI9DC": {
    "title": "On Using Certified Training towards Empirical Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro De Palma",
      "Serge Durand",
      "Zakaria Chihani",
      "François Terrier",
      "Caterina Urban"
    ]
  },
  "https://openreview.net/forum?id=Nu6N69i8SB": {
    "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixin Liang",
      "LILI YU",
      "Liang Luo",
      "Srini Iyer",
      "Ning Dong",
      "Chunting Zhou",
      "Gargi Ghosh",
      "Mike Lewis",
      "Wen-tau Yih",
      "Luke Zettlemoyer",
      "Xi Victoria Lin"
    ]
  },
  "https://openreview.net/forum?id=nMCJ8bFq4B": {
    "title": "Multiplayer Information Asymmetric Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chang",
      "Yuanhao Lu"
    ]
  },
  "https://openreview.net/forum?id=pqZ6nOm3WF": {
    "title": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo-Line-Search Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Tsukada",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=OGCuDFab4b": {
    "title": "Daphne: Multi-Pass Compilation of Probabilistic Programs into Graphical Models and Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Dietrich Weilbach",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=ELtNtkGXoK": {
    "title": "Cluster Tree for Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Kushnir",
      "Sandeep Silwal"
    ]
  },
  "https://openreview.net/forum?id=UWNa9Pv6qA": {
    "title": "Neuron-based explanations of neural networks sacrifice completeness and interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nolan Simran Dey",
      "Eric Taylor",
      "Alexander Wong",
      "Bryan P. Tripp",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=vRYt8QLKqK": {
    "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein",
      "Azarm Nowzad",
      "Fikret Sivrikaya",
      "Sahin Albayrak"
    ]
  },
  "https://openreview.net/forum?id=msI02LXVJX": {
    "title": "Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hagmann",
      "Michael Staniek",
      "Stefan Riezler"
    ]
  },
  "https://openreview.net/forum?id=oAzu0gzUUb": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Liu",
      "Juncheng Dong",
      "Ziyang Jiang",
      "Ahmed Aloui",
      "Keyu Li",
      "Michael Hunter Klein",
      "Vahid Tarokh",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=hDywd5AbIM": {
    "title": "SAFE-NID: Self-Attention with Normalizing-Flow Encodings for Network Intrusion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Matejek",
      "Ashish Gehani",
      "Nathaniel D. Bastian",
      "Daniel J Clouse",
      "Bradford J Kline",
      "Susmit Jha"
    ]
  },
  "https://openreview.net/forum?id=aPyJilTiIb": {
    "title": "A Unified View of Double-Weighting for Marginal Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José I. Segovia-Martín",
      "Santiago Mazuelas",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=qsipSdfWeV": {
    "title": "Distilling Datasets Into Less Than One Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asaf Shul",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=uxyWlXPuIg": {
    "title": "On Using Secure Aggregation in Differentially Private Federated Learning with Multiple Local Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. Heikkilä"
    ]
  },
  "https://openreview.net/forum?id=XVSQnnf7QT": {
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Jeevan P",
      "Amit Sethi"
    ]
  },
  "https://openreview.net/forum?id=tIfS6jyO9f": {
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aurélien Chateigner"
    ]
  },
  "https://openreview.net/forum?id=EoiuRII7MQ": {
    "title": "Lower Ricci Curvature for Efficient Community Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Jin Park",
      "Didong Li"
    ]
  },
  "https://openreview.net/forum?id=uRbf9ANAns": {
    "title": "Meta-learning Optimizers for Communication-Efficient Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles-Étienne Joseph",
      "Benjamin Thérien",
      "Abhinav Moudgil",
      "Boris Knyazev",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=HTpMOl6xSI": {
    "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shwai He",
      "Daize Dong",
      "Liang Ding",
      "Ang Li"
    ]
  },
  "https://openreview.net/forum?id=MGdydNfWzQ": {
    "title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramansh Sharma",
      "Varun Shankar"
    ]
  },
  "https://openreview.net/forum?id=56EBglCFvx": {
    "title": "HARE: Human-in-the-Loop Algorithmic Recourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Srinivas Kancheti",
      "Rahul Vigneswaran",
      "Bamdev Mishra",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=nNN1pPJRVL": {
    "title": "Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hana YAHIA",
      "Bruno Figliuzzi",
      "Florent Di Meglio",
      "Gerbaud",
      "Stephane Menand",
      "Mohamed MAHJOUB"
    ]
  },
  "https://openreview.net/forum?id=VNM6V1gi3k": {
    "title": "Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kumar",
      "Jarvis Haupt"
    ]
  },
  "https://openreview.net/forum?id=HaAg9RN7Hi": {
    "title": "Unlabelled Compressive Sensing under Sparse Permutation and Prior Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garweet Sresth",
      "Satish Mulleti",
      "Ajit Rajwade"
    ]
  },
  "https://openreview.net/forum?id=osesw2V10u": {
    "title": "A unifying framework for generalised Bayesian online learning in non-stationary environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerardo Duran-Martin",
      "Leandro Sánchez-Betancourt",
      "Alex Shestopaloff",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=GEilvtsFNV": {
    "title": "Variational Neural Stochastic Differential Equations with Change Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef El-Laham",
      "Zhongchang Sun",
      "Haibei Zhu",
      "Tucker Balch",
      "Svitlana Vyetrenko"
    ]
  },
  "https://openreview.net/forum?id=y5Hf0otJLk": {
    "title": "Respecting the limit: Bayesian optimization with a bound on the optimal value",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Juergen Branke",
      "Matthias Poloczek"
    ]
  },
  "https://openreview.net/forum?id=dg1tqNIWg3": {
    "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danil Provodin",
      "Bram van den Akker",
      "Christina Katsimerou",
      "Maurits Clemens Kaptein",
      "Mykola Pechenizkiy"
    ]
  },
  "https://openreview.net/forum?id=FecsgPCOHk": {
    "title": "Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashka Shah",
      "Adela Frances DePavia",
      "Nathaniel C Hudson",
      "Ian Foster",
      "Rick Stevens"
    ]
  },
  "https://openreview.net/forum?id=uafxqhImPM": {
    "title": "On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Alter",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=uA19Xo1o31": {
    "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Faysse",
      "Patrick Fernandes",
      "Nuno M Guerreiro",
      "António Loison",
      "Duarte Miguel Alves",
      "Caio Corro",
      "Nicolas Boizard",
      "João Alves",
      "Ricardo Rei",
      "Pedro Henrique Martins",
      "Antoni Bigata Casademunt",
      "François Yvon",
      "Andre Martins",
      "Gautier Viaud",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=uPCvfyr2KP": {
    "title": "Reheated Gradient-based Discrete Sampling for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muheng Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=5zRs34Ls3C": {
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ]
  },
  "https://openreview.net/forum?id=Xv3ZrFayIO": {
    "title": "Attention Overlap Is Responsible for The Entity Missing Problem in Text-to-image Diffusion Models!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Mari Oriyad",
      "Mohammadali Banayeeanzade",
      "Reza Abbasi",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openreview.net/forum?id=iHYCdTAOqF": {
    "title": "The Time-Energy Model: Selective Time-Series Forecasting Using Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Brusokas",
      "Seshu Tirupathi",
      "Dalin Zhang",
      "Torben Bach Pedersen"
    ]
  },
  "https://openreview.net/forum?id=Is9APiPg4V": {
    "title": "Characterizing the Convergence of Game Dynamics via Potentialness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Bichler",
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Matthias Oberlechner",
      "Bary Pradelski"
    ]
  },
  "https://openreview.net/forum?id=OGifiton47": {
    "title": "Active Diffusion Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oisín Nolan",
      "Tristan Stevens",
      "Wessel L. van Nierop",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=XosdLS7KVE": {
    "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for Transformer Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pihe Hu",
      "Shaolong Li",
      "Xun Wang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LDzvZEVl5H": {
    "title": "Online Control-Informed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Liang",
      "Tianyu Zhou",
      "Zehui Lu",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=D3DA7pgpvn": {
    "title": "Visual Privacy Auditing with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristian Schwethelm",
      "Johannes Kaiser",
      "Moritz Knolle",
      "Sarah Lockfisch",
      "Daniel Rueckert",
      "Alexander Ziller"
    ]
  },
  "https://openreview.net/forum?id=nuIUTHGlM5": {
    "title": "Calibrated Probabilistic Forecasts for Arbitrary Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Marx",
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ]
  },
  "https://openreview.net/forum?id=QlBaDKb370": {
    "title": "State space models can express $n$-gram languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinoth Nandakumar",
      "Qiang Qu",
      "Peng Mi",
      "Tongliang Liu"
    ]
  },
  "https://openreview.net/forum?id=VxC4PZ71Ym": {
    "title": "Unlearning Personal Data from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas De Min",
      "Massimiliano Mancini",
      "Stéphane Lathuilière",
      "Subhankar Roy",
      "Elisa Ricci"
    ]
  },
  "https://openreview.net/forum?id=pF2ukh7HxA": {
    "title": "FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Abbott",
      "Gioele Zardini"
    ]
  },
  "https://openreview.net/forum?id=EEeVYfXor5": {
    "title": "Out of Spuriousity: Improving Robustness to Spurious Correlations without Group Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Quynh Le",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  "https://openreview.net/forum?id=5PPbvCExZs": {
    "title": "No Need for Ad-hoc Substitutes: The Expected Cost is a Principled All-purpose Classification Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer"
    ]
  },
  "https://openreview.net/forum?id=HOnL5hjaIt": {
    "title": "Generalized Tangent Kernel: A Unified Geometric Foundation for Natural Gradient and Standard Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Wei Xu"
    ]
  },
  "https://openreview.net/forum?id=Yk7GUlJwGa": {
    "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David OSOWIECHI",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openreview.net/forum?id=FoQK84nwY3": {
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Han Zhong",
      "Zhihan Liu",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan Awadalla",
      "Zhaoran Wang"
    ]
  },
  "https://openreview.net/forum?id=RXoSmiyObR": {
    "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisuke Hatano",
      "Satoshi Hara",
      "Hiromi Arai"
    ]
  },
  "https://openreview.net/forum?id=03UB1MCAMr": {
    "title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Bresson",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Michail Chatzianastasis",
      "Jun Pang",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=Xz5IcOizQ6": {
    "title": "Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghong Dai",
      "Jy-yong Sohn",
      "Yicong Chen",
      "S M Iftekharul Alam",
      "Ravikumar Balakrishnan",
      "Suman Banerjee",
      "Nageen Himayat",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=38cwP8xVxD": {
    "title": "The 2024 Foundation Model Transparency Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=dczXe0S1oL": {
    "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Serra",
      "Ben Werner",
      "Florian Buettner"
    ]
  },
  "https://openreview.net/forum?id=ZInwrlkQ3f": {
    "title": "An elementary concentration bound for Gibbs measures arising in statistical learning theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Ramsay",
      "Aukosh Jagannath",
      "Shojaeddin Chenouri"
    ]
  },
  "https://openreview.net/forum?id=tSFpsfndE7": {
    "title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Bernecker",
      "Ghalia Rehawi",
      "Francesco Paolo Casale",
      "Janine Knauer-Arloth",
      "Annalisa Marsico"
    ]
  },
  "https://openreview.net/forum?id=N28FdYO2sH": {
    "title": "Learning Linear Polytree Structural Equation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingmei Lou",
      "Yu Hu",
      "Xiaodong Li"
    ]
  },
  "https://openreview.net/forum?id=6nBIweDYzZ": {
    "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejumade Afonja",
      "Hui-Po Wang",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=BPDVZajOW5": {
    "title": "Optimizing Estimators of Squared Calibration Errors in Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gregor Gruber",
      "Francis R. Bach"
    ]
  },
  "https://openreview.net/forum?id=ZdMIXltJzK": {
    "title": "Reset-free Reinforcement Learning with World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat",
      "Edward S. Hu"
    ]
  },
  "https://openreview.net/forum?id=QIzRdjIWnS": {
    "title": "Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yi Zhou",
      "Shaofeng Zou"
    ]
  },
  "https://openreview.net/forum?id=UV58hNygne": {
    "title": "HoSNNs: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hejia Geng",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=8Q4qxe9a9Z": {
    "title": "A Self-Explainable Heterogeneous GNN for Relational Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Ferrini",
      "Antonio Longa",
      "Andrea Passerini",
      "Manfred Jaeger"
    ]
  },
  "https://openreview.net/forum?id=9NVJ0ZgEfT": {
    "title": "Long Short-Term Imputer: Handling Consecutive Missing Values in Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng You",
      "Xinyang Chen",
      "Yu Sun",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openreview.net/forum?id=58gPkcVbFL": {
    "title": "Evolution of Discriminator and Generator Gradients in GAN Training: From Fitting to Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ]
  },
  "https://openreview.net/forum?id=GkYOcbNLaW": {
    "title": "Cycle Conditioning for Robust Representation Learning from Categorical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Tabejamaat",
      "Farzaneh Etminani",
      "Mattias Ohlsson"
    ]
  },
  "https://openreview.net/forum?id=CrKMqRAhBo": {
    "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roozbeh Yousefzadeh",
      "Xuenan Cao"
    ]
  },
  "https://openreview.net/forum?id=HkmymFPODz": {
    "title": "Deep Active Learning in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Xie",
      "Jifan Zhang",
      "Haoyue Bai",
      "Robert D Nowak"
    ]
  },
  "https://openreview.net/forum?id=J7cY9Jr9WM": {
    "title": "A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amadou Siaka SANGARE",
      "Nicolas Dunou",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=bqMJToTkvT": {
    "title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Kong",
      "Hangyu Mao",
      "Zhao Qi",
      "Bin Zhang",
      "Jingqing Ruan",
      "Li Shen",
      "Yongzhe Chang",
      "Xueqian Wang",
      "Rui Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=jRbKsQ3sYO": {
    "title": "Combating Inter-Task Confusion and Catastrophic Forgetting by Metric Learning and Re-Using a Past Trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayedmoslem Shokrolahi",
      "IL MIN KIM"
    ]
  },
  "https://openreview.net/forum?id=prVLANCshF": {
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Jieru Mei",
      "Bingchen Zhao",
      "Yisen Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=IaUh7CSD3k": {
    "title": "Metalearning Continual Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Irie",
      "Róbert Csordás",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=CAkt3DsAZs": {
    "title": "Meta-Learning for Graphs with Heterogeneous Node Attribute Spaces for Few-Shot Edge Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong Chuang",
      "Yusuke Tanaka",
      "Tomoharu Iwata"
    ]
  },
  "https://openreview.net/forum?id=jJOVpnNrEp": {
    "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minttu Alakuijala",
      "Reginald McLean",
      "Isaac Woungang",
      "Nariman Farsad",
      "Samuel Kaski",
      "Pekka Marttinen",
      "Kai Yuan"
    ]
  },
  "https://openreview.net/forum?id=2Zan4ATYsh": {
    "title": "DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi WANG",
      "Yuhang Zhou",
      "Zhixiong Zhang",
      "Qiguang Chen",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=k4AxEwTaHq": {
    "title": "FaAlGrad: Fairness through Alignment of Gradients across Different Subpopulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Malik",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=xXs2GKXPnH": {
    "title": "Faster Diffusion Through Temporal Attention Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Wentian Zhang",
      "Jinheng Xie",
      "Francesco Faccio",
      "Mengmeng Xu",
      "Tao Xiang",
      "Mike Zheng Shou",
      "Juan-Manuel Perez-Rua",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=Za9Tm07fig": {
    "title": "TACO Vision Models Can Be Efficiently Specialized via Few-Shot Task-Aware Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kuznedelev",
      "Soroush Tabesh",
      "Kimia Noorbakhsh",
      "Elias Frantar",
      "Sara Beery",
      "Eldar Kurtic",
      "Dan Alistarh"
    ]
  },
  "https://openreview.net/forum?id=XPREcQlAM0": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan V. Truong"
    ]
  },
  "https://openreview.net/forum?id=ZckLMG00sO": {
    "title": "Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjeev Raja",
      "Ishan Amin",
      "Fabian Pedregosa",
      "Aditi S. Krishnapriyan"
    ]
  },
  "https://openreview.net/forum?id=6jTQrr3APY": {
    "title": "Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prabhu babu",
      "Petre Stoica",
      "Astha Saini"
    ]
  },
  "https://openreview.net/forum?id=EWT4GxjGDS": {
    "title": "Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Acharya",
      "Juba Ziani",
      "Jingyan Wang",
      "Varun Vangala"
    ]
  },
  "https://openreview.net/forum?id=dvRysCqmYQ": {
    "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Yang",
      "Han Yu",
      "Peikun Guo",
      "Khadija Zanna",
      "Xiaoxue Yang",
      "Akane Sano"
    ]
  },
  "https://openreview.net/forum?id=zSK81A2hxQ": {
    "title": "A Neural Material Point Method for Particle-based Emulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Rochman-Sharabi",
      "Sacha Lewin",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=0RJvZY0h6O": {
    "title": "Lognormal Mutations and their Use in Detecting Surreptitious Fake Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Teytaud",
      "Mariia Zameshina",
      "Tom Sander",
      "Pierre Fernandez",
      "Furong Ye",
      "Laurent Najman",
      "Thomas Bäck",
      "Ismail Labiad"
    ]
  },
  "https://openreview.net/forum?id=k3Ab6RuJE9": {
    "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Robert Bamler",
      "Bernhard Schölkopf",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=fC4bh1PmZr": {
    "title": "Counterfactual Learning of Stochastic Policies with Continuous Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houssam Zenati",
      "Alberto Bietti",
      "Matthieu Martin",
      "Eustache Diemert",
      "Pierre Gaillard",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Vwgjk5ysWn": {
    "title": "Why is constrained neural language generation particularly challenging?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Garbacea",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=K6CvWPtF62": {
    "title": "Provable Quantum Algorithm Advantage for Gaussian Process Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristian A. Galvis-Florez",
      "Ahmad Farooq",
      "Simo Särkkä"
    ]
  },
  "https://openreview.net/forum?id=ojeCoOKwWp": {
    "title": "Differentially Private Source-Target Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Schnapp",
      "Sivan Sabato"
    ]
  },
  "https://openreview.net/forum?id=WxHTSPS2pi": {
    "title": "Uncertainty-Based Experience Replay for Task-Agnostic Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Remonda",
      "Cole Corbitt Terrell",
      "Eduardo E. Veas",
      "Marc Masana"
    ]
  },
  "https://openreview.net/forum?id=E2zKNuwNDc": {
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Peter Shaw",
      "Jonathan Berant"
    ]
  },
  "https://openreview.net/forum?id=6LO1y8ZE0F": {
    "title": "SimPLR: A Simple and Plain Transformer for Efficient Object Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Kien Nguyen",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openreview.net/forum?id=TnT59yz7lc": {
    "title": "Exploiting Benford's Law for Weight Regularization of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Ott",
      "Huawei Sun",
      "Enrico Rinaldi",
      "Gianfranco Mauro",
      "Lorenzo Servadei",
      "Robert Wille"
    ]
  },
  "https://openreview.net/forum?id=rWSiBknwQa": {
    "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Guozheng Ma",
      "Cong Yu",
      "Ning Gui",
      "Linrui Zhang",
      "Zhiqi Huang",
      "Suwei Ma",
      "Yongzhe Chang",
      "Sen Zhang",
      "Li Shen",
      "Xueqian Wang",
      "Peilin Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=42v6I5Ut9a": {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pSk5qyt1ob": {
    "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudi Coppola",
      "Manuel Mazo Espinosa"
    ]
  },
  "https://openreview.net/forum?id=EcMVskXo1n": {
    "title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Yaochen Zhu",
      "Chuxu Zhang",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=69RntSRF5K": {
    "title": "An Analytical Model for Overparameterized Learning Under Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliav Mor",
      "Yair Carmon"
    ]
  },
  "https://openreview.net/forum?id=t5cy5v9wph": {
    "title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martha Lewis",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=adhsMqURI1": {
    "title": "Comparing the information content of probabilistic representation spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kieran A. Murphy",
      "Sam Dillavou",
      "Danielle Bassett"
    ]
  },
  "https://openreview.net/forum?id=jAHEBivObO": {
    "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyush Tiwary",
      "Atri Guha",
      "Subhodip Panda",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=D3DBqvSDbj": {
    "title": "On Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangming Gu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan Li",
      "Min Lin",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=dNJmJ8bh1M": {
    "title": "The Sparse Matrix-Based Random Projection: A Study of Binary and Ternary Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Lu",
      "Zhongzheng Li",
      "Mingrui Chen",
      "Weiyu Li"
    ]
  },
  "https://openreview.net/forum?id=Sx1khIIi95": {
    "title": "Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Caron",
      "Fadhel Ayed",
      "Paul Jung",
      "Hoil Lee",
      "Juho Lee",
      "Hongseok Yang"
    ]
  },
  "https://openreview.net/forum?id=rfPns0WJyg": {
    "title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos E. Luis",
      "Alessandro Giacomo Bottero",
      "Julia Vinogradska",
      "Felix Berkenkamp",
      "Jan Peters"
    ]
  },
  "https://openreview.net/forum?id=hCxtlfvL22": {
    "title": "Latent Space Energy-based Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Deqian Kong",
      "Jianwen Xie",
      "Kookjin Lee",
      "Ying Nian Wu",
      "Yezhou Yang"
    ]
  },
  "https://openreview.net/forum?id=U8EMkndyq4": {
    "title": "Using representation balancing to learn conditional-average dose responses from clustered data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bockel-Rickermann",
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Tim Verdonck",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=TRKwzPnXWQ": {
    "title": "ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Hongru Zhu",
      "Chen Wei",
      "Yijiang Li",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=GGHk5ukO6t": {
    "title": "Dynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=h751wl9xiR": {
    "title": "ALTA: Compiler-Based Analysis of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Shaw",
      "James Cohan",
      "Jacob Eisenstein",
      "Kenton Lee",
      "Jonathan Berant",
      "Kristina Toutanova"
    ]
  },
  "https://openreview.net/forum?id=BLDtWlFKhn": {
    "title": "Density of states in neural networks: an in-depth exploration of learning in parameter space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margherita Mele",
      "Roberto Menichetti",
      "Alessandro Ingrosso",
      "Raffaello Potestio"
    ]
  },
  "https://openreview.net/forum?id=HjpD5kpfa3": {
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chaolong Ying",
      "Yimu Wang",
      "Yaoyao Xu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=JQ0agisXny": {
    "title": "A Strong Baseline for Molecular Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philippe Formont",
      "Hugo Jeannin",
      "Pablo Piantanida",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=u9EHndbiVw": {
    "title": "PROXI: Challenging the GNNs for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Astrit Tola",
      "Jack Myrick",
      "Baris Coskunuzer"
    ]
  },
  "https://openreview.net/forum?id=RfFqBXLDQk": {
    "title": "On Space Folds of ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lewandowski",
      "Hamid Eghbalzadeh",
      "Bernhard Heinzl",
      "Raphael Pisoni",
      "Bernhard A. Moser"
    ]
  },
  "https://openreview.net/forum?id=asiBW1bB9b": {
    "title": "Improving Consistency in Large Language Models through Chain of Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Raj",
      "Vipul Gupta",
      "Domenic Rosati",
      "Subhabrata Majumdar"
    ]
  },
  "https://openreview.net/forum?id=H4S4ETc8c9": {
    "title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Ichihara",
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Kenshi Abe",
      "Kaito Ariu",
      "Mitsuki Sakamoto",
      "Eiji Uchibe"
    ]
  },
  "https://openreview.net/forum?id=ScEv13W2f1": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Hong-Xing Yu",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Wt6Iz5XNIO": {
    "title": "Understanding LLM Embeddings for Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Tang",
      "Bangding Yang",
      "Xingyou Song"
    ]
  },
  "https://openreview.net/forum?id=5qKI2dkrjL": {
    "title": "APR-CNN: Convolutional Neural Networks for the Adaptive Particle Representation of Large Microscopy Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Jonsson",
      "Bevan Leslie Cheeseman",
      "Ivo Sbalzarini"
    ]
  },
  "https://openreview.net/forum?id=zjxKrb4ehr": {
    "title": "On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Bruno",
      "Ying Zhang",
      "Dongyoung Lim",
      "Omer Deniz Akyildiz",
      "Sotirios Sabanis"
    ]
  },
  "https://openreview.net/forum?id=A1R1cQ93Cb": {
    "title": "Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Venturini",
      "Marianna De Santis",
      "Jordan Patracone",
      "Martin Schmidt",
      "Francesco Rinaldi",
      "Saverio Salzo"
    ]
  },
  "https://openreview.net/forum?id=nmBleuFzaN": {
    "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Dufort-Labbé",
      "Pierluca D'Oro",
      "Evgenii Nikishin",
      "Irina Rish",
      "Pierre-Luc Bacon",
      "Razvan Pascanu",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=ZMliWjMCor": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahrokh Ghoddousi Boroujeni",
      "Andreas Krause",
      "Giancarlo Ferrari-Trecate"
    ]
  },
  "https://openreview.net/forum?id=DrMCDS88IL": {
    "title": "Wasserstein Coreset via Sinkhorn Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyun Yin",
      "Yixuan Qiu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=tzW948kU6x": {
    "title": "Diffusion on Graph: Augmentation of Graph Structure for Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Changyu Liu",
      "Yingzhen Yang"
    ]
  },
  "https://openreview.net/forum?id=8rxtL0kZnX": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lev Telyatnikov",
      "Maria Sofia Bucarelli",
      "Guillermo Bernardez",
      "Olga Zaghen",
      "Simone Scardapane",
      "Pietro Lio"
    ]
  },
  "https://openreview.net/forum?id=sbmp55k6iE": {
    "title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikaru Umeda",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=JEHIVfjmOf": {
    "title": "JoIN: Joint GANs Inversion for Intrinsic Image Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Shah",
      "Svetlana Lazebnik",
      "Julien Philip"
    ]
  },
  "https://openreview.net/forum?id=1QeI99nH9k": {
    "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cullen Anderson",
      "Jeff M. Phillips"
    ]
  },
  "https://openreview.net/forum?id=7CUluLpLxV": {
    "title": "Explaining Explainability: Recommendations for Effective Use of Concept Activation Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Nicolson",
      "Lisa Schut",
      "Alison Noble",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=IrBYuh9W3T": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Shirali",
      "Moritz Hardt"
    ]
  },
  "https://openreview.net/forum?id=Bmy82p2eez": {
    "title": "Continual Learning from Simulated Interactions via Multitask Prospective Rehearsal for Bionic Limb Behavior Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharmita Dey",
      "Benjamin Paassen",
      "Sarath Ravindran Nair",
      "Sabri Boughorbel",
      "Arndt F. Schilling"
    ]
  },
  "https://openreview.net/forum?id=DYCSRf3vby": {
    "title": "Geometry-Aware visualization of high dimensional Symmetric Positive Definite matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault de Surrel",
      "Sylvain Chevallier",
      "Fabien Lotte",
      "Florian Yger"
    ]
  },
  "https://openreview.net/forum?id=VM8bNd5A09": {
    "title": "CNN Interpretability with Multivector Tucker Saliency Maps for Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aymene Mohammed Bouayed",
      "Samuel Deslauriers-gauthier",
      "Adrian IACOVELLI",
      "David Naccache"
    ]
  },
  "https://openreview.net/forum?id=YxXyRSlZ4b": {
    "title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Luca Marchetti",
      "Gabriele Cesa",
      "Kumar Pratik",
      "Arash Behboodi"
    ]
  },
  "https://openreview.net/forum?id=tzFjcVqmxw": {
    "title": "Enhancing Remaining Useful Life Prediction with Ensemble Multi-Term Fourier Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Song",
      "Laurens Bliek",
      "Yaoxin Wu",
      "Yingqian Zhang"
    ]
  },
  "https://openreview.net/forum?id=Wnd0XY0twh": {
    "title": "Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liran Nochumsohn",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=M3SkSMfWcP": {
    "title": "Adaptive Multi-step Refinement Network for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Chen",
      "Yufan Ren",
      "Tong Zhang",
      "Zheng Dang",
      "Wenbing Tao",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=zKv8qULV6n": {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Yuanhan Zhang",
      "Dong Guo",
      "Renrui Zhang",
      "Feng Li",
      "Hao Zhang",
      "Kaichen Zhang",
      "Peiyuan Zhang",
      "Yanwei Li",
      "Ziwei Liu",
      "Chunyuan Li"
    ]
  },
  "https://openreview.net/forum?id=F5ALCh3GWG": {
    "title": "On the Regularization of Learnable Embeddings for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Butera",
      "Giovanni De Felice",
      "Andrea Cini",
      "Cesare Alippi"
    ]
  },
  "https://openreview.net/forum?id=JQGmbVK4Fr": {
    "title": "Towards context and domain-aware algorithms for scene analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Serouis",
      "Florence Sèdes"
    ]
  },
  "https://openreview.net/forum?id=P5y82LKGbY": {
    "title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyun Wang",
      "Yadi Cao",
      "Chris Russell",
      "Yanxin Shen",
      "Junyu Luo",
      "Ming Zhang",
      "Siyu Heng",
      "Xiao Luo"
    ]
  },
  "https://openreview.net/forum?id=gwXfZ3xkUq": {
    "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qian Liu",
      "Chao Du",
      "Tongyao Zhu",
      "Cunxiao Du",
      "Kenji Kawaguchi",
      "Tianyu Pang"
    ]
  },
  "https://openreview.net/forum?id=UYXPt7HUdl": {
    "title": "Score-Based Denoising Diffusion Models for Photon-Starved Image Restoration Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savvas Melidonis",
      "Yiming Xi",
      "Konstantinos C. Zygalakis",
      "Yoann Altmann",
      "Marcelo Pereyra"
    ]
  },
  "https://openreview.net/forum?id=W50i7r3DHE": {
    "title": "Instance-Aware Graph Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Jundong Li",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=xpnPYfufhz": {
    "title": "Partially Frozen Random Networks Contain Compact Strong Lottery Tickets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Daiki Chijiwa",
      "Ángel López García-Arias",
      "Yasuyuki Okoshi",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Susumu Takeuchi",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=c7AAHdEYz5": {
    "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minguk Jang",
      "Hye Won Chung"
    ]
  },
  "https://openreview.net/forum?id=Q7aXOnEGgU": {
    "title": "On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Behboodi",
      "Gabriele Cesa"
    ]
  },
  "https://openreview.net/forum?id=2wgnepQjyF": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Israfil Bahceci",
      "Akram Bin Sediq",
      "HAMZA SOKUN",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=pxxmUKKgel": {
    "title": "How Does Code Pretraining Affect Language Model Task Performance?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Petty",
      "Sjoerd van Steenkiste",
      "Tal Linzen"
    ]
  },
  "https://openreview.net/forum?id=uDRzORdPT7": {
    "title": "DeepRRTime: Robust Time-series Forecasting with a Regularized INR Basis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Mahdi Gilany",
      "Kry Yik-Chau Lui",
      "Martin Magill",
      "Alexander Pashevich"
    ]
  },
  "https://openreview.net/forum?id=Lt2H8Bd8jF": {
    "title": "Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Vincent",
      "Daniel Palenicek",
      "Boris Belousov",
      "Jan Peters",
      "Carlo D'Eramo"
    ]
  },
  "https://openreview.net/forum?id=jZBAVFGUUo": {
    "title": "Towards Measuring Predictability: To which extent data-driven approaches can extract deterministic relations from data exemplified with time series prediction and classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh GHOLAM ZADEH",
      "Vaisakh Shaj",
      "Patrick Jahnke",
      "Gerhard Neumann",
      "Tim Breitenbach"
    ]
  },
  "https://openreview.net/forum?id=wIgRV336hC": {
    "title": "Minimax Lower Bounds for Estimating Distributions on Low-dimensional Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Chakraborty"
    ]
  },
  "https://openreview.net/forum?id=XWAXcxNg4n": {
    "title": "Test-Time Adaptation with Source Based Auxiliary Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motasem Alfarra",
      "Alvaro Correia",
      "Bernard Ghanem",
      "Christos Louizos"
    ]
  },
  "https://openreview.net/forum?id=HlzjI2fn2T": {
    "title": "On the stability of gradient descent with second order dynamics for time-varying cost functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis E Gibson",
      "Sawal Acharya",
      "Anjali Parashar",
      "Joseph Emilio Gaudio",
      "Anuradha Annaswamy"
    ]
  },
  "https://openreview.net/forum?id=O4CQ5AM5yP": {
    "title": "REX: GPU-Accelerated Sim2Real Framework with Delay and Dynamics Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Jens Kober",
      "Robert Babuska",
      "Laura Ferranti"
    ]
  },
  "https://openreview.net/forum?id=udVkqIDYSM": {
    "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidan Wang",
      "Rui Shen",
      "Bradly C. Stadie"
    ]
  },
  "https://openreview.net/forum?id=v47f4DwYZb": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geri Skenderi",
      "Hang Li",
      "Jiliang Tang",
      "Marco Cristani"
    ]
  },
  "https://openreview.net/forum?id=L7sQ8CW2FY": {
    "title": "Conformalized Credal Regions for Classification with Ambiguous Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Caprio",
      "David Stutz",
      "Shuo Li",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=2nRcWy3RLM": {
    "title": "Bridging Causality, Individual Fairness, and Adversarial Robustness in the Absence of Structural Causal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ]
  },
  "https://openreview.net/forum?id=8tMMCf4YYn": {
    "title": "Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Mishchenko",
      "Rustem Islamov",
      "Eduard Gorbunov",
      "Samuel Horváth"
    ]
  },
  "https://openreview.net/forum?id=gLQ801ewwp": {
    "title": "Identifying Axiomatic Mathematical Transformation Steps using Tree-Structured Pointer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Wankerl",
      "Jan Pfister",
      "Andrzej Dulny",
      "Gerhard Götz",
      "Andreas Hotho"
    ]
  },
  "https://openreview.net/forum?id=p9KSFrTLx0": {
    "title": "Mixture Degree-Corrected Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noureddine Henka",
      "Mohamad Assaad",
      "Sami Tazi"
    ]
  },
  "https://openreview.net/forum?id=x1dXvvElVd": {
    "title": "Interpreting Neurons in Deep Vision Networks with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Bai",
      "Rahul Ajay Iyer",
      "Tuomas Oikarinen",
      "Akshay R. Kulkarni",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=f6yMdmrD2g": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed Fatih Balin",
      "Dominique LaSalle",
      "Umit Catalyurek"
    ]
  },
  "https://openreview.net/forum?id=Ss9MTTN7OL": {
    "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Miranda",
      "Elena Sofia Ruzzetti",
      "Andrea Santilli",
      "Fabio Massimo Zanzotto",
      "Sébastien Bratières",
      "Emanuele Rodolà"
    ]
  },
  "https://openreview.net/forum?id=OGaTF9iOxi": {
    "title": "Maximum Mean Discrepancy on Exponential Windows for Online Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Kalinke",
      "Marco Heyden",
      "Georg Gntuni",
      "Edouard Fouché",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=aWRMvXTvPf": {
    "title": "Shapley Values of Structured Additive Regression Models and Application to RKHS Weightings of Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Dubé",
      "Mario Marchand"
    ]
  },
  "https://openreview.net/forum?id=WppTEs4Kkn": {
    "title": "On the effects of similarity metrics in decentralized deep learning under distribution shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edvin Listo Zec",
      "Tom Hagander",
      "Eric Ihre-Thomason",
      "Sarunas Girdzijauskas"
    ]
  },
  "https://openreview.net/forum?id=VmfWywWuYQ": {
    "title": "Interactive Task Planning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Li",
      "Philipp Wu",
      "Pieter Abbeel",
      "Jitendra Malik"
    ]
  },
  "https://openreview.net/forum?id=4o8lIFkpn2": {
    "title": "\\copyright Plug-in Authorization for Human Copyright Protection in Text-to-Image Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhou",
      "Huishuai Zhang",
      "Jiang Bian",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openreview.net/forum?id=3Y3o0yFZfu": {
    "title": "Private Fine-tuning of Large Language Models with Zeroth-order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tang",
      "Ashwinee Panda",
      "Milad Nasr",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ]
  },
  "https://openreview.net/forum?id=bZzXgheUSD": {
    "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masih Eskandar",
      "Tooba Imtiaz",
      "Zifeng Wang",
      "Jennifer Dy"
    ]
  },
  "https://openreview.net/forum?id=MO1slfU9xy": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Mougan",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ]
  },
  "https://openreview.net/forum?id=zg0hPlABfY": {
    "title": "Enhancing Parameter Efficiency and Generalization in Large Models: A Regularized and Masked Low-Rank Adaptation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Mao",
      "Zihao Zhao",
      "Siqi Ping",
      "Yang Liu",
      "Wenbo Ding"
    ]
  },
  "https://openreview.net/forum?id=nu1SjVgSuy": {
    "title": "SPFormer: Enhancing Vision Transformer with Superpixel Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieru Mei",
      "Liang-Chieh Chen",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=pWSrm3oP8b": {
    "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhepeng Cen",
      "Yao Liu",
      "Siliang Zeng",
      "Pratik Chaudhari",
      "Huzefa Rangwala",
      "George Karypis",
      "Rasool Fakoor"
    ]
  },
  "https://openreview.net/forum?id=oeg2ncuSPz": {
    "title": "Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasis Kratsios",
      "Haitz Sáez de Ocáriz Borde",
      "Takashi Furuya",
      "Marc T. Law"
    ]
  },
  "https://openreview.net/forum?id=hJHf7PCuVt": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Ling",
      "Zhimeng Jiang",
      "Na Zou",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=uF9ZdAwrCT": {
    "title": "In-distribution adversarial attacks on object recognition models using gradient-free search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spandan Madan",
      "Tomotake Sasaki",
      "Hanspeter Pfister",
      "Tzu-Mao Li",
      "Xavier Boix"
    ]
  },
  "https://openreview.net/forum?id=KbteA50cni": {
    "title": "Distributed Quasi-Newton Method for Fair and Fast Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi",
      "Linfeng Ye"
    ]
  },
  "https://openreview.net/forum?id=hMO8sT9qaD": {
    "title": "Making Reliable and Flexible Decisions in Long-tailed Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolian Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=dzQCRHKRdC": {
    "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Derezinski"
    ]
  },
  "https://openreview.net/forum?id=GDN5cFTNaL": {
    "title": "Adjacency Search Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meher Chaitanya",
      "Kshitijaa Jaglan",
      "Ulrik Brandes"
    ]
  },
  "https://openreview.net/forum?id=4Xz0WBAiX4": {
    "title": "ExCeL: Combined Extreme and Collective Logit Information for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen Karunanayake",
      "Suranga Seneviratne",
      "Sanjay Chawla"
    ]
  },
  "https://openreview.net/forum?id=8C8LJIqF4y": {
    "title": "Time Series Domain Adaptation via Channel-Selective Representation Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nauman Ahad",
      "Mark A. Davenport",
      "Eva L Dyer"
    ]
  },
  "https://openreview.net/forum?id=OOgsAZdFOt": {
    "title": "Can AI-Generated Text be Reliably Detected? Stress Testing AI Text Detectors Under Various Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinu Sankar Sadasivan",
      "Aounon Kumar",
      "Sriram Balasubramanian",
      "Wenxiao Wang",
      "Soheil Feizi"
    ]
  },
  "https://openreview.net/forum?id=aiOHc1LGpD": {
    "title": "Differentially Private Gradient Flow based on the Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilana Sebag",
      "Muni Sreenivas Pydi",
      "Jean-Yves Franceschi",
      "Alain Rakotomamonjy",
      "Mike Gartrell",
      "Jamal Atif",
      "Alexandre Allauzen"
    ]
  },
  "https://openreview.net/forum?id=dbaGuiYsTl": {
    "title": "Wasserstein Modality Alignment Makes Your Multimodal Transformer More Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhuo zhi",
      "Yuxuan Sun",
      "Qiangqiang Wu",
      "Ziquan Liu",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=AFxEdJwQcp": {
    "title": "A thorough reproduction and evaluation of $\\mu$P",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Vlassis",
      "David Belius",
      "Volodymyr Fomichov"
    ]
  },
  "https://openreview.net/forum?id=avDr56QjSI": {
    "title": "Semantic Alignment for Prompt-Tuning in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hari Chandana Kuchibhotla",
      "Sai Srinivas Kancheti",
      "Abbavaram Gowtham Reddy",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=Utjw2z1ale": {
    "title": "Identifying Spurious Correlations using Counterfactual Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Paul Cohen",
      "Louis Blankemeier",
      "Akshay S Chaudhari"
    ]
  },
  "https://openreview.net/forum?id=LVQ8BEL5n3": {
    "title": "Numerically Robust Fixed-Point Smoothing Without State Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Krämer"
    ]
  },
  "https://openreview.net/forum?id=r8UFp9olQ0": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Majellaro",
      "Jonathan Collu",
      "Aske Plaat",
      "Thomas M. Moerland"
    ]
  },
  "https://openreview.net/forum?id=Gb4HBGG9re": {
    "title": "Enhanced Federated Optimization: Adaptive Unbiased Client Sampling with Reduced Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dun Zeng",
      "Zenglin Xu",
      "Yu Pan",
      "Xu Luo",
      "Qifan Wang",
      "Xiaoying Tang"
    ]
  },
  "https://openreview.net/forum?id=vmmgFW3ztz": {
    "title": "Leveraging a Simulator for Learning Causal Representations from Post-Treatment Covariates for CATE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Nagalapatti",
      "Pranava Singhal",
      "Avishek Ghosh",
      "Sunita Sarawagi"
    ]
  },
  "https://openreview.net/forum?id=INijCSPtbQ": {
    "title": "Preventing Conflicting Gradients in Neural Marked Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=LZ9FmeFeLV": {
    "title": "Towards LifeSpan Cognitive Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Chi Han",
      "Tongtong Wu",
      "Xiaoxin He",
      "Wangchunshu Zhou",
      "Nafis Sadeq",
      "Xiusi Chen",
      "Zexue He",
      "Wei Wang",
      "Gholamreza Haffari",
      "Heng Ji",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=IIVr4Hu3Oi": {
    "title": "Distributed Multi-Agent Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prithviraj Tarale",
      "Edward Rietman",
      "Hava T Siegelmann"
    ]
  },
  "https://openreview.net/forum?id=T5OuTgPxHS": {
    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://openreview.net/forum?id=0mGho8wrv5": {
    "title": "SelfEval: Leveraging discriminative nature of generative models for evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Saketh Rambhatla",
      "Ishan Misra"
    ]
  },
  "https://openreview.net/forum?id=dHljjaNHh1": {
    "title": "Fairness Through Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ]
  },
  "https://openreview.net/forum?id=V2SD2uVKEE": {
    "title": "Zero-shot CLIP Class Forgetting via Text-image Space Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Kravets",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openreview.net/forum?id=lTX4bYREAZ": {
    "title": "A Scalable Approach for Mapper via Efficient Spatial Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Simi"
    ]
  },
  "https://openreview.net/forum?id=VIkycTWDWo": {
    "title": "Doubly Robust Conditional VAE via Decoder Calibration: An Implicit KL Annealing Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhui Liu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=IZrt6hB2sI": {
    "title": "Improving CLIP Counting Accuracy via Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisu Zhang",
      "Yicong Chen",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=ccu0M3nmlF": {
    "title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koki Okajima",
      "Tomoyuki Obuchi"
    ]
  },
  "https://openreview.net/forum?id=qbrE0LR7fF": {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer",
      "Daniel Ramos"
    ]
  },
  "https://openreview.net/forum?id=LdflD41Gn8": {
    "title": "On the Properties and Estimation of Pointwise Mutual Information Profiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paweł Czyż",
      "Frederic Grabowski",
      "Julia E Vogt",
      "Niko Beerenwinkel",
      "Alexander Marx"
    ]
  },
  "https://openreview.net/forum?id=BlYIPa0Fx1": {
    "title": "An analysis of the noise schedule for score-based generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislas Strasman",
      "Antonio Ocello",
      "Claire Boyer",
      "Sylvain Le Corff",
      "Vincent Lemaire"
    ]
  },
  "https://openreview.net/forum?id=PtD2gVmb3J": {
    "title": "Global Safe Sequential Learning via Efficient Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cen-You Li",
      "Olaf Dünnbier",
      "Marc Toussaint",
      "Barbara Rakitsch",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=QQE5j2OsLW": {
    "title": "Can Optimization Trajectories Explain Multi-Task Transfer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mueller",
      "Mark Dredze",
      "Nicholas Andrews"
    ]
  },
  "https://openreview.net/forum?id=yzbAFf8vd5": {
    "title": "A comparison between humans and AI at recognizing objects in unusual poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Netta Ollikka",
      "Amro Kamal Mohamed Abbas",
      "Andrea Perin",
      "Markku Kilpeläinen",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=V6ia5hWIMD": {
    "title": "νSAM: Memory-Efficient Sharpness-Aware Minimization via Nuclear Norm Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Parameswaran Raman",
      "Lenon Minorics",
      "Mingyi Hong",
      "Shoham Sabach",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WEYMCLu8u7": {
    "title": "Event-Triggered Time-Varying Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Brunzema",
      "Alexander von Rohr",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=PTTa3U29NR": {
    "title": "Optimization Dynamics of Equivariant and Augmented Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oskar Nordenfors",
      "Fredrik Ohlsson",
      "Axel Flinth"
    ]
  },
  "https://openreview.net/forum?id=QplBL2pV4Z": {
    "title": "Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Yin Huang",
      "Ruinan Jin",
      "Can Zhao",
      "Daguang Xu",
      "Xiaoxiao Li"
    ]
  },
  "https://openreview.net/forum?id=XL1N6iLr0G": {
    "title": "An Attribute-based Method for Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Reiss",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=8mgX3Uw2Ea": {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Zhu",
      "Sheng Liu",
      "Carlos Fernandez-Granda",
      "Narges Razavian"
    ]
  },
  "https://openreview.net/forum?id=tRpWaK3pWh": {
    "title": "A Generalization Bound for Nearly-Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Golikov"
    ]
  },
  "https://openreview.net/forum?id=Qq4ge9Qe31": {
    "title": "Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Perini",
      "Maja Rudolph",
      "Sabrina Schmedding",
      "Chen Qiu"
    ]
  },
  "https://openreview.net/forum?id=BhOJreYmur": {
    "title": "MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Guerra-Manzanares",
      "Farah Shamout"
    ]
  },
  "https://openreview.net/forum?id=b68QOenPWy": {
    "title": "Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leah Bar",
      "Boaz Lerner",
      "Nir Darshan",
      "Rami Ben-Ari"
    ]
  },
  "https://openreview.net/forum?id=I4IAwVOZrM": {
    "title": "Lifelong Learning in StyleGAN through Latent Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Kappiyath",
      "ANMOL GARG",
      "Ramya Hebbalaguppe",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=bwRxXiGO9A": {
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Boizard",
      "Kevin El Haddad",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=CTkABQvnkm": {
    "title": "Decoupled Sequence and Structure Generation for Realistic Antibody Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nayoung Kim",
      "Minsu Kim",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://openreview.net/forum?id=pxdSm7PW5Q": {
    "title": "Reviving Life on the Edge: Joint Score-Based Graph Generation of Rich Edge Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrod Berman",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=60Gi1w6hte": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc T. Law",
      "Karsten Kreis",
      "Haggai Maron"
    ]
  },
  "https://openreview.net/forum?id=eakh1Edffd": {
    "title": "Reinforcement learning with non-ergodic reward increments: robustness via ergodicity transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Erfaun Noorani",
      "James Price",
      "Ole Peters",
      "Colm Connaughton",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=bHdEtW5E7O": {
    "title": "Federated Learning with Efficient Local Adaptation for Realized Volatility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhao",
      "Lin Cai",
      "Wu-Sheng Lu"
    ]
  },
  "https://openreview.net/forum?id=MvYddudHuE": {
    "title": "Reweighting Improves Conditional Risk Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Jiahe Lin",
      "Fengpei Li",
      "Songzhu Zheng",
      "Anant Raj",
      "Anderson Schneider",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://openreview.net/forum?id=vZGZIIgcG4": {
    "title": "Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Rahbar",
      "Niklas Åkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=DqWvxSQ1TK": {
    "title": "From Promise to Practice: A Study of Common Pitfalls Behind the Generalization Gap in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeideh Ghanbari Azar",
      "Lorenzo Tronchin",
      "Attila Simkó",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=3mJZfL77WM": {
    "title": "Highway Graph to Accelerate Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidu Yin",
      "Zhen Zhang",
      "Dong Gong",
      "Stefano V Albrecht",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=DCAeXwLenB": {
    "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Fred Maurice NGOLE MBOULA",
      "Antoine Souloumiac"
    ]
  },
  "https://openreview.net/forum?id=gpHOtQQPJG": {
    "title": "Optimization and Generalization Guarantees for Weight Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Cisneros-Velarde",
      "Zhijie Chen",
      "Sanmi Koyejo",
      "Arindam Banerjee"
    ]
  },
  "https://openreview.net/forum?id=LzmsvRTqaJ": {
    "title": "Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative model for Quasar spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidhi Lalchand",
      "Anna-Christina Eilers"
    ]
  },
  "https://openreview.net/forum?id=fqkq1MgONB": {
    "title": "BM$^2$: Coupled Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Peluchetti"
    ]
  },
  "https://openreview.net/forum?id=wS1fD0ofay": {
    "title": "Partial-Label Learning with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fuchs",
      "Florian Kalinke",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=34vtRA3Nvu": {
    "title": "PRIMO: Private Regression in Multiple Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seth Neel"
    ]
  },
  "https://openreview.net/forum?id=ytKFKoCpyK": {
    "title": "ODNet: Opinion Dynamics-Inspired Neural Message Passing for Graphs and Hypergraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Zhou",
      "Outongyi Lv",
      "Jing Wang",
      "Xiang Xiao",
      "Weishu Zhao"
    ]
  },
  "https://openreview.net/forum?id=0u7pWfjri5": {
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=UrSgGSTM2J": {
    "title": "Minimax Posterior Contraction Rates for Unconstrained Distribution Estimation on $[0,1]^d$ under Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Matthew Jacobs",
      "Lekha Patel",
      "Anirban Bhattacharya",
      "Debdeep Pati"
    ]
  },
  "https://openreview.net/forum?id=9CWU8Oi86d": {
    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Yuzheng Hu",
      "Yong Lin",
      "Tong Zhang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=yawWz4qWkF": {
    "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Alina Fastowski",
      "Felix Pfeiffer",
      "Gjergji Kasneci"
    ]
  },
  "https://openreview.net/forum?id=CNaiJRcX84": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Paul E. Apolinario",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=iVV7IzI55V": {
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=AjJTg5M0r8": {
    "title": "Slicing Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Bonet",
      "Kimia Nadjahi",
      "Thibault Sejourne",
      "Kilian FATRAS",
      "Nicolas Courty"
    ]
  },
  "https://openreview.net/forum?id=yBgTVWccIx": {
    "title": "DafnyBench: A Benchmark for Formal Software Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chloe R Loughridge",
      "Qinyi Sun",
      "Seth Ahrenbach",
      "Federico Cassano",
      "Chuyue Sun",
      "Ying Sheng",
      "Anish Mudide",
      "Md Rakib Hossain Misu",
      "Nada Amin",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=x8wscCAJ2m": {
    "title": "Sparse Neural Architectures via Deterministic Ramanujan Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suryam Arnav Kalra",
      "Arindam Biswas",
      "Pabitra Mitra",
      "BISWAJIT BASU"
    ]
  },
  "https://openreview.net/forum?id=JHxrh00W1j": {
    "title": "Masked Capsule Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=gqh0yzPYdo": {
    "title": "No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu Gaur",
      "Darshan Singh S",
      "Makarand Tapaswi"
    ]
  },
  "https://openreview.net/forum?id=Conma3qnaT": {
    "title": "Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Verma",
      "Gantavya Bhatt",
      "Avi Schwarzschild",
      "Soumye Singhal",
      "Arnav Mohanty Das",
      "Chirag Shah",
      "John P Dickerson",
      "Pin-Yu Chen",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=XDbY3qhM42": {
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang ZHANG",
      "Joshua M. Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ]
  },
  "https://openreview.net/forum?id=oYP2Pd5aQt": {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Gao",
      "Chuanyang Zheng",
      "Enze Xie",
      "Han Shi",
      "Tianyang Hu",
      "Yu Li",
      "Michael Ng",
      "Zhenguo Li",
      "Zhaoqiang Liu"
    ]
  },
  "https://openreview.net/forum?id=Og3VxBFhwj": {
    "title": "Linear Convergence of Decentralized FedAvg for PL Objectives: The Interpolation Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti P Maralappanavar",
      "Prashant Khanduri",
      "Bharath B N"
    ]
  },
  "https://openreview.net/forum?id=XxbQAsxrRC": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franka Bause",
      "Fabian Jogl",
      "Patrick Indri",
      "Tamara Drucks",
      "David Penz",
      "Nils Morten Kriege",
      "Thomas Gärtner",
      "Pascal Welke",
      "Maximilian Thiessen"
    ]
  },
  "https://openreview.net/forum?id=aV6dCg1VFV": {
    "title": "Investigating the impact of missing value handling on Boosted trees and Deep learning for Tabular data: A Claim Reserving case study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Larionov",
      "Niall M. Adams",
      "Kevin N. Webster"
    ]
  },
  "https://openreview.net/forum?id=IK2cR89z45": {
    "title": "Personalized Privacy Amplification via Importance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fay",
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=bwyHf5eery": {
    "title": "A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data and Overparameterization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Johannes Zenn",
      "Robert Bamler"
    ]
  },
  "https://openreview.net/forum?id=kzPNHQ8ByY": {
    "title": "Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihong Yu",
      "Manav Mishra",
      "Alec Koppel",
      "Carl Busart",
      "Priya Narayan",
      "Dinesh Manocha",
      "Amrit Singh Bedi",
      "Pratap Tokekar"
    ]
  },
  "https://openreview.net/forum?id=Vq0wMFBjo2": {
    "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zang",
      "Tian Yun",
      "Hao Tan",
      "Trung Bui",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=o58uy91V2V": {
    "title": "On the Detection of Reviewer-Author Collusion Rings From Paper Bidding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Jecmen",
      "Nihar B Shah",
      "Fei Fang",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=ZA7D4nQuQF": {
    "title": "Transformers in Uniform TC$^0$",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=SeGNvJJjbs": {
    "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo"
    ]
  },
  "https://openreview.net/forum?id=KqRnsEMYLx": {
    "title": "Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madison Cooley",
      "Varun Shankar",
      "Mike Kirby",
      "Shandian Zhe"
    ]
  },
  "https://openreview.net/forum?id=nxQtoHHcj9": {
    "title": "An Analysis of Model Robustness across Concurrent Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongho Jeon",
      "Suhwan Choi",
      "Hyoje Lee",
      "Teresa Yeo"
    ]
  },
  "https://openreview.net/forum?id=ZnWqtPhHM7": {
    "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarshi Brahma",
      "Anuska Roy",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=JN7iNWaPTe": {
    "title": "Mental Modelling of Reinforcement Learning Agents by Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Lu",
      "Xufeng Zhao",
      "Josua Spisak",
      "Jae Hee Lee",
      "Stefan Wermter"
    ]
  },
  "https://openreview.net/forum?id=PzmaWLqK0e": {
    "title": "Reward-based Autonomous Online Learning Framework for Resilient Cooperative Target Monitoring using a Swarm of Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhankar Gupta",
      "Saksham Sharma",
      "Suresh Sundaram"
    ]
  },
  "https://openreview.net/forum?id=edULLIVnoc": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Cohen-Wang",
      "Joshua Vendrow",
      "Aleksander Madry"
    ]
  },
  "https://openreview.net/forum?id=xQbRFHfgGL": {
    "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivanshu Shekhar",
      "Shreyas Singh",
      "Tong Zhang"
    ]
  },
  "https://openreview.net/forum?id=Y8EspxaksH": {
    "title": "Faithful Interpretation for Graph Neural Networks",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Lu Yu",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ]
  },
  "https://openreview.net/forum?id=mAiMKnr9r5": {
    "title": "Random Policy Enables In-Context Reinforcement Learning within Trust Horizons",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqin Chen",
      "Santiago Paternain"
    ]
  },
  "https://openreview.net/forum?id=1p9hQTbjgo": {
    "title": "MiniFold: Simple, Fast, and Accurate Protein Structure Prediction",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Wohlwend",
      "Mateo Reveiz",
      "Matt McPartlon",
      "Axel Feldmann",
      "Wengong Jin",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=ssXSrZ94sR": {
    "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kay",
      "Timm Haucke",
      "Suzanne Stathatos",
      "Siqi Deng",
      "Erik Young",
      "Pietro Perona",
      "Sara Beery",
      "Grant Van Horn"
    ]
  },
  "https://openreview.net/forum?id=x6fXnsM9Ez": {
    "title": "The 2023 Foundation Model Transparency Index",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Betty Xiong",
      "Daniel Zhang",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=1Avb4jYjLb": {
    "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Nikhil Anand",
      "Nikhil Vyas",
      "Eran Malach",
      "Sham M. Kakade"
    ]
  },
  "https://openreview.net/forum?id=Y7dRmpGiHj": {
    "title": "What is the Relationship between Tensor Factorizations and Circuits (and How Can We Exploit it)?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Loconte",
      "Antonio Mari",
      "Gennaro Gala",
      "Robert Peharz",
      "Cassio de Campos",
      "Erik Quaeghebeur",
      "Gennaro Vessio",
      "Antonio Vergari"
    ]
  },
  "https://openreview.net/forum?id=YCt8lsIDwA": {
    "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yu",
      "Chenchen Zhu",
      "Sean Culatana",
      "Raghuraman Krishnamoorthi",
      "Fanyi Xiao",
      "Yong Jae Lee"
    ]
  },
  "https://openreview.net/forum?id=vKUPXuEzj8": {
    "title": "Reproducibility Study of 'SLICE: Stabilized LIME for Consistent Explanations for Image Classification",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Bandyopadhyay",
      "Chiranjeev Bindra",
      "Roan van Blanken",
      "Arijit Ghosh"
    ]
  },
  "https://openreview.net/forum?id=TJRyDi7mwH": {
    "title": "NeoBERT: A Next Generation BERT",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lola Le Breton",
      "Quentin Fournier",
      "Mariam El Mezouar",
      "Sarath Chandar"
    ]
  },
  "https://openreview.net/forum?id=855yo1Ubt2": {
    "title": "An Expanded Benchmark that Rediscovers and Affirms the Edge of Uncertainty Sampling for Active Learning in Tabular Datasets",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po-Yi Lu",
      "Yi-Jie Cheng",
      "Chun-Liang Li",
      "Hsuan-Tien Lin"
    ]
  },
  "https://openreview.net/forum?id=H6DtMcZf5s": {
    "title": "Remembering to Be Fair Again: Reproducing Non-Markovian Fairness in Sequential Decision Making",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Domonkos Nagy",
      "Lohithsai Yadala Chanchu",
      "Krystof Bobek",
      "Xin Zhou",
      "Jacobus Smit"
    ]
  },
  "https://openreview.net/forum?id=zLfLTHOdZW": {
    "title": "[RE] GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyme Chatupanyachotikul",
      "Leonard Horns",
      "Matei Nastase"
    ]
  },
  "https://openreview.net/forum?id=l9rATNBB8Y": {
    "title": "Privacy Awareness for Information-Sharing Assistants: A Case-study on Form-filling with Contextual Integrity",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahra Ghalebikesabi",
      "Eugene Bagdasarian",
      "Ren Yi",
      "Itay Yona",
      "Ilia Shumailov",
      "Aneesh Pappu",
      "Chongyang Shi",
      "Laura Weidinger",
      "Robert Stanforth",
      "Leonard Berrada",
      "Pushmeet Kohli",
      "Po-Sen Huang",
      "Borja Balle"
    ]
  },
  "https://openreview.net/forum?id=sXr1fRjs1N": {
    "title": "Contextualized Messages Boost Graph Representations",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Godwin Lim",
      "Galvin Brice Sy Lim",
      "Renzo Roel Tan",
      "Kazushi Ikeda"
    ]
  },
  "https://openreview.net/forum?id=yeITEuhv4Q": {
    "title": "Revisiting Deep Hybrid Models for Out-of-Distribution Detection",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Ruben Schlumbom",
      "Eibe Frank"
    ]
  },
  "https://openreview.net/forum?id=mSoDRZXsqj": {
    "title": "Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Billy Joe Franks",
      "Moshe Eliasof",
      "Semih Cantürk",
      "Guy Wolf",
      "Carola-Bibiane Schönlieb",
      "Sophie Fellenz",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=rKAkp1f3R7": {
    "title": "Shedding Light on Problems with Hyperbolic Graph Learning",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isay Katsman",
      "Anna Gilbert"
    ]
  },
  "https://openreview.net/forum?id=IPmzyQSiQE": {
    "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zach Nussbaum",
      "John Xavier Morris",
      "Andriy Mulyar",
      "Brandon Duderstadt"
    ]
  },
  "https://openreview.net/forum?id=wcxrJcJ7vq": {
    "title": "The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgi Ganev",
      "Meenatchi Sundaram Muthu Selva Annamalai",
      "Emiliano De Cristofaro"
    ]
  },
  "https://openreview.net/forum?id=wF3ZtSlOcT": {
    "title": "Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Sidiropoulos",
      "Samarth Bhargav",
      "Panagiotis Eustratiadis",
      "Evangelos Kanoulas"
    ]
  },
  "https://openreview.net/forum?id=knv4lQFVoE": {
    "title": "A general framework of Riemannian adaptive optimization methods with a convergence analysis",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Sakai",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=tf6A9EYMo6": {
    "title": "Personalization of Large Language Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Ryan A. Rossi",
      "Branislav Kveton",
      "Yijia Shao",
      "Diyi Yang",
      "Hamed Zamani",
      "Franck Dernoncourt",
      "Joe Barrow",
      "Tong Yu",
      "Sungchul Kim",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Tyler Derr",
      "Hongjie Chen",
      "Junda Wu",
      "Xiang Chen",
      "Zichao Wang",
      "Subrata Mitra",
      "Nedim Lipka",
      "Nesreen K. Ahmed",
      "Yu Wang"
    ]
  },
  "https://openreview.net/forum?id=wHECkBOwyt": {
    "title": "Efficient Diffusion Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Shen",
      "Jingxuan Zhang",
      "Boning Xiong",
      "Rui Hu",
      "Shoufa Chen",
      "Zhongwei Wan",
      "Xin Wang",
      "Yu Zhang",
      "Zixuan Gong",
      "Guangyin Bao",
      "Chaofan Tao",
      "Yongfeng Huang",
      "Ye Yuan",
      "Mi Zhang"
    ]
  },
  "https://openreview.net/forum?id=ewwNKwh6SK": {
    "title": "Conditional Image Synthesis with Diffusion Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhan",
      "Defang Chen",
      "Jian-Ping Mei",
      "Zhenghe Zhao",
      "Jiawei Chen",
      "Chun Chen",
      "Siwei Lyu",
      "Can Wang"
    ]
  },
  "https://openreview.net/forum?id=ZiJYahyXLU": {
    "title": "Machine Learning with Physics Knowledge for Prediction: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Watson",
      "Chen Song",
      "Oliver Weeger",
      "Theo Gruner",
      "An Thai Le",
      "Kay Hansel",
      "Ahmed Hendawy",
      "Oleg Arenz",
      "Will Trojak",
      "Miles Cranmer",
      "Carlo D'Eramo",
      "Fabian Buelow",
      "Tanmay Goyal",
      "Jan Peters",
      "Martin W Hoffmann"
    ]
  },
  "https://openreview.net/forum?id=CsoSWpR5xC": {
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiachong Feng",
      "Longxu Dou",
      "Minzhi Li",
      "Qinghao Wang",
      "Yu Guo",
      "Haochuan Wang",
      "Chang Ma",
      "Lingpeng Kong"
    ]
  },
  "https://openreview.net/forum?id=D1PPuk8ZBI": {
    "title": "When Should Reinforcement Learning Use Causal Reasoning?",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Schulte",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=fHf4jbIfex": {
    "title": "Graph Theory-Based Deep Graph Similarity Learning: A Unified Survey of Pipeline, Techniques, and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouyang LIU",
      "Ning Liu",
      "Yixin Chen",
      "Ziqing Wen",
      "Jiezhong He",
      "Dongsheng Li"
    ]
  },
  "https://openreview.net/forum?id=u0azVc9Y0y": {
    "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Colin Raffel",
      "Mohammed Muqeeth",
      "Lucas Caccia",
      "Haokun Liu",
      "Tianlong Chen",
      "Mohit Bansal",
      "Leshem Choshen",
      "Alessandro Sordoni"
    ]
  },
  "https://openreview.net/forum?id=1BqXkjNEGP": {
    "title": "Autoregressive Models in Vision: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Xiong",
      "Gongye Liu",
      "Lun Huang",
      "Chengyue Wu",
      "Taiqiang Wu",
      "Yao Mu",
      "Yuan Yao",
      "Hui Shen",
      "Zhongwei Wan",
      "Jinfa Huang",
      "Chaofan Tao",
      "Shen Yan",
      "Huaxiu Yao",
      "Lingpeng Kong",
      "Hongxia Yang",
      "Mi Zhang",
      "Guillermo Sapiro",
      "Jiebo Luo",
      "Ping Luo",
      "Ngai Wong"
    ]
  },
  "https://openreview.net/forum?id=M7Lhr2anjg": {
    "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofiane ENNADIR",
      "Gabriela Zarzar Gandler",
      "Filip Cornell",
      "Lele Cao",
      "Oleg Smirnov",
      "Tianze Wang",
      "Levente Zólyomi",
      "Björn Brinne",
      "Sahar Asadi"
    ]
  },
  "https://openreview.net/forum?id=vz5P1Kbt6t": {
    "title": "Adaptive Physics-informed Neural Networks: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edgar Torres",
      "Mathias Niepert"
    ]
  },
  "https://openreview.net/forum?id=1nO4qFMiS0": {
    "title": "Open Problems in Technical AI Governance",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anka Reuel",
      "Benjamin Bucknall",
      "Stephen Casper",
      "Timothy Fist",
      "Lisa Soder",
      "Onni Aarne",
      "Lewis Hammond",
      "Lujain Ibrahim",
      "Alan Chan",
      "Peter Wills",
      "Markus Anderljung",
      "Ben Garfinkel",
      "Lennart Heim",
      "Andrew Trask",
      "Gabriel Mukobi",
      "Rylan Schaeffer",
      "Mauricio Baker",
      "Sara Hooker",
      "Irene Solaiman",
      "Sasha Luccioni",
      "Nitarshan Rajkumar",
      "Nicolas Moës",
      "Jeffrey Ladish",
      "David Bau",
      "Paul Bricman",
      "Neel Guha",
      "Jessica Newman",
      "Yoshua Bengio",
      "Tobin South",
      "Alex Pentland",
      "Sanmi Koyejo",
      "Mykel Kochenderfer",
      "Robert Trager"
    ]
  },
  "https://openreview.net/forum?id=FJgtVfUxLQ": {
    "title": "A Survey on the Honesty of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ]
  },
  "https://openreview.net/forum?id=QTsJXSvAI2": {
    "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amer Essakine",
      "Yanqi Cheng",
      "Chun-Wun Cheng",
      "Lipei Zhang",
      "Zhongying Deng",
      "Lei Zhu",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=sZdtTJInUg": {
    "title": "Class Incremental Learning from First Principles: A Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Ashtekar",
      "Jingxi Zhu",
      "Vasant G Honavar"
    ]
  },
  "https://openreview.net/forum?id=ukLxqA8zXj": {
    "title": "Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Hedström",
      "Philine Lou Bommer",
      "Thomas F Burns",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=RGsdAwWuu6": {
    "title": "Unified Risk Analysis for Weakly Supervised Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=YxKJihRcby": {
    "title": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SUBBA REDDY OOTA",
      "Zijiao Chen",
      "Manish Gupta",
      "Bapi Raju Surampudi",
      "Gael Jobard",
      "Frederic Alexandre",
      "Xavier Hinaut"
    ]
  },
  "https://openreview.net/forum?id=WUQsBiJqyP": {
    "title": "A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiliang Liu",
      "Sheng Xu",
      "Shicheng Liu",
      "Ashish Gaurav",
      "Sriram Ganapathi Subramanian",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=wZLWuFHxt5": {
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "XIAOYU XU",
      "Xiaobao Wu",
      "Jie Fu",
      "Feng Yichao",
      "Fengjun Pan",
      "Anh Tuan Luu"
    ]
  },
  "https://openreview.net/forum?id=RJT1baPhdV": {
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ]
  },
  "https://openreview.net/forum?id=ON7dtdEHVQ": {
    "title": "(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=7A96yteeF9": {
    "title": "Latent mixed-effect models for high-dimensional longitudinal data",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priscilla Ong",
      "Manuel Haussmann",
      "Otto Lönnroth",
      "Harri Lähdesmäki"
    ]
  },
  "https://openreview.net/forum?id=9L0B5N5hUX": {
    "title": "Investigating Generalization Behaviours of Generative Flow Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lazar Atanackovic",
      "Emmanuel Bengio"
    ]
  },
  "https://openreview.net/forum?id=xdWP1d8BxI": {
    "title": "Sparse Decomposition of Graph Neural Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Hu",
      "Mai Zeng",
      "Ge Zhang",
      "Pavel Rumiantsev",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=5298fKGmv3": {
    "title": "The BrowserGym Ecosystem for Web Agent Research",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Le Sellier de Chezelles",
      "Maxime Gasse",
      "Alexandre Lacoste",
      "Massimo Caccia",
      "Alexandre Drouin",
      "Léo Boisvert",
      "Megh Thakkar",
      "Tom Marty",
      "Rim Assouel",
      "Sahar Omidi Shayegan",
      "Lawrence Keunho Jang",
      "Xing Han Lù",
      "Ori Yoran",
      "Dehan Kong",
      "Frank F. Xu",
      "Siva Reddy",
      "Graham Neubig",
      "Quentin Cappart",
      "Russ Salakhutdinov",
      "Nicolas Chapados"
    ]
  },
  "https://openreview.net/forum?id=SbGt90dxdp": {
    "title": "Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Rumiantsev",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=FcyHZ6Q4k0": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=jrUUk5Fskm": {
    "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonios Valkanas",
      "Yuening Wang",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=hGaWq5Buj7": {
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussein Mozannar",
      "Valerie Chen",
      "Mohammed Alsobay",
      "Subhro Das",
      "Sebastian Zhao",
      "Dennis Wei",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Ameet Talwalkar",
      "David Sontag"
    ]
  }
}