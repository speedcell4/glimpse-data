{
  "https://openreview.net/forum?id=uKZ0R4IQaO": {
    "title": "Dynamic Pricing in the Linear Valuation Model using Shape Constraints",
    "volume": "main",
    "abstract": "We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model eliminating the need for tuning parameters commonly required by existing methods. Previous works have addressed the challenge of unknown market noise distribution $F_0$ using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the assumption that $F_0$ satisfies Lipschitz (or stronger) conditions. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\\alpha$-H\\\"older continuous for some $\\alpha \\in (0,1]$, for which we derive a regret upper bound. Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains lower empirical regret in comparison to several existing methods in the literature while offering the advantage of being tuning-parameter free",
    "checked": true,
    "id": "8e9541327a293fa72d43c2ec2f2b0e0bfb66349b",
    "semantic_title": "dynamic pricing in the linear valuation model using shape constraints",
    "citation_count": 1,
    "authors": [
      "Daniele Bracale",
      "Moulinath Banerjee",
      "Yuekai Sun",
      "Salam Turki",
      "Kevin Stoll"
    ]
  },
  "https://openreview.net/forum?id=9Xj5w4DX0t": {
    "title": "Rank Suggestion in Non-negative Matrix Factorization: Residual Sensitivity to Initial Conditions (RSIC)",
    "volume": "main",
    "abstract": "Determining the appropriate rank in Non-negative Matrix Factorization (NMF) is a critical challenge that often requires extensive parameter tuning and domain-specific knowledge. Traditional methods for rank determination focus on identifying a single optimal rank, which may not capture the complex structure inherent in real-world datasets. In this study, we introduce a novel approach called Residual Sensitivity to Intial Conditions (RSIC) that suggests potentially multiple ranks of interest by analyzing the sensitivity of the relative residuals (e.g., relative reconstruction error) to different initializations. By computing the Mean Coordinatewise Interquartile Range (MCI) of the residuals across multiple random initializations, our method identifies regions where the NMF solutions are less sensitive to initial conditions and potentially more meaningful. We evaluate RSIC on a diverse set of datasets, including single-cell gene expression data, image data, and text data, and compare it against current state-of-the-art rank determination methods. Our experiments demonstrate that RSIC effectively identifies relevant ranks consistent with the underlying structure of the data, outperforming traditional methods in scenarios where they are computationally infeasible or less accurate. This approach provides a more scalable and generalizable solution for rank determination in NMF that does not rely on domain-specific knowledge or assumptions",
    "checked": true,
    "id": "ef655348d40b0fc09f1b126af8d822476bb07f3c",
    "semantic_title": "rank suggestion in non-negative matrix factorization: residual sensitivity to initial conditions (rsic)",
    "citation_count": 0,
    "authors": [
      "Marc A. Tunnell",
      "Zachary DeBruine",
      "Erin Carrier"
    ]
  },
  "https://openreview.net/forum?id=cFmmaxkD5A": {
    "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
    "volume": "main",
    "abstract": "Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at https://github.com/Alexiland/MLO-MAE",
    "checked": true,
    "id": "3b8fc96f41c8ce86b2b8b3b872407e8d1644c944",
    "semantic_title": "downstream task guided masking learning in masked autoencoders using multi-level optimization",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Ramtin Hosseini",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Ranak Roy Chowdhury",
      "Rajesh K. Gupta",
      "Pengtao Xie"
    ]
  },
  "https://openreview.net/forum?id=ntGPYNUF3t": {
    "title": "Latte: Latent Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, \\textit{i.e.}, FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation",
    "checked": true,
    "id": "e0eac8c64be3313e581c28a495bec192e7e67284",
    "semantic_title": "latte: latent diffusion transformer for video generation",
    "citation_count": 207,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Gengyun Jia",
      "Ziwei Liu",
      "Yuan-Fang Li",
      "Cunjian Chen",
      "Yu Qiao"
    ]
  },
  "https://openreview.net/forum?id=LJHVPWNnV6": {
    "title": "Graph Potential Field Neural Network for Massive Agents Group-wise Path Planning",
    "volume": "main",
    "abstract": "Multi-agent path planning is important in both multi-agent path finding and multi-agent reinforcement learning areas. However, continual group-wise multi-agent path planning that requires the agents to perform as a team to pursue high team scores instead of individually is less studied. To address this problem, we propose a novel graph potential field-based neural network (GPFNN), which models a valid potential field map for path planning. Our GPFNN unfolds the T-step iterative optimization of the potential field maps as a T-layer feedforward neural network. Thus, a deeper GPFNN leads to more precise potential field maps without the over-smoothing issue. A potential field map inherently provides a monotonic potential flow from any source node to the target nodes to construct the optimal path (w.r.t. the potential decay), equipping our GPFNN with an elegant planning ability. Moreover, we incorporate dynamically updated boundary conditions into our GPFNN to address group-wise multi-agent path planning that supports both static targets and dynamic moving targets. Empirically, experiments on three different-sized mazes (up to $1025 \\times 1025$ sized mazes) with up to 1,000 agents demonstrate the planning ability of our GPFNN to handle both static and dynamic moving targets. Experiments on extensive graph node classification tasks on six graph datasets (up to millions of nodes) demonstrate the learning ability of our GPFNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming Lyu",
      "Xiaowei Zhou",
      "Xingrui Yu",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=JT2KMuo2BV": {
    "title": "Rethinking Patch Dependence for Masked Autoencoders",
    "volume": "main",
    "abstract": "In this work, we examine the impact of inter-patch dependencies in the decoder of masked autoencoders (MAE) on representation learning. We decompose the decoding mechanism for masked reconstruction into self-attention between mask tokens and cross-attention between masked and visible tokens. Our findings reveal that MAE reconstructs coherent images from visible patches not through interactions between patches in the decoder but by learning a global representation within the encoder. This discovery leads us to propose a simple visual pretraining framework: cross-attention masked autoencoders (CrossMAE). This framework employs only cross-attention in the decoder to independently read out reconstructions for a small subset of masked patches from encoder outputs. This approach achieves comparable or superior performance to traditional MAE across models ranging from ViT-S to ViT-H and significantly reduces computational requirements. By its design, CrossMAE challenges the necessity of interaction between mask tokens for effective masked pretraining. Code and models are publicly available: https://crossmae.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Fu",
      "Long Lian",
      "Renhao Wang",
      "Baifeng Shi",
      "XuDong Wang",
      "Adam Yala",
      "Trevor Darrell",
      "Alexei A Efros",
      "Ken Goldberg"
    ]
  },
  "https://openreview.net/forum?id=FkKBxp0FhR": {
    "title": "A Systematic Evaluation of the Planning and Scheduling Abilities of the Reasoning Model o1",
    "volume": "main",
    "abstract": "OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive Large Language Models (LLMs)–making it a new kind of model: a Large Reasoning Model (LRM)–and be generally capable of tackling procedural reasoning tasks. We present the first comprehensive evaluation of these models on the fundamental tasks of planning and scheduling. Previous research attempted to use LLMs' expressive generation capabilities to solve these problems, but met with only limited success. We fill in the gaps in this literature by testing a larger suite of state-of-the-art LLMs on a set of large benchmarks, and then use this as a baseline to evaluate o1-preview and o1-mini. We see that while they can offer significant accuracy improvements over LLMs, this single metric is misleading and incomplete, as LRM queries demand large and unpredictable costs and take significant amounts of time to complete. We provide a case study demonstrating that, at those same price points, other methods of inference time scaling can do just as well. We also show that, contrary to OpenAI's injunctions, o1's performance can be improved further by embedding it in compound systems that separately, but complementarily, scale inference time further. Finally, while the paper is focused on o1, we provide similar evaluations of a more recent (and open-weight) LRM -- DeepSeek R1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Atharva Gundawar",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=7bIfe2I7bK": {
    "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models",
    "volume": "main",
    "abstract": "The visual world is fundamentally compositional. Visual scenes are defined by the composition of objects and their relations. Hence, it is essential for computer vision systems to reflect and exploit this compositionality to achieve robust and generalizable scene understanding. While major strides have been made toward the development of general-purpose, multimodal generative models, including both text-to-image models and multimodal vision-language models, it remains unclear whether these systems are capable of accurately generating and interpreting scenes involving the composition of multiple objects and relations. In this work, we present an evaluation of the compositional visual processing capabilities in the current generation of text-to-image (DALL-E 3) and multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5, QWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these systems to human participants. The results suggest that these systems display some ability to solve compositional and relational tasks, showing notable improvements over the previous generation of multimodal models, but with performance nevertheless well below the level of human participants, particularly for more complex scenes involving many (>5) objects and multiple relations. These results highlight the need for further progress toward compositional understanding of visual scenes",
    "checked": true,
    "id": "670aed8dcbd20ed5f8c8228497f434e191106228",
    "semantic_title": "evaluating compositional scene understanding in multimodal generative models",
    "citation_count": 1,
    "authors": [
      "Shuhao Fu",
      "Andrew Jun Lee",
      "Yixin Anna Wang",
      "Ida Momennejad",
      "Trevor Bihl",
      "Hongjing Lu",
      "Taylor Whittington Webb"
    ]
  },
  "https://openreview.net/forum?id=3jdI0aEW3k": {
    "title": "Distributed and Secure Kernel-Based Quantum Machine Learning",
    "volume": "main",
    "abstract": "Quantum computing promises to revolutionize machine learning, offering significant efficiency gains for tasks such as clustering and distance estimation. Additionally, it provides enhanced security through fundamental principles like the measurement postulate and the no-cloning theorem, enabling secure protocols such as quantum teleportation and quantum key distribution. While advancements in secure quantum machine learning are notable, the development of secure and distributed quantum analogs of kernel-based machine learning techniques remains underexplored. In this work, we present a novel approach for securely computing three commonly used kernels: the polynomial, radial basis function (RBF), and Laplacian kernels, when data is distributed, using quantum feature maps. Our methodology formalizes a robust framework that leverages quantum teleportation to enable secure and distributed kernel learning. The proposed architecture is validated using IBM's Qiskit Aer Simulator on various public datasets",
    "checked": true,
    "id": "764cfd1d64a18a869b41942b7bc64cc518cc8799",
    "semantic_title": "distributed and secure kernel-based quantum machine learning",
    "citation_count": 0,
    "authors": [
      "Arjhun Swaminathan",
      "Mete Akgün"
    ]
  },
  "https://openreview.net/forum?id=X3gSvQjShh": {
    "title": "An Embedding is Worth a Thousand Noisy Labels",
    "volume": "main",
    "abstract": "The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems. Existing strategies to address label noise exhibit severe limitations due to computational complexity and application dependency. In this work, we propose WANN, a Weighted Adaptive Nearest Neighbor approach that builds on self-supervised feature representations obtained from foundation models. To guide the weighted voting scheme, we introduce a reliability score $\\eta$, which measures the likelihood of a data label being correct. WANN outperforms reference methods, including a linear layer trained with robust loss functions, on diverse datasets of varying size and under various noise types and severities. WANN also exhibits superior generalization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed k-NNs. Furthermore, the proposed weighting scheme enhances supervised dimensionality reduction under noisy labels. This yields a significant boost in classification performance with 10x and 100x smaller image embeddings, minimizing latency and storage requirements. Our approach, emphasizing efficiency and explainability, emerges as a simple, robust solution to overcome inherent limitations of deep neural network training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Salvo",
      "Sebastian Doerrich",
      "Ines Rieger",
      "Christian Ledig"
    ]
  },
  "https://openreview.net/forum?id=gxUp2d4JTw": {
    "title": "LTL-Constrained Policy Optimization with Cycle Experience Replay",
    "volume": "main",
    "abstract": "Linear Temporal Logic (LTL) offers a precise means for constraining the behavior of reinforcement learning agents. However, in many settings where both satisfaction and optimality conditions are present, LTL is insufficient to capture both. Instead, LTL-constrained policy optimization, where the goal is to optimize a scalar reward under LTL constraints, is needed. This constrained optimization problem proves difficult in deep Reinforcement Learning (DRL) settings, where learned policies often ignore the LTL constraint due to the sparse nature of LTL satisfaction. To alleviate the sparsity issue, we introduce Cycle Experience Replay (CyclER), a novel reward shaping technique that exploits the underlying structure of the LTL constraint to guide a policy towards satisfaction by encouraging partial behaviors compliant with the constraint. We provide a theoretical guarantee that optimizing CyclER will achieve policies that satisfy the LTL constraint with near-optimal probability. We evaluate CyclER in three continuous control domains. Our experimental results show that optimizing CyclER in tandem with the existing scalar reward outperforms existing reward-shaping methods at finding performant LTL-satisfying policies",
    "checked": true,
    "id": "ac80c658a9e1d428734ec0848b2ad7099d8cb110",
    "semantic_title": "ltl-constrained policy optimization with cycle experience replay",
    "citation_count": 1,
    "authors": [
      "Ameesh Shah",
      "Cameron Voloshin",
      "Chenxi Yang",
      "Abhinav Verma",
      "Swarat Chaudhuri",
      "Sanjit A. Seshia"
    ]
  },
  "https://openreview.net/forum?id=I1gALvbRxj": {
    "title": "Bézier Flow: a Surface-wise Gradient Descent Method for Multi-objective Optimization",
    "volume": "main",
    "abstract": "This paper proposes a framework to construct a multi-objective optimization algorithm from a single-objective optimization algorithm by using the Bézier simplex model. Additionally, we extend the stability of optimization algorithms in the sense of Probably Approximately Correct (PAC) learning and define the PAC stability. We prove that it leads to an upper bound on the generalization error with high probability. Furthermore, we show that multi-objective optimization algorithms derived from a gradient descent-based single-objective optimization algorithm are PAC stable. We conducted numerical experiments with synthetic and real multi-objective optimization problem instances and demonstrated that our method achieved lower generalization errors than the existing multi-objective optimization algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Sannai",
      "Yasunari Hikima",
      "Ken Kobayashi",
      "Akinori Tanaka",
      "Naoki Hamada"
    ]
  },
  "https://openreview.net/forum?id=SBM9yeNZz5": {
    "title": "Maximising the Utility of Validation Sets for Imbalanced Noisy-label Meta-learning",
    "volume": "main",
    "abstract": "Meta-learning is an effective method to handle imbalanced and noisy-label learning, but it generally depends on a clean validation set. Unfortunately, this validation set has poor scalability when the number of classes increases, as traditionally these samples need to be randomly selected, manually labelled and balanced-distributed. This problem therefore has motivated the development of meta-learning methods to automatically select validation samples that are likely to have clean labels and balanced class distribution. Unfortunately, a common missing point of existing meta-learning methods for noisy label learning is the lack of consideration for data informativeness when constructing the validation set. The construction of an informative validation set requires hard samples, i.e., samples that the model has low confident prediction, but these samples are more likely to be noisy, which can degrade the meta reweighting process. Therefore, the balance between sample informativeness and cleanness is an important criteria for validation set optimization. In this paper, we propose new criteria to characterise the utility of such meta-learning validation sets, based on: 1) sample informativeness; 2) balanced class distribution; and 3) label cleanliness. We also introduce a new imbalanced noisy-label meta-learning (INOLML) algorithm that auto- matically builds a validation set by maximising such utility criteria. The proposed method shows state-of-the-art (SOTA) results compared to previous meta-learning and noisy-label learning approaches on several noisy-label learning benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Anh Dung",
      "Cuong C. Nguyen",
      "Vasileios Belagiannis",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=sSOxuUjE2o": {
    "title": "Controlled Training Data Generation with Diffusion Models",
    "volume": "main",
    "abstract": "We present a method to control a text-to-image generative model to produce training data useful for supervised learning. Unlike previous works that employ an open-loop approach via pre-defined prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system that involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model to find adversarial prompts that result in generated images that maximize the model's loss and, consequently, expose its vulnerabilities. While these adversarial prompts generate training examples curated for improving the given model, they are not curated for a specific target distribution of interest, which can be inefficient. Therefore, we introduce the second feedback mechanism that can optionally guide the generation process towards a desirable target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. The proposed closed-loop system allows us to control the training data generation for a given model and target image distribution. We evaluate on different tasks, datasets, and architectures, with different types of distribution shifts (corruptions, spurious correlations, unseen domains) and illustrate the advantages of the proposed feedback mechanisms compared to open-loop approaches",
    "checked": true,
    "id": "45f9ca4f97fadf37fb7276f26a1d35905077be49",
    "semantic_title": "controlled training data generation with diffusion models",
    "citation_count": 2,
    "authors": [
      "Teresa Yeo",
      "Andrei Atanov",
      "Harold Luc Benoit",
      "Aleksandr Alekseev",
      "Ruchira Ray",
      "Pooya Esmaeil Akhoondi",
      "Amir Zamir"
    ]
  },
  "https://openreview.net/forum?id=Okxp1W8If0": {
    "title": "(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum",
    "volume": "main",
    "abstract": "Stochastic heavy ball momentum (SHB) is commonly used to train machine learning models, and often provides empirical improvements over stochastic gradient descent. By primarily focusing on strongly-convex quadratics, we aim to better understand the theoretical advantage of SHB and subsequently improve the method. For strongly-convex quadratics, Kidambi et al. (2018) show that SHB (with a mini-batch of size $1$) cannot attain accelerated convergence, and hence has no theoretical benefit over SGD. They conjecture that the practical gain of SHB is a by-product of using larger mini-batches. We first substantiate this claim by showing that SHB can attain an accelerated rate when the mini-batch size is larger than a threshold $b^*$ that depends on the condition number $\\kappa$. Specifically, we prove that with the same step-size and momentum parameters as in the deterministic setting, SHB with a sufficiently large mini-batch size results in an $O\\left(\\exp(-\\frac{T}{\\sqrt{\\kappa}}) + \\sigma \\right)$ convergence when measuring the distance to the optimal solution in the $\\ell_2$ norm, where $T$ is the number of iterations and $\\sigma^2$ is the variance in the stochastic gradients. We prove a lower-bound which demonstrates that a $\\kappa$ dependence in $b^*$ is necessary. To ensure convergence to the minimizer, we design a noise-adaptive multi-stage algorithm that results in an $O\\left(\\exp\\left(-\\frac{T}{\\sqrt{\\kappa}}\\right) + \\frac{\\sigma}{\\sqrt{T}}\\right)$ rate when measuring the distance to the optimal solution in the $\\ell_2$ norm. We also consider the general smooth, strongly-convex setting and propose the first noise-adaptive SHB variant that converges to the minimizer at an $O(\\exp(-\\frac{T}{\\kappa}) + \\frac{\\sigma^2}{T})$ rate when measuring the distance to the optimal solution in the squared $\\ell_2$ norm. We empirically demonstrate the effectiveness of the proposed algorithms",
    "checked": true,
    "id": "f340532ef3da3400b4310531ec549b1767bba953",
    "semantic_title": "(accelerated) noise-adaptive stochastic heavy-ball momentum",
    "citation_count": 0,
    "authors": [
      "Anh Quang Dang",
      "Reza Babanezhad Harikandeh",
      "Sharan Vaswani"
    ]
  },
  "https://openreview.net/forum?id=nWk5OtZ7ze": {
    "title": "Quantile Activation: Correcting a failure mode of traditional ML models",
    "volume": "main",
    "abstract": "Standard ML models fail to infer the context distribution and suitably adapt. For instance, the learning fails when the underlying distribution is actually a mixture of distributions with contradictory labels. Learning also fails if there is a shift between train and test distributions. Standard neural network architectures like MLPs or CNNs are not equipped to handle this. In this article, we propose a simple activation function, quantile activation (QAct), that addresses this problem without significantly increasing computational costs. The core idea is to \"adapt\" the outputs of each neuron to its context distribution. The proposed quantile activation (QAct) outputs the relative quantile position of neuron activations within their context distribution, diverging from the direct numerical outputs common in traditional networks. A specific case of the above failure mode is when there is an inherent distribution shift, i.e the test distribution differs slightly from the train distribution. We validate the proposed activation function under covariate shifts, using datasets designed to test robustness against distortions. Our results demonstrate significantly better generalisation across distortions compared to conventional classifiers and other adaptive methods, across various architectures. Although this paper presents a proof of concept, we find that this approach unexpectedly outperforms DINOv2 (small), despite DINOv2 being trained with a much larger network and dataset",
    "checked": false,
    "id": "451d57bf84f776b631fddaf05127169838e71051",
    "semantic_title": "quantile activation: correcting a failure mode of ml models",
    "citation_count": 0,
    "authors": [
      "Aditya Challa",
      "Sravan Danda",
      "Laurent Najman",
      "Snehanshu Saha"
    ]
  },
  "https://openreview.net/forum?id=hCyT4RsF27": {
    "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
    "volume": "main",
    "abstract": "Graphs are growing rapidly and so are the number of different categories associated with it. Applications like e-commerce, healthcare, recommendation systems, and various social media platforms are rapidly moving towards graph representation of data due to their ability to capture both structural and attribute information. One crucial task in graph analysis is node classification, where unlabeled nodes are categorized into predefined classes. In practice, novel classes appear incrementally sometimes with just a few labels (seen classes) or even without any labels (unseen classes), either because they are new or haven't been explored much. Traditional methods assume abundant labeled data for training, which isn't always feasible. We investigate a broader objective: Graph Class Incremental Learning under Weak Supervision (GCL), addressing this challenge by meta-training on base classes with limited labeled instances. During the incremental streams, novel classes can have few-shot or zero-shot representation. Our proposed framework GOTHAM efficiently accommodates these unlabeled nodes by finding the closest prototype representation, serving as class representatives in the attribute space. For Text-Attributed Graphs (TAGs), our framework additionally incorporates semantic information to enhance the representation. By employing teacher-student knowledge distillation to mitigate forgetting, GOTHAM achieves promising results across various tasks. Experiments on datasets such as Cora-ML, Amazon, and OBGN-Arxiv showcase the effectiveness of our approach in handling evolving graph data under limited supervision",
    "checked": true,
    "id": "4f4b5bcb828c5e8171092c48a56780c7d2113e90",
    "semantic_title": "gotham: graph class incremental learning framework under weak supervision",
    "citation_count": 0,
    "authors": [
      "Aditya Hemant Shahane",
      "Prathosh AP",
      "Sandeep Kumar"
    ]
  },
  "https://openreview.net/forum?id=dNWaTuKV9M": {
    "title": "Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning",
    "volume": "main",
    "abstract": "The primary objective of methods in continual learning is to learn tasks in a sequential manner over time (sometimes from a stream of data), while mitigating the detrimental phenomenon of catastrophic forgetting. This paper proposes a method to learn an effective representation between previous and newly encountered class prototypes. We propose a prototypical network with a Bayesian learning-driven contrastive loss (BLCL), tailored specifically for class-incremental learning scenarios. We introduce a contrastive loss that incorporates novel classes into the latent representation by reducing intra-class and increasing inter-class distance. Our approach dynamically adapts the balance between the cross-entropy and contrastive loss functions with a Bayesian learning technique. Experimental results conducted on the CIFAR-10, CIFAR-100, and ImageNet100 datasets for image classification and images of a GNSS-based dataset for interference classification validate the efficacy of our method, showcasing its superiority over existing state-of-the-art approaches",
    "checked": true,
    "id": "655b66f54d2a9ce2f5c0e66db1fdacdad06aa9f9",
    "semantic_title": "bayesian learning-driven prototypical contrastive loss for class-incremental learning",
    "citation_count": 4,
    "authors": [
      "Nisha L. Raichur",
      "Lucas Heublein",
      "Tobias Feigl",
      "Alexander Rügamer",
      "Christopher Mutschler",
      "Felix Ott"
    ]
  },
  "https://openreview.net/forum?id=s1zfBJysbI": {
    "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. This approach ensures the temporal consistency in the edited videos while maintaining high fidelity to the editing text prompt. We further propose a recursive and ensembled refinement by revisiting the denoising step and guidance scale used in video diffusion process with Video-3DGS. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (×1.9, ×4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos",
    "checked": true,
    "id": "4437854b36a71d150cda9812c2e9f337f60f457d",
    "semantic_title": "enhancing temporal consistency in video editing by reconstructing videos with 3d gaussian splatting",
    "citation_count": 1,
    "authors": [
      "Inkyu Shin",
      "Qihang Yu",
      "Xiaohui Shen",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ]
  },
  "https://openreview.net/forum?id=GXlsrvOGIK": {
    "title": "On Learning Representations for Tabular Data Distillation",
    "volume": "main",
    "abstract": "Dataset distillation generates a small set of information-rich instances from a large dataset, resulting in reduced storage requirements, privacy or copyright risks, and computational costs for downstream modeling, though much of the research has focused on the image data modality. We study tabular data distillation, which brings in novel challenges such as the inherent feature heterogeneity and the common use of non-differentiable learning models (such as decision tree ensembles and nearest-neighbor predictors). To mitigate these challenges, we present $\\texttt{TDColER}$, a tabular data distillation framework via column embeddings-based representation learning. To evaluate this framework, we also present a tabular data distillation benchmark, ${{\\sf \\small TDBench}}$. Based on an elaborate evaluation on ${{\\sf \\small TDBench}}$, resulting in 226,200 distilled datasets and 541,980 models trained on them, we demonstrate that $\\texttt{TDColER}$ is able to boost the distilled data quality of off-the-shelf distillation schemes by 0.5-143% across 7 different tabular learning models. All of the code used in the experiments can be found in http://github.com/inwonakng/tdbench",
    "checked": true,
    "id": "7978a8bdb10a1eadd9a3ab772d6fc8be4f961e4e",
    "semantic_title": "on learning representations for tabular data distillation",
    "citation_count": 0,
    "authors": [
      "Inwon Kang",
      "Parikshit Ram",
      "Yi Zhou",
      "Horst Samulowitz",
      "Oshani Seneviratne"
    ]
  },
  "https://openreview.net/forum?id=baZLwdphqw": {
    "title": "Stabilizing the Kumaraswamy Distribution",
    "volume": "main",
    "abstract": "Large-scale latent variable models require expressive continuous distributions that support efficient sampling and low-variance differentiation, achievable through the reparameterization trick. The Kumaraswamy (KS) distribution is both expressive and supports the reparameterization trick with a simple closed-form inverse CDF. Yet, its adoption remains limited. We identify and resolve numerical instabilities in the log-pdf, CDF, and inverse CDF, exposing issues in libraries like PyTorch and TensorFlow. We then introduce simple and scalable latent variable models to address exploration-exploitation trade-offs in contextual multi-armed bandits and facilitate uncertainty quantification for link prediction with graph neural networks. We find these models to be most performant when paired with the stable KS. Our results support the stabilized KS distribution as a core component in scalable variational models for bounded latent variables",
    "checked": true,
    "id": "aba259db543fd9c00a7884eb73efab1aa4ac6c91",
    "semantic_title": "stabilizing the kumaraswamy distribution",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Gonzalo Mateos"
    ]
  },
  "https://openreview.net/forum?id=AHTz2mTlKk": {
    "title": "Empirical Bayes Trend Filtering Through a Variational Inference Framework",
    "volume": "main",
    "abstract": "This paper introduces a novel framework for Bayesian trend filtering using an empirical Bayes approach and a variational inference algorithm. Trend filtering is a nonparametric regression technique that has gained popularity for its simple formulation and local adaptability. Bayesian adaptations of trend filtering have been proposed as an alternative method, while they often rely on computationally intensive sampling-based methods for posterior inference. We propose an empirical Bayes trend filtering (EBTF) that leverages shrinkage priors, estimated through an empirical Bayes procedure by maximizing the marginal likelihood. To address the computational challenges posed by large datasets, we implement a variational inference algorithm for posterior computation, ensuring scalability and efficiency. Our framework is flexible, allowing the incorporation of various shrinkage priors, and optimizes the level of smoothness directly from the data. We also discuss alternative formulations of the EBTF model, along with their pros and cons. We demonstrate the performance of our EBTF method through comprehensive simulations and real-world data applications, highlighting its ability to maintain computational efficiency while providing accurate trend estimation",
    "checked": false,
    "id": "f0ff40fbdaf24354eeee2b84eb435c1bccaa7baa",
    "semantic_title": "mimo detection with spatial sigma-delta adcs: a variational bayesian approach",
    "citation_count": 1,
    "authors": [
      "Dongyue Xie"
    ]
  },
  "https://openreview.net/forum?id=MJOKrHqiV1": {
    "title": "Multi-Output Distributional Fairness via Post-Processing",
    "volume": "main",
    "abstract": "The post-processing approaches are becoming prominent techniques to enhance machine learning models' fairness because of their intuitiveness, low computational cost, and excellent scalability. However, most existing post-processing methods are designed for task-specific fairness measures and are limited to single-output models. In this paper, we introduce a post-processing method for multi-output models, such as the ones used for multi-task/multi-class classification and representation learning, to enhance a model's distributional parity, a task-agnostic fairness measure. Existing methods for achieving distributional parity rely on the (inverse) cumulative density function of a model's output, restricting their applicability to single-output models. Extending previous works, we propose to employ optimal transport mappings to move a model's outputs across different groups towards their empirical Wasserstein barycenter. An approximation technique is applied to reduce the complexity of computing the exact barycenter and a kernel regression method is proposed to extend this process to out-of-sample data. Our empirical studies evaluate the proposed approach against various baselines on multi-task/multi-class classification and representation learning tasks, demonstrating the effectiveness of the proposed approach",
    "checked": true,
    "id": "b5025e687cb2001e92ea2872d1e9d9db6e4179c1",
    "semantic_title": "multi-output distributional fairness via post-processing",
    "citation_count": 0,
    "authors": [
      "Gang Li",
      "Qihang Lin",
      "Ayush Ghosh",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=uJELgNGiMW": {
    "title": "Meta-Learning to Teach Semantic Prompts for Open Domain Generalization in Vision-Language Models",
    "volume": "main",
    "abstract": "Open Domain Generalization (ODG) addresses the challenges posed by domain and category shifts between labeled training sources and unlabeled target domains. Current state-of-the-art methods struggle with the limitations of traditional CNN backbones, leading to reduced generalization and increased error rates in detecting target open samples without prior knowledge. Additionally, recent CLIP-based prompt learning approaches fail to distinguish between known and unknown classes effectively, resulting in suboptimal performance. To address these challenges, we propose MetaPrompt, which leverages the semantic strengths of the vision-language model CLIP and the ''learning-to-learn'' capabilities of Meta-Learning to achieve robust generalization across domain and category shifts. Our framework introduces three key innovations: First, we approach ODG as a multi-class classification problem that includes both known and novel categories, designing novel prompts capable of detecting unknown class samples across multiple domains. These prompts are trained using Meta-Learning with momentum updates, enabling smooth and accurate differentiation between known and unknown classes. Second, we introduce a novel domain-agnostic semantic attention-based prompt alongside domain-focused prompts to enhance robustness in classifying unknown classes across various domains. Finally, we incorporate an unsupervised contrastive loss during episodic Meta-Training, which reinforces the boundaries in the metric space between known and unknown classes, thereby enhancing ''unknown'' class awareness in the prompts. MetaPrompt has demonstrated its superiority through extensive testing on diverse datasets, excelling in both closed and open-set DG scenarios and consistently outperforming existing solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirsha Bose",
      "Mainak Singha",
      "Ankit Jha",
      "Souradeep Mukhopadhyay",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=nay3Kvw8BD": {
    "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
    "volume": "main",
    "abstract": "Large-scale machine learning (ML) models are increasingly being used in critical domains like education, lending, recruitment, healthcare, criminal justice, etc. However, the training, deployment, and utilization of these models demand substantial computational resources. To decrease computation and memory costs, machine learning models with sparse weight matrices are widely used in the literature. Among sparse models, those with special sparse structures (e.g., models with block-wise sparse weight matrices) fit better with the hardware accelerators and can decrease the memory and computation costs during the inference. Unfortunately, while there are several efficient training methods, none of them are designed to train a block-wise sparse model efficiently. As a result, the current methods for training block-wise sparse models start with full and dense models leading to inefficient training. In this work, we focus on training models with \\textit{block-wise sparse matrices} and propose an efficient training algorithm to decrease both computation and memory costs during training and inference. In addition, we will show that our proposed method enables us to efficiently find the right block size for the sparsity pattern during the training process. Our extensive empirical and theoretical analyses show that our algorithms can decrease the computation and memory costs significantly without a performance drop compared to baselines",
    "checked": true,
    "id": "df5f3641603273efdeedbed9052fa558c383d2b2",
    "semantic_title": "an efficient training algorithm for models with block-wise sparsity",
    "citation_count": 0,
    "authors": [
      "Ding Zhu",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili"
    ]
  },
  "https://openreview.net/forum?id=OTwnNBxZFB": {
    "title": "Almost Sure Convergence of Stochastic Gradient Methods under Gradient Domination",
    "volume": "main",
    "abstract": "Stochastic gradient methods are among the most important algorithms in training machine learning problems. While classical assumptions such as strong convexity allow a simple analysis they are rarely satisfied in applications. In recent years, global and local gradient domination properties have shown to be a more realistic replacement of strong convexity. They were proved to hold in diverse settings such as (simple) policy gradient methods in reinforcement learning and training of deep neural networks with analytic activation functions. We prove almost sure convergence rates $f(X_n)-f^*\\in o\\big( n^{-\\frac{1}{4\\beta-1}+\\epsilon}\\big)$ of the last iterate for stochastic gradient descent (with and without momentum) under global and local $\\beta$-gradient domination assumptions. The almost sure rates get arbitrarily close to recent rates in expectation. Finally, we demonstrate how to apply our results to the training task in both supervised and reinforcement learning",
    "checked": false,
    "id": "f7e7c2a5ff982fd08bb81436c81981bd350742d3",
    "semantic_title": "almost sure convergence rates of stochastic gradient methods under gradient domination",
    "citation_count": 3,
    "authors": [
      "Simon Weissmann",
      "Sara Klein",
      "Waïss Azizian",
      "Leif Döring"
    ]
  },
  "https://openreview.net/forum?id=WfAvMdwiE8": {
    "title": "Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot Class-Incremental Tuning of Foundation Models",
    "volume": "main",
    "abstract": "We propose Consistency-guided Asynchronous Contrastive Tuning (CoACT), a novel method for continuously tuning foundation models to learn new classes in few-shot settings. CoACT consists of three key components: (i) asynchronous contrastive tuning, which learns new classes by including LoRA modules in the pre-trained encoder while enforcing consistency between two asynchronous encoders; (ii) controlled fine-tuning, which facilitates effective tuning of a subset of the foundation model; and (iii) consistency-guided incremental tuning, which enforces additional regularization during later sessions to reduce forgetting of the learned classes. We evaluate our proposed solution on Few-Shot Class-Incremental Learning (FSCIL) as well as a new and more challenging setup called Few-Shot Class-Incremental Tuning (FSCIT), which facilitates the continual tuning of vision foundation models to learn new classes with only a few samples per class. Unlike traditional FSCIL, FSCIT does not require a large in-distribution base session for initial fully supervised training prior to the incremental few-shot sessions. We conduct extensive evaluations across 16 diverse datasets, demonstrating the effectiveness of CoACT in both FSCIL and FSCIT setups. CoACT outperforms existing methods by up to 5.02% in FSCIL and up to 12.51% in FSCIT for individual datasets, with an average improvement of 2.47%. Furthermore, CoACT exhibits reduced forgetting and enhanced robustness in low-shot experiments. Detailed ablation and sensitivity studies highlight the contribution of each component of CoACT. We make our code publicly available at https://github.com/ShuvenduRoy/CoACT-FSCIL",
    "checked": true,
    "id": "a30c3e02307682c3274036e8be41bada4442a163",
    "semantic_title": "consistency-guided asynchronous contrastive tuning for few-shot class-incremental tuning of foundation models",
    "citation_count": 1,
    "authors": [
      "Shuvendu Roy",
      "Elham Dolatabadi",
      "Arash Afkanpour",
      "Ali Etemad"
    ]
  },
  "https://openreview.net/forum?id=heeJqQXKg7": {
    "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
    "volume": "main",
    "abstract": "Literature reviews are an essential component of scientific research, but they remain time-intensive and challenging to write, especially due to the recent influx of research papers. This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract. We decompose the task into two components: (1) Retrieving related works given a query abstract and (2) Writing a literature review based on the retrieved results. We analyze how effective LLMs are for both components. For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base. Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods while providing insights into the LLM's decision-making process. In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review. To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation protocol to promote additional research and development in this regard. Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning. Particularly, we find that combining keyword-based and document-embedding-based search improves precision and recall during retrieval by 10% and 30%, respectively, compared to using either of the methods in isolation. Further, we demonstrate that our planning-based approach achieves higher-quality reviews by minimizing hallucinated references in the generated review by 18-26% compared to existing simpler LLM-based generation methods. Our project page including a demonstration system and toolkit can be accessed here: https://litllm.github.io",
    "checked": true,
    "id": "3bf29a0420b1042f5e0a319c27cb32d46d9cde3e",
    "semantic_title": "litllms, llms for literature review: are we there yet?",
    "citation_count": 0,
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy Dj Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=M62P7iOT7d": {
    "title": "DeformTime: capturing variable dependencies with deformable attention for time series forecasting",
    "volume": "main",
    "abstract": "In multivariable time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and often overlook the potential of using exogenous variables in enhancing the prediction of the target endogenous variable. To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy. It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB). Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB. We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables. The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 7.2% on average. Notably, performance gains remain consistent across longer forecasting horizons",
    "checked": true,
    "id": "6c5ad359f1ca77ebaca62cae8263c48864b786d4",
    "semantic_title": "deformtime: capturing variable dependencies with deformable attention for time series forecasting",
    "citation_count": 0,
    "authors": [
      "Yuxuan Shu",
      "Vasileios Lampos"
    ]
  },
  "https://openreview.net/forum?id=9kFlOyLwyf": {
    "title": "Latent Covariate Shift: Unlocking Partial Identifiability for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics. Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, which we refer to as the latent content variable. Within this new paradigm, we present an intricate causal generative model by introducing latent noises across domains, along with a latent content variable and a latent style variable to achieve more nuanced rendering of observational data. We demonstrate that the latent content variable can be identified up to block identifiability due to its versatile yet distinct causal structure. We anchor our theoretical insights into a novel MSDA method, which learns the label distribution conditioned on the identifiable latent content variable, thereby accommodating more substantial distribution shifts. The proposed approach showcases exceptional performance and efficacy on both simulated and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Anton van den Hengel",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=Wj8yFjIpom": {
    "title": "$f$-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
    "volume": "main",
    "abstract": "Independent learning is a straightforward solution for fully decentralized learning in cooperative multi-agent reinforcement learning (MARL). The study of independent learning has a history of decades, and the representatives, such as independent Q-learning and independent PPO, can achieve good performances on several benchmarks. However, most independent learning algorithms lack convergence guarantees or theoretical support. In this paper, we propose a general formulation of independent policy optimization, $f$-divergence policy optimization. We hope that a more general policy optimization formulation will provide deeper insights into fully decentralized learning. We demonstrate the generality of this formulation and analyze its limitations. Based on this formulation, we further propose a novel independent learning algorithm, TVPO, which theoretically guarantees convergence. Empirically, we demonstrate that TVPO outperforms state-of-the-art fully decentralized learning methods on three popular cooperative MARL benchmarks, thereby verifying the efficacy of TVPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=vQDKYYuqWA": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "main",
    "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times",
    "checked": true,
    "id": "4b1278b2266ce5009e70f2efe85ccff87350de9c",
    "semantic_title": "vision-language models provide promptable representations for reinforcement learning",
    "citation_count": 19,
    "authors": [
      "William Chen",
      "Oier Mees",
      "Aviral Kumar",
      "Sergey Levine"
    ]
  },
  "https://openreview.net/forum?id=pKilnjQsb0": {
    "title": "Implicit Bias and Fast Convergence Rates for Self-attention",
    "volume": "main",
    "abstract": "We study the fundamental optimization principles of self-attention, the defining mechanism of transformers, by analyzing the implicit bias of gradient-based optimizers in training a self-attention layer with a linear decoder in binary classification. Building on prior studies in linear logistic regression, recent findings demonstrate that the key-query matrix $W_t$ from gradient-descent (GD) converges in direction towards $W_{mm}$, which maximizes the margin between optimal and non-optimal tokens across sequences. However, this convergence is local, dependent on initial conditions, only holds asymptotically as the number of iterations increases, and leaves questions about the potential benefits of adaptive step-size rules unaddressed. To bridge this gap, we first establish scenarios for which convergence is provably global. We then analyze two adaptive step-size strategies: normalized GD and Polyak step-size, demonstrating finite-time convergence rates for $W_t$ to $W_{mm}$, and quantifying the sparsification rate of the attention map. These findings not only show that these strategies can accelerate parameter convergence over standard GD in a non-convex setting but also deepen the understanding of the implicit bias in self-attention, linking it more closely to the phenomena observed in linear logistic regression despite its intricate non-convex nature",
    "checked": true,
    "id": "621fed685b47d70d2b3bf4ed6c7c7c623b5e715d",
    "semantic_title": "implicit bias and fast convergence rates for self-attention",
    "citation_count": 10,
    "authors": [
      "Bhavya Vasudeva",
      "Puneesh Deora",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=sXq3Wb3vef": {
    "title": "Decomposing The Dark Matter of Sparse Autoencoders",
    "volume": "main",
    "abstract": "Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in ``dark matter'': unexplained variance in activations. This work investigates dark matter as an object of study in its own right. Surprisingly, we find that much of SAE dark matter---about half of the error vector itself and $>90\\% $ of its norm---can be linearly predicted from the initial activation vector. Additionally, we find that the scaling behavior of SAE error norms at a per token level is remarkably predictable: larger SAEs mostly struggle to reconstruct the same contexts as smaller SAEs. We build on the linear representation hypothesis to propose models of activations that might lead to these observations, including postulating a new type of ``introduced error''; these insights imply that the part of the SAE error vector that cannot be linearly predicted (``nonlinear'' error) might be fundamentally different from the linearly predictable component. To validate this hypothesis, we empirically analyze nonlinear SAE error and show that 1) it contains fewer not yet learned features, 2) SAEs trained on it are quantitatively worse, 3) it helps predict SAE per-token scaling behavior, and 4) it is responsible for a proportional amount of the downstream increase in cross entropy loss when SAE activations are inserted into the model. Finally, we examine two methods to reduce nonlinear SAE error: inference time gradient pursuit, which leads to a very slight decrease in nonlinear error, and linear transformations from earlier layer SAE outputs, which leads to a larger reduction",
    "checked": true,
    "id": "74485409331b414368616c5acdcaced4f1b4506b",
    "semantic_title": "decomposing the dark matter of sparse autoencoders",
    "citation_count": 7,
    "authors": [
      "Joshua Engels",
      "Logan Riggs Smith",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=Mae23iEqPS": {
    "title": "Predicting sub-population specific viral evolution",
    "volume": "main",
    "abstract": "Forecasting the change in the distribution of viral variants is crucial for therapeutic design and disease surveillance. This task poses significant modeling challenges due to the sharp differences in virus distributions across sub-populations (e.g., countries) and their dynamic interactions. Existing machine learning approaches that model the variant distribution as a whole are incapable of making location-specific predictions and ignore transmissions that shape the viral landscape. In this paper, we propose a sub-population specific protein evolution model, which predicts the time-resolved distributions of viral proteins in different locations. The algorithm explicitly models the transmission rates between sub-populations and learns their interdependence from data. The change in protein distributions across all sub-populations is defined through a linear ordinary differential equation (ODE) parametrized by transmission rates. Solving this ODE yields the likelihood of a given protein occurring in particular sub-populations. Multi-year evaluation on both SARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms baselines in accurately predicting distributions of viral proteins across continents and countries. We also find that the transmission rates learned from data are consistent with the transmission pathways discovered by retrospective phylogenetic analysis",
    "checked": true,
    "id": "9e552f28fccc526cad55a876a3b8613b6a925804",
    "semantic_title": "predicting sub-population specific viral evolution",
    "citation_count": 0,
    "authors": [
      "Wenxian Shi",
      "Menghua Wu",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=SB7JzhDG45": {
    "title": "Simulation-based Bayesian Inference from Privacy Protected Data",
    "volume": "main",
    "abstract": "Many modern statistical analysis and machine learning applications require training models on sensitive user data. Under a formal definition of privacy protection, differentially private algorithms inject calibrated noise into the confidential data or during the data analysis process to produce privacy-protected datasets or queries. However, restricting access to only privatized data during statistical analysis makes it computationally challenging to make valid statistical inferences. In this work, we propose simulation-based inference methods from privacy-protected datasets. In addition to sequential Monte Carlo approximate Bayesian computation, we adopt neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and with ordinary linear regression models. Illustrating the privacy-utility trade-off, our experiments and analysis demonstrate the necessity and feasibility of designing valid statistical inference procedures to correct for biases introduced by the privacy-protection mechanisms",
    "checked": true,
    "id": "08cb14d5314dbe317fdb3e2b7dec0a7b7e151745",
    "semantic_title": "simulation-based bayesian inference from privacy protected data",
    "citation_count": 1,
    "authors": [
      "Yifei Xiong",
      "Nianqiao Ju",
      "Sanguo Zhang"
    ]
  },
  "https://openreview.net/forum?id=0AOUWC4ss8": {
    "title": "Illustrated Landmark Graphs for Long-horizon Policy Learning",
    "volume": "main",
    "abstract": "Applying learning-based approaches to long-horizon sequential decision-making tasks requires a human teacher to carefully craft reward functions or curate demonstrations to elicit desired behaviors. To simplify this, we first introduce an alternative form of task-specification, Illustrated Landmark Graph (ILG), that represents the task as a directed graph where each vertex corresponds to a region of the state space (a landmark), and each edge represents an easier to achieve sub-task. A landmark in the ILG is conveyed to the agent through a few illustrative examples grounded in the agent's observation space. Second, we propose ILG-Learn, a human in the loop algorithm that interleaves planning over the ILG and sub-task policy learning. ILG-Learn adaptively plans through the ILG by relying on the human teacher's feedback to estimate the success rates of learned policies. We conduct experiments on long-horizon block stacking and point maze navigation tasks, and find that our approach achieves considerably higher success rates (~ 50% improvement) compared to hierarchical reinforcement learning and imitation learning baselines. Additionally, we highlight how the flexibility of the ILG specification allows the agent to learn a sequence of sub-tasks that is better suited to its limited capabilities",
    "checked": true,
    "id": "0a653ecb7cab770a3eddf270864aabc1cc05dd47",
    "semantic_title": "illustrated landmark graphs for long-horizon policy learning",
    "citation_count": 0,
    "authors": [
      "Christopher Watson",
      "Arjun Krishna",
      "Rajeev Alur",
      "Dinesh Jayaraman"
    ]
  },
  "https://openreview.net/forum?id=Rwf31BYTAU": {
    "title": "Adaptive Incentive Design for Markov Decision Processes with Unknown Rewards",
    "volume": "main",
    "abstract": "Incentive design, also known as model design or environment design for Markov decision processes(MDPs), refers to a class of problems in which a leader can incentivize his follower by modifying the follower's reward function, in anticipation that the follower's optimal policy in the resulting MDP can be desirable for the leader's objective. In this work, we propose gradient-ascent algorithms to compute the leader's optimal incentive design, despite the lack of knowledge about the follower's reward function. First, we formulate the incentive design problem as a bi-level optimization problem and demonstrate that, by the softmax temporal consistency between the follower's policy and value function, the bi-level optimization problem can be reduced to single-level optimization, for which a gradient-based algorithm can be developed to optimize the leader's objective. We establish several key properties of incentive design in MDPs and prove the convergence of the proposed gradient-based method. Next, we show that the gradient terms can be estimated from observations of the follower's best response policy, enabling the use of a stochastic gradient-ascent algorithm to compute a locally optimal incentive design without knowing or learning the follower's reward function. Finally, we analyze the conditions under which an incentive design remains optimal for two different rewards which are policy invariant. The effectiveness of the proposed algorithm is demonstrated using a small probabilistic transition system and a stochastic gridworld",
    "checked": false,
    "id": "31c85aa0d48a8249b15876501f7c1e2105fe699a",
    "semantic_title": "adaptive incentive design for markov decision processes with unknown rewards ∗",
    "citation_count": 0,
    "authors": [
      "Haoxiang Ma",
      "Shuo Han",
      "Ahmed Hemida",
      "Charles A kamhoua",
      "Jie Fu"
    ]
  },
  "https://openreview.net/forum?id=tUnyInYbjK": {
    "title": "Influence Learning in Complex Systems",
    "volume": "main",
    "abstract": "High sample complexity hampers the successful application of reinforcement learning methods, especially in real-world problems where simulating complex dynamics is computationally demanding. Influence-based abstraction (IBA) was proposed to mitigate this issue by breaking down the global model of large-scale distributed systems, such as traffic control problems, into small local sub-models. Each local model includes only a few state variables and a representation of the influence exerted by the external portion of the system. This approach allows converting a complex simulator into local lightweight simulators, enabling more effective applications of planning and reinforcement learning methods. However, the effectiveness of IBA critically depends on the ability to accurately approximate the influence of each local model. While there are a few examples showing promising results in benchmark problems, the question of whether this approach is feasible in more practical scenarios remains open. In this work, we take steps towards addressing this question by conducting an extensive empirical study of learning models for influence approximations in various realistic domains, and evaluating how these models generalize over long horizons. We find that learning the influence is often a manageable learning task, even for complex and large systems. Additionally, we demonstrate the efficacy of the approximation models for long-horizon problems. By using short trajectories, we can learn accurate influence approximations for much longer horizons",
    "checked": true,
    "id": "3499e4f9ed483674c9b2c2b7170f05ce8d6773a8",
    "semantic_title": "influence learning in complex systems",
    "citation_count": 0,
    "authors": [
      "Elena Congeduti",
      "roberto rocchetta",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=t1utIThKHD": {
    "title": "An Information Theoretic Approach to Machine Unlearning",
    "volume": "main",
    "abstract": "To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. We explore unlearning from an information theoretic perspective, connecting the influence of a sample to the information gain a model receives by observing it. From this, we derive a simple but principled zero-shot unlearning method based on the geometry of the model. Our approach takes the form of minimising the gradient of a learned function with respect to a small neighbourhood around a target forget point. This induces a smoothing effect, causing forgetting by moving the boundary of the classifier. We explore the intuition behind why this approach can jointly unlearn forget samples while preserving general model performance through a series of low-dimensional experiments. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method is competitive with state-of-the-art performance under the strict constraints of zero-shot unlearning",
    "checked": true,
    "id": "385f2bdc5cb58d83dd4878cac771f6961933411d",
    "semantic_title": "an information theoretic approach to machine unlearning",
    "citation_count": 4,
    "authors": [
      "Jack Foster",
      "Kyle Fogarty",
      "Stefan Schoepf",
      "Zack Dugue",
      "Cengiz Oztireli",
      "Alexandra Brintrup"
    ]
  },
  "https://openreview.net/forum?id=JhYbGiFn3Y": {
    "title": "Emergent representations in networks trained with the Forward-Forward algorithm",
    "volume": "main",
    "abstract": "The Backpropagation algorithm has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, the recently introduced Forward-Forward algorithm replaces the forward and backward passes of Backpropagation with two forward passes. In this work, we show that the internal representations obtained by the Forward-Forward algorithm can organise into category-specific ensembles exhibiting high sparsity -- composed of a low number of active units. This situation is reminiscent of what has been observed in cortical sensory areas, where neuronal ensembles are suggested to serve as the functional building blocks for perception and action. Interestingly, while this sparse pattern does not typically arise in models trained with standard Backpropagation, it can emerge in networks trained with Backpropagation on the same objective proposed for the Forward-Forward algorithm",
    "checked": true,
    "id": "5c3de66ded77aefa17bf77d12f0130fc4a383628",
    "semantic_title": "emergent representations in networks trained with the forward-forward algorithm",
    "citation_count": 9,
    "authors": [
      "Niccolo Tosato",
      "Lorenzo Basile",
      "Emanuele Ballarin",
      "Giuseppe De Alteriis",
      "Alberto Cazzaniga",
      "Alessio ansuini"
    ]
  },
  "https://openreview.net/forum?id=ZfPbCFZQbx": {
    "title": "Robust Symbolic Regression for Dynamical System Identification",
    "volume": "main",
    "abstract": "Real-world complex systems often miss high-fidelity physical descriptions and are typically subject to partial observability. Learning the dynamics of such systems is a challenging and ubiquitous problem, encountered in diverse critical applications which require interpretability and qualitative guarantees.Our paper addresses this problem in the case of sparsely observed probability distribution flows, governed by ODEs. Specifically, we devise a {\\it white box} approach -dubbed Symbolic Distribution Flow Learner (\\texttt{SDFL})- leveraging symbolic search with a Wasserstein-based loss function, resulting in a robust model-recovery scheme which naturally lends itself to cope with partial observability. Additionally, we furnish the proposed framework with theoretical guarantees on the number of required {\\it snapshots} to achieve a certain level of fidelity in the model-discovery. We illustrate the performance of the proposed scheme on the prototypical problem of Kuramoto networks and a standard benchmark of single-cell RNA sequence trajectory data. The numerical experiments demonstrate the competitive performance of \\texttt{SDFL} in comparison to the state-of-the-art",
    "checked": true,
    "id": "23fc1e9f5b28138fa74c42690bc57cdd37ec480a",
    "semantic_title": "robust symbolic regression for dynamical system identification",
    "citation_count": 0,
    "authors": [
      "Ramzi Dakhmouche",
      "Ivan Lunati",
      "Hossein Gorji"
    ]
  },
  "https://openreview.net/forum?id=0yPWtbR3MC": {
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "volume": "main",
    "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training. We examine the seemingly obvious question: \\emph{how to effectively prompt VLMs for semantic segmentation}. To that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the out-of-distribution MESS dataset collection. We introduce a scalable prompting scheme, \\emph{few-shot prompted semantic segmentation}, inspired by open-vocabulary segmentation and few-shot learning. It turns out that VLMs lag far behind specialist models trained for a specific segmentation task, by about 30\\% on average on the Intersection-over-Union metric. Moreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve. Our analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11\\% improvement in performance. Motivated by our findings, we propose PromptMatcher, a remarkably simple training-free baseline that combines both text and visual prompts, achieving state-of-the-art results outperforming the best text-prompted VLM by 2.5\\%, and the top visual-prompted VLM by 3.5\\% on few-shot prompted semantic segmentation",
    "checked": true,
    "id": "b6dfc7d08361e1626bc6453fc687dfb8aa27e4c0",
    "semantic_title": "show or tell? effectively prompting vision-language models for semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Niccolò Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "A. Cristiano I. Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ]
  },
  "https://openreview.net/forum?id=gangoPXSRw": {
    "title": "Probabilistic neural operators for functional uncertainty quantification",
    "volume": "main",
    "abstract": "Neural operators aim to approximate the solution operator of a system of differential equations purely from data. They have shown immense success in modeling complex dynamical systems across various domains. However, the occurrence of uncertainties inherent in both model and data has so far rarely been taken into account\\textemdash{}a critical limitation in complex, chaotic systems such as weather forecasting. In this paper, we introduce the probabilistic neural operator (PNO), a framework for learning probability distributions over the output function space of neural operators. PNO extends neural operators with generative modeling based on strictly proper scoring rules, integrating uncertainty information directly into the training process. We provide a theoretical justification for the approach and demonstrate improved performance in quantifying uncertainty across different domains and with respect to different baselines. Furthermore, PNO requires minimal adjustment to existing architectures, shows improved performance for most probabilistic prediction tasks, and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. Implementing our approach into large-scale models for physical applications can lead to improvements in corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models",
    "checked": true,
    "id": "c98c3cae4c1cf4281962822079995b5f59809a5b",
    "semantic_title": "probabilistic neural operators for functional uncertainty quantification",
    "citation_count": 0,
    "authors": [
      "Christopher Bülte",
      "Philipp Scholl",
      "Gitta Kutyniok"
    ]
  },
  "https://openreview.net/forum?id=A6tOXkkE4Z": {
    "title": "Decision-Focused Surrogate Modeling for Mixed-Integer Linear Optimization",
    "volume": "main",
    "abstract": "Mixed-integer optimization is at the core of many online decision-making systems that demand frequent updates of decisions in real time. However, due to their combinatorial nature, mixed-integer linear programs (MILPs) can be difficult to solve, rendering them often unsuitable for time-critical online applications. To address this challenge, we develop a data-driven approach for constructing surrogate optimization models in the form of linear programs (LPs) that can be solved much more efficiently than the corresponding MILPs. We train these surrogate LPs in a decision-focused manner such that for different model inputs, they achieve the same or close to the same optimal solutions as the original MILPs. One key advantage of the proposed method is that it allows the incorporation of all of the original MILP's linear constraints, which significantly increases the likelihood of obtaining feasible predicted solutions. Results from two computational case studies indicate that this decision-focused surrogate modeling approach is highly data-efficient and provides very accurate predictions of the optimal solutions. In these examples, the resulting surrogate LPs outperform state-of-the-art neural-network-based optimization proxies",
    "checked": true,
    "id": "670b6e19dae39f03a0f1c91734502f1bddaf6fbd",
    "semantic_title": "decision-focused surrogate modeling for mixed-integer linear optimization",
    "citation_count": 0,
    "authors": [
      "Shivi Dixit",
      "Rishabh Gupta",
      "Qi Zhang"
    ]
  },
  "https://openreview.net/forum?id=4ZJjr9YbBw": {
    "title": "A Vector Bernstein Inequality for Self-Normalized Martingales",
    "volume": "main",
    "abstract": "We prove a Bernstein inequality for vector-valued self-normalized martingales. We first give an alternative perspective of the corresponding sub-Gaussian bound due to Abbasi-Yadkori et al. via a PAC-Bayesian argument with Gaussian priors. By instantiating this argument to priors drawn uniformly over well-chosen ellipsoids, we obtain a Bernstein bound",
    "checked": true,
    "id": "e595a287eeca558c7e40bdd036ca4f0f95fd2f3f",
    "semantic_title": "a vector bernstein inequality for self-normalized martingales",
    "citation_count": 1,
    "authors": [
      "Ingvar Ziemann"
    ]
  },
  "https://openreview.net/forum?id=Cw2xlg0e46": {
    "title": "Long-context LLMs Struggle with Long In-context Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs",
    "checked": true,
    "id": "2717e5c7384ec12cfd6cf9c34897c6adad3230ed",
    "semantic_title": "long-context llms struggle with long in-context learning",
    "citation_count": 132,
    "authors": [
      "Tianle Li",
      "Ge Zhang",
      "Quy Duc Do",
      "Xiang Yue",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=d9htascfP8": {
    "title": "Meta-learning Population-based Methods for Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) algorithms are highly sensitive to their hyperparameter settings. Recently, numerous methods have been proposed to dynamically optimize these hyperparameters. One prominent approach is Population-Based Bandits (PB2), which uses time-varying Gaussian processes (GP) to dynamically optimize hyperparameters with a population of parallel agents. Despite its strong overall performance, PB2 experiences slow starts due to the GP initially lacking sufficient information. To mitigate this issue, we propose four different methods that utilize meta-data from various environments. These approaches are novel in that they adapt meta-learning methods to accommodate the time-varying setting. Among these approaches, MultiTaskPB2, which uses meta-learning for the surrogate model, stands out as the most promising approach. It outperforms PB2 and other baselines in both anytime and final performance across two RL environment families",
    "checked": true,
    "id": "1090b0bd3c8734e571b126f8403128f2d8c7d9ef",
    "semantic_title": "meta-learning population-based methods for reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Johannes Hog",
      "Raghu Rajan",
      "André Biedenkapp",
      "Noor Awad",
      "Frank Hutter",
      "Vu Nguyen"
    ]
  },
  "https://openreview.net/forum?id=kd6CfmdPfX": {
    "title": "Posterior Sampling for Reinforcement Learning on Graphs",
    "volume": "main",
    "abstract": "Many Markov Decision Processes (MDPs) exhibit structure in their state and action spaces that is not exploited. We consider the case where the structure can be modelled using a directed acyclic graph (DAG) composed of nodes and edges. In this case, each node has a state, and the state transition dynamics are influenced by the states and actions at its parent nodes. We propose an MDP framework, \\emph{Directed Acyclic Markov Decision Process} (DAMDP) that formalises this problem, and we develop algorithms to perform planning and learning. Crucially, DAMDPs retain many of the benefits of MDPs, as we can show that Dynamic Programming can find the optimal policy in known DAMDPs. We also demonstrate how to perform Reinforcement Learning in DAMDPs when the transition probabilities and the reward function are unknown. To this end, we derive a posterior sampling-based algorithm that is able to leverage the graph structure to boost learning efficiency. Moreover, we obtain a theoretical bound on the Bayesian regret for this algorithm, which directly shows the efficiency gain from considering the graph structure. We then conclude by empirically demonstrating that by harnessing the DAMDP, our algorithm outperforms traditional posterior sampling for Reinforcement Learning in both a maximum flow problem and a real-world wind farm optimisation task",
    "checked": false,
    "id": "65608bd85f132d78b5edecf5c640bbca6fb7f69d",
    "semantic_title": "exploiting causal graph priors with posterior sampling for reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Arnaud Robert",
      "Aldo A. Faisal",
      "Ciara Pike-Burke"
    ]
  },
  "https://openreview.net/forum?id=wPHVijYksq": {
    "title": "A limitation on black-box dynamics approaches to Reinforcement Learning",
    "volume": "main",
    "abstract": "We prove a fundamental limitation on the computational efficiency of a large class of Reinforcement Learning (RL) methods. This limitation applies to model-free RL methods as well as some model-based methods, such as AlphaZero. We provide a formalism that describes this class and present a family of RL problems provably intractable for these methods. Conversely, the problems in the family can be efficiently solved by toy methods. We identify several types of algorithms proposed in the literature that can avoid our limitation, including algorithms that construct an inverse dynamics model, and planning algorithms that leverage an explicit model of the dynamics",
    "checked": true,
    "id": "f2fc4c68f9a79f512f9821625627a0fbf8c41d72",
    "semantic_title": "a limitation on black-box dynamics approaches to reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Brieuc Pinon",
      "Raphael Jungers",
      "Jean-Charles Delvenne"
    ]
  },
  "https://openreview.net/forum?id=w4nd5695sq": {
    "title": "Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors",
    "volume": "main",
    "abstract": "Learning with Errors (LWE) is a hard math problem underlying recently standardized post-quantum cryptography (PQC) systems for key exchange and digital signatures. Prior work proposed new machine learning (ML)-based attacks on LWE problems with small, sparse secrets, but these attacks require millions of LWE samples to train on and take days to recover secrets. We propose three key methods---better preprocessing, angular embeddings and model pre-training---to improve these attacks, speeding up preprocessing by $25\\times$ and improving model sample efficiency by $10\\times$. We demonstrate for the first time that pre-training improves and reduces the cost of ML attacks on LWE. Our architecture improvements enable scaling to larger-dimension LWE problems: this work is the first instance of ML attacks recovering sparse binary secrets in dimension $n=1024$, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed, albeit for larger modulus $q$. Our ML-based approach is the only attack which has successfully recovered secrets for these parameters",
    "checked": true,
    "id": "d9e76ae6480114d81da2e9eb98f848df120be057",
    "semantic_title": "salsa fresca: angular embeddings and pre-training for ml attacks on learning with errors",
    "citation_count": 6,
    "authors": [
      "Samuel Stevens",
      "Emily Wenger",
      "Cathy Yuanchen Li",
      "Niklas Nolte",
      "Eshika Saxena",
      "Francois Charton",
      "Kristin E. Lauter"
    ]
  },
  "https://openreview.net/forum?id=xBbj46Y2fN": {
    "title": "What's Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias",
    "volume": "main",
    "abstract": "Knowledge Distillation is a commonly used Deep Neural Network (DNN) compression method, which often maintains overall generalization performance. However, we show that even for balanced image classification datasets, such as CIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are statistically significantly affected by distillation when comparing class-wise accuracy (i.e. class bias) between a teacher/distilled student or distilled student/non-distilled student model. Changes in class bias are not necessarily an undesirable outcome when considered outside of the context of a model's usage. Using two common fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD) on models trained with the CelebA, Trifeature, and HateXplain datasets, our results suggest that increasing the distillation temperature improves the distilled student model's fairness, and the distilled student fairness can even surpass the fairness of the teacher model at high temperatures. Additionally, we examine individual fairness, ensuring similar instances receive similar predictions. Our results confirm that higher temperatures also improve the distilled student model's individual fairness. This study highlights the uneven effects of distillation on certain classes and its potentially significant role in fairness, emphasizing that caution is warranted when using distilled models for sensitive application domains",
    "checked": false,
    "id": "7235f2e446d470920d2063796baeaa2f0b5e1c74",
    "semantic_title": "what is left after distillation? how knowledge transfer impacts fairness and bias",
    "citation_count": 1,
    "authors": [
      "Aida Mohammadshahi",
      "Yani Ioannou"
    ]
  },
  "https://openreview.net/forum?id=CeNNIQ8GJf": {
    "title": "Efficient Multi-Agent Cooperation Learning through Teammate Lookahead",
    "volume": "main",
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) is a rapidly growing research field that has achieved outstanding results across a variety of challenging cooperation tasks. However, existing MARL algorithms typically overlook the concurrent updates of teammate agents. An agent always learns from the data that it cooperates with one set of (current) teammates, but then practices with another set of (updated) teammates. This phenomenon, termed as ``teammate delay'', leads to a discrepancy between the agent's learning objective and the actual evaluation scenario, which can degrade learning stability and efficiency. In this paper, we tackle this challenge by introducing a lookahead strategy that enables agents to learn to cooperate with predicted future teammates, allowing the explicit awareness of concurrent teammate updates. This lookahead strategy is designed to seamlessly integrate with existing policy-gradient-based MARL methods, enhancing their performance without significant modifications to their underlying structures. The extensive experiments demonstrate the effectiveness of this approach, showing that the lookahead strategy can enhance the cooperation learning efficiency and achieve superior performance over the state-of-the-art MARL algorithms",
    "checked": false,
    "id": "69b3592be220e5641eeeca96b70427615d428a93",
    "semantic_title": "efficient multi-agent cooperation learning through team-mate lookahead",
    "citation_count": 0,
    "authors": [
      "Feng Chen",
      "Xinwei Chen",
      "Rong-Jun Qin",
      "Cong Guan",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://openreview.net/forum?id=DcIW0idrg8": {
    "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement",
    "volume": "main",
    "abstract": "We propose a novel memory-modular learner for image classification that separates knowledge memorization from reasoning. Our model enables effective generalization to new classes by simply replacing the memory contents, without the need for model retraining. Unlike traditional models that encode both world knowledge and task-specific skills into their weights during training, our model stores knowledge in the external memory of web-crawled image and text data. At inference time, the model dynamically selects relevant content from the memory based on the input image, allowing it to adapt to arbitrary classes by simply replacing the memory contents. The key differentiator that our learner meta-learns to perform classification tasks with noisy web data from unseen classes, resulting in robust performance across various classification scenarios. Experimental results demonstrate the promising performance and versatility of our approach in handling diverse classification tasks, including zero-shot/few-shot classification of unseen classes, fine-grained classification, and class-incremental classification",
    "checked": true,
    "id": "04e3f0005e1c3aff8d1c9c56843bb9aaef517f3b",
    "semantic_title": "memory-modular classification: learning to generalize with memory replacement",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Ahmet Iscen",
      "Eunchan Jo",
      "Sua Choi",
      "Minsu Cho",
      "Cordelia Schmid"
    ]
  },
  "https://openreview.net/forum?id=HJbcwRbMQQ": {
    "title": "Efficient Training of Multi-task Neural Solver for Combinatorial Optimization",
    "volume": "main",
    "abstract": "Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. Naive application of conventional multi-task learning approaches often falls short in delivering a high-quality, unified neural solver. This deficiency primarily stems from the significant computational demands and a lack of adequate consideration for the complexities inherent in COPs. In this paper, we propose a general and efficient training paradigm to deliver a unified combinarotial multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. By employing theoretically grounded approximations, our method significantly enhances overall performance, regardless of whether it is within constrained training budgets, across equivalent training epochs, or in terms of generalization capabilities, when compared to conventional training schedules. On the real-world datasets of TSPLib and CVRPLib, our method also achieved the best results compared to single task learning and multi-task learning approaches. Additionally, the influence matrix provides empirical evidence supporting common practices in the field of learning to optimize, further substantiating the effectiveness of our approach. Our code is open-sourced and available at \\url{https://github.com/LOGO-CUHKSZ/MTL-COP}",
    "checked": true,
    "id": "bb3794057dac136aa9eddcc7a26af94c00cf883a",
    "semantic_title": "efficient training of multi-task neural solver for combinatorial optimization",
    "citation_count": 1,
    "authors": [
      "Chenguang Wang",
      "Zhang-Hua Fu",
      "Pinyan Lu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=B1q9po4LPl": {
    "title": "Uncovering Strong Lottery Tickets in Graph Transformers: A Path to Memory Efficient and Robust Graph Learning",
    "volume": "main",
    "abstract": "Graph Transformers (GTs) have recently demonstrated strong capabilities for capturing complex relationships in graph-structured data using global self-attention mechanisms. On the other hand, their high memory requirements during inference remain a challenge for practical deployment. In this study, we investigate the existence of strong lottery tickets (SLTs) — subnetworks within randomly initialized neural networks that can attain competitive accuracy without weight training — in GTs. Previous studies have explored SLTs in message-passing neural networks (MPNNs), showing that SLTs not only exist in MPNNs but also help mitigate over-smoothing problems and improve robustness. However, the potential of SLTs in GTs remains unexplored. With GTs having 4.5$\\times$ more parameters than MPNNs, SLTs hold even greater application value in this context. We find that fixed random weights with a traditional SLT search method cannot adapt to imbalances of features in GTs, leading to highly biased attention that destabilizes model performance. To overcome this issue and efficiently search for SLTs, we introduce a novel approach called Adaptive Scaling. We empirically confirm the existence of SLTs within GTs and demonstrate their versatility through extensive experiments across different GT architectures, including NodeFormer, GRIT, and GraphGPS. Our findings demonstrate that SLTs achieve comparable accuracy while reducing memory usage by 2--32$\\times$, effectively generalize to out-of-distribution data, and enhance robustness against adversarial perturbations. This work highlights that SLTs offer a resource-efficient approach to improving the scalability, efficiency, and robustness of GTs, with broad implications for applications involving graph data",
    "checked": true,
    "id": "605358e411f6dda7c52b34dec718924468bb9f94",
    "semantic_title": "uncovering strong lottery tickets in graph transformers: a path to memory efficient and robust graph learning",
    "citation_count": 0,
    "authors": [
      "Hiroaki Ito",
      "Jiale Yan",
      "Hikari Otsuka",
      "Kazushi Kawamura",
      "Masato Motomura",
      "Thiem Van Chu",
      "Daichi Fujiki"
    ]
  },
  "https://openreview.net/forum?id=MKCwO34oIq": {
    "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Jiang",
      "Negar Hassanpour",
      "Mohammad Salameh",
      "Mohan Sai Singamsetti",
      "Fengyu Sun",
      "Wei Lu",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=4zGPT0ZwnU": {
    "title": "Theoretical Insights into Overparameterized Models in Multi-Task and Replay-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadamin Banayeeanzade",
      "Mahdi Soltanolkotabi",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=a6WthNFhL2": {
    "title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongyoon Kim",
      "Minchan Jeong",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Sumyeong Ahn",
      "Se-Young Yun"
    ]
  },
  "https://openreview.net/forum?id=G1p0YwrX8X": {
    "title": "Sparsified State-Space Models are Efficient Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woomin Song",
      "Jihoon Tack",
      "Sangwoo Mo",
      "Seunghyuk Oh",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=BvKYsaOVEn": {
    "title": "Removing Structured Noise using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tristan Stevens",
      "Hans van Gorp",
      "Faik C Meral",
      "Junseob Shin",
      "Jason Yu",
      "Jean-luc Robert",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=UaaT2fI9DC": {
    "title": "On Using Certified Training towards Empirical Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro De Palma",
      "Serge Durand",
      "Zakaria Chihani",
      "François Terrier",
      "Caterina Urban"
    ]
  },
  "https://openreview.net/forum?id=Nu6N69i8SB": {
    "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixin Liang",
      "LILI YU",
      "Liang Luo",
      "Srini Iyer",
      "Ning Dong",
      "Chunting Zhou",
      "Gargi Ghosh",
      "Mike Lewis",
      "Wen-tau Yih",
      "Luke Zettlemoyer",
      "Xi Victoria Lin"
    ]
  },
  "https://openreview.net/forum?id=nMCJ8bFq4B": {
    "title": "Multiplayer Information Asymmetric Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chang",
      "Yuanhao Lu"
    ]
  },
  "https://openreview.net/forum?id=pqZ6nOm3WF": {
    "title": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo-Line-Search Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Tsukada",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=OGCuDFab4b": {
    "title": "Daphne: Multi-Pass Compilation of Probabilistic Programs into Graphical Models and Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Dietrich Weilbach",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=ELtNtkGXoK": {
    "title": "Cluster Tree for Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Kushnir",
      "Sandeep Silwal"
    ]
  },
  "https://openreview.net/forum?id=UWNa9Pv6qA": {
    "title": "Neuron-based explanations of neural networks sacrifice completeness and interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nolan Simran Dey",
      "Eric Taylor",
      "Alexander Wong",
      "Bryan P. Tripp",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=vRYt8QLKqK": {
    "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein",
      "Azarm Nowzad",
      "Fikret Sivrikaya",
      "Sahin Albayrak"
    ]
  },
  "https://openreview.net/forum?id=msI02LXVJX": {
    "title": "Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hagmann",
      "Michael Staniek",
      "Stefan Riezler"
    ]
  },
  "https://openreview.net/forum?id=oAzu0gzUUb": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Liu",
      "Juncheng Dong",
      "Ziyang Jiang",
      "Ahmed Aloui",
      "Keyu Li",
      "Michael Hunter Klein",
      "Vahid Tarokh",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=hDywd5AbIM": {
    "title": "SAFE-NID: Self-Attention with Normalizing-Flow Encodings for Network Intrusion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Matejek",
      "Ashish Gehani",
      "Nathaniel D. Bastian",
      "Daniel J Clouse",
      "Bradford J Kline",
      "Susmit Jha"
    ]
  },
  "https://openreview.net/forum?id=aPyJilTiIb": {
    "title": "A Unified View of Double-Weighting for Marginal Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José I. Segovia-Martín",
      "Santiago Mazuelas",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=qsipSdfWeV": {
    "title": "Distilling Datasets Into Less Than One Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asaf Shul",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=uxyWlXPuIg": {
    "title": "On Using Secure Aggregation in Differentially Private Federated Learning with Multiple Local Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. Heikkilä"
    ]
  },
  "https://openreview.net/forum?id=XVSQnnf7QT": {
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Jeevan P",
      "Amit Sethi"
    ]
  },
  "https://openreview.net/forum?id=tIfS6jyO9f": {
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aurélien Chateigner"
    ]
  },
  "https://openreview.net/forum?id=EoiuRII7MQ": {
    "title": "Lower Ricci Curvature for Efficient Community Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Jin Park",
      "Didong Li"
    ]
  },
  "https://openreview.net/forum?id=uRbf9ANAns": {
    "title": "Meta-learning Optimizers for Communication-Efficient Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles-Étienne Joseph",
      "Benjamin Thérien",
      "Abhinav Moudgil",
      "Boris Knyazev",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=HTpMOl6xSI": {
    "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shwai He",
      "Daize Dong",
      "Liang Ding",
      "Ang Li"
    ]
  },
  "https://openreview.net/forum?id=MGdydNfWzQ": {
    "title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramansh Sharma",
      "Varun Shankar"
    ]
  },
  "https://openreview.net/forum?id=56EBglCFvx": {
    "title": "HARE: Human-in-the-Loop Algorithmic Recourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Srinivas Kancheti",
      "Rahul Vigneswaran",
      "Bamdev Mishra",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=nNN1pPJRVL": {
    "title": "Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hana YAHIA",
      "Bruno Figliuzzi",
      "Florent Di Meglio",
      "Gerbaud",
      "Stephane Menand",
      "Mohamed MAHJOUB"
    ]
  },
  "https://openreview.net/forum?id=VNM6V1gi3k": {
    "title": "Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kumar",
      "Jarvis Haupt"
    ]
  },
  "https://openreview.net/forum?id=HaAg9RN7Hi": {
    "title": "Unlabelled Compressive Sensing under Sparse Permutation and Prior Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garweet Sresth",
      "Satish Mulleti",
      "Ajit Rajwade"
    ]
  },
  "https://openreview.net/forum?id=osesw2V10u": {
    "title": "A unifying framework for generalised Bayesian online learning in non-stationary environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerardo Duran-Martin",
      "Leandro Sánchez-Betancourt",
      "Alex Shestopaloff",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=GEilvtsFNV": {
    "title": "Variational Neural Stochastic Differential Equations with Change Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef El-Laham",
      "Zhongchang Sun",
      "Haibei Zhu",
      "Tucker Balch",
      "Svitlana Vyetrenko"
    ]
  },
  "https://openreview.net/forum?id=y5Hf0otJLk": {
    "title": "Respecting the limit: Bayesian optimization with a bound on the optimal value",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Juergen Branke",
      "Matthias Poloczek"
    ]
  },
  "https://openreview.net/forum?id=dg1tqNIWg3": {
    "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danil Provodin",
      "Bram van den Akker",
      "Christina Katsimerou",
      "Maurits Clemens Kaptein",
      "Mykola Pechenizkiy"
    ]
  },
  "https://openreview.net/forum?id=FecsgPCOHk": {
    "title": "Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashka Shah",
      "Adela Frances DePavia",
      "Nathaniel C Hudson",
      "Ian Foster",
      "Rick Stevens"
    ]
  },
  "https://openreview.net/forum?id=uafxqhImPM": {
    "title": "On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Alter",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=uA19Xo1o31": {
    "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Faysse",
      "Patrick Fernandes",
      "Nuno M Guerreiro",
      "António Loison",
      "Duarte Miguel Alves",
      "Caio Corro",
      "Nicolas Boizard",
      "João Alves",
      "Ricardo Rei",
      "Pedro Henrique Martins",
      "Antoni Bigata Casademunt",
      "François Yvon",
      "Andre Martins",
      "Gautier Viaud",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=uPCvfyr2KP": {
    "title": "Reheated Gradient-based Discrete Sampling for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muheng Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=5zRs34Ls3C": {
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ]
  },
  "https://openreview.net/forum?id=Xv3ZrFayIO": {
    "title": "Attention Overlap Is Responsible for The Entity Missing Problem in Text-to-image Diffusion Models!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Mari Oriyad",
      "Mohammadali Banayeeanzade",
      "Reza Abbasi",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openreview.net/forum?id=iHYCdTAOqF": {
    "title": "The Time-Energy Model: Selective Time-Series Forecasting Using Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Brusokas",
      "Seshu Tirupathi",
      "Dalin Zhang",
      "Torben Bach Pedersen"
    ]
  },
  "https://openreview.net/forum?id=Is9APiPg4V": {
    "title": "Characterizing the Convergence of Game Dynamics via Potentialness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Bichler",
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Matthias Oberlechner",
      "Bary Pradelski"
    ]
  },
  "https://openreview.net/forum?id=OGifiton47": {
    "title": "Active Diffusion Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oisín Nolan",
      "Tristan Stevens",
      "Wessel L. van Nierop",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=XosdLS7KVE": {
    "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for Transformer Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pihe Hu",
      "Shaolong Li",
      "Xun Wang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LDzvZEVl5H": {
    "title": "Online Control-Informed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Liang",
      "Tianyu Zhou",
      "Zehui Lu",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=D3DA7pgpvn": {
    "title": "Visual Privacy Auditing with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristian Schwethelm",
      "Johannes Kaiser",
      "Moritz Knolle",
      "Sarah Lockfisch",
      "Daniel Rueckert",
      "Alexander Ziller"
    ]
  },
  "https://openreview.net/forum?id=nuIUTHGlM5": {
    "title": "Calibrated Probabilistic Forecasts for Arbitrary Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Marx",
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ]
  },
  "https://openreview.net/forum?id=QlBaDKb370": {
    "title": "State space models can express $n$-gram languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinoth Nandakumar",
      "Qiang Qu",
      "Peng Mi",
      "Tongliang Liu"
    ]
  },
  "https://openreview.net/forum?id=VxC4PZ71Ym": {
    "title": "Unlearning Personal Data from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas De Min",
      "Massimiliano Mancini",
      "Stéphane Lathuilière",
      "Subhankar Roy",
      "Elisa Ricci"
    ]
  },
  "https://openreview.net/forum?id=pF2ukh7HxA": {
    "title": "FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Abbott",
      "Gioele Zardini"
    ]
  },
  "https://openreview.net/forum?id=EEeVYfXor5": {
    "title": "Out of Spuriousity: Improving Robustness to Spurious Correlations without Group Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Quynh Le",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  "https://openreview.net/forum?id=5PPbvCExZs": {
    "title": "No Need for Ad-hoc Substitutes: The Expected Cost is a Principled All-purpose Classification Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer"
    ]
  },
  "https://openreview.net/forum?id=HOnL5hjaIt": {
    "title": "Generalized Tangent Kernel: A Unified Geometric Foundation for Natural Gradient and Standard Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Wei Xu"
    ]
  },
  "https://openreview.net/forum?id=Yk7GUlJwGa": {
    "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David OSOWIECHI",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openreview.net/forum?id=FoQK84nwY3": {
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Han Zhong",
      "Zhihan Liu",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan Awadalla",
      "Zhaoran Wang"
    ]
  },
  "https://openreview.net/forum?id=RXoSmiyObR": {
    "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisuke Hatano",
      "Satoshi Hara",
      "Hiromi Arai"
    ]
  },
  "https://openreview.net/forum?id=03UB1MCAMr": {
    "title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Bresson",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Michail Chatzianastasis",
      "Jun Pang",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=Xz5IcOizQ6": {
    "title": "Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghong Dai",
      "Jy-yong Sohn",
      "Yicong Chen",
      "S M Iftekharul Alam",
      "Ravikumar Balakrishnan",
      "Suman Banerjee",
      "Nageen Himayat",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=38cwP8xVxD": {
    "title": "The 2024 Foundation Model Transparency Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=dczXe0S1oL": {
    "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Serra",
      "Ben Werner",
      "Florian Buettner"
    ]
  },
  "https://openreview.net/forum?id=ZInwrlkQ3f": {
    "title": "An elementary concentration bound for Gibbs measures arising in statistical learning theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Ramsay",
      "Aukosh Jagannath",
      "Shojaeddin Chenouri"
    ]
  },
  "https://openreview.net/forum?id=tSFpsfndE7": {
    "title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Bernecker",
      "Ghalia Rehawi",
      "Francesco Paolo Casale",
      "Janine Knauer-Arloth",
      "Annalisa Marsico"
    ]
  },
  "https://openreview.net/forum?id=N28FdYO2sH": {
    "title": "Learning Linear Polytree Structural Equation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingmei Lou",
      "Yu Hu",
      "Xiaodong Li"
    ]
  },
  "https://openreview.net/forum?id=6nBIweDYzZ": {
    "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejumade Afonja",
      "Hui-Po Wang",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=BPDVZajOW5": {
    "title": "Optimizing Estimators of Squared Calibration Errors in Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gregor Gruber",
      "Francis R. Bach"
    ]
  },
  "https://openreview.net/forum?id=ZdMIXltJzK": {
    "title": "Reset-free Reinforcement Learning with World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat",
      "Edward S. Hu"
    ]
  },
  "https://openreview.net/forum?id=QIzRdjIWnS": {
    "title": "Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yi Zhou",
      "Shaofeng Zou"
    ]
  },
  "https://openreview.net/forum?id=UV58hNygne": {
    "title": "HoSNNs: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hejia Geng",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=8Q4qxe9a9Z": {
    "title": "A Self-Explainable Heterogeneous GNN for Relational Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Ferrini",
      "Antonio Longa",
      "Andrea Passerini",
      "Manfred Jaeger"
    ]
  },
  "https://openreview.net/forum?id=9NVJ0ZgEfT": {
    "title": "Long Short-Term Imputer: Handling Consecutive Missing Values in Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng You",
      "Xinyang Chen",
      "Yu Sun",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openreview.net/forum?id=58gPkcVbFL": {
    "title": "Evolution of Discriminator and Generator Gradients in GAN Training: From Fitting to Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ]
  },
  "https://openreview.net/forum?id=GkYOcbNLaW": {
    "title": "Cycle Conditioning for Robust Representation Learning from Categorical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Tabejamaat",
      "Farzaneh Etminani",
      "Mattias Ohlsson"
    ]
  },
  "https://openreview.net/forum?id=CrKMqRAhBo": {
    "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roozbeh Yousefzadeh",
      "Xuenan Cao"
    ]
  },
  "https://openreview.net/forum?id=HkmymFPODz": {
    "title": "Deep Active Learning in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Xie",
      "Jifan Zhang",
      "Haoyue Bai",
      "Robert D Nowak"
    ]
  },
  "https://openreview.net/forum?id=J7cY9Jr9WM": {
    "title": "A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amadou Siaka SANGARE",
      "Nicolas Dunou",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=bqMJToTkvT": {
    "title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Kong",
      "Hangyu Mao",
      "Zhao Qi",
      "Bin Zhang",
      "Jingqing Ruan",
      "Li Shen",
      "Yongzhe Chang",
      "Xueqian Wang",
      "Rui Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=jRbKsQ3sYO": {
    "title": "Combating Inter-Task Confusion and Catastrophic Forgetting by Metric Learning and Re-Using a Past Trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayedmoslem Shokrolahi",
      "IL MIN KIM"
    ]
  },
  "https://openreview.net/forum?id=prVLANCshF": {
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Jieru Mei",
      "Bingchen Zhao",
      "Yisen Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=IaUh7CSD3k": {
    "title": "Metalearning Continual Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Irie",
      "Róbert Csordás",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=CAkt3DsAZs": {
    "title": "Meta-Learning for Graphs with Heterogeneous Node Attribute Spaces for Few-Shot Edge Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong Chuang",
      "Yusuke Tanaka",
      "Tomoharu Iwata"
    ]
  },
  "https://openreview.net/forum?id=jJOVpnNrEp": {
    "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minttu Alakuijala",
      "Reginald McLean",
      "Isaac Woungang",
      "Nariman Farsad",
      "Samuel Kaski",
      "Pekka Marttinen",
      "Kai Yuan"
    ]
  },
  "https://openreview.net/forum?id=2Zan4ATYsh": {
    "title": "DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi WANG",
      "Yuhang Zhou",
      "Zhixiong Zhang",
      "Qiguang Chen",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=k4AxEwTaHq": {
    "title": "FaAlGrad: Fairness through Alignment of Gradients across Different Subpopulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Malik",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=xXs2GKXPnH": {
    "title": "Faster Diffusion Through Temporal Attention Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Wentian Zhang",
      "Jinheng Xie",
      "Francesco Faccio",
      "Mengmeng Xu",
      "Tao Xiang",
      "Mike Zheng Shou",
      "Juan-Manuel Perez-Rua",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=Za9Tm07fig": {
    "title": "TACO Vision Models Can Be Efficiently Specialized via Few-Shot Task-Aware Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kuznedelev",
      "Soroush Tabesh",
      "Kimia Noorbakhsh",
      "Elias Frantar",
      "Sara Beery",
      "Eldar Kurtic",
      "Dan Alistarh"
    ]
  },
  "https://openreview.net/forum?id=XPREcQlAM0": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan V. Truong"
    ]
  },
  "https://openreview.net/forum?id=ZckLMG00sO": {
    "title": "Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjeev Raja",
      "Ishan Amin",
      "Fabian Pedregosa",
      "Aditi S. Krishnapriyan"
    ]
  },
  "https://openreview.net/forum?id=6jTQrr3APY": {
    "title": "Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prabhu babu",
      "Petre Stoica",
      "Astha Saini"
    ]
  },
  "https://openreview.net/forum?id=EWT4GxjGDS": {
    "title": "Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Acharya",
      "Juba Ziani",
      "Jingyan Wang",
      "Varun Vangala"
    ]
  },
  "https://openreview.net/forum?id=dvRysCqmYQ": {
    "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Yang",
      "Han Yu",
      "Peikun Guo",
      "Khadija Zanna",
      "Xiaoxue Yang",
      "Akane Sano"
    ]
  },
  "https://openreview.net/forum?id=zSK81A2hxQ": {
    "title": "A Neural Material Point Method for Particle-based Emulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Rochman-Sharabi",
      "Sacha Lewin",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=0RJvZY0h6O": {
    "title": "Lognormal Mutations and their Use in Detecting Surreptitious Fake Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Teytaud",
      "Mariia Zameshina",
      "Tom Sander",
      "Pierre Fernandez",
      "Furong Ye",
      "Laurent Najman",
      "Thomas Bäck",
      "Ismail Labiad"
    ]
  },
  "https://openreview.net/forum?id=k3Ab6RuJE9": {
    "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Robert Bamler",
      "Bernhard Schölkopf",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=fC4bh1PmZr": {
    "title": "Counterfactual Learning of Stochastic Policies with Continuous Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houssam Zenati",
      "Alberto Bietti",
      "Matthieu Martin",
      "Eustache Diemert",
      "Pierre Gaillard",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Vwgjk5ysWn": {
    "title": "Why is constrained neural language generation particularly challenging?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Garbacea",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=K6CvWPtF62": {
    "title": "Provable Quantum Algorithm Advantage for Gaussian Process Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristian A. Galvis-Florez",
      "Ahmad Farooq",
      "Simo Särkkä"
    ]
  },
  "https://openreview.net/forum?id=ojeCoOKwWp": {
    "title": "Differentially Private Source-Target Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Schnapp",
      "Sivan Sabato"
    ]
  },
  "https://openreview.net/forum?id=WxHTSPS2pi": {
    "title": "Uncertainty-Based Experience Replay for Task-Agnostic Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Remonda",
      "Cole Corbitt Terrell",
      "Eduardo E. Veas",
      "Marc Masana"
    ]
  },
  "https://openreview.net/forum?id=E2zKNuwNDc": {
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Peter Shaw",
      "Jonathan Berant"
    ]
  },
  "https://openreview.net/forum?id=6LO1y8ZE0F": {
    "title": "SimPLR: A Simple and Plain Transformer for Efficient Object Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Kien Nguyen",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openreview.net/forum?id=TnT59yz7lc": {
    "title": "Exploiting Benford's Law for Weight Regularization of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Ott",
      "Huawei Sun",
      "Enrico Rinaldi",
      "Gianfranco Mauro",
      "Lorenzo Servadei",
      "Robert Wille"
    ]
  },
  "https://openreview.net/forum?id=rWSiBknwQa": {
    "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Guozheng Ma",
      "Cong Yu",
      "Ning Gui",
      "Linrui Zhang",
      "Zhiqi Huang",
      "Suwei Ma",
      "Yongzhe Chang",
      "Sen Zhang",
      "Li Shen",
      "Xueqian Wang",
      "Peilin Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=42v6I5Ut9a": {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pSk5qyt1ob": {
    "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudi Coppola",
      "Manuel Mazo Espinosa"
    ]
  },
  "https://openreview.net/forum?id=EcMVskXo1n": {
    "title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Yaochen Zhu",
      "Chuxu Zhang",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=69RntSRF5K": {
    "title": "An Analytical Model for Overparameterized Learning Under Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliav Mor",
      "Yair Carmon"
    ]
  },
  "https://openreview.net/forum?id=t5cy5v9wph": {
    "title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martha Lewis",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=adhsMqURI1": {
    "title": "Comparing the information content of probabilistic representation spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kieran A. Murphy",
      "Sam Dillavou",
      "Danielle Bassett"
    ]
  },
  "https://openreview.net/forum?id=jAHEBivObO": {
    "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyush Tiwary",
      "Atri Guha",
      "Subhodip Panda",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=D3DBqvSDbj": {
    "title": "On Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangming Gu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan Li",
      "Min Lin",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=dNJmJ8bh1M": {
    "title": "The Sparse Matrix-Based Random Projection: A Study of Binary and Ternary Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Lu",
      "Zhongzheng Li",
      "Mingrui Chen",
      "Weiyu Li"
    ]
  },
  "https://openreview.net/forum?id=Sx1khIIi95": {
    "title": "Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Caron",
      "Fadhel Ayed",
      "Paul Jung",
      "Hoil Lee",
      "Juho Lee",
      "Hongseok Yang"
    ]
  },
  "https://openreview.net/forum?id=rfPns0WJyg": {
    "title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos E. Luis",
      "Alessandro Giacomo Bottero",
      "Julia Vinogradska",
      "Felix Berkenkamp",
      "Jan Peters"
    ]
  },
  "https://openreview.net/forum?id=hCxtlfvL22": {
    "title": "Latent Space Energy-based Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Deqian Kong",
      "Jianwen Xie",
      "Kookjin Lee",
      "Ying Nian Wu",
      "Yezhou Yang"
    ]
  },
  "https://openreview.net/forum?id=U8EMkndyq4": {
    "title": "Using representation balancing to learn conditional-average dose responses from clustered data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bockel-Rickermann",
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Tim Verdonck",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=TRKwzPnXWQ": {
    "title": "ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Hongru Zhu",
      "Chen Wei",
      "Yijiang Li",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=GGHk5ukO6t": {
    "title": "Dynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=h751wl9xiR": {
    "title": "ALTA: Compiler-Based Analysis of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Shaw",
      "James Cohan",
      "Jacob Eisenstein",
      "Kenton Lee",
      "Jonathan Berant",
      "Kristina Toutanova"
    ]
  },
  "https://openreview.net/forum?id=BLDtWlFKhn": {
    "title": "Density of states in neural networks: an in-depth exploration of learning in parameter space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margherita Mele",
      "Roberto Menichetti",
      "Alessandro Ingrosso",
      "Raffaello Potestio"
    ]
  },
  "https://openreview.net/forum?id=HjpD5kpfa3": {
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chaolong Ying",
      "Yimu Wang",
      "Yaoyao Xu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=JQ0agisXny": {
    "title": "A Strong Baseline for Molecular Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philippe Formont",
      "Hugo Jeannin",
      "Pablo Piantanida",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=u9EHndbiVw": {
    "title": "PROXI: Challenging the GNNs for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Astrit Tola",
      "Jack Myrick",
      "Baris Coskunuzer"
    ]
  },
  "https://openreview.net/forum?id=RfFqBXLDQk": {
    "title": "On Space Folds of ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lewandowski",
      "Hamid Eghbalzadeh",
      "Bernhard Heinzl",
      "Raphael Pisoni",
      "Bernhard A. Moser"
    ]
  },
  "https://openreview.net/forum?id=asiBW1bB9b": {
    "title": "Improving Consistency in Large Language Models through Chain of Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Raj",
      "Vipul Gupta",
      "Domenic Rosati",
      "Subhabrata Majumdar"
    ]
  },
  "https://openreview.net/forum?id=H4S4ETc8c9": {
    "title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Ichihara",
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Kenshi Abe",
      "Kaito Ariu",
      "Mitsuki Sakamoto",
      "Eiji Uchibe"
    ]
  },
  "https://openreview.net/forum?id=ScEv13W2f1": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Hong-Xing Yu",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Wt6Iz5XNIO": {
    "title": "Understanding LLM Embeddings for Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Tang",
      "Bangding Yang",
      "Xingyou Song"
    ]
  },
  "https://openreview.net/forum?id=5qKI2dkrjL": {
    "title": "APR-CNN: Convolutional Neural Networks for the Adaptive Particle Representation of Large Microscopy Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Jonsson",
      "Bevan Leslie Cheeseman",
      "Ivo Sbalzarini"
    ]
  },
  "https://openreview.net/forum?id=zjxKrb4ehr": {
    "title": "On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Bruno",
      "Ying Zhang",
      "Dongyoung Lim",
      "Omer Deniz Akyildiz",
      "Sotirios Sabanis"
    ]
  },
  "https://openreview.net/forum?id=A1R1cQ93Cb": {
    "title": "Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Venturini",
      "Marianna De Santis",
      "Jordan Patracone",
      "Martin Schmidt",
      "Francesco Rinaldi",
      "Saverio Salzo"
    ]
  },
  "https://openreview.net/forum?id=nmBleuFzaN": {
    "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Dufort-Labbé",
      "Pierluca D'Oro",
      "Evgenii Nikishin",
      "Irina Rish",
      "Pierre-Luc Bacon",
      "Razvan Pascanu",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=ZMliWjMCor": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahrokh Ghoddousi Boroujeni",
      "Andreas Krause",
      "Giancarlo Ferrari-Trecate"
    ]
  },
  "https://openreview.net/forum?id=DrMCDS88IL": {
    "title": "Wasserstein Coreset via Sinkhorn Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyun Yin",
      "Yixuan Qiu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=tzW948kU6x": {
    "title": "Diffusion on Graph: Augmentation of Graph Structure for Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Changyu Liu",
      "Yingzhen Yang"
    ]
  },
  "https://openreview.net/forum?id=8rxtL0kZnX": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lev Telyatnikov",
      "Maria Sofia Bucarelli",
      "Guillermo Bernardez",
      "Olga Zaghen",
      "Simone Scardapane",
      "Pietro Lio"
    ]
  },
  "https://openreview.net/forum?id=sbmp55k6iE": {
    "title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikaru Umeda",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=JEHIVfjmOf": {
    "title": "JoIN: Joint GANs Inversion for Intrinsic Image Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Shah",
      "Svetlana Lazebnik",
      "Julien Philip"
    ]
  },
  "https://openreview.net/forum?id=1QeI99nH9k": {
    "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cullen Anderson",
      "Jeff M. Phillips"
    ]
  },
  "https://openreview.net/forum?id=7CUluLpLxV": {
    "title": "Explaining Explainability: Recommendations for Effective Use of Concept Activation Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Nicolson",
      "Lisa Schut",
      "Alison Noble",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=IrBYuh9W3T": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Shirali",
      "Moritz Hardt"
    ]
  },
  "https://openreview.net/forum?id=Bmy82p2eez": {
    "title": "Continual Learning from Simulated Interactions via Multitask Prospective Rehearsal for Bionic Limb Behavior Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharmita Dey",
      "Benjamin Paassen",
      "Sarath Ravindran Nair",
      "Sabri Boughorbel",
      "Arndt F. Schilling"
    ]
  },
  "https://openreview.net/forum?id=DYCSRf3vby": {
    "title": "Geometry-Aware visualization of high dimensional Symmetric Positive Definite matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault de Surrel",
      "Sylvain Chevallier",
      "Fabien Lotte",
      "Florian Yger"
    ]
  },
  "https://openreview.net/forum?id=VM8bNd5A09": {
    "title": "CNN Interpretability with Multivector Tucker Saliency Maps for Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aymene Mohammed Bouayed",
      "Samuel Deslauriers-gauthier",
      "Adrian IACOVELLI",
      "David Naccache"
    ]
  },
  "https://openreview.net/forum?id=YxXyRSlZ4b": {
    "title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Luca Marchetti",
      "Gabriele Cesa",
      "Kumar Pratik",
      "Arash Behboodi"
    ]
  },
  "https://openreview.net/forum?id=tzFjcVqmxw": {
    "title": "Enhancing Remaining Useful Life Prediction with Ensemble Multi-Term Fourier Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Song",
      "Laurens Bliek",
      "Yaoxin Wu",
      "Yingqian Zhang"
    ]
  },
  "https://openreview.net/forum?id=Wnd0XY0twh": {
    "title": "Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liran Nochumsohn",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=M3SkSMfWcP": {
    "title": "Adaptive Multi-step Refinement Network for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Chen",
      "Yufan Ren",
      "Tong Zhang",
      "Zheng Dang",
      "Wenbing Tao",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=zKv8qULV6n": {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Yuanhan Zhang",
      "Dong Guo",
      "Renrui Zhang",
      "Feng Li",
      "Hao Zhang",
      "Kaichen Zhang",
      "Peiyuan Zhang",
      "Yanwei Li",
      "Ziwei Liu",
      "Chunyuan Li"
    ]
  },
  "https://openreview.net/forum?id=F5ALCh3GWG": {
    "title": "On the Regularization of Learnable Embeddings for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Butera",
      "Giovanni De Felice",
      "Andrea Cini",
      "Cesare Alippi"
    ]
  },
  "https://openreview.net/forum?id=JQGmbVK4Fr": {
    "title": "Towards context and domain-aware algorithms for scene analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Serouis",
      "Florence Sèdes"
    ]
  },
  "https://openreview.net/forum?id=P5y82LKGbY": {
    "title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyun Wang",
      "Yadi Cao",
      "Chris Russell",
      "Yanxin Shen",
      "Junyu Luo",
      "Ming Zhang",
      "Siyu Heng",
      "Xiao Luo"
    ]
  },
  "https://openreview.net/forum?id=gwXfZ3xkUq": {
    "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qian Liu",
      "Chao Du",
      "Tongyao Zhu",
      "Cunxiao Du",
      "Kenji Kawaguchi",
      "Tianyu Pang"
    ]
  },
  "https://openreview.net/forum?id=UYXPt7HUdl": {
    "title": "Score-Based Denoising Diffusion Models for Photon-Starved Image Restoration Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savvas Melidonis",
      "Yiming Xi",
      "Konstantinos C. Zygalakis",
      "Yoann Altmann",
      "Marcelo Pereyra"
    ]
  },
  "https://openreview.net/forum?id=W50i7r3DHE": {
    "title": "Instance-Aware Graph Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Jundong Li",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=xpnPYfufhz": {
    "title": "Partially Frozen Random Networks Contain Compact Strong Lottery Tickets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Daiki Chijiwa",
      "Ángel López García-Arias",
      "Yasuyuki Okoshi",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Susumu Takeuchi",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=c7AAHdEYz5": {
    "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minguk Jang",
      "Hye Won Chung"
    ]
  },
  "https://openreview.net/forum?id=Q7aXOnEGgU": {
    "title": "On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Behboodi",
      "Gabriele Cesa"
    ]
  },
  "https://openreview.net/forum?id=2wgnepQjyF": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Israfil Bahceci",
      "Akram Bin Sediq",
      "HAMZA SOKUN",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=pxxmUKKgel": {
    "title": "How Does Code Pretraining Affect Language Model Task Performance?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Petty",
      "Sjoerd van Steenkiste",
      "Tal Linzen"
    ]
  },
  "https://openreview.net/forum?id=uDRzORdPT7": {
    "title": "DeepRRTime: Robust Time-series Forecasting with a Regularized INR Basis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Mahdi Gilany",
      "Kry Yik-Chau Lui",
      "Martin Magill",
      "Alexander Pashevich"
    ]
  },
  "https://openreview.net/forum?id=Lt2H8Bd8jF": {
    "title": "Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Vincent",
      "Daniel Palenicek",
      "Boris Belousov",
      "Jan Peters",
      "Carlo D'Eramo"
    ]
  },
  "https://openreview.net/forum?id=jZBAVFGUUo": {
    "title": "Towards Measuring Predictability: To which extent data-driven approaches can extract deterministic relations from data exemplified with time series prediction and classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh GHOLAM ZADEH",
      "Vaisakh Shaj",
      "Patrick Jahnke",
      "Gerhard Neumann",
      "Tim Breitenbach"
    ]
  },
  "https://openreview.net/forum?id=wIgRV336hC": {
    "title": "Minimax Lower Bounds for Estimating Distributions on Low-dimensional Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Chakraborty"
    ]
  },
  "https://openreview.net/forum?id=XWAXcxNg4n": {
    "title": "Test-Time Adaptation with Source Based Auxiliary Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motasem Alfarra",
      "Alvaro Correia",
      "Bernard Ghanem",
      "Christos Louizos"
    ]
  },
  "https://openreview.net/forum?id=HlzjI2fn2T": {
    "title": "On the stability of gradient descent with second order dynamics for time-varying cost functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis E Gibson",
      "Sawal Acharya",
      "Anjali Parashar",
      "Joseph Emilio Gaudio",
      "Anuradha Annaswamy"
    ]
  },
  "https://openreview.net/forum?id=O4CQ5AM5yP": {
    "title": "REX: GPU-Accelerated Sim2Real Framework with Delay and Dynamics Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Jens Kober",
      "Robert Babuska",
      "Laura Ferranti"
    ]
  },
  "https://openreview.net/forum?id=udVkqIDYSM": {
    "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidan Wang",
      "Rui Shen",
      "Bradly C. Stadie"
    ]
  },
  "https://openreview.net/forum?id=v47f4DwYZb": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geri Skenderi",
      "Hang Li",
      "Jiliang Tang",
      "Marco Cristani"
    ]
  },
  "https://openreview.net/forum?id=L7sQ8CW2FY": {
    "title": "Conformalized Credal Regions for Classification with Ambiguous Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Caprio",
      "David Stutz",
      "Shuo Li",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=2nRcWy3RLM": {
    "title": "Bridging Causality, Individual Fairness, and Adversarial Robustness in the Absence of Structural Causal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ]
  },
  "https://openreview.net/forum?id=8tMMCf4YYn": {
    "title": "Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Mishchenko",
      "Rustem Islamov",
      "Eduard Gorbunov",
      "Samuel Horváth"
    ]
  },
  "https://openreview.net/forum?id=gLQ801ewwp": {
    "title": "Identifying Axiomatic Mathematical Transformation Steps using Tree-Structured Pointer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Wankerl",
      "Jan Pfister",
      "Andrzej Dulny",
      "Gerhard Götz",
      "Andreas Hotho"
    ]
  },
  "https://openreview.net/forum?id=p9KSFrTLx0": {
    "title": "Mixture Degree-Corrected Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noureddine Henka",
      "Mohamad Assaad",
      "Sami Tazi"
    ]
  },
  "https://openreview.net/forum?id=x1dXvvElVd": {
    "title": "Interpreting Neurons in Deep Vision Networks with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Bai",
      "Rahul Ajay Iyer",
      "Tuomas Oikarinen",
      "Akshay R. Kulkarni",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=f6yMdmrD2g": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed Fatih Balin",
      "Dominique LaSalle",
      "Umit Catalyurek"
    ]
  },
  "https://openreview.net/forum?id=Ss9MTTN7OL": {
    "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Miranda",
      "Elena Sofia Ruzzetti",
      "Andrea Santilli",
      "Fabio Massimo Zanzotto",
      "Sébastien Bratières",
      "Emanuele Rodolà"
    ]
  },
  "https://openreview.net/forum?id=OGaTF9iOxi": {
    "title": "Maximum Mean Discrepancy on Exponential Windows for Online Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Kalinke",
      "Marco Heyden",
      "Georg Gntuni",
      "Edouard Fouché",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=aWRMvXTvPf": {
    "title": "Shapley Values of Structured Additive Regression Models and Application to RKHS Weightings of Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Dubé",
      "Mario Marchand"
    ]
  },
  "https://openreview.net/forum?id=WppTEs4Kkn": {
    "title": "On the effects of similarity metrics in decentralized deep learning under distribution shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edvin Listo Zec",
      "Tom Hagander",
      "Eric Ihre-Thomason",
      "Sarunas Girdzijauskas"
    ]
  },
  "https://openreview.net/forum?id=VmfWywWuYQ": {
    "title": "Interactive Task Planning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Li",
      "Philipp Wu",
      "Pieter Abbeel",
      "Jitendra Malik"
    ]
  },
  "https://openreview.net/forum?id=4o8lIFkpn2": {
    "title": "\\copyright Plug-in Authorization for Human Copyright Protection in Text-to-Image Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhou",
      "Huishuai Zhang",
      "Jiang Bian",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openreview.net/forum?id=3Y3o0yFZfu": {
    "title": "Private Fine-tuning of Large Language Models with Zeroth-order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tang",
      "Ashwinee Panda",
      "Milad Nasr",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ]
  },
  "https://openreview.net/forum?id=bZzXgheUSD": {
    "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masih Eskandar",
      "Tooba Imtiaz",
      "Zifeng Wang",
      "Jennifer Dy"
    ]
  },
  "https://openreview.net/forum?id=MO1slfU9xy": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Mougan",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ]
  },
  "https://openreview.net/forum?id=zg0hPlABfY": {
    "title": "Enhancing Parameter Efficiency and Generalization in Large Models: A Regularized and Masked Low-Rank Adaptation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Mao",
      "Zihao Zhao",
      "Siqi Ping",
      "Yang Liu",
      "Wenbo Ding"
    ]
  },
  "https://openreview.net/forum?id=nu1SjVgSuy": {
    "title": "SPFormer: Enhancing Vision Transformer with Superpixel Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieru Mei",
      "Liang-Chieh Chen",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=pWSrm3oP8b": {
    "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhepeng Cen",
      "Yao Liu",
      "Siliang Zeng",
      "Pratik Chaudhari",
      "Huzefa Rangwala",
      "George Karypis",
      "Rasool Fakoor"
    ]
  },
  "https://openreview.net/forum?id=oeg2ncuSPz": {
    "title": "Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasis Kratsios",
      "Haitz Sáez de Ocáriz Borde",
      "Takashi Furuya",
      "Marc T. Law"
    ]
  },
  "https://openreview.net/forum?id=hJHf7PCuVt": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Ling",
      "Zhimeng Jiang",
      "Na Zou",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=uF9ZdAwrCT": {
    "title": "In-distribution adversarial attacks on object recognition models using gradient-free search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spandan Madan",
      "Tomotake Sasaki",
      "Hanspeter Pfister",
      "Tzu-Mao Li",
      "Xavier Boix"
    ]
  },
  "https://openreview.net/forum?id=KbteA50cni": {
    "title": "Distributed Quasi-Newton Method for Fair and Fast Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi",
      "Linfeng Ye"
    ]
  },
  "https://openreview.net/forum?id=hMO8sT9qaD": {
    "title": "Making Reliable and Flexible Decisions in Long-tailed Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolian Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=dzQCRHKRdC": {
    "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Derezinski"
    ]
  },
  "https://openreview.net/forum?id=GDN5cFTNaL": {
    "title": "Adjacency Search Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meher Chaitanya",
      "Kshitijaa Jaglan",
      "Ulrik Brandes"
    ]
  },
  "https://openreview.net/forum?id=4Xz0WBAiX4": {
    "title": "ExCeL: Combined Extreme and Collective Logit Information for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen Karunanayake",
      "Suranga Seneviratne",
      "Sanjay Chawla"
    ]
  },
  "https://openreview.net/forum?id=8C8LJIqF4y": {
    "title": "Time Series Domain Adaptation via Channel-Selective Representation Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nauman Ahad",
      "Mark A. Davenport",
      "Eva L Dyer"
    ]
  },
  "https://openreview.net/forum?id=OOgsAZdFOt": {
    "title": "Can AI-Generated Text be Reliably Detected? Stress Testing AI Text Detectors Under Various Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinu Sankar Sadasivan",
      "Aounon Kumar",
      "Sriram Balasubramanian",
      "Wenxiao Wang",
      "Soheil Feizi"
    ]
  },
  "https://openreview.net/forum?id=aiOHc1LGpD": {
    "title": "Differentially Private Gradient Flow based on the Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilana Sebag",
      "Muni Sreenivas Pydi",
      "Jean-Yves Franceschi",
      "Alain Rakotomamonjy",
      "Mike Gartrell",
      "Jamal Atif",
      "Alexandre Allauzen"
    ]
  },
  "https://openreview.net/forum?id=dbaGuiYsTl": {
    "title": "Wasserstein Modality Alignment Makes Your Multimodal Transformer More Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhuo zhi",
      "Yuxuan Sun",
      "Qiangqiang Wu",
      "Ziquan Liu",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=AFxEdJwQcp": {
    "title": "A thorough reproduction and evaluation of $\\mu$P",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Vlassis",
      "David Belius",
      "Volodymyr Fomichov"
    ]
  },
  "https://openreview.net/forum?id=avDr56QjSI": {
    "title": "Semantic Alignment for Prompt-Tuning in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hari Chandana Kuchibhotla",
      "Sai Srinivas Kancheti",
      "Abbavaram Gowtham Reddy",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=Utjw2z1ale": {
    "title": "Identifying Spurious Correlations using Counterfactual Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Paul Cohen",
      "Louis Blankemeier",
      "Akshay S Chaudhari"
    ]
  },
  "https://openreview.net/forum?id=LVQ8BEL5n3": {
    "title": "Numerically Robust Fixed-Point Smoothing Without State Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Krämer"
    ]
  },
  "https://openreview.net/forum?id=r8UFp9olQ0": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Majellaro",
      "Jonathan Collu",
      "Aske Plaat",
      "Thomas M. Moerland"
    ]
  },
  "https://openreview.net/forum?id=Gb4HBGG9re": {
    "title": "Enhanced Federated Optimization: Adaptive Unbiased Client Sampling with Reduced Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dun Zeng",
      "Zenglin Xu",
      "Yu Pan",
      "Xu Luo",
      "Qifan Wang",
      "Xiaoying Tang"
    ]
  },
  "https://openreview.net/forum?id=vmmgFW3ztz": {
    "title": "Leveraging a Simulator for Learning Causal Representations from Post-Treatment Covariates for CATE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Nagalapatti",
      "Pranava Singhal",
      "Avishek Ghosh",
      "Sunita Sarawagi"
    ]
  },
  "https://openreview.net/forum?id=INijCSPtbQ": {
    "title": "Preventing Conflicting Gradients in Neural Marked Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=LZ9FmeFeLV": {
    "title": "Towards LifeSpan Cognitive Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Chi Han",
      "Tongtong Wu",
      "Xiaoxin He",
      "Wangchunshu Zhou",
      "Nafis Sadeq",
      "Xiusi Chen",
      "Zexue He",
      "Wei Wang",
      "Gholamreza Haffari",
      "Heng Ji",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=IIVr4Hu3Oi": {
    "title": "Distributed Multi-Agent Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prithviraj Tarale",
      "Edward Rietman",
      "Hava T Siegelmann"
    ]
  },
  "https://openreview.net/forum?id=T5OuTgPxHS": {
    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://openreview.net/forum?id=0mGho8wrv5": {
    "title": "SelfEval: Leveraging discriminative nature of generative models for evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Saketh Rambhatla",
      "Ishan Misra"
    ]
  },
  "https://openreview.net/forum?id=dHljjaNHh1": {
    "title": "Fairness Through Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ]
  },
  "https://openreview.net/forum?id=V2SD2uVKEE": {
    "title": "Zero-shot CLIP Class Forgetting via Text-image Space Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Kravets",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openreview.net/forum?id=lTX4bYREAZ": {
    "title": "A Scalable Approach for Mapper via Efficient Spatial Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Simi"
    ]
  },
  "https://openreview.net/forum?id=VIkycTWDWo": {
    "title": "Doubly Robust Conditional VAE via Decoder Calibration: An Implicit KL Annealing Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhui Liu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=IZrt6hB2sI": {
    "title": "Improving CLIP Counting Accuracy via Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisu Zhang",
      "Yicong Chen",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=ccu0M3nmlF": {
    "title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koki Okajima",
      "Tomoyuki Obuchi"
    ]
  },
  "https://openreview.net/forum?id=qbrE0LR7fF": {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer",
      "Daniel Ramos"
    ]
  },
  "https://openreview.net/forum?id=LdflD41Gn8": {
    "title": "On the Properties and Estimation of Pointwise Mutual Information Profiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paweł Czyż",
      "Frederic Grabowski",
      "Julia E Vogt",
      "Niko Beerenwinkel",
      "Alexander Marx"
    ]
  },
  "https://openreview.net/forum?id=BlYIPa0Fx1": {
    "title": "An analysis of the noise schedule for score-based generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislas Strasman",
      "Antonio Ocello",
      "Claire Boyer",
      "Sylvain Le Corff",
      "Vincent Lemaire"
    ]
  },
  "https://openreview.net/forum?id=PtD2gVmb3J": {
    "title": "Global Safe Sequential Learning via Efficient Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cen-You Li",
      "Olaf Dünnbier",
      "Marc Toussaint",
      "Barbara Rakitsch",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=QQE5j2OsLW": {
    "title": "Can Optimization Trajectories Explain Multi-Task Transfer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mueller",
      "Mark Dredze",
      "Nicholas Andrews"
    ]
  },
  "https://openreview.net/forum?id=yzbAFf8vd5": {
    "title": "A comparison between humans and AI at recognizing objects in unusual poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Netta Ollikka",
      "Amro Kamal Mohamed Abbas",
      "Andrea Perin",
      "Markku Kilpeläinen",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=V6ia5hWIMD": {
    "title": "νSAM: Memory-Efficient Sharpness-Aware Minimization via Nuclear Norm Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Parameswaran Raman",
      "Lenon Minorics",
      "Mingyi Hong",
      "Shoham Sabach",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WEYMCLu8u7": {
    "title": "Event-Triggered Time-Varying Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Brunzema",
      "Alexander von Rohr",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=PTTa3U29NR": {
    "title": "Optimization Dynamics of Equivariant and Augmented Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oskar Nordenfors",
      "Fredrik Ohlsson",
      "Axel Flinth"
    ]
  },
  "https://openreview.net/forum?id=QplBL2pV4Z": {
    "title": "Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Yin Huang",
      "Ruinan Jin",
      "Can Zhao",
      "Daguang Xu",
      "Xiaoxiao Li"
    ]
  },
  "https://openreview.net/forum?id=XL1N6iLr0G": {
    "title": "An Attribute-based Method for Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Reiss",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=8mgX3Uw2Ea": {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Zhu",
      "Sheng Liu",
      "Carlos Fernandez-Granda",
      "Narges Razavian"
    ]
  },
  "https://openreview.net/forum?id=tRpWaK3pWh": {
    "title": "A Generalization Bound for Nearly-Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Golikov"
    ]
  },
  "https://openreview.net/forum?id=Qq4ge9Qe31": {
    "title": "Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Perini",
      "Maja Rudolph",
      "Sabrina Schmedding",
      "Chen Qiu"
    ]
  },
  "https://openreview.net/forum?id=BhOJreYmur": {
    "title": "MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Guerra-Manzanares",
      "Farah Shamout"
    ]
  },
  "https://openreview.net/forum?id=b68QOenPWy": {
    "title": "Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leah Bar",
      "Boaz Lerner",
      "Nir Darshan",
      "Rami Ben-Ari"
    ]
  },
  "https://openreview.net/forum?id=I4IAwVOZrM": {
    "title": "Lifelong Learning in StyleGAN through Latent Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Kappiyath",
      "ANMOL GARG",
      "Ramya Hebbalaguppe",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=bwRxXiGO9A": {
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Boizard",
      "Kevin El Haddad",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=CTkABQvnkm": {
    "title": "Decoupled Sequence and Structure Generation for Realistic Antibody Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nayoung Kim",
      "Minsu Kim",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://openreview.net/forum?id=pxdSm7PW5Q": {
    "title": "Reviving Life on the Edge: Joint Score-Based Graph Generation of Rich Edge Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrod Berman",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=60Gi1w6hte": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc T. Law",
      "Karsten Kreis",
      "Haggai Maron"
    ]
  },
  "https://openreview.net/forum?id=eakh1Edffd": {
    "title": "Reinforcement learning with non-ergodic reward increments: robustness via ergodicity transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Erfaun Noorani",
      "James Price",
      "Ole Peters",
      "Colm Connaughton",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=bHdEtW5E7O": {
    "title": "Federated Learning with Efficient Local Adaptation for Realized Volatility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhao",
      "Lin Cai",
      "Wu-Sheng Lu"
    ]
  },
  "https://openreview.net/forum?id=MvYddudHuE": {
    "title": "Reweighting Improves Conditional Risk Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Jiahe Lin",
      "Fengpei Li",
      "Songzhu Zheng",
      "Anant Raj",
      "Anderson Schneider",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://openreview.net/forum?id=vZGZIIgcG4": {
    "title": "Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Rahbar",
      "Niklas Åkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=DqWvxSQ1TK": {
    "title": "From Promise to Practice: A Study of Common Pitfalls Behind the Generalization Gap in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeideh Ghanbari Azar",
      "Lorenzo Tronchin",
      "Attila Simkó",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=3mJZfL77WM": {
    "title": "Highway Graph to Accelerate Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidu Yin",
      "Zhen Zhang",
      "Dong Gong",
      "Stefano V Albrecht",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=DCAeXwLenB": {
    "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Fred Maurice NGOLE MBOULA",
      "Antoine Souloumiac"
    ]
  },
  "https://openreview.net/forum?id=gpHOtQQPJG": {
    "title": "Optimization and Generalization Guarantees for Weight Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Cisneros-Velarde",
      "Zhijie Chen",
      "Sanmi Koyejo",
      "Arindam Banerjee"
    ]
  },
  "https://openreview.net/forum?id=LzmsvRTqaJ": {
    "title": "Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative model for Quasar spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidhi Lalchand",
      "Anna-Christina Eilers"
    ]
  },
  "https://openreview.net/forum?id=fqkq1MgONB": {
    "title": "BM$^2$: Coupled Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Peluchetti"
    ]
  },
  "https://openreview.net/forum?id=wS1fD0ofay": {
    "title": "Partial-Label Learning with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fuchs",
      "Florian Kalinke",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=34vtRA3Nvu": {
    "title": "PRIMO: Private Regression in Multiple Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seth Neel"
    ]
  },
  "https://openreview.net/forum?id=ytKFKoCpyK": {
    "title": "ODNet: Opinion Dynamics-Inspired Neural Message Passing for Graphs and Hypergraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Zhou",
      "Outongyi Lv",
      "Jing Wang",
      "Xiang Xiao",
      "Weishu Zhao"
    ]
  },
  "https://openreview.net/forum?id=0u7pWfjri5": {
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=UrSgGSTM2J": {
    "title": "Minimax Posterior Contraction Rates for Unconstrained Distribution Estimation on $[0,1]^d$ under Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Matthew Jacobs",
      "Lekha Patel",
      "Anirban Bhattacharya",
      "Debdeep Pati"
    ]
  },
  "https://openreview.net/forum?id=9CWU8Oi86d": {
    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Yuzheng Hu",
      "Yong Lin",
      "Tong Zhang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=yawWz4qWkF": {
    "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Alina Fastowski",
      "Felix Pfeiffer",
      "Gjergji Kasneci"
    ]
  },
  "https://openreview.net/forum?id=CNaiJRcX84": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Paul E. Apolinario",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=iVV7IzI55V": {
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=AjJTg5M0r8": {
    "title": "Slicing Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Bonet",
      "Kimia Nadjahi",
      "Thibault Sejourne",
      "Kilian FATRAS",
      "Nicolas Courty"
    ]
  },
  "https://openreview.net/forum?id=yBgTVWccIx": {
    "title": "DafnyBench: A Benchmark for Formal Software Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chloe R Loughridge",
      "Qinyi Sun",
      "Seth Ahrenbach",
      "Federico Cassano",
      "Chuyue Sun",
      "Ying Sheng",
      "Anish Mudide",
      "Md Rakib Hossain Misu",
      "Nada Amin",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=x8wscCAJ2m": {
    "title": "Sparse Neural Architectures via Deterministic Ramanujan Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suryam Arnav Kalra",
      "Arindam Biswas",
      "Pabitra Mitra",
      "BISWAJIT BASU"
    ]
  },
  "https://openreview.net/forum?id=JHxrh00W1j": {
    "title": "Masked Capsule Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=gqh0yzPYdo": {
    "title": "No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu Gaur",
      "Darshan Singh S",
      "Makarand Tapaswi"
    ]
  },
  "https://openreview.net/forum?id=Conma3qnaT": {
    "title": "Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Verma",
      "Gantavya Bhatt",
      "Avi Schwarzschild",
      "Soumye Singhal",
      "Arnav Mohanty Das",
      "Chirag Shah",
      "John P Dickerson",
      "Pin-Yu Chen",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=XDbY3qhM42": {
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang ZHANG",
      "Joshua M. Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ]
  },
  "https://openreview.net/forum?id=oYP2Pd5aQt": {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Gao",
      "Chuanyang Zheng",
      "Enze Xie",
      "Han Shi",
      "Tianyang Hu",
      "Yu Li",
      "Michael Ng",
      "Zhenguo Li",
      "Zhaoqiang Liu"
    ]
  },
  "https://openreview.net/forum?id=Og3VxBFhwj": {
    "title": "Linear Convergence of Decentralized FedAvg for PL Objectives: The Interpolation Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti P Maralappanavar",
      "Prashant Khanduri",
      "Bharath B N"
    ]
  },
  "https://openreview.net/forum?id=XxbQAsxrRC": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franka Bause",
      "Fabian Jogl",
      "Patrick Indri",
      "Tamara Drucks",
      "David Penz",
      "Nils Morten Kriege",
      "Thomas Gärtner",
      "Pascal Welke",
      "Maximilian Thiessen"
    ]
  },
  "https://openreview.net/forum?id=aV6dCg1VFV": {
    "title": "Investigating the impact of missing value handling on Boosted trees and Deep learning for Tabular data: A Claim Reserving case study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Larionov",
      "Niall M. Adams",
      "Kevin N. Webster"
    ]
  },
  "https://openreview.net/forum?id=IK2cR89z45": {
    "title": "Personalized Privacy Amplification via Importance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fay",
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=bwyHf5eery": {
    "title": "A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data and Overparameterization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Johannes Zenn",
      "Robert Bamler"
    ]
  },
  "https://openreview.net/forum?id=kzPNHQ8ByY": {
    "title": "Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihong Yu",
      "Manav Mishra",
      "Alec Koppel",
      "Carl Busart",
      "Priya Narayan",
      "Dinesh Manocha",
      "Amrit Singh Bedi",
      "Pratap Tokekar"
    ]
  },
  "https://openreview.net/forum?id=Vq0wMFBjo2": {
    "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zang",
      "Tian Yun",
      "Hao Tan",
      "Trung Bui",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=o58uy91V2V": {
    "title": "On the Detection of Reviewer-Author Collusion Rings From Paper Bidding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Jecmen",
      "Nihar B Shah",
      "Fei Fang",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=ZA7D4nQuQF": {
    "title": "Transformers in Uniform TC$^0$",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=SeGNvJJjbs": {
    "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo"
    ]
  },
  "https://openreview.net/forum?id=KqRnsEMYLx": {
    "title": "Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madison Cooley",
      "Varun Shankar",
      "Mike Kirby",
      "Shandian Zhe"
    ]
  },
  "https://openreview.net/forum?id=nxQtoHHcj9": {
    "title": "An Analysis of Model Robustness across Concurrent Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongho Jeon",
      "Suhwan Choi",
      "Hyoje Lee",
      "Teresa Yeo"
    ]
  },
  "https://openreview.net/forum?id=ZnWqtPhHM7": {
    "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarshi Brahma",
      "Anuska Roy",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=JN7iNWaPTe": {
    "title": "Mental Modelling of Reinforcement Learning Agents by Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Lu",
      "Xufeng Zhao",
      "Josua Spisak",
      "Jae Hee Lee",
      "Stefan Wermter"
    ]
  },
  "https://openreview.net/forum?id=PzmaWLqK0e": {
    "title": "Reward-based Autonomous Online Learning Framework for Resilient Cooperative Target Monitoring using a Swarm of Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhankar Gupta",
      "Saksham Sharma",
      "Suresh Sundaram"
    ]
  },
  "https://openreview.net/forum?id=edULLIVnoc": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Cohen-Wang",
      "Joshua Vendrow",
      "Aleksander Madry"
    ]
  },
  "https://openreview.net/forum?id=ssXSrZ94sR": {
    "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kay",
      "Timm Haucke",
      "Suzanne Stathatos",
      "Siqi Deng",
      "Erik Young",
      "Pietro Perona",
      "Sara Beery",
      "Grant Van Horn"
    ]
  },
  "https://openreview.net/forum?id=x6fXnsM9Ez": {
    "title": "The 2023 Foundation Model Transparency Index",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Betty Xiong",
      "Daniel Zhang",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=1Avb4jYjLb": {
    "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Nikhil Anand",
      "Nikhil Vyas",
      "Eran Malach",
      "Sham M. Kakade"
    ]
  },
  "https://openreview.net/forum?id=Y7dRmpGiHj": {
    "title": "What is the Relationship between Tensor Factorizations and Circuits (and How Can We Exploit it)?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Loconte",
      "Antonio Mari",
      "Gennaro Gala",
      "Robert Peharz",
      "Cassio de Campos",
      "Erik Quaeghebeur",
      "Gennaro Vessio",
      "Antonio Vergari"
    ]
  },
  "https://openreview.net/forum?id=YCt8lsIDwA": {
    "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yu",
      "Chenchen Zhu",
      "Sean Culatana",
      "Raghuraman Krishnamoorthi",
      "Fanyi Xiao",
      "Yong Jae Lee"
    ]
  },
  "https://openreview.net/forum?id=sXr1fRjs1N": {
    "title": "Contextualized Messages Boost Graph Representations",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Godwin Lim",
      "Galvin Brice Sy Lim",
      "Renzo Roel Tan",
      "Kazushi Ikeda"
    ]
  },
  "https://openreview.net/forum?id=yeITEuhv4Q": {
    "title": "Revisiting Deep Hybrid Models for Out-of-Distribution Detection",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Ruben Schlumbom",
      "Eibe Frank"
    ]
  },
  "https://openreview.net/forum?id=mSoDRZXsqj": {
    "title": "Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Billy Joe Franks",
      "Moshe Eliasof",
      "Semih Cantürk",
      "Guy Wolf",
      "Carola-Bibiane Schönlieb",
      "Sophie Fellenz",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=rKAkp1f3R7": {
    "title": "Shedding Light on Problems with Hyperbolic Graph Learning",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isay Katsman",
      "Anna Gilbert"
    ]
  },
  "https://openreview.net/forum?id=IPmzyQSiQE": {
    "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zach Nussbaum",
      "John Xavier Morris",
      "Andriy Mulyar",
      "Brandon Duderstadt"
    ]
  },
  "https://openreview.net/forum?id=wcxrJcJ7vq": {
    "title": "The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgi Ganev",
      "Meenatchi Sundaram Muthu Selva Annamalai",
      "Emiliano De Cristofaro"
    ]
  },
  "https://openreview.net/forum?id=wF3ZtSlOcT": {
    "title": "Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Sidiropoulos",
      "Samarth Bhargav",
      "Panagiotis Eustratiadis",
      "Evangelos Kanoulas"
    ]
  },
  "https://openreview.net/forum?id=knv4lQFVoE": {
    "title": "A general framework of Riemannian adaptive optimization methods with a convergence analysis",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Sakai",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=FJgtVfUxLQ": {
    "title": "A Survey on the Honesty of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ]
  },
  "https://openreview.net/forum?id=QTsJXSvAI2": {
    "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amer Essakine",
      "Yanqi Cheng",
      "Chun-Wun Cheng",
      "Lipei Zhang",
      "Zhongying Deng",
      "Lei Zhu",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=sZdtTJInUg": {
    "title": "Class Incremental Learning from First Principles: A Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Ashtekar",
      "Jingxi Zhu",
      "Vasant G Honavar"
    ]
  },
  "https://openreview.net/forum?id=ukLxqA8zXj": {
    "title": "Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Hedström",
      "Philine Lou Bommer",
      "Thomas F Burns",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=RGsdAwWuu6": {
    "title": "Unified Risk Analysis for Weakly Supervised Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=YxKJihRcby": {
    "title": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SUBBA REDDY OOTA",
      "Zijiao Chen",
      "Manish Gupta",
      "Bapi Raju Surampudi",
      "Gael Jobard",
      "Frederic Alexandre",
      "Xavier Hinaut"
    ]
  },
  "https://openreview.net/forum?id=WUQsBiJqyP": {
    "title": "A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiliang Liu",
      "Sheng Xu",
      "Shicheng Liu",
      "Ashish Gaurav",
      "Sriram Ganapathi Subramanian",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=wZLWuFHxt5": {
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "XIAOYU XU",
      "Xiaobao Wu",
      "Jie Fu",
      "Feng Yichao",
      "Fengjun Pan",
      "Anh Tuan Luu"
    ]
  },
  "https://openreview.net/forum?id=RJT1baPhdV": {
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ]
  },
  "https://openreview.net/forum?id=xdWP1d8BxI": {
    "title": "Sparse Decomposition of Graph Neural Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Hu",
      "Mai Zeng",
      "Ge Zhang",
      "Pavel Rumiantsev",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=5298fKGmv3": {
    "title": "The BrowserGym Ecosystem for Web Agent Research",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Le Sellier de Chezelles",
      "Maxime Gasse",
      "Alexandre Lacoste",
      "Massimo Caccia",
      "Alexandre Drouin",
      "Léo Boisvert",
      "Megh Thakkar",
      "Tom Marty",
      "Rim Assouel",
      "Sahar Omidi Shayegan",
      "Lawrence Keunho Jang",
      "Xing Han Lù",
      "Ori Yoran",
      "Dehan Kong",
      "Frank F. Xu",
      "Siva Reddy",
      "Graham Neubig",
      "Quentin Cappart",
      "Russ Salakhutdinov",
      "Nicolas Chapados"
    ]
  },
  "https://openreview.net/forum?id=SbGt90dxdp": {
    "title": "Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Rumiantsev",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=FcyHZ6Q4k0": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=jrUUk5Fskm": {
    "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonios Valkanas",
      "Yuening Wang",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=hGaWq5Buj7": {
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussein Mozannar",
      "Valerie Chen",
      "Mohammed Alsobay",
      "Subhro Das",
      "Sebastian Zhao",
      "Dennis Wei",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Ameet Talwalkar",
      "David Sontag"
    ]
  }
}