{
  "https://openreview.net/forum?id=gwUOzI4DuV": {
    "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment",
    "volume": "main",
    "abstract": "Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges because interactions with an environment are prohibited. In this paper, we propose a new framework, namely \\textbf{M}ulti-\\textbf{A}gent \\textbf{C}ausal \\textbf{C}redit \\textbf{A}ssignment (\\textbf{MACCA}), to address credit assignment in the offline MARL setting. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to integrate with various offline MARL methods seamlessly. Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. In our experiments, we demonstrate that MACCA not only outperforms state-of-the-art methods but also enhances performance when integrated with other backbones",
    "checked": true,
    "id": "80e2f19b543685bbcfc4137216ebe688060adf00",
    "semantic_title": "macca: offline multi-agent reinforcement learning with causal credit assignment",
    "citation_count": 3,
    "authors": [
      "Ziyan Wang",
      "Yali Du",
      "Yudi Zhang",
      "Meng Fang",
      "Biwei Huang"
    ]
  },
  "https://openreview.net/forum?id=9oToxYVOSW": {
    "title": "Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation",
    "volume": "main",
    "abstract": "Gradient-based optimization has been a cornerstone of machine learning that enabled the vast ad- vances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can be- come limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications — neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) — we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function",
    "checked": true,
    "id": "e007954131351392042d9176e862f66dbdc43696",
    "semantic_title": "efficient and flexible neural network training through layer-wise feedback propagation",
    "citation_count": 2,
    "authors": [
      "Leander Weber",
      "Jim Berend",
      "Moritz Weckbecker",
      "Alexander Binder",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ]
  },
  "https://openreview.net/forum?id=Tnwci2kLna": {
    "title": "CXAD: Contrastive Explanations for Anomaly Detection: Algorithms, Complexity Results and Experiments",
    "volume": "main",
    "abstract": "Anomaly/Outlier detection (AD/OD) is often used in controversial applications to detect unusual behavior which is then further investigated or policed. This means an explanation of why something was predicted as an anomaly is desirable not only for individuals but also for the general population and policy-makers. However, existing explainable AI (XAI) methods are not well suited for Explainable Anomaly detection (XAD). In particular, most XAI methods provide instance-level explanations, whereas a model/global-level explanation is desirable for a complete understanding of the definition of normality or abnormality used by an AD algorithm. Further, existing XAI methods try to explain an algorithm's behavior by finding an explanation of why an instance belongs to a category. However, by definition, anomalies/outliers are chosen because they are different from the normal instances. We propose a new style of model agnostic explanation, called contrastive explanation, that is designed specifically for AD algorithms. It addresses the novel challenge of providing a model-agnostic and global-level explanation by finding contrasts between the outlier group of instances and the normal group. We propose three formulations: (i) Contrastive Explanation, (ii) Strongly Contrastive Explanation, and (iii) Multiple Strong Contrastive Explanations. The last formulation is specifically for the case where a given dataset is believed to have many types of anomalies. For the first two formulations, we show the underlying problem is in the computational class P by presenting linear and polynomial time exact algorithms. We show that the last formulation is computationally intractable, and we use an integer linear program for that version to generate experimental results. We demonstrate our work on several data sets such as the CelebA image data set, the HateXplain language data set, and the COMPAS dataset on fairness. These data sets are chosen as their ground truth explanations are clear or well-known",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Davidson",
      "Nicolás Kennedy",
      "S. S. Ravi"
    ]
  },
  "https://openreview.net/forum?id=FPJKZDzdsW": {
    "title": "Fairness with respect to Stereotype Predictors: Impossibilities and Best Practices",
    "volume": "main",
    "abstract": "As AI systems increasingly influence decision-making from consumer recommendations to educational opportunities, their accountability becomes paramount. This need for oversight has driven extensive research into algorithmic fairness, a body of work that has examined both allocative and representational harms. However, numerous works examining representational harms such as stereotypes encompass many different concepts measured by different criteria, yielding many, potentially conflicting, characterizations of harm. The abundance of measurement approaches makes the mitigation of stereotypes in downstream machine learning models highly challenging. Our work introduces and unifies a broad class of auditors through the framework of \\textit{stereotype predictors}. We map notions of fairness with respect to these predictors to existing notions of group fairness. We give guidance, with theoretical foundations, for selecting one or a set of stereotype predictors and provide algorithms for achieving fairness with respect to stereotype predictors under various fairness notions. We demonstrate the effectiveness of our algorithms with different stereotype predictors in two empirical case studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inbal Rachel Livni Navon",
      "Omer Reingold",
      "Judy Hanwen Shen"
    ]
  },
  "https://openreview.net/forum?id=6Aj0aNXfRy": {
    "title": "Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) often suffer from performance degradation as the network depth increases. This paper addresses this issue by introducing initialization methods that enhance signal propagation (SP) within GNNs. We propose three key metrics for effective SP in GNNs: forward propagation, backward propagation, and graph embedding variation (GEV). While the first two metrics derive from classical SP theory, the third is specifically designed for GNNs. We theoretically demonstrate that a broad range of commonly used initialization methods for GNNs, which exhibit performance degradation with increasing depth, fail to control these three metrics simultaneously. To deal with this limitation, a direct exploitation of the SP analysis--searching for weight initialization variances that optimize the three metrics--is shown to significantly enhance the SP in deep GCNs. This approach is called \\textit{\\textbf{S}ignal \\textbf{P}ropagation \\textbf{o}n \\textbf{G}raph-guided \\textbf{Init}ialization (\\textbf{SPoGInit})}. Our experiments demonstrate that SPoGInit outperforms commonly used initialization methods on various tasks and architectures. Notably, SPoGInit enables performance improvements as GNNs deepen, which represents a significant advancement in addressing depth-related challenges and highlights the validity and effectiveness of the SP analysis framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senmiao Wang",
      "Yupeng Chen",
      "Yushun Zhang",
      "Ruoyu Sun",
      "Tian Ding"
    ]
  },
  "https://openreview.net/forum?id=p0KTYl2B9T": {
    "title": "Spaced Scheduling for Large Language Model Training",
    "volume": "main",
    "abstract": "Recent breakthroughs in deep learning have accelerated progress toward increasingly capable large language models (LLMs), even sparking discussions about the path to Artificial General Intelligence (AGI). Yet, current LLM training pipelines continue to depend on heuristics and human-driven empirical analysis to curate data. In practice, more sophisticated data selection methods often incur high costs, exhibit limited adaptability, or do not consistently surpass simple random baselines across various models and datasets. In this work, we propose Spaced Scheduled Training (Sst), a novel adaptive data selection strategy that prioritizes training examples based solely on per-example perplexity computed from the model's own evolving parameters. By obviating the need for external reference models, Sst customizes data selection to the model's unique characteristics, including its pre-training data composition, and eliminates biases commonly introduced by these external models. Extensive experiments on seven LLMs (0.5B to 32B parameters) in the instruction-finetuning (IFT) setting show that Sst consistently outperforms representative state-of-the-art selection approaches like Deita and InsTag on the Open LLM Leaderboard. For instance, with Qwen2.5-32B and a 30k examples data budget, Sst achieved a 42.75% Open LLM Leaderboard score, exceeding a leading data-selection baseline (38.56%) and the full-100k dataset baseline (39.58%). We further present a theoretical framework to assess computational overhead of model-based selection methods, showing that Sst remains efficient in practical scenarios, and propose strategies to mitigate the overhead in worst-case scenarios. Our findings underscore the potential of model-informed dynamic data selection, offering an efficient, adaptable, and cost-effective approach. We release our training code, trained models, and data mixes in our public repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine El hattami",
      "Nicolas Chapados",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=PMO30TLI4l": {
    "title": "Selective Concept Bottleneck Models Without Predefined Concepts",
    "volume": "main",
    "abstract": "Concept-based models like Concept Bottleneck Models (CBMs) have garnered significant interest for improving model interpretability by first predicting human-understandable concepts before mapping them to the output classes. Early approaches required costly concept annotations. To alleviate this, recent methods utilized large language models to automatically generate class-specific concept descriptions and learned mappings from a pretrained black-box model's raw features to these concepts using vision-language models. However, these approaches assume prior knowledge of which concepts the black-box model has learned. In this work, we discover the concepts encoded by the model through unsupervised concept discovery techniques instead. We further leverage a simple input-dependent concept selection mechanism that dynamically retains a sparse set of relevant concepts of each input, enhancing both sparsity and interpretability. Our approach not only improves downstream performance, but also needs significantly fewer concepts for accurate classification. Lastly, we show how large vision-language models can guide the editing of our models' weights to correct model errors",
    "checked": false,
    "id": "984be2ce918e3d495fc0174519f34d7b472445ab",
    "semantic_title": "concept bottleneck models without predefined concepts",
    "citation_count": 12,
    "authors": [
      "Simon Schrodi",
      "Julian Schur",
      "Max Argus",
      "Thomas Brox"
    ]
  },
  "https://openreview.net/forum?id=T49vPTkIt5": {
    "title": "Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments. However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which always results in algorithmic instability, difficulty in convergence, or entrapment in local optima. While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques. In this paper, we introduce \\textbf{E}volutionary action \\textbf{SPA}ce \\textbf{R}eduction with \\textbf{K}nowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback. In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively. Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents. The code can be found in https://github.com/LiuZhihao2022/eSpark",
    "checked": true,
    "id": "d7a505964defe667f9c7aa8cb7b79f1caef89614",
    "semantic_title": "knowing what not to do: leverage language model insights for action space pruning in multi-agent reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Zhihao Liu",
      "Xianliang Yang",
      "Zichuan Liu",
      "Yifan Xia",
      "Wei Jiang",
      "Yuanyu Zhang",
      "Lijuan Li",
      "Guoliang Fan",
      "Lei Song",
      "Jiang Bian"
    ]
  },
  "https://openreview.net/forum?id=kEUvWFHEsn": {
    "title": "[RE] GNNBoundary: Finding Boundaries and Going Beyond Them",
    "volume": "main",
    "abstract": "Graph classification models are becoming increasingly popular, while explainability methods face challenges due to the discrete nature of graphs and other factors. However, investigating model decision-making, such as through decision-boundary regions, helps prevent misclassification and improve model robustness. This study aims to reproduce the findings of GNNBoundary: Towards Explaining Graph Neural Networks Through the Lens of Decision Boundaries (Wang & Shen, 2024). Their work supports 3 main claims: (1) their proposed algorithm can identify adjacent class pairs reliably, (2) their GNNBoundary can effectively and consistently generate near-boundary graphs outperforming the cross entropy baseline and (3) the generated near-boundary graphs can be used to accurately assess key properties of the decision boundary; margin, thickness, and complexity. We reproduce the experiments on the same datasets and extended them to two additional real-world datasets. Beyond that, we test different boundary probability ranges and their effect on decision boundary metrics, develop an additional baseline, and conduct hyperparameter tuning. We confirm the first claim regarding the adjacency discovery as well as the second claim that GNNBoundary outperforms the cross-entropy baseline under the limitation that it requires intensive hyperparameter tuning for convergence. The third claim is partially accepted as we observe a high variance between reported and obtained results, disproving the reliability and precision of the boundary statistics. Code and instructions are available at: https://github.com/jhb300/re_gnnboundary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Henrik Bertrand",
      "Lukas Bierling",
      "Ina Klaric",
      "Aron Wezenberg"
    ]
  },
  "https://openreview.net/forum?id=lTt2cTW8h1": {
    "title": "Return-Aligned Decision Transformer",
    "volume": "main",
    "abstract": "Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. It is increasingly important to adjust the performance of AI agents to meet human requirements, for example, in applications like video games and education tools. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and includes a mechanism to control the agent's performance using the target return. However, the action generation is hardly influenced by the target return because DT's self-attention allocates scarce attention scores to the return tokens. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to more effectively align the actual return with the target return. RADT leverages features extracted by paying attention solely to the return, enabling action generation to consistently depend on the target return. Extensive experiments show that RADT significantly reduces the discrepancies between the actual return and the target return compared to DT-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsunehiko Tanaka",
      "Kenshi Abe",
      "Kaito Ariu",
      "Tetsuro Morimura",
      "Edgar Simo-Serra"
    ]
  },
  "https://openreview.net/forum?id=R7QFlwvnne": {
    "title": "Unified Preference Optimization: Language Model Alignment Beyond the Preference Frontier",
    "volume": "main",
    "abstract": "For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to easily tune language models to maximize auxiliary, non-preferential objectives according to the LLM designer's preferences (e.g., tuning lexical style or minimizing specific kinds of harmful content). Critically, these designer objectives may not be amply human-labeled or represented in available data, align with user preferences, or even be able to be captured tractably by binary preference pairs. To leverage the simplicity and performance of DPO with the generality of RL, we propose a unified approach. Based on a simple decomposition of preference and auxiliary objectives, we allow for tuning LLMs to optimize user and designer preferences without any additional specialized or preference data, computational cost, stability \"tweaks\", hyperparameter tuning, or training instability. The proposed method, Unified Preference Optimization, shows the ability to effectively generalize to user preferences and auxiliary objectives, while preserving or surpassing alignment performance on challenging benchmarks across a range of model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudhan Badrinath",
      "Prabhat Agarwal",
      "Jiajing Xu"
    ]
  },
  "https://openreview.net/forum?id=Z0DhgU8fBt": {
    "title": "[Re] Improving Interpretation Faithfulness for Vision Transformers",
    "volume": "main",
    "abstract": "This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by Hu et al. (2024) alongside interpretability methods for Vision Transformers from Chefer et al. (2021) and Xu et al. (2022). We investigate claims made by Hu et al. (2024), namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Izabela Kurek",
      "Wojciech Trejter",
      "Stipe Frkovic",
      "Andro Erdelez"
    ]
  },
  "https://openreview.net/forum?id=y8VXikiIU0": {
    "title": "Enhancing Sample Generation of Diffusion Models using Noise Level Correction",
    "volume": "main",
    "abstract": "The denoising process of diffusion models can be interpreted as an approximate projection of noisy samples onto the data manifold. Moreover, the noise level in these samples approximates their distance to the underlying manifold. Building on this insight, we propose a novel method to enhance sample generation by aligning the estimated noise level with the true distance of noisy samples to the manifold. Specifically, we introduce a noise level correction network, leveraging a pre-trained denoising network, to refine noise level estimates during the denoising process. Additionally, we extend this approach to various image restoration tasks by integrating task-specific constraints, including inpainting, deblurring, super-resolution, colorization, and compressed sensing. Experimental results demonstrate that our method significantly improves sample quality in both unconstrained and constrained generation scenarios. Notably, the proposed noise level correction framework is compatible with existing denoising schedulers (e.g., DDIM), offering additional performance improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abulikemu Abuduweili",
      "Chenyang Yuan",
      "Changliu Liu",
      "Frank Permenter"
    ]
  },
  "https://openreview.net/forum?id=YCBVcGSZeR": {
    "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
    "volume": "main",
    "abstract": "Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using Bayesian optimization, our parametric Markov-copula model yields more favorable error-cost trade-offs, improving the area under the error-cost curve by 4.3% on average for cascades with $k\\geq 3$ models. In the low-sample regime with $n \\leq 30$ training examples, the performance improvement widens to 10.2%, suggesting that our framework's inductive assumptions about the interactions between the error rates of different LLMs enhance sample efficiency. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing systems of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael J. Zellinger",
      "Matt Thomson"
    ]
  },
  "https://openreview.net/forum?id=WfVXe88oMh": {
    "title": "Proximal Policy Distillation",
    "volume": "main",
    "abstract": "We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that integrates student-driven distillation and Proximal Policy Optimization (PPO) to increase sample efficiency and to leverage the additional rewards that the student policy collects during distillation. To assess the efficacy of our method, we compare PPD with two common alternatives, student-distill and teacher-distill, over a wide range of reinforcement learning environments that include discrete actions and continuous control (ATARI, Mujoco, and Procgen). For each environment and method, we perform distillation to a set of target student neural networks that are smaller, identical (self-distillation), or larger than the teacher network. Our findings indicate that PPD improves sample efficiency and produces better student policies compared to typical policy distillation approaches. Moreover, PPD demonstrates greater robustness than alternative methods when distilling policies from imperfect demonstrations. The code for the paper is released as part of a new Python library built on top of stable-baselines3 to facilitate policy distillation: <Anonymized GitHub Repository>",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Spigler"
    ]
  },
  "https://openreview.net/forum?id=6RCs2tLsHq": {
    "title": "Metamorphic Forward Adaptation Network: Dynamically Adaptive and Modular Multi-layer Learning",
    "volume": "main",
    "abstract": "Back-propagation is a widely used algorithm for training neural networks by adjusting weights based on error gradients. However, back-propagation is biologically implausible with global derivative computation and lacks robustness in long-term dynamic learning. A previously proposed alternative to back-propagation is the Forward-Forward algorithm, which bypasses global gradient dependency and localises computations, making it a more biologically plausible approach. However, Forward-Forward has been evaluated in limited environments, does not yet match back-propagation's performance, and only supports classification, not regression. This research introduces the Metamorphic Forward Adaptation Network (MFAN), using a contrastive learning property as its core, and retaining the layer-wise architecture of the Forward-Forward algorithm. Compared to the Forward-Forward model being limited to discrete classification, MFAN can process discrete and continuous data, showing stability, adaptability, and the ability to handle evolving data. MFAN performs well in continuous data stream scenarios, demonstrating superior adaptability and robustness compared to back-propagation, particularly in tasks requiring dynamic, long-term learning",
    "checked": false,
    "id": "8fad565cd99d91f115afb29e958ec6cdedac70cc",
    "semantic_title": "context-aware feature extraction network for high-precision uav-based vehicle detection in urban environments",
    "citation_count": 0,
    "authors": [
      "Yu Sun",
      "Vijja Wichitwechkarn",
      "Ronald Clark",
      "Mirko Kovac",
      "Basaran Bahadir Kocer"
    ]
  },
  "https://openreview.net/forum?id=rkfop9GyxB": {
    "title": "Lie Symmetry Net: Preserving Conservation Laws in Modelling Financial Market Dynamics via Differential Equations",
    "volume": "main",
    "abstract": "This paper employs a novel Lie symmetries-based framework to model the intrinsic symmetries within financial market. Specifically, we introduce Lie symmetry net (LSN), which characterises the Lie symmetries of the differential equations (DE) estimating financial market dynamics, such as the Black-Scholes equation. To simulate these differential equations in a symmetry-aware manner, LSN incorporates a Lie symmetry risk derived from the conservation laws associated with the Lie symmetry operators of the target differential equations. This risk measures how well the Lie symmetries are realised and guides the training of LSN under the structural risk minimisation framework. Extensive numerical experiments demonstrate that LSN effectively realises the Lie symmetries and achieves an error reduction of more than one order of magnitude compared to state-of-the-art methods. The code is available at https://github.com/Jxl163/LSN_code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuelian Jiang",
      "Tongtian Zhu",
      "Yingxiang Xu",
      "Can Wang",
      "Yeyu Zhang",
      "Fengxiang He"
    ]
  },
  "https://openreview.net/forum?id=NbRybPuWCv": {
    "title": "A Framework for Finding Local Saddle Points in Two-Player Zero-Sum Black-Box Games",
    "volume": "main",
    "abstract": "Saddle point optimization is a critical problem employed in numerous real-world applications, including portfolio optimization, generative adversarial networks, and robotics. It has been extensively studied in cases where the objective function is known and differentiable. Existing work in black-box settings with unknown objectives that can only be sampled either assumes convexity-concavity in the objective to simplify the problem or operates with noisy gradient estimators. In contrast, we introduce a framework inspired by Bayesian optimization which utilizes Gaussian processes to model the unknown (potentially nonconvex-nonconcave) objective and requires only zeroth-order samples. Our approach frames the saddle point optimization problem as a two-level process which can flexibly leverage existing general-sum Nash game solvers to solve for saddle points of zero-sum games. The upper level of our framework produces a model of the objective function by sampling in promising locations, and the lower level of our framework uses the existing model to frame and solve a general-sum game to identify locations to sample. This lower level procedure can be designed in complementary ways, and we demonstrate the flexibility of our approach by introducing variants which appropriately trade off between factors like runtime, the cost of function evaluations, and the number of available initial samples. We experimentally demonstrate these algorithms on synthetic and realistic datasets in black-box nonconvex-nonconcave settings, showcasing their ability to efficiently locate local saddle points in these contexts",
    "checked": true,
    "id": "f1496bf0f1182376c322c6effae9961db1a17954",
    "semantic_title": "a framework for finding local saddle points in two-player zero-sum black-box games",
    "citation_count": 0,
    "authors": [
      "Shubhankar Agarwal",
      "Hamzah I Khan",
      "Sandeep P. Chinchali",
      "David Fridovich-Keil"
    ]
  },
  "https://openreview.net/forum?id=kK0WrBZAli": {
    "title": "Scalable Multi-Output Gaussian Processes with Stochastic Variational Inference",
    "volume": "main",
    "abstract": "The Multi-Output Gaussian Process (MOGP) is a popular tool for modelling data from multiple sources. A typical choice to build a covariance function for a MOGP is the Linear Model of Coregionalisation (LMC) which parametrically models the covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises this idea by modelling the covariance between outputs using a kernel applied to latent variables, one per output, leading to a flexible MOGP model that allows efficient generalisation to new outputs with few data points. The computational complexity in LV-MOGP grows linearly with the number of outputs, which makes it unsuitable for problems with a large number of outputs. In this paper, we propose a stochastic variational inference approach for the LV-MOGP that allows mini-batches for both inputs and outputs, making computational complexity per training iteration independent of the number of outputs. We demonstrate the performance of the model by benchmarking against some other MOGP models in several real-world datasets, including spatial-temporal climate modelling and spatial transcriptomics",
    "checked": true,
    "id": "3094b1bf913ab6af02a9d15235eb1658a73e6ba0",
    "semantic_title": "scalable multi-output gaussian processes with stochastic variational inference",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Jiang",
      "Sokratia Georgaka",
      "Magnus Rattray",
      "Mauricio A Álvarez"
    ]
  },
  "https://openreview.net/forum?id=IGsEgWM4to": {
    "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized code generation but are require significant resources and tend to over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs is a cost-effective alternative, yet standard supervised approaches rely solely on correct examples, overlooking valuable insights from failures. We introduce CodeLutra, a new framework that leverages both correct and incorrect code attempts. Instead of purely instructing with correct solutions, CodeLutra uses iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This process narrows the performance gap with state-of-the-art, larger models, without requiring massive datasets or auxiliary models. For example, on a challenging data science coding task, using only 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's level. By capitalizing on both successes and mistakes, \\textsc{CodeLutra} offers a scalable, efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leitian Tao",
      "Xiang Chen",
      "Tong Yu",
      "Tung Mai",
      "Ryan A. Rossi",
      "Yixuan Li",
      "Saayan Mitra"
    ]
  },
  "https://openreview.net/forum?id=bpaLYaf6Dp": {
    "title": "Disappearance of Timestep Embedding: A Case Study on Neural ODE and Diffusion Models",
    "volume": "main",
    "abstract": "Dynamical systems are often time-varying, whose modeling requires a function that evolves with respect to time. Recent studies such as the neural ordinary differential equation proposed a time-dependent neural network, which provides a neural network varying with respect to time. However, we claim that the architectural choice to build a time-dependent neural network significantly affects its time-awareness but still lacks sufficient validation in its current states. In this study, we conduct an in-depth analysis of the architecture of neural ordinary differential equations. Here, we report a vulnerability of vanishing timestep embedding, which disables the time-awareness of a time-dependent neural network. Specifically, we find that the ConcatConv operation, which is widely used in neural ordinary differential equations, causes an additive effect of timestep embedding, which is readily canceled out by the subsequent batch normalization. This vanishing timestep embedding also arises for group normalization and is analyzed thoroughly with respect to the number of channels, groups, and relative variance. Furthermore, we find that this vulnerability can also be observed in diffusion models because they employ a similar architecture that incorporates timestep embedding to discriminate between different timesteps during a diffusion process. Our analysis provides a detailed description of this phenomenon as well as several solutions to address the root cause. Through experiments on neural ordinary differential equations and diffusion models, we observed that ensuring alive time-awareness via proposed solutions boosted their performance, such as classification accuracy, FID, and inception score, which implies that their current implementations lack sufficient time-dependency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bum Jun Kim",
      "Yoshinobu Kawahara",
      "Sang Woo Kim"
    ]
  },
  "https://openreview.net/forum?id=GtXSN52nIW": {
    "title": "Sparser, Better, Faster, Stronger: Sparsity Detection for Efficient Automatic Differentiation",
    "volume": "main",
    "abstract": "From implicit differentiation to probabilistic modeling, Jacobian and Hessian matrices have many potential use cases in Machine Learning (ML), but they are viewed as computationally prohibitive. Fortunately, these matrices often exhibit sparsity, which can be leveraged to speed up the process of Automatic Differentiation (AD). This paper presents advances in sparsity detection, previously the performance bottleneck of Automatic Sparse Differentiation (ASD). Our implementation of sparsity detection is based on operator overloading, able to detect both local and global sparsity patterns, and supports flexible index set representations. It is fully automatic and requires no modification of user code, making it compatible with existing ML codebases. Most importantly, it is highly performant, unlocking Jacobians and Hessians at scales where they were considered too expensive to compute. On real-world problems from scientific ML, graph neural networks and optimization, we show significant speed-ups of up to three orders of magnitude. Notably, using our sparsity detection system, ASD outperforms standard AD for one-off computations, without amortization of either sparsity detection or matrix coloring",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Hill",
      "Guillaume Dalle"
    ]
  },
  "https://openreview.net/forum?id=IcOBCufqFO": {
    "title": "Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations",
    "volume": "main",
    "abstract": "Vision-language contrastive learning frameworks such as CLIP enable learning representations from natural language supervision and provide strong zero-shot classification capabilities. However, due to the nature of the supervisory signal in these paradigms, they lack the ability to learn localized features, leading to degraded performance on dense prediction tasks such as segmentation and detection. On the other hand, self-supervised learning methods have shown the ability to learn granular representations, complementing the high-level features in vision-language training. In this work, we present Harmony, a framework that combines vision-language training with discriminative and generative self-supervision to learn visual features that can be generalized across different downstream vision tasks. Our framework is specifically designed to work on web-scraped data by not relying on negative examples in the self-supervised learning path and addressing the one-to-one correspondence issue using soft CLIP targets generated by an EMA model. Moreover, Harmony optimizes for five different objectives simultaneously, efficiently utilizing the supervision in each data example, making it even more suited in data-constrained settings. We comprehensively evaluate Harmony across various vision downstream tasks and find that it significantly outperforms the baseline CLIP and outperforms the previously leading joint self- and weakly supervised methods, SLIP, MaskCLIP, and DetailCLIP. Specifically, when compared against these methods, Harmony shows superior performance in linear-probing, fine-tuning, and zero-shot classification on ImageNet-1k, semantic segmentation on ADE20K, and both object detection and instance segmentation on MS-COCO, when pre-training a ViT-B on CC3M. We also show that Harmony outperforms SILC on detection, linear and fine-tuning classification, and outperforms other self-supervised learning methods like iBOT and MAE across all tasks evaluated. Our code is publicly available at https://github.com/MohammedSB/Harmony",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Baharoon",
      "Jonathan Klein",
      "Dominik Michels"
    ]
  },
  "https://openreview.net/forum?id=3ECbEZg2If": {
    "title": "Full-Rank Unsupervised Node Embeddings for Directed Graphs via Message Aggregation",
    "volume": "main",
    "abstract": "Linear message-passing models have emerged as compelling alternatives to non-linear graph neural networks for unsupervised node embedding learning, due to their scalability and competitive performance on downstream tasks. However, we identify a fundamental flaw in recently proposed linear models that combine embedding aggregation with concatenation during each message-passing iteration: rank deficiency. A rank-deficient embedding matrix contains column vectors which take arbitrary values, leading to ill-conditioning that degrades downstream task accuracy, particularly in unsupervised tasks such as graph alignment. We deduce that repeated embedding aggregation and concatenation introduces linearly dependent features, causing rank deficiency. To address this, we propose ACC (Aggregate, Compress, Concatenate), a novel model that avoids redundant feature computation by applying aggregation to the messages from the previous iteration, rather than the embeddings. Consequently, ACC generates full-rank embeddings, significantly improving graph alignment accuracy from 10% to 60% compared to rank-deficient embeddings, while also being faster to compute. Additionally, ACC employs directed message-passing and achieves node classification accuracies comparable to state-of-the-art self-supervised graph neural networks on directed graph benchmarks, while also being over 70 times faster on graphs with over 1 million edges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ciwan Ceylan",
      "Kambiz Ghoorchian",
      "Danica Kragic"
    ]
  },
  "https://openreview.net/forum?id=u4YDVFodYX": {
    "title": "Prior Learning in Introspective VAEs",
    "volume": "main",
    "abstract": "Variational Autoencoders (VAEs) are a popular framework for unsupervised learning and data generation. A plethora of methods have been proposed focusing on improving VAEs, with the incorporation of adversarial objectives and the integration of prior learning mechanisms being prominent directions. When it comes to the former, an indicative instance is the recently introduced family of Introspective VAEs aiming at ensuring that a low likelihood is assigned to unrealistic samples. In this study, we focus on the Soft-IntroVAE (S-IntroVAE), one of only two members of the Introspective VAE family, the other being the original IntroVAE. We select S-IntroVAE for its state-of-the-art status and its training stability. In particular, we investigate the implication of incorporating a multimodal and trainable prior into this S-IntroVAE. Namely, we formulate the prior as a third player and show that when trained in cooperation with the decoder constitutes an effective way for prior learning, which shares the Nash Equilibrium with the vanilla S-IntroVAE. Furthermore, based on a modified formulation of the optimal ELBO in S-IntroVAE, we develop theoretically motivated regularizations, namely (i) adaptive variance clipping to stabilize training when learning the prior and (ii) responsibility regularization to discourage the formation of inactive prior modes. Finally, we perform a series of targeted experiments on a 2D density estimation benchmark and in an image generation setting comprised of the (F)-MNIST and CIFAR-10 datasets demonstrating the effect of prior learning in S-IntroVAE in generation and representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Athanasiadis",
      "Fredrik Lindsten",
      "Michael Felsberg"
    ]
  },
  "https://openreview.net/forum?id=EDQ8QOGqjr": {
    "title": "Learning Using a Single Forward Pass",
    "volume": "main",
    "abstract": "We propose a learning algorithm to overcome the limitations of traditional backpropagation in resource-constrained environments: Solo Pass Embedded Learning Algorithm (SPELA). SPELA operates with local loss functions to update weights, significantly saving on resources allocated to the propagation of gradients and storing computational graphs while being sufficiently accurate. Consequently, SPELA can closely match backpropagation using less memory. Moreover, SPELA can effectively fine-tune pre-trained image recognition models for new tasks. Further, SPELA is extended with significant modifications to train CNN networks, which we evaluate on CIFAR-10, CIFAR-100, and SVHN 10 datasets, showing equivalent performance compared to backpropagation. Our results indicate that SPELA, with its features such as local learning and early exit, is a potential candidate for learning in resource-constrained edge AI applications",
    "checked": true,
    "id": "52f29792f2901985bd44c78de09e16dcbd258836",
    "semantic_title": "learning using a single forward pass",
    "citation_count": 0,
    "authors": [
      "Aditya Somasundaram",
      "Pushkal Mishra",
      "Ayon Borthakur"
    ]
  },
  "https://openreview.net/forum?id=hQjwDqfSzj": {
    "title": "Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making",
    "volume": "main",
    "abstract": "Statistical models are often defined by a generative process for simulating synthetic data, but this can lead to intractable likelihoods. Likelihood free inference (LFI) methods enable Bayesian inference to be performed in this case. Extending a popular approach to simulation-efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood Free Inference (MOBOLFI) to perform LFI using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a separate discrepancy for each data source. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling detection of conflicting information and deeper understanding of the importance of different data sources in estimating individual parameters. The adaptive choice of simulation parameters using multi-objective Bayesian optimization ensures simulation efficient approximation of likelihood components for all data sources. We illustrate our approach in sequential sampling models (SSMs), which are widely used in psychology and consumer-behavior modeling. SSMs are often fitted using multi-source data, such as choice and response time. The advantages of our approach are illustrated in comparison with a single discrepancy for an SSM fitted to data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles",
    "checked": true,
    "id": "ede1395863e3c9e8a41812b9589eff46b248dc27",
    "semantic_title": "multi-objective bayesian optimization for likelihood-free inference in sequential sampling models of decision making",
    "citation_count": 0,
    "authors": [
      "David Chen",
      "Xinwei Li",
      "Eui-Jin Kim",
      "Prateek Bansal",
      "David J Nott"
    ]
  },
  "https://openreview.net/forum?id=FNRdaHz3qN": {
    "title": "Change Point Detection in the Frequency Domain with Statistical Reliability",
    "volume": "main",
    "abstract": "Effective condition monitoring in complex systems requires identifying change points (CPs) in the frequency domain, as the structural changes often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akifumi Yamada",
      "Tomohiro Shiraishi",
      "Shuichi Nishino",
      "Teruyuki Katsuoka",
      "Kouichi Taji",
      "Ichiro Takeuchi"
    ]
  },
  "https://openreview.net/forum?id=HBZoXjUAqV": {
    "title": "Recall and Refine: A Simple but Effective Source-free Open- set Domain Adaptation Framework",
    "volume": "main",
    "abstract": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where novel classes — also referred to as target-private unknown classes — are present. Source-free Open-set Domain Adaptation (SF-OSDA) methods address OSDA without accessing labeled source data, making them particularly relevant under privacy constraints. However, SF-OSDA presents significant challenges due to distribution shifts and the introduction of novel classes. Existing SF-OSDA methods typically rely on thresholding the prediction entropy of a sample to identify it as either a known or unknown class, but fail to explicitly learn discriminative features for the target-private unknown classes. We propose Recall and Refine (RRDA), a novel SF-OSDA framework designed to address these limitations by explicitly learning features for target-private unknown classes. RRDA employs a two-stage process. First, we enhance the model's capacity to recognize unknown classes by training a target classifier with an additional decision boundary, guided by synthetic samples generated from target domain features. This enables the classifier to effectively separate known and unknown classes. Second, we adapt the entire model to the target domain, addressing both domain shifts and distinguishability to unknown classes. Any off-the-shelf source-free domain adaptation method (e.g.\\ SHOT, AaD) can be seamlessly integrated into our framework at this stage. Extensive experiments on three benchmark datasets demonstrate that RRDA significantly outperforms existing SF-OSDA and OSDA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ismail Nejjar",
      "Hao Dong",
      "Olga Fink"
    ]
  },
  "https://openreview.net/forum?id=ylUVRikhTL": {
    "title": "Mixed-View Panorama Synthesis using Geospatially Guided Diffusion",
    "volume": "main",
    "abstract": "We introduce the task of mixed-view panorama synthesis, where the goal is to synthesize a novel panorama given a small set of input panoramas and a satellite image of the area. This contrasts with previous work which only uses input panoramas (same-view synthesis), or an input satellite image (cross-view synthesis). We argue that the mixed-view setting is the most natural to support panorama synthesis for arbitrary locations worldwide. A critical challenge is that the spatial coverage of panoramas is uneven, with few panoramas available in many regions of the world. We introduce an approach that utilizes diffusion-based modeling and an attention-based architecture for extracting information from all available input imagery. Experimental results demonstrate the effectiveness of our proposed method. In particular, our model can handle scenarios when the available panoramas are sparse or far from the location of the panorama we are attempting to synthesize",
    "checked": true,
    "id": "9f52715bd5bf9328e0565e9fcbbed323278d596a",
    "semantic_title": "mixed-view panorama synthesis using geospatially guided diffusion",
    "citation_count": 1,
    "authors": [
      "Zhexiao Xiong",
      "Xin Xing",
      "Scott Workman",
      "Subash Khanal",
      "Nathan Jacobs"
    ]
  },
  "https://openreview.net/forum?id=S6fe4aH6YA": {
    "title": "Link Prediction with Relational Hypergraphs",
    "volume": "main",
    "abstract": "Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to inductive link prediction with relational hypergraphs, where the task is over $k$-ary relations, substantially harder than link prediction on knowledge graphs with binary relations only. In this paper, we propose a framework for link prediction with relational hypergraphs, empowering applications of graph neural networks on fully relational structures. Theoretically, we conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms and also via logical expressiveness. Empirically, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction and also lead to competitive results for transductive link prediction",
    "checked": true,
    "id": "d5591f0ffa407eae914b45e5329768f8ce553862",
    "semantic_title": "link prediction with relational hypergraphs",
    "citation_count": 4,
    "authors": [
      "Xingyue Huang",
      "Miguel Romero Orth",
      "Pablo Barcelo",
      "Michael M. Bronstein",
      "Ismail Ilkan Ceylan"
    ]
  },
  "https://openreview.net/forum?id=DDUsc1lD27": {
    "title": "Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), agents have no incentive to exhibit predictable trajectories, and are often pushed (through e.g. policy entropy regularisation) to randomise their actions in favor of exploration. This lack of predictability awareness often makes it challenging for other agents and humans to predict an agent's trajectories, possibly triggering unsafe scenarios (e.g. in human-robot interaction). We propose a novel method to induce predictable trajectories in RL agents, termed Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate to quantify predictability. Our method maximizes a linear combination of a standard discounted reward and the negative entropy rate, thus trading off optimality with predictability. We show how the entropy rate can be formally cast as an average reward, how entropy-rate value functions can be estimated from a learned model and incorporate this in policy-gradient algorithms, and demonstrate how this approach produces predictable (near-optimal) policies in tasks inspired by human-robot use-cases",
    "checked": true,
    "id": "8aaca0d0fff646acf187628d679844b4bbfeee70",
    "semantic_title": "predictable reinforcement learning dynamics through entropy rate minimization",
    "citation_count": 2,
    "authors": [
      "Daniel Jarne Ornia",
      "Giannis Delimpaltadakis",
      "Jens Kober",
      "Javier Alonso-Mora"
    ]
  },
  "https://openreview.net/forum?id=FVFqrxeF8e": {
    "title": "Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients",
    "volume": "main",
    "abstract": "We propose Mirror Descent Optimal Transport (MDOT), a novel method for solving discrete optimal transport (OT) problems with high precision, by unifying temperature annealing in entropic-regularized OT (EOT) with mirror descent techniques. In this framework, temperature annealing produces a sequence of EOT dual problems, whose solution gradually gets closer to the solution of the original OT problem. We solve each problem efficiently using a GPU-parallel nonlinear conjugate gradients algorithm (PNCG) that outperforms traditional Sinkhorn iterations under weak regularization. Moreover, our investigation also reveals that the theoretical convergence rate of Sinkhorn iterations can exceed existing non-asymptotic bounds when its stopping criterion is tuned in a manner analogous to MDOT. Our comprehensive ablation studies of MDOT-PNCG affirm its robustness across a wide range of algorithmic parameters. Benchmarking on 24 problem sets of size $n=4096$ in a GPU environment demonstrate that our method attains high-precision, feasible solutions significantly faster than a representative set of existing OT solvers—including accelerated gradient methods and advanced Sinkhorn variants—in both wall-clock time and number of operations. Empirical convergence rates range between $O(n^2 \\varepsilon^{-1/4})$ and $O(n^2 \\varepsilon^{-1})$, where $\\varepsilon$ is the optimality gap. For problem sizes up to $n=16\\,384$, the empirical runtime scales as $\\widetilde{O}(n^2)$ for moderate precision and as $\\widetilde{O}(n^{5/2})$ at worst for high precision. These findings establish MDOT-PNCG as a compelling alternative to current OT solvers, particularly in challenging weak-regularization regimes",
    "checked": true,
    "id": "aeae241d9dbd868d00c1e982f6c8e9664cf31c38",
    "semantic_title": "efficient and accurate optimal transport with mirror descent and conjugate gradients",
    "citation_count": 3,
    "authors": [
      "Mete Kemertas",
      "Allan Douglas Jepson",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=p7jQEf3wlh": {
    "title": "Efficient Hardware Scaling and Diminishing Returns in Large-Scale Training of Language Models",
    "volume": "main",
    "abstract": "To train the exceedingly large neural networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model training. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies with current best practices. In experiments with model sizes up to 70B parameters and utilizing up to 2048 H100 GPUs, we demonstrate that: (1) Naive scale out with Fully Sharded Data Parallelism (FSDP) incurs communication overhead which leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jared Fernandez",
      "Luca Wehrstedt",
      "Leonid Shamis",
      "Mostafa Elhoushi",
      "Kalyan Saladi",
      "Yonatan Bisk",
      "Emma Strubell",
      "Jacob Kahn"
    ]
  },
  "https://openreview.net/forum?id=cqDH0e6ak2": {
    "title": "Flow map matching with stochastic interpolants: A mathematical framework for consistency models",
    "volume": "main",
    "abstract": "Generative models based on dynamical equations such as flows and diffusions offer exceptional sample quality, but require computationally expensive numerical integration during inference. The advent of consistency models has enabled efficient one-step or few-step generation, yet despite their practical success, a systematic understanding of their design has been hindered by the lack of a comprehensive theoretical framework. Here we introduce Flow Map Matching (FMM), a principled framework for learning the two-time flow map of an underlying dynamical generative model, thereby providing this missing mathematical foundation. Leveraging stochastic interpolants, we propose training objectives both for distillation from a pre-trained velocity field and for direct training of a flow map over an interpolant or a forward diffusion process. Theoretically, we show that FMM unifies and extends a broad class of existing approaches for fast sampling, including consistency models, consistency trajectory models, and progressive distillation. Experiments on CIFAR-10 and ImageNet-32 highlight that our approach can achieve sample quality comparable to flow matching while reducing generation time by a factor of 10-20",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Matthew Boffi",
      "Michael Samuel Albergo",
      "Eric Vanden-Eijnden"
    ]
  },
  "https://openreview.net/forum?id=qVUEuhlaEa": {
    "title": "Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning is an evolving machine learning paradigm, in which multiple clients perform computations based on their individual private data, interspersed by communication with a remote server. A common strategy to curtail communication costs is Local Training, which consists in performing multiple local stochastic gradient descent steps between successive communication rounds. However, the conventional approach to local training overlooks the practical necessity for client-specific personalization, a technique to tailor local models to individual needs. We introduce Scafflix, a novel algorithm that efficiently integrates explicit personalization with local training. This innovative approach benefits from these two techniques, thereby achieving doubly accelerated communication, as we demonstrate both in theory and practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Yi",
      "Laurent Condat",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=z3RIiidJgD": {
    "title": "MemBench: Memorized Image Trigger Prompt Dataset for Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in Text-to-Image generation tasks, leading to the development of many commercial models. However, recent studies have reported that diffusion models often repeatedly generate memorized images in train data when triggered by specific prompts, potentially raising social issues ranging from copyright to privacy concerns. To sidestep the memorization, recent studies have been conducted to develop memorization mitigation methods for diffusion models. Nevertheless, the lack of benchmarks hinders the assessment of the true effectiveness of these methods. In this work, we present MemBench, the first benchmark for evaluating image memorization mitigation methods. Our benchmark includes a large number of memorized image trigger prompts in various Text-to-Image diffusion models. Furthermore, in contrast to the prior work evaluating mitigation performance only on trigger prompts, we present metrics evaluating on both trigger prompts and general prompts, so that we can see whether mitigation methods address the memorization issue while maintaining performance for general prompts. Through our MemBench evaluation, we revealed that existing memorization mitigation methods notably degrade the overall performance of diffusion models and need to be further developed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunsan Hong",
      "Tae-Hyun Oh",
      "Minhyuk Sung"
    ]
  },
  "https://openreview.net/forum?id=3HKNwejEEq": {
    "title": "NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA",
    "volume": "main",
    "abstract": "The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future",
    "checked": true,
    "id": "66aba2df8c6994aefe8136d98d93eba63f36f385",
    "semantic_title": "neurips 2023 competition: privacy preserving federated learning document vqa",
    "citation_count": 0,
    "authors": [
      "Marlon Tobaben",
      "Mohamed Ali Souibgui",
      "Rubèn Tito",
      "Khanh Nguyen",
      "Raouf Kerkouche",
      "Kangsoo Jung",
      "Joonas Jälkö",
      "Lei Kang",
      "Andrey Barsky",
      "Vincent Poulain d'Andecy",
      "Aurélie JOSEPH",
      "Aashiq Muhamed",
      "Kevin Kuo",
      "Virginia Smith",
      "Yusuke Yamasaki",
      "Takumi Fukami",
      "Kenta Niwa",
      "Iifan Tyou",
      "Hiro Ishii",
      "Rio Yokota",
      "Ragul N",
      "Rintu Kutum",
      "Josep Llados",
      "Ernest Valveny",
      "Antti Honkela",
      "Mario Fritz",
      "Dimosthenis Karatzas"
    ]
  },
  "https://openreview.net/forum?id=R2rasAEPVi": {
    "title": "Leopard: A Vision Language Model for Text-Rich Multi- Image Tasks",
    "volume": "main",
    "abstract": "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose Leopard, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we proposed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of images. Experiments on a diverse set of benchmarks reveal that our model consistently outperforms state-of-the-art systems, such as Llama-3.2 and Qwen2-VL, in challenging text-rich, multi-image evaluations. Remarkably, our approach achieves outstanding performance using only 1.2M fully open-sourced training instances, outperforming models that rely on large-scale in-house data, highlighting its efficiency and effectiveness. Our code and data are available at https://anonymous.4open.science/r/Leopard-908F",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Jia",
      "Wenhao Yu",
      "Kaixin Ma",
      "Tianqing Fang",
      "Zhihan Zhang",
      "Siru Ouyang",
      "Hongming Zhang",
      "Dong Yu",
      "Meng Jiang"
    ]
  },
  "https://openreview.net/forum?id=bAM8y3Hm0p": {
    "title": "Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity Resolution",
    "volume": "main",
    "abstract": "The entity resolution problem requires finding pairs across datasets that belong to different owners but refer to the same entity in the real world. To train and evaluate solutions (either rule-based or machine-learning-based) to the entity resolution problem, generating a ground truth dataset with entity pairs or clusters is needed. However, such a data annotation process involves humans as domain oracles to review the plaintext data for all candidate record pairs from different parties, which inevitably infringes the privacy of data owners, especially in privacy-sensitive cases like medical records. To the best of our knowledge, there is no prior work on privacy-preserving ground truth labeling in the context of entity resolution. We propose a novel blind annotation protocol based on homomorphic encryption that allows domain oracles to collaboratively label ground truth without sharing data in plaintext with other parties. In addition, we design a domain-specific, user-friendly language that conceals the complex underlying homomorphic encryption circuits, making it more accessible and easier for users to adopt this technique. The empirical experiments indicate the feasibility of our privacy-preserving protocol (f-measure on average achieves more than 90\\% compared with the real ground truth)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiang Yao",
      "Weizhao Jin",
      "Srivatsan Ravi"
    ]
  },
  "https://openreview.net/forum?id=B0E2yjrNb8": {
    "title": "Dynamic Schwartz-Fourier Neural Operator for Enhanced Expressive Power",
    "volume": "main",
    "abstract": "Recently, neural operators have emerged as a prevailing approach for learning discretization-invariant mappings between function spaces. A particular example is the Fourier Neural Operator (FNO), which constrains integral kernels to be convolutions and learns the kernel directly in the frequency domain. Due to the capacity of Fourier transforms to effectively reduce the dimensionality and preserve information, FNOs demonstrate superior performance in terms of both efficiency and accuracy. In FNOs, the convolution kernel is fixed as a point-wise multiplication in the frequency domain; however, these translation-invariant kernels might limit the expressiveness of FNOs. For instance, if the underlying system lacks translational symmetries, the kernels learned by the FNO will still exhibit translational invariance, thereby limiting the model's expressive power. We propose a dynamic Schwartz operator that induces interactions between modes to enhance the expressiveness of FNOs. In this work, we introduce a novel approach that equips FNOs with Schwartz operators to learn dynamic kernels, termed Dynamic Kernel Fourier Neural Operators (DSFNOs). By incorporating this dynamic mechanism, our model gains the ability to capture relevant frequency information patterns, facilitating a better understanding and representation of complex physical phenomena. Through experiments, we demonstrate that DSFNOs can improve FNOs on a range of tasks, highlighting the effectiveness of our proposed approach. The code is available at https://github.com/wenhangao21/TMLR25_DSFNO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Gao",
      "Jian Luo",
      "Ruichen Xu",
      "Yi Liu"
    ]
  },
  "https://openreview.net/forum?id=z27hb0rmLT": {
    "title": "Normality-Guided Distributional Reinforcement Learning for Continuous Control",
    "volume": "main",
    "abstract": "Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirically quite close to normal. We design a method that exploits this property, employing variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically significant improvements in 10 out of 16 continuous task settings, while utilizing a reduced number of weights and achieving faster training time compared to an ensemble-based method for quantifying value distribution uncertainty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Seung Byun",
      "Andrew Perrault"
    ]
  },
  "https://openreview.net/forum?id=VdW9SkALSd": {
    "title": "Mathematical Characterization of Better-than-Random Multiclass Models",
    "volume": "main",
    "abstract": "A binary supervised model outperforms chance if and only if the determinant of the confusion matrix is positive. This is equivalent to saying that the associated point in the ROC space is above the random guessing line. This also means that Youden's J, Cohen's $\\kappa$ and Matthews' correlation coefficient are positive. We extend these results to any number of classes: for a target variable with $m \\geq 2$ classes, we show that a model does better than chance if and only if the entries of the confusion matrix verify $m(m-1)$ homogeneous polynomial inequalities of degree 2, which can be expressed using generalized likelihood ratios. We also obtain a more theoretical formulation: a model does better than chance if and only if it is a maximum likelihood estimator of the target variable. When this is the case, we find that the multiclass versions of the previous metrics remain positive. If $m>2$, we notice that no-skill classifiers are only a small part of the topological boundary between better-than-random models and bad models. For $m=3$, we show that bad models occupy exactly 90\\% of the ROC space, far more than the 50\\% of the two-class problems. Finally, we propose to define weak multiclass classifiers by conditions on these generalized likelihood ratios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sébastien Foulle"
    ]
  },
  "https://openreview.net/forum?id=3qmnxysNbi": {
    "title": "To Be Greedy, or Not to Be – That Is the Question for Population Based Training Variants",
    "volume": "main",
    "abstract": "Achieving excellent results with neural networks requires careful hyperparameter tuning, which can be automated via hyperparameter optimization algorithms such as Population Based Training (PBT). PBT stands out for its capability to efficiently optimize hyperparameter schedules in parallel and within the wall-clock time of training a single network. Several PBT variants have been proposed that improve performance in the experimental settings considered in the associated publications. However, the experimental settings and tasks vary across publications, while the best previous PBT variant is not always included in the comparisons, thus making the relative performance of PBT variants unclear. In this work, we empirically evaluate five single-objective PBT variants on a set of image classification and reinforcement learning tasks with different setups (such as increasingly large search spaces). We find that the Bayesian Optimization (BO) variants of PBT tend to behave greedier than the non-BO ones, which is beneficial when aggressively pursuing short-term gains improves long-term performance and harmful otherwise. This is a previously overlooked caveat to the reported improvements of the BO PBT variants. Examining their theoretical properties, we find that the returns of BO PBT variants are guaranteed to asymptotically approach the returns of the greedy hyperparameter schedule (rather than the optimal one, as claimed in prior work). Together with our empirical results, this leads us to conclude that there is currently no single best PBT variant capable of outperforming others both when pursuing short-term gains is helpful in the long term, and when it is harmful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Chebykin",
      "Tanja Alderliesten",
      "Peter Bosman"
    ]
  },
  "https://openreview.net/forum?id=XPEEsKneKs": {
    "title": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems",
    "volume": "main",
    "abstract": "When solving inverse problems, one increasingly popular approach is to use pre-trained diffusion models as plug-and-play priors. This framework can accommodate different forward models without re-training while preserving the generative capability of diffusion models. Despite their success in many imaging inverse problems, most existing methods rely on privileged information such as derivative, pseudo-inverse, or full knowledge about the forward model. This reliance poses a substantial limitation that restricts their use in a wide range of problems where such information is unavailable, such as in many scientific applications. We propose Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free approach that can solve inverse problems by only accessing forward model evaluations and a pre-trained diffusion model prior. We study the empirical effectiveness of EnKG across various inverse problems, including scientific settings such as inferring fluid flows and astronomical objects, which are highly non-linear inverse problems that often only permit black-box access to the forward model. We open-source our code at https://github.com/devzhk/enkg-pytorch",
    "checked": true,
    "id": "ffde2508da9e9e1dc7e571c57c4f84005813265f",
    "semantic_title": "ensemble kalman diffusion guidance: a derivative-free method for inverse problems",
    "citation_count": 5,
    "authors": [
      "Hongkai Zheng",
      "Wenda Chu",
      "Austin Wang",
      "Nikola Borislavov Kovachki",
      "Ricardo Baptista",
      "Yisong Yue"
    ]
  },
  "https://openreview.net/forum?id=5EXrH2h3I5": {
    "title": "A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models",
    "volume": "main",
    "abstract": "Few-shot semantic segmentation (FSS) is a crucial challenge in computer vision, driving extensive research into a diverse range of methods, from advanced meta-learning techniques to simple transfer learning baselines. With the emergence of vision foundation models (VFM) serving as generalist feature extractors, we seek to explore the adaptation of these models for FSS. While current FSS benchmarks focus on adapting pre-trained models to new tasks with few images, they emphasize in-domain generalization, making them less suitable for VFM trained on large-scale web datasets. To address this, we propose a novel realistic benchmark with a simple and straightforward adaptation process tailored for this task. Using this benchmark, we conduct a comprehensive comparative analysis of prominent VFM and semantic segmentation models. To evaluate their effectiveness, we leverage various adaption methods, ranging from linear probing to parameter efficient fine-tuning (PEFT) and full fine-tuning. Our findings show that models designed for segmentation can be outperformed by self-supervised (SSL) models. On the other hand, while PEFT methods yields competitive performance, they provide little discrepancy in the obtained results compared to other methods, highlighting the critical role of the feature extractor in determining results. To our knowledge, this is the first study on the adaptation of VFM for FSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reda Bensaid",
      "Vincent Gripon",
      "François Leduc-Primeau",
      "Lukas Mauch",
      "Ghouthi BOUKLI HACENE",
      "Fabien Cardinaux"
    ]
  },
  "https://openreview.net/forum?id=7aJxaPg30d": {
    "title": "Extending Graph Condensation to Multi-Label Datasets: A Benchmark Study",
    "volume": "main",
    "abstract": "As graph data grows increasingly complicated, training graph neural networks (GNNs) on large-scale datasets presents significant challenges, including computational resource constraints, data redundancy, and transmission inefficiencies. While existing graph condensation techniques have shown promise in addressing these issues, they are predominantly designed for single-label datasets, where each node is associated with a single class label. However, many real-world applications, such as social network analysis and bioinformatics, involve multi-label graph datasets, where one node can have various related labels. To deal with this problem, we extend traditional graph condensation approaches to accommodate multi-label datasets by introducing modifications to synthetic dataset initialization and condensing optimization. Through experiments on eight real-world multi-label graph datasets, we prove the effectiveness of our method. In the experiment, the GCond framework, combined with K-Center initialization and binary cross-entropy loss (BCELoss), generally achieves the best performance. This benchmark for multi-label graph condensation not only enhances the scalability and efficiency of GNNs for multi-label graph data but also offers substantial benefits for diverse real-world applications",
    "checked": true,
    "id": "cd9a56e56f037bc962043448b3fe007f2f72a4ce",
    "semantic_title": "extending graph condensation to multi-label datasets: a benchmark study",
    "citation_count": 1,
    "authors": [
      "Liangliang Zhang",
      "Haoran Bao",
      "Yao Ma"
    ]
  },
  "https://openreview.net/forum?id=pvtgffHtJm": {
    "title": "Diffusion Model Predictive Control",
    "volume": "main",
    "abstract": "We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach that learns a multi-step action proposal and a multi-step dynamics model, both using diffusion models, and combines them for use in online MPC. On the popular D4RL benchmark, we show performance that is significantly better than existing model-based offline planning methods using MPC (e.g. MBOP) and competitive with state-of-the-art (SOTA) model-based and model-free reinforcement learning methods. We additionally illustrate D-MPC's ability to optimize novel reward functions at run time and adapt to novel dynamics, and highlight its advantages compared to existing diffusion-based planning baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyao Zhou",
      "Sivaramakrishnan Swaminathan",
      "Rajkumar Vasudeva Raju",
      "J Swaroop Guntupalli",
      "Wolfgang Lehrach",
      "Joseph Ortiz",
      "Antoine Dedieu",
      "Miguel Lazaro-Gredilla",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=aFAMPSmNHR": {
    "title": "Do Think Tags Really Help LLMs Plan? A Critical Evaluation of ReAct-Style Prompting",
    "volume": "main",
    "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of considerable interest and debate. Among the original papers arguing for emergent reasoning abilities of LLMs, ReAct became particularly popular by claiming to tease out LLM reasoning abilities with special prompting involving \"interleaving reasoning trace with action execution\". In this paper, we critically examine the claims of ReAct style prompting for planning and sequential decision-making problems. By introducing systematic variations to the input prompt, we perform a sensitivity analysis along the original claims of ReAct. Our experiments in AlfWorld and WebShop, domains that were used in the original ReAct work, show that the performance is minimally influenced by the interleaved reasoning trace or by the content of these generated reasoning traces. Instead, the performance of LLMs is primarily driven by the unreasonably high degree of similarity between input example tasks and queries, with shockingly little ability to generalize. In addition to raising questions on claims about reasoning abilities, this lack of generalization also implicitly forces the prompt designer to provide instance-specific examples, significantly increasing the cognitive burden on the human. Our empirical results show that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities, thereby leading to severe lack of generalization beyond the few-shot examples given in the prompts. Our code and prompt settings can be found here on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Bhambri",
      "Mudit Verma",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=3q1bUIHTJK": {
    "title": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting",
    "volume": "main",
    "abstract": "Obeying precise constraints on top of multiple external attributes is a common computational problem underlying seemingly different domains, from controlled text generation to protein engineering. Existing language model (LM) controllability methods for multi-attribute constraint satisfaction often rely on specialized architectures or gradient-based classifiers, limiting their flexibility to work with arbitrary black-box evaluators and pretrained models. Current general-purpose large language models, while capable, cannot achieve fine-grained multi-attribute control over external attributes. Thus, we create Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of finetuning language models on any sequential domain to satisfy user-specified constraints on multiple external real-value attributes. Our method trains LMs as editors by sampling diverse multi-attribute edit pairs from an initial set of paraphrased outputs. During inference, LM iteratively improves upon its previous solution to satisfy constraints for all attributes by leveraging our designed constraint satisfaction reward. We additionally experiment with reward-weighted behavior cloning to further improve the constraint satisfaction rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text Style Transfer, where the goal is to simultaneously modify the sentiment and complexity of reviews, and (2) Protein Design, focusing on modulating fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical results show that MACS achieves the highest threshold satisfaction in both FineCS tasks, outperforming strong domain-specific baselines. Our work opens new avenues for generalized and real-value multi-attribute control, with implications for diverse applications spanning natural language processing and bioinformatics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Baheti",
      "Debanjana Chakraborty",
      "Faeze Brahman",
      "Ronan Le Bras",
      "Ximing Lu",
      "Nouha Dziri",
      "Yejin Choi",
      "Mark Riedl",
      "Maarten Sap"
    ]
  },
  "https://openreview.net/forum?id=0jhoriH9yA": {
    "title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development",
    "volume": "main",
    "abstract": "Transformer architectures have revolutionized a broad spectrum of AI applications by leveraging attention mechanisms for parallelized and long-range sequence processing. Despite their remarkable success, building and customizing transformers remains prohibitively complex for many domain experts who lack deep knowledge of low-level implementations. We introduce AttentionSmithy, a modular software package that lowers the barrier to transformer innovation by decomposing key components---attention modules, feed-forward networks, normalization layers, and positional encodings---into reusable building blocks. By disentangling architectural elements into well-defined interfaces, users can rapidly prototype, adapt, and evaluate transformer variants without extensive coding overhead. Our framework currently supports four distinct positional encoding strategies (sinusoidal, learned, rotary, and ALiBi), offers modular integration of multiple attention methods (including standard attention, Longformer, and Linformer), and integrates seamlessly with neural architecture search (NAS) for automated design exploration. The system is designed to support future extensions with minimal overhead. We validate AttentionSmithy by replicating the original ``Attention Is All You Need'' transformer under resource constraints, demonstrating robust performance on a machine translation task. Leveraging the package's integrated NAS capability, we identified an optimized model configuration that outperformed our baseline, demonstrating the framework's effectiveness for automated architecture search and model improvement. We further illustrate AttentionSmithy's adaptability through gene-specific modeling, where a variant of a BERT-style architecture achieves over 95\\% accuracy on downstream cell type classification tasks using ranked transcriptomic data. These case studies underscore AttentionSmithy's core advantage: enabling specialized experimentation across diverse application domains---from natural language processing to genomic analysis---by obviating the need for labor-intensive, low-level framework manipulation. We anticipate that AttentionSmithy will serve as a foundation for creative transformer-based solutions, expediting research and development in numerous scientific and industrial fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caleb Cranney",
      "Jesse G Meyer"
    ]
  },
  "https://openreview.net/forum?id=JzmXo0rfry": {
    "title": "Evaluating explainability techniques on discrete-time graph neural networks",
    "volume": "main",
    "abstract": "Discrete-time temporal Graph Neural Networks (GNNs) are powerful tools for modeling evolving graph-structured data and are widely used in decision-making processes across domains such as social network analysis, financial systems, and collaboration networks. Explaining the predictions of these models is an important research area due to the critical role their decisions play in building trust in social or financial systems. However, the explainability of Temporal Graph Neural Networks remains a challenging and relatively unexplored field. Hence, in this work, we propose a novel framework to evaluate explainability techniques tailored for discrete-time temporal GNNs. Our framework introduces new training and evaluation settings that capture the evolving nature of temporal data, defines metrics to assess the temporal aspects of explanations, and establishes baselines and models specific to discrete-time temporal networks. Through extensive experiments, we outline the best explainability techniques for discrete-time GNNs in terms of fidelity, efficiency, and human-readability trade-offs. By addressing the unique challenges of temporal graph data, our framework sets the stage for future advancements in explaining discrete-time GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Dileo",
      "Matteo Zignani",
      "Sabrina Tiziana Gaito"
    ]
  },
  "https://openreview.net/forum?id=Q70C1HQ0VO": {
    "title": "Alternators For Sequence Modeling",
    "volume": "main",
    "abstract": "This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and often outperform strong baselines such as Mambas, neural ODEs, and diffusion models in the domains we studied",
    "checked": true,
    "id": "b82554a8e084b487ded937ee84d493d4e6271977",
    "semantic_title": "alternators for sequence modeling",
    "citation_count": 1,
    "authors": [
      "Mohammad Reza Rezaei",
      "Adji Bousso Dieng"
    ]
  },
  "https://openreview.net/forum?id=yzACI2vFaX": {
    "title": "Evaluating Long Range Dependency Handling in Code Generation LLMs",
    "volume": "main",
    "abstract": "As language models support larger and larger context sizes, evaluating their ability to make effective use of that context becomes increasingly important. We analyze the ability of several code generation models to handle long range dependencies using a suite of multi-step key retrieval tasks in context windows up to 8k tokens in length. The tasks progressively increase in difficulty and allow more nuanced evaluation of model capabilities than tests like the popular needle-in-the-haystack test. We find that performance degrades significantly for many models (up to 2x) when a function references another function that is defined later in the prompt. We also observe that models that use sliding window attention mechanisms have difficulty handling references further than the size of a single window. We perform simple prompt modifications using call graph information to improve multi-step retrieval performance up to 3x. Our analysis highlights ways that long-context performance needs deeper consideration beyond retrieval of single facts within a document",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannick Assogba",
      "Donghao Ren"
    ]
  },
  "https://openreview.net/forum?id=FHkWY4aGsN": {
    "title": "CLImage: Human-Annotated Datasets for Complementary-Label Learning",
    "volume": "main",
    "abstract": "Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical applicability remains unverified for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels, and it is not clear how far the assumptions are from reality. Secondly, their evaluation has been limited to synthetically labeled datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels from human annotators. Our efforts resulted in the creation of four datasets: CLCIFAR10, CLCIFAR20, CLMicroImageNet10, and CLMicroImageNet20, derived from well-known classification datasets CIFAR10, CIFAR100, and TinyImageNet200. These datasets represent the very first real-world CLL datasets, namely CLImage, which are publicly available at: https://github.com/ntucllab/CLImage_Dataset. Through extensive benchmark experiments, we discovered a notable decrease in performance when transitioning from synthetically labeled datasets to real-world datasets. We investigated the key factors contributing to the decrease with a thorough dataset-level ablation study. Our analyses highlight annotation noise as the most influential factor in the real-world datasets. In addition, we discover that the biased-nature of human-annotated complementary labels and the difficulty to validate with only complementary labels are two outstanding barriers to practical CLL. These findings suggest that the community focus more research efforts on developing CLL algorithms and validation schemes that are robust to noisy and biased complementary-label distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiu-Hsuan Wang",
      "Mai Tan Ha",
      "Nai-Xuan Ye",
      "Wei-I Lin",
      "Hsuan-Tien Lin"
    ]
  },
  "https://openreview.net/forum?id=amUisgrmte": {
    "title": "ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis",
    "volume": "main",
    "abstract": "Deep learning is providing a wealth of new approaches to the problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with limitations in their applicability. This work introduces ViewFusion, an end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target view only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely underdetermined conditions (thanks to its generative nature)---all while generating views of quality on par or even better than comparable methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small Neural 3D Mesh Renderer dataset. Code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernard Spiegl",
      "Andrea Perin",
      "Stephane Deny",
      "Alexander Ilin"
    ]
  },
  "https://openreview.net/forum?id=tXnVRpRlR8": {
    "title": "Learning Actionable Counterfactual Explanations in Large State Spaces",
    "volume": "main",
    "abstract": "Recourse generators provide actionable insights, often through feature-based counterfactual explanations (CFEs), to help negatively classified individuals understand how to adjust their input features to achieve a positive classification. These feature-based CFEs, which we refer to as \\emph{low-level} CFEs, are overly specific (e.g., coding experience: \\(4 \\to 5+\\) years) and often recommended in a feature space that doesn't straightforwardly align with real-world actions. To bridge this gap, we introduce three novel recourse types grounded in real-world actions: high-level continuous (\\emph{hl-continuous}), high-level discrete (\\emph{hl-discrete}), and high-level ID (\\emph{hl-id}) CFEs. We formulate single-agent CFE generation methods for hl-discrete and hl-continuous CFEs. For the hl-discrete CFE, we cast the task as a weighted set cover problem that selects the least cost set of hl-discrete actions that satisfy the eligibility of features, and model the hl-continuous CFE as a solution to an integer linear program that identifies the least cost set of hl-continuous actions capable of favorably altering the prediction of a linear classifier. Since these methods require costly optimization per agent, we propose data-driven CFE generation approaches that, given instances of agents and their optimal CFEs, learn a CFE generator that quickly provides optimal CFEs for new agents. This approach, also viewed as one of learning an optimal policy in a family of large but deterministic MDPs, considers several problem formulations, including formulations in which the actions and their effects are unknown, and therefore addresses informational and computational challenges. We conduct extensive empirical evaluations using publicly available healthcare datasets (BRFSS, Foods, and NHANES) and fully-synthetic data. For negatively classified agents identified by linear and threshold-based binary classifiers, we compare the proposed forms of recourse to low-level CFEs, which suggest how the agent can transition from state \\(\\mathbf{x}\\) to a new state \\(\\mathbf{x}'\\) where the model prediction is desirable. We also extensively evaluate the effectiveness of our neural network-based, data-driven CFE generation approaches. Empirical results show that the proposed data-driven CFE generators are accurate and resource-efficient, and the proposed forms of recourse offer various advantages over the low-level CFEs",
    "checked": true,
    "id": "1942d245a3deb92ef63bd56251f2d1b6977f4699",
    "semantic_title": "learning actionable counterfactual explanations in large state spaces",
    "citation_count": 0,
    "authors": [
      "Keziah Naggita",
      "Matthew Walter",
      "Avrim Blum"
    ]
  },
  "https://openreview.net/forum?id=9M4NKMZOPu": {
    "title": "Learning distributed representations with efficient SoftMax normalization",
    "volume": "main",
    "abstract": "Learning distributed representations, or embeddings, that encode the relational similarity patterns among objects is a relevant task in machine learning. A popular method to learn the embedding matrices $X, Y$ is optimizing a loss function of the term ${\\rm SoftMax}(XY^T)$. The complexity required to calculate this term, however, runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time heuristic approximation to compute the normalization constants of ${\\rm SoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some pre-trained embedding datasets that the proposed estimation method achieves higher or comparable accuracy with competing methods. From this result, we design an efficient and task-agnostic algorithm that learns the embeddings by optimizing the cross entropy between the softmax and a set of probability distributions given as inputs. The proposed algorithm is interpretable and easily adapted to arbitrary embedding problems. We consider a few use cases and observe similar or higher performances and a lower computational time than similar ``2Vec'' algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Dall'Amico",
      "Enrico Maria Belliardo"
    ]
  },
  "https://openreview.net/forum?id=QQZ8uPxFb3": {
    "title": "Explaining Node Embeddings",
    "volume": "main",
    "abstract": "Node embedding algorithms produce low-dimensional latent representations of nodes in a graph. These embeddings are often used for downstream tasks, such as node classification and link prediction. In this paper, we investigate the following two questions: (Q1) Can we explain each embedding dimension with human-understandable graph features (e.g. degree, clustering coefficient and PageRank). (Q2) How can we modify existing node embedding algorithms to produce embeddings that can be easily explained by human-understandable graph features? We find that the answer to Q1 is yes and introduce a new framework called XM (short for eXplain eMbedding) to answer Q2. A key aspect of XM involves minimizing the nuclear norm of the generated explanations. We show that by minimizing the nuclear norm, we minimize the lower bound on the entropy of the generated explanations. We test XM on a variety of real-world graphs and show that XM not only preserves the performance of existing node embedding methods, but also enhances their explainability",
    "checked": true,
    "id": "29977715c0f556cbf2274a409a3beb879987306c",
    "semantic_title": "explaining node embeddings",
    "citation_count": 0,
    "authors": [
      "Zohair Shafi",
      "Ayan Chatterjee",
      "Tina Eliassi-Rad"
    ]
  },
  "https://openreview.net/forum?id=F42CRfcp3D": {
    "title": "Diversity-Driven View Subset Selection for Indoor Novel View Synthesis",
    "volume": "main",
    "abstract": "Novel view synthesis of indoor scenes can be achieved by capturing a monocular video sequence of the environment. However, redundant information caused by artificial movements in the input video data reduces the efficiency of scene modeling. To address this, we formulate the problem as a combinatorial optimization task for view subset selection. In this work, we propose a novel subset selection framework that integrates a comprehensive diversity-based measurement with well-designed utility functions. We provide a theoretical analysis of these utility functions and validate their effectiveness through extensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset designed for indoor novel view synthesis, featuring complex and extended trajectories that simulate intricate human behaviors. Experiments on IndoorTraj show that our framework consistently outperforms baseline strategies while using only 5–20% of the data, highlighting its remarkable efficiency and effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Wang",
      "Han Zhou",
      "Matthew B. Blaschko",
      "Tinne Tuytelaars",
      "Minye Wu"
    ]
  },
  "https://openreview.net/forum?id=Q2M4yijKSo": {
    "title": "Flexible Infinite-Width Graph Convolutional Neural Networks",
    "volume": "main",
    "abstract": "A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed and tunable only through a small number of hyperparameters, thus eliminating the possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well because they are able to flexibly learn representations for the task at hand. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of node classification tasks on graphs. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a \"knob\" to control the amount of flexibility and hence representation learning. We found that representation learning gives noticeable performance improvements for heterophilous node classification tasks, but less so for homophilous node classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Anson",
      "Edward Milsom",
      "Laurence Aitchison"
    ]
  },
  "https://openreview.net/forum?id=QI0l842vSq": {
    "title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) learn to represent nodes by aggregating information from their neighbors. As GNNs increase in depth, their receptive field grows exponentially, leading to high memory costs. Several works in the literature proposed to address this shortcoming by sampling subgraphs, or by using historical embeddings. These methods have mostly focused on benchmarks of single-label node classification on homophilous graphs, where neighboring nodes often share the same label. However, most of these methods rely on static heuristics that may not generalize across different graphs or tasks. We argue that the sampling method should be adaptive, adjusting to the complex structural properties of each graph. To this end, we introduce GRAPES, an adaptive sampling method that learns to identify the set of nodes crucial for training a GNN. GRAPES trains a second GNN to predict node sampling probabilities by optimizing the downstream task objective. We evaluate GRAPES on various node classification benchmarks involving homophilous as well as heterophilous graphs. We demonstrate GRAPES' effectiveness in accuracy and scalability, particularly in multi-label heterophilous graphs. Additionally, GRAPES uses orders of magnitude less GPU memory than a strong baseline based on historical embeddings. Unlike other sampling methods, GRAPES maintains high accuracy even with smaller sample sizes and, therefore, can scale to massive graphs. Our implementation is publicly available online",
    "checked": true,
    "id": "0e98e6bf270463cd2b5841a669b34a90494441d5",
    "semantic_title": "grapes: learning to sample graphs for scalable graph neural networks",
    "citation_count": 5,
    "authors": [
      "Taraneh Younesian",
      "Daniel Daza",
      "Emile van Krieken",
      "Thiviyan Thanapalasingam",
      "Peter Bloem"
    ]
  },
  "https://openreview.net/forum?id=zo5b60AuAH": {
    "title": "Local Differential Privacy-Preserving Spectral Clustering for General Graphs",
    "volume": "main",
    "abstract": "Spectral clustering is a widely used algorithm to find clusters in networks. Several researchers have studied the stability of spectral clustering under local differential privacy with the additional assumption that the underlying networks are generated from the stochastic block model (SBM). However, we argue that this assumption is too restrictive since social networks do not originate from the SBM. Thus, we delve into an analysis for general graphs in this work. Our primary focus is the edge flipping method -- a common technique for protecting local differential privacy. We show that, when the edges of an $n$-vertex graph satisfying some reasonable well-clustering assumptions are flipped with a probability of $O(\\log n/n)$, the clustering outcomes are largely consistent. Empirical tests further corroborate these theoretical findings. Conversely, although clustering outcomes have been stable for non-sparse and well-clustered graphs produced from the SBM, we show that in general, spectral clustering may yield highly erratic results on certain graphs when the flipping probability is $\\omega(\\log n/n)$. This indicates that the best privacy budget obtainable for general graphs is $\\Theta(\\log n)$",
    "checked": true,
    "id": "50808d60fed0c31674cb813365533a8b57e00032",
    "semantic_title": "local differential privacy-preserving spectral clustering for general graphs",
    "citation_count": 1,
    "authors": [
      "Sayan Mukherjee",
      "Vorapong Suppakitpaisarn"
    ]
  },
  "https://openreview.net/forum?id=WzS33L1iPC": {
    "title": "Visually Descriptive Language Model for Vector Graphics Reasoning",
    "volume": "main",
    "abstract": "Despite significant advancements, current large multimodal models (LMMs) struggle to bridge the gap between low-level visual perception—focusing on shapes, sizes, and layouts—and high-level language reasoning involving semantics, events, and logic. This limitation becomes evident in tasks requiring precise visual perception, such as comparing geometric properties or solving visual algorithmic reasoning problems. To study this failure mode, we focus on an important visual domain: vector graphics —images composed purely of 2D objects and shapes, which are prevalent in Web, PC, and Mobile environments. Importantly, we consider rasterized vector graphics without assuming access to their underlying vector code. We identify two key research questions: how can we enable precise visual perception, and how can we facilitate high-level reasoning based on such low-level perceptions? To accurately capture low-level visual details, we explore using SVG for the precise encoding of visual scenes. However, SVGs are not readily interpretable by LLMs or LMMs in a zero-shot manner. To address this challenge, we propose the Visually Descriptive Language Model (VDLM) to build a bridge between low-level visual perception and high-level language reasoning. VDLM learns an intermediate symbolic representation called Primal Visual Description (PVD), which translates raw SVGs into a higher-level abstraction comprising primitive attributes. This abstraction allows for direct interpretation by foundation models for zero-shot generalization to different reasoning tasks. Without any human-annotated data, VDLM leads to significant improvements in state-of-the-art LMMs, such as GPT-4o, across various low-level multimodal perception and reasoning tasks on rasterized vector graphics. Additionally, we provide extensive analyses of VDLM's performance, showing that our framework offers improved interpretability due to its disentangled perception and reasoning processes. As the first attempt to construct a descriptive intermediate representation for low-level visual reasoning, we also conduct an in-depth error analysis, highlighting remaining limitations and suggesting directions for future research",
    "checked": true,
    "id": "6fde101147cf3b1980f1ac54a45026dddf110f3a",
    "semantic_title": "visually descriptive language model for vector graphics reasoning",
    "citation_count": 4,
    "authors": [
      "Zhenhailong Wang",
      "Joy Hsu",
      "Xingyao Wang",
      "Kuan-Hao Huang",
      "Manling Li",
      "Jiajun Wu",
      "Heng Ji"
    ]
  },
  "https://openreview.net/forum?id=PPGJ3EvENv": {
    "title": "Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport",
    "volume": "main",
    "abstract": "Detecting anomalies in datasets is a longstanding problem in machine learning. In this context, anomalies are defined as a sample that significantly deviates from the remaining data. Meanwhile, Optimal Transport (OT) is a field of mathematics concerned with the transportation, between two probability distribution, at least effort. In classical OT, the optimal transportation strategy of a distribution to itself is the identity, i.e., each sample keeps its mass. In this paper, we tackle anomaly detection by forcing samples to displace its mass, while keeping the least effort objective. We call this new transportation problem Mass Repulsing Optimal Transport (MROT). Naturally, samples lying in low density regions of space will be forced to displace mass very far, incurring a higher transportation cost. In contrast, samples on high density regions are able to send their mass just outside an \\emph{exclusion zone}. We use these concepts to design a new anomaly score. Through a series of experiments in existing benchmarks, and fault detection problems, we show that our algorithm improves over existing methods. Our code is publicly available at https://github.com/eddardd/MROT",
    "checked": true,
    "id": "90fda71788307ea96e198939231b24874ac935f2",
    "semantic_title": "unsupervised anomaly detection through mass repulsing optimal transport",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "EL HABAZI Adel",
      "Fred Maurice NGOLE MBOULA"
    ]
  },
  "https://openreview.net/forum?id=Xmk1or5eH8": {
    "title": "Algorithm Configuration for Structured Pfaffian Settings",
    "volume": "main",
    "abstract": "Data-driven algorithm design uses historical problem instances to automatically adjust and optimize algorithms to their application domain, typically by selecting algorithms from parameterized families. While the approach has been highly successful in practice, providing theoretical guarantees for several algorithmic families remains challenging. This is due to the intricate dependence of the algorithmic performance on the parameters, often exhibiting a piecewise discontinuous structure. In this work, we present new frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both statistical and online learning settings. For the statistical learning setting, we introduce the Pfaffian GJ framework, an extension of the classical Goldberg-Jerrum (GJ) framework (Bartlett et al., 2022; Goldberg & Jerrum, 1993), that is capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions (quotients of two polynomials), our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a refined piecewise structure, which automatically translates to learning guarantees using our proposed framework. For the online learning setting, we provide a new tool for verifying the dispersion property of a sequence of loss functions, a sufficient condition that allows no-regret learning for sequences of piecewise structured loss functions where the piecewise structure involves Pfaffian transition boundaries. We use our framework to provide novel learning guarantees for many challenging data-driven design problems of interest, including data-driven linkage-based clustering, graph-based semi-supervised learning, and regularized logistic regression",
    "checked": true,
    "id": "bd060781042b776f1ea0299155c4fb9c88d4d74b",
    "semantic_title": "algorithm configuration for structured pfaffian settings",
    "citation_count": 1,
    "authors": [
      "Maria Florina Balcan",
      "Anh Tuan Nguyen",
      "Dravyansh Sharma"
    ]
  },
  "https://openreview.net/forum?id=zF9IrMTjCC": {
    "title": "Generalizable Representation Learning for fMRI-based Neurological Disorder Identification",
    "volume": "main",
    "abstract": "Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhui Cui",
      "Haleh Akrami",
      "Anand Joshi",
      "Richard Leahy"
    ]
  },
  "https://openreview.net/forum?id=Uz9J77Riul": {
    "title": "Information Theoretic Guarantees For Policy Alignment In Large Language Models",
    "volume": "main",
    "abstract": "Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy based on an $f$-divergence like $\\mathsf{KL}$ divergence. The best of $n$ alignment policy selects the sample with the highest reward from $n$ independent samples. Recent work shows that the reward improvement of the aligned policy scales as $\\sqrt{\\mathsf{KL}}$, with an explicit bound on the $\\mathsf{KL}$ for best of $n$ policies. We show that this $\\sqrt{\\mathsf{KL}}$ bound holds if the reference policy's reward has sub-gaussian tails. For best of $n$ policies, the $\\mathsf{KL}$ bound applies to any $f$-divergence through a reduction to exponential order statistics using the Rényi representation. Tighter control can be achieved with Rényi divergence if additional tail information is known. Finally, we demonstrate how these bounds transfer to golden rewards, resulting in decreased golden reward improvement due to proxy reward overestimation and approximation errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Mroueh",
      "Apoorva Nitsure"
    ]
  },
  "https://openreview.net/forum?id=o6ukhJLzMQ": {
    "title": "Augmented Invertible Koopman Autoencoder for long-term time series forecasting",
    "volume": "main",
    "abstract": "Following the introduction of Dynamic Mode Decomposition and its numerous extensions, many neural autoencoder-based implementations of the Koopman operator have recently been proposed. This class of methods appears to be of interest for modeling dynamical systems, either through direct long-term prediction of the evolution of the state or as a powerful embedding for downstream methods. In particular, a recent line of work has developed invertible Koopman autoencoders (IKAEs), which provide an exact reconstruction of the input state thanks to their analytically invertible encoder, based on coupling layer normalizing flow models. We identify that the conservation of the dimension imposed by the normalizing flows is a limitation for the IKAE models, and thus we propose to augment the latent state with a second, non-invertible encoder network. This results in our new model: the Augmented Invertible Koopman AutoEncoder (AIKAE). We demonstrate the relevance of the AIKAE through a series of long-term time series forecasting experiments, on satellite image time series as well as on a benchmark involving predictions based on a large lookback window of observations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Frion",
      "Lucas Drumetz",
      "Mauro Dalla Mura",
      "Guillaume Tochon",
      "Abdeldjalil AISSA EL BEY"
    ]
  },
  "https://openreview.net/forum?id=wyOv4kGkbU": {
    "title": "Test-time Contrastive Concepts for Open-world Semantic Segmentation with Vision-Language Models",
    "volume": "main",
    "abstract": "Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts of image-text pairs to align both modalities with a simple contrastive objective, have paved the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image pixels are assigned the closest query in feature space. However, this works well when a user exhaustively lists all possible visual concepts in an image that contrast against each other for the assignment. This corresponds to the current evaluation setup in the literature, which relies on having access to a list of in-domain relevant concepts, typically classes of a benchmark dataset. Here, we consider the more challenging (and realistic) scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic \"background\" text, we propose two different approaches to automatically generate, at test time, query-specific textual contrastive concepts. We do so by leveraging the distribution of texts in the VLM's training set or crafted LLM prompts. We also propose a metric designed to evaluate this scenario and show the relevance of our approach on commonly used datasets",
    "checked": false,
    "id": "d1635967b7f44ce3714d78f2cf0eea4cd5636e8e",
    "semantic_title": "test-time contrastive concepts for open-world semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Monika Wysoczańska",
      "Antonin Vobecky",
      "Amaia Cardiel",
      "Tomasz Trzcinski",
      "Renaud Marlet",
      "Andrei Bursuc",
      "Oriane Siméoni"
    ]
  },
  "https://openreview.net/forum?id=qmHlTkLdbL": {
    "title": "Online Bandit Nonlinear Control with Dynamic Batch Length and Adaptive Learning Rate",
    "volume": "main",
    "abstract": "This paper is concerned with the online bandit nonlinear control, which aims to learn the best stabilizing controller from a pool of stabilizing and destabilizing controllers of unknown types for a given nonlinear dynamical system. We develop an algorithm, named Dynamic Batch length and Adaptive learning Rate (DBAR), and study its stability and regret. Unlike the existing Exp3 algorithm requiring an exponentially stabilizing controller, DBAR only needs a significantly weaker notion of controller stability, in which case substantial time may be required to certify the system stability. Dynamic batch length in DBAR effectively addresses this issue and enables the system to attain asymptotic stability, where the algorithm behaves as if there were no destabilizing controllers. Moreover, adaptive learning rate in DBAR only uses the state norm information to achieve a tight regret bound even when none of the stabilizing controllers in the pool are exponentially stabilizing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Kim",
      "Javad Lavaei"
    ]
  },
  "https://openreview.net/forum?id=L33DSu3zvq": {
    "title": "Tighter sparse variational Gaussian processes",
    "volume": "main",
    "abstract": "Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang D Bui",
      "Matthew Ashman",
      "Richard E. Turner"
    ]
  },
  "https://openreview.net/forum?id=BbwlJpNXgW": {
    "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
    "volume": "main",
    "abstract": "Large language models trained on web-scale corpora can memorize undesirable data containing misinformation, copyrighted material, or private or sensitive information. Recently, several machine unlearning algorithms have been proposed to eliminate the effect of such datapoints from trained models-- that is, to approximate *a model that had never been trained on these datapoints in the first place*. However, evaluating the effectiveness of unlearning algorithms remains an open challenge. Previous work has relied on heuristics-- such as verifying that the model can no longer reproduce the specific information targeted for removal while maintaining accuracy on unrelated test data. These approaches inadequately capture the complete effect of reversing the influence of datapoints on a trained model. In this work, we propose the RESTOR framework for machine unlearning evaluation, which assesses the ability of unlearning algorithms for targeted data erasure, by evaluating the ability of models to forget the knowledge introduced in these datapoints, while simultaneously recovering the model's knowledge state had it never encountered these datapoints. RESTOR helps uncover several novel insights about popular unlearning algorithms, and the mechanisms through which they operate-- for instance, identifying that some algorithms merely emphasize forgetting but not recovering knowledge, and that localizing unlearning targets can enhance unlearning performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keivan Rezaei",
      "Khyathi Chandu",
      "Soheil Feizi",
      "Yejin Choi",
      "Faeze Brahman",
      "Abhilasha Ravichander"
    ]
  },
  "https://openreview.net/forum?id=fuOHI59rUW": {
    "title": "MarDini: Masked Auto-regressive Diffusion for Video Generation at Scale",
    "volume": "main",
    "abstract": "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Shikun Liu",
      "Zijian Zhou",
      "Mengmeng Xu",
      "Yanping Xie",
      "Xiao Han",
      "Juan Camilo Perez",
      "Ding Liu",
      "Kumara Kahatapitiya",
      "Menglin Jia",
      "Jui-Chieh Wu",
      "Sen He",
      "Tao Xiang",
      "Jürgen Schmidhuber",
      "Juan-Manuel Perez-Rua"
    ]
  },
  "https://openreview.net/forum?id=LLWJkR6gaI": {
    "title": "Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control",
    "volume": "main",
    "abstract": "Imitation learning is an efficient method for teaching robots a variety of tasks. Diffusion Policy, which uses a conditional denoising diffusion process to generate actions, has demonstrated superior performance, particularly in learning from multi-modal demonstrates. However, it relies on executing multiple actions predicted from the same inference step to retain performance and prevent mode bouncing, which limits its responsiveness, as actions are not conditioned on the most recent observations. To address this, we introduce Responsive Noise-Relaying Diffusion Policy (RNR-DP), which maintains a noise-relaying buffer with progressively increasing noise levels and employs a sequential denoising mechanism that generates immediate, noise-free actions at the head of the sequence, while appending noisy actions at the tail. This ensures that actions are responsive and conditioned on the latest observations, while maintaining motion consistency through the noise-relaying buffer. This design enables the handling of tasks requiring responsive control, and accelerates action generation by reusing denoising steps. Experiments on response-sensitive tasks demonstrate that, compared to Diffusion Policy, ours achieves 18% improvement in success rate. Further evaluation on regular tasks demonstrates that RNR-DP also exceeds the best acceleration method (DDIM) by 6.9% in success rate, highlighting its computational efficiency advantage in scenarios where responsiveness is less critical",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoqun Chen",
      "Xiu Yuan",
      "Tongzhou Mu",
      "Hao Su"
    ]
  },
  "https://openreview.net/forum?id=Ed1DBB3sBQ": {
    "title": "Conformal Prediction: A Theoretical Note and Benchmarking Transductive Node Classification in Graphs",
    "volume": "main",
    "abstract": "Conformal prediction has become increasingly popular for quantifying the uncertainty associated with machine learning models. Recent work in graph uncertainty quantification has built upon this approach for conformal graph prediction. The nascent nature of these explorations has led to conflicting choices for implementations, baselines, and method evaluation. In this work, we analyze the design choices made in the literature and discuss the tradeoffs associated with existing methods. Building on the existing implementations for existing methods, we introduce techniques to scale existing methods to large-scale graph datasets without sacrificing performance. Our theoretical and empirical results justify our recommendations for future scholarship in graph conformal prediction",
    "checked": true,
    "id": "bee6768e7c5fde8e1e0385d2fb085c584f0ed61a",
    "semantic_title": "conformal prediction: a theoretical note and benchmarking transductive node classification in graphs",
    "citation_count": 0,
    "authors": [
      "Pranav Maneriker",
      "Aditya T. Vadlamani",
      "Anutam Srinivasan",
      "Yuntian He",
      "Ali Payani",
      "srinivasan parthasarathy"
    ]
  },
  "https://openreview.net/forum?id=A4RLpHPXCu": {
    "title": "Offset Unlearning for Large Language Models",
    "volume": "main",
    "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs",
    "checked": true,
    "id": "72010b7fe48301a38e8063109b8ef8fcfd573e05",
    "semantic_title": "offset unlearning for large language models",
    "citation_count": 14,
    "authors": [
      "James Y. Huang",
      "Wenxuan Zhou",
      "Fei Wang",
      "Fred Morstatter",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ]
  },
  "https://openreview.net/forum?id=oYmRiWCQ1W": {
    "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation",
    "volume": "main",
    "abstract": "Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praveen Srinivasa Varadhan",
      "amogh gulati",
      "Ashwin Sankar",
      "Srija Anand",
      "Anirudh Gupta",
      "Anirudh Mukherjee",
      "Shiva Kumar Marepally",
      "Ankur Bhatia",
      "Saloni Jaju",
      "Suvrat Bhooshan",
      "Mitesh M Khapra"
    ]
  },
  "https://openreview.net/forum?id=cCQKwd5MFP": {
    "title": "Part-aware Prompted Segment Anything Model for Adaptive Segmentation",
    "volume": "main",
    "abstract": "Precision medicine, such as patient-adaptive treatments assisted by medical image analysis, poses new challenges for image segmentation algorithms due to the large variability across different patients and the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}rompted}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, $\\text{P}^2\\text{SAM}$ enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the part-aware prompt mechanism, we propose a distribution-guided retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^2\\text{SAM}$ improves the performance by $\\texttt{+} 8.0\\%$ and $\\texttt{+} 2.0\\%$ mean Dice score for two different patient-adaptive segmentation applications, respectively. In addition, $\\text{P}^2\\text{SAM}$ also exhibits impressive generalizability in other adaptive segmentation tasks in the natural image domain, $\\textit{e.g.}$, $\\texttt{+} 6.4\\%$ mIoU within personalized object segmentation task. Code will be released upon acceptance",
    "checked": true,
    "id": "470cacf502daf31e7da621666eb012ffa49305c1",
    "semantic_title": "part-aware prompted segment anything model for adaptive segmentation",
    "citation_count": 3,
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ]
  },
  "https://openreview.net/forum?id=kY2fKLOGkI": {
    "title": "On the Utility of Existing Fine-Tuned Models on Data-Scarce Domains",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been observed to perform well on a wide range of downstream tasks when fine-tuned on domain-specific data. However, such data may not be readily available in many applications, motivating zero-shot or few-shot approaches using existing domain or task adjacent (fine-tuned) models, which we call DAFT. While several fine-tuned models for various tasks are available, finding one appropriate DAFT model for a given task is often not straight forward. In this paper, we explore different utilization techniques of these existing DAFT models for data-scarce problems, i.e., tasks for which data is not available or limited. We observe that for zero-shot problems, ensembling of DAFT models provides an accuracy performance close to that of the single best model. With few-shot problems (few data from target domain available), this performance can be improved further by picking or putting more weights to the DAFT models that are expected to perform better on the target task",
    "checked": true,
    "id": "2abf7bd98138a35eb377babfcf04c067e437f7af",
    "semantic_title": "on the utility of existing fine-tuned models on data-scarce domains",
    "citation_count": 0,
    "authors": [
      "Md Ibrahim Ibne Alam",
      "Parikshit Ram",
      "Soham Dan",
      "Horst Samulowitz",
      "Koushik Kar"
    ]
  },
  "https://openreview.net/forum?id=k8x44wVIs1": {
    "title": "Group Fair Federated Learning via Stochastic Kernel Regularization",
    "volume": "main",
    "abstract": "Ensuring \\textbf{group fairness} in federated learning (FL) presents unique challenges due to data heterogeneity and communication constraints. We propose Kernel Fair Federated Learning (\\texttt{KFFL}), a novel framework that incorporates group fairness into FL models using the Kernel Hilbert-Schmidt Independence Criterion (KHSIC) as a fairness regularizer. To address scalability, \\texttt{KFFL} approximates KHSIC with Random Feature Maps (RFMs), significantly reducing computational and communication overhead while achieving \\textit{group fairness}. To address the resulting non-convex optimization problem, we propose \\texttt{FedProxGrad}, a federated proximal gradient algorithm that guarantees convergence. Through experiments on standard benchmark datasets across both IID and Non-IID settings for regression and classification tasks, \\texttt{KFFL} demonstrates its ability to balance accuracy and fairness effectively, outperforming existing methods by comprehensively exploring the Pareto Frontier. Furthermore, we introduce \\texttt{KFFL-TD}, a time-delayed variant that further reduces communication rounds, enhancing efficiency in decentralized environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huzaifa Arif",
      "Pin-Yu Chen",
      "Keerthiram Murugesan",
      "Alex Gittens"
    ]
  },
  "https://openreview.net/forum?id=quE8gDDegf": {
    "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification",
    "volume": "main",
    "abstract": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, \\emph{class prototype learning} (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67\\% improvement over strong baseline methods",
    "checked": false,
    "id": "4a3cda4ee6bcb5d40604d93cb37c3a82b89c2bee",
    "semantic_title": "e xploring w eak - to -s trong g eneralization for clip-based c lassification",
    "citation_count": 0,
    "authors": [
      "Jinhao Li",
      "Sarah Monazam Erfani",
      "Lei Feng",
      "James Bailey",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=OjWB2671AR": {
    "title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning",
    "volume": "main",
    "abstract": "Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation---a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization",
    "checked": true,
    "id": "1869e4b3ec0a248c6ba360bf17706621207d310e",
    "semantic_title": "deep augmentation: dropout as augmentation for self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Rickard Brüel Gabrielsson",
      "Tongzhou Wang",
      "Manel Baradad",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=haf78jerSt": {
    "title": "Forecasting Company Fundamentals",
    "volume": "main",
    "abstract": "Company fundamentals are key to assessing companies' financial and overall success and stability. Forecasting them is important in multiple fields, including investing and econometrics. While statistical and contemporary machine learning methods have been applied to many time series tasks, there is a lack of comparison of these approaches on this particularly challenging data regime. To this end, we try to bridge this gap and thoroughly evaluate the theoretical properties and practical performance of 24 deterministic and probabilistic company fundamentals forecasting models on real company data. We observe that deep learning models provide superior forecasting performance to classical models, in particular when considering uncertainty estimation. To validate the findings, we compare them to human analyst expectations and find that their accuracy is comparable to the automatic forecasts. We further show how these high-quality forecasts can benefit automated stock allocation. We close by presenting possible ways of integrating domain experts to further improve performance and increase reliability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Divo",
      "Eric Endress",
      "Kevin Endler",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ]
  },
  "https://openreview.net/forum?id=6g1WJ55N51": {
    "title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control",
    "volume": "main",
    "abstract": "We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $\\epsilon t$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting algorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon t$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting",
    "checked": true,
    "id": "331586853f0b2f45d66fa8bde7bdc5dde07725cf",
    "semantic_title": "etgl-ddpg: a deep deterministic policy gradient algorithm for sparse reward continuous control",
    "citation_count": 1,
    "authors": [
      "Ehsan Futuhi",
      "Shayan Karimi",
      "Chao Gao",
      "Martin Müller"
    ]
  },
  "https://openreview.net/forum?id=kHl4JzyNzF": {
    "title": "Music Foundation Model as Generic Booster for Music Downstream Tasks",
    "volume": "main",
    "abstract": "We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions",
    "checked": true,
    "id": "fa689259129b052f5f97934128d476f7c82d4a23",
    "semantic_title": "music foundation model as generic booster for music downstream tasks",
    "citation_count": 1,
    "authors": [
      "Wei-Hsiang Liao",
      "Yuhta Takida",
      "Yukara Ikemiya",
      "Zhi Zhong",
      "Chieh-Hsin Lai",
      "Giorgio Fabbro",
      "Kazuki Shimada",
      "Keisuke Toyama",
      "Kin Wai Cheuk",
      "Marco A. Martínez-Ramírez",
      "Shusuke Takahashi",
      "Stefan Uhlich",
      "Taketo Akama",
      "Woosung Choi",
      "Yuichiro Koyama",
      "Yuki Mitsufuji"
    ]
  },
  "https://openreview.net/forum?id=pdC092Nn8N": {
    "title": "Studying Exploration in RL: An Optimal Transport Analysis of Occupancy Measure Trajectories",
    "volume": "main",
    "abstract": "The rising successes of RL are propelled by combining smart algorithmic strategies and deep architectures to optimize the distribution of returns and visitations over the state-action space. A quantitative framework to compare the learning processes of these eclectic RL algorithms is currently absent but desired in practice. We address this gap by representing the learning process of an RL algorithm as a sequence of policies generated during training, and then studying the policy trajectory induced in the manifold of state-action occupancy measures. Using an optimal transport-based metric, we measure the length of the paths induced by the policy sequence yielded by an RL algorithm between an initial policy and a final optimal policy. Hence, we first define the Effort of Sequential Learning (ESL). ESL quantifies the relative distance that an RL algorithm travels compared to the shortest path from the initial to the optimal policy. Furthermore, we connect the dynamics of policies in the occupancy measure space and regret (another metric to understand the suboptimality of an RL algorithm), by defining the Optimal Movement Ratio (OMR). OMR assesses the fraction of movements in the occupancy measure space that effectively reduce an analogue of regret. Finally, we derive approximation guarantees to estimate ESL and OMR with a finite number of samples and without access to an optimal policy. Through empirical analyses across various environments and algorithms, we demonstrate that ESL and OMR provide insights into the exploration processes of RL algorithms and the hardness of different tasks in discrete and continuous MDPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reabetswe M. Nkhumise",
      "Debabrota Basu",
      "Tony J. Prescott",
      "Aditya Gilra"
    ]
  },
  "https://openreview.net/forum?id=gG8sQUUtN7": {
    "title": "LASP: Linear Attention Sequence Parallelism",
    "volume": "main",
    "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: \\url{https://github.com/OpenNLPLab/LASP}",
    "checked": false,
    "id": "660d80773f55c5dc889de3ce93b71672133a91bc",
    "semantic_title": "linear attention sequence parallelism",
    "citation_count": 2,
    "authors": [
      "Weigao Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Yu Qiao",
      "Yiran Zhong"
    ]
  },
  "https://openreview.net/forum?id=PHsfZnF2FC": {
    "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design choices and added complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Importantly, MOORL addresses the key challenges of hybrid RL in terms of being design-free. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without relying on extensive design choices. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Chaudhary",
      "Washim Uddin Mondal",
      "Laxmidhar Behera"
    ]
  },
  "https://openreview.net/forum?id=2NSb3cJE03": {
    "title": "Time-Uniform Confidence Spheres for Means of Random Vectors",
    "volume": "main",
    "abstract": "We study sequential mean estimation in $\\mathbb{R}^d$. In particular, we derive time-uniform confidence spheres---\\emph{confidence sphere sequences} (CSSs)---which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\\psi$ random vectors (which includes sub-gamma, and sub-exponential distributions). Many of our results are optimal. For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Chugg",
      "Hongjian Wang",
      "Aaditya Ramdas"
    ]
  },
  "https://openreview.net/forum?id=6BlOCx5c5T": {
    "title": "How far away are truly hyperparameter-free learning algorithms?",
    "volume": "main",
    "abstract": "Despite major advances in methodology, hyperparameter tuning remains a crucial (and expensive) part of the development of machine learning systems. Even ignoring architectural choices, deep neural networks have a large number of optimization and regularization hyperparameters that need to be tuned carefully per workload in order to obtain the best results. In a perfect world, training algorithms would not require workload-specific hyperparameter tuning, but would instead have default settings that performed well across many workloads. Recently, there has been a growing literature on optimization methods which attempt to reduce the number of hyperparameters---particularly the learning rate and its accompanying schedule. Given these developments, how far away is the dream of neural network training algorithms that completely obviate the need for painful tuning? In this paper, we evaluate the potential of learning-rate-free methods as components of hyperparameter-free methods. We freeze their (non-learning rate) hyperparameters to default values, and score their performance using the recently-proposed AlgoPerf: Training Algorithms benchmark. We found that literature-supplied default settings performed poorly on the benchmark, so we performed a search for hyperparameter configurations that performed well across all workloads simultaneously. The best \"algoperf-calibrated\" learning-rate-free methods had much improved performance but still lagged slightly behind a similarly calibrated NadamW baseline in overall benchmark score. Our results suggest that there is still much room for improvement for learning-rate-free methods, and that testing against a strong, workload-agnostic baseline is important to improve hyperparameter reduction techniques",
    "checked": true,
    "id": "eb917217a143ceb9059bbf6659020741a1fba00e",
    "semantic_title": "how far away are truly hyperparameter-free learning algorithms?",
    "citation_count": 0,
    "authors": [
      "Priya Kasimbeg",
      "Vincent Roulet",
      "Naman Agarwal",
      "Sourabh Medapati",
      "Fabian Pedregosa",
      "Atish Agarwala",
      "George E. Dahl"
    ]
  },
  "https://openreview.net/forum?id=VTgixSbrJI": {
    "title": "Hitchhiker's guide on the relation of Energy-Based Models with other generative models, sampling and statistical physics: a comprehensive review",
    "volume": "main",
    "abstract": "Energy-Based Models (EBMs) have emerged as a powerful framework in the realm of generative modeling, offering a unique perspective that aligns closely with principles of statistical mechanics. This review aims to provide physicists with a comprehensive understanding of EBMs, delineating their connection to other generative models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Normalizing Flows. We explore the sampling techniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods, and draw parallels between EBM concepts and statistical mechanics, highlighting the significance of energy functions and partition functions. Furthermore, we delve into state-of-the-art training methodologies for EBMs, covering recent advancements and their implications for enhanced model performance and efficiency. This review is designed to clarify the often complex interconnections between these models, which can be challenging due to the diverse communities working on the topic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Carbone"
    ]
  },
  "https://openreview.net/forum?id=VPl3T43Hxb": {
    "title": "A Local Polyak-Łojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models",
    "volume": "main",
    "abstract": "Most prior work on the convergence of gradient descent (GD) for overparameterized neural networks relies on strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (large, spectral, balanced). Recent efforts to relax these assumptions focus on two-layer linear networks trained with the squared loss. In this work, we derive a linear convergence rate for training two-layer linear neural networks with GD for general losses and under relaxed assumptions on the step size, width, and initialization. A key challenge in deriving this result is that classical ingredients for deriving convergence rates for nonconvex problems, such as the Polyak-Łojasiewicz (PL) condition and Descent Lemma, do not hold globally for overparameterized neural networks. Here, we prove that these two conditions hold locally with local constants that depend on the weights. Then, we provide bounds on these local constants, which depend on the initialization of the weights, the current loss, and the global PL and smoothness constants of the non-overparameterized model. Based on these bounds, we derive a linear convergence rate for GD. Our convergence analysis not only improves upon prior results but also suggests a better choice for the step size, as verified through our numerical experiments",
    "checked": false,
    "id": "ad848a24708cf438340f97023c091908789c94db",
    "semantic_title": "a local polyak-lojasiewicz and descent lemma of gradient descent for overparametrized linear models",
    "citation_count": 0,
    "authors": [
      "Ziqing Xu",
      "Hancheng Min",
      "Salma Tarmoun",
      "Enrique Mallada",
      "Rene Vidal"
    ]
  },
  "https://openreview.net/forum?id=p499xXaclC": {
    "title": "Pruning Feature Extractor Stacking for Cross-domain Few-shot Learning",
    "volume": "main",
    "abstract": "Combining knowledge from source domains to learn efficiently from a few labelled instances in a target domain is a transfer learning problem known as cross-domain few-shot learning (CDFSL). Feature extractor stacking (FES) is a state-of-the-art CDFSL method that maintains a collection of source domain feature extractors instead of a single universal extractor. FES uses stacked generalisation to build an ensemble from extractor snapshots saved during target domain fine-tuning. It outperforms several contemporary universal model-based CDFSL methods in the Meta-Dataset benchmark. However, it incurs higher storage cost because it saves a snapshot for every fine-tuning iteration for every extractor. In this work, we propose a bidirectional snapshot selection strategy for FES, leveraging its cross-validation process and the ordered nature of its snapshots, and demonstrate that a 95% snapshot reduction can be achieved while retaining the same level of accuracy",
    "checked": true,
    "id": "6ed6f78ce0411a6f2bbf374741ffacc3d2e97f86",
    "semantic_title": "pruning feature extractor stacking for cross-domain few-shot learning",
    "citation_count": 0,
    "authors": [
      "Hongyu Wang",
      "Eibe Frank",
      "Bernhard Pfahringer",
      "Geoff Holmes"
    ]
  },
  "https://openreview.net/forum?id=nuN1mRrrjX": {
    "title": "Cometh: A continuous-time discrete-state graph diffusion model",
    "volume": "main",
    "abstract": "Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, tailored to the specificities of graph data. In addition, we also successfully replaced the set of structural encodings previously used in the discrete graph diffusion model with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a near-perfect performance of 99.5% on the planar graph dataset and outperforms DiGress by 12.6% on the large GuacaMol dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Siraudin",
      "Fragkiskos D. Malliaros",
      "Christopher Morris"
    ]
  },
  "https://openreview.net/forum?id=LDBjgS5Ez7": {
    "title": "Uniform Noise Distribution and Compact Clusters: Unveiling the Success of Self-Supervised Learning in Label Noise",
    "volume": "main",
    "abstract": "Label noise is ubiquitous in real-world datasets, posing significant challenges to machine learning models. While self-supervised learning (SSL) algorithms have empirically demonstrated effectiveness in learning noisy labels, the theoretical understanding of their effectiveness remains underexplored. In this paper, we present a theoretical framework to understand how SSL methods enhance learning with noisy labels, especially for the instance-dependent label noise. We reveal that the uniform and compact cluster structures induced by contrastive SSL play a crucial role in mitigating the adverse effects of label noise. Specifically, we theoretically show that a classifier trained on SSL-learned representations significantly outperforms one trained using traditional supervised learning methods. This results from two key merits of SSL representations over label noise: 1. Uniform Noise Distribution: Label noise becomes uniformly distributed over SSL representations with respect to the true class labels, rather than the noisy ones, leading to an easier learning task. 2. Enhanced Cluster Structure: SSL enhances the formation of well-separated and compact categorical clusters, increasing inter-class distances while tightening intra-class clusters. We further theoretically justify the benefits of training a classifier on such structured representations, demonstrating that it encourages the classifier trained on noisy data to be aligned with the optimal classifier. Extensive experiments validate the robustness of SSL representations in combating label noise, confirming the practical values of our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Xu",
      "Li Yi",
      "Gezheng Xu",
      "Xi Chen",
      "Ian McLeod",
      "Charles Ling",
      "Boyu Wang"
    ]
  },
  "https://openreview.net/forum?id=S6JpSsYBDZ": {
    "title": "RefinedFields: Radiance Fields Refinement for Planar Scene Representations",
    "volume": "main",
    "abstract": "Planar scene representations have recently witnessed increased interests for modeling scenes from images, as their lightweight planar structure enables compatibility with image-based models. Notably, K-Planes have gained particular attention as they extend planar scene representations to support in-the-wild scenes, in addition to object-level scenes. However, their visual quality has recently lagged behind that of state-of-the-art techniques. To reduce this gap, we propose RefinedFields, a method that leverages pre-trained networks to refine K-Planes scene representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and improves upon its base representation on the task of novel view synthesis. Our project page can be found at https://refinedfields.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Kassab",
      "Antoine Schnepf",
      "Jean-Yves Franceschi",
      "Laurent Caraffa",
      "Jeremie Mary",
      "Valerie Gouet-Brunet"
    ]
  },
  "https://openreview.net/forum?id=NeQYi56MFj": {
    "title": "M3CoL: Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification",
    "volume": "main",
    "abstract": "Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive Learning approach to capture nuanced shared relations inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with their corresponding samples from other modalities thereby capturing shared relations between them. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research",
    "checked": false,
    "id": "baa3f5de929e76427312ae2c13d7d42444c355ca",
    "semantic_title": "harnessing shared relations via multimodal mixup contrastive learning for multimodal classification",
    "citation_count": 0,
    "authors": [
      "Raja Kumar",
      "Raghav Singhal",
      "Pranamya Prashant Kulkarni",
      "Deval Mehta",
      "Kshitij Sharad Jadhav"
    ]
  },
  "https://openreview.net/forum?id=N2rWhTgits": {
    "title": "Guided Discrete Diffusion for Electronic Health Record Generation",
    "volume": "main",
    "abstract": "Electronic health records (EHRs) are a pivotal data source that enables numerous applications in computational medicine, e.g., disease progression prediction, clinical trial design, and health economics and outcomes research. Despite wide usability, their sensitive nature raises privacy and confidentially concerns, which limit potential use cases. To tackle these challenges, we explore the use of generative models to synthesize artificial, yet realistic EHRs. While diffusion-based methods have recently demonstrated state-of-the-art performance in generating other data modalities and overcome the training instability and mode collapse issues that plague previous GAN-based approaches, their applications in EHR generation remain underexplored. The discrete nature of tabular medical code data in EHRs poses challenges for high-quality data generation, especially for continuous diffusion models. To this end, we introduce a novel tabular EHR generation method, EHR-D3PM, which enables both unconditional and conditional generation using the discrete diffusion model. Our experiments demonstrate that EHR-D3PM significantly outperforms existing generative baselines on comprehensive fidelity and utility metrics while maintaining less attribute and membership vulnerability risks. Furthermore, we show EHR-D3PM is effective as a data augmentation method and enhances performance on downstream tasks when combined with real data",
    "checked": true,
    "id": "3643812e72325a7d92e57b21ad2cb24faf563f30",
    "semantic_title": "guided discrete diffusion for electronic health record generation",
    "citation_count": 7,
    "authors": [
      "Jun Han",
      "Zixiang Chen",
      "Yongqian Li",
      "Yiwen Kou",
      "Eran Halperin",
      "Robert E. Tillman",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=sq5AJvVuha": {
    "title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models",
    "volume": "main",
    "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget",
    "checked": true,
    "id": "aeb6fbef9a2f455a71773488e09f1c4f6686ac55",
    "semantic_title": "dygmamba: efficiently modeling long-term temporal dependency on continuous-time dynamic graphs with state space models",
    "citation_count": 3,
    "authors": [
      "Zifeng Ding",
      "Yifeng Li",
      "Yuan He",
      "Antonio Norelli",
      "Jingcheng Wu",
      "Volker Tresp",
      "Michael M. Bronstein",
      "Yunpu Ma"
    ]
  },
  "https://openreview.net/forum?id=5f7YlSKG1l": {
    "title": "Towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion",
    "volume": "main",
    "abstract": "Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine whether these effects can be identified from observational data. Identifying total effects in fully specified causal graphs has received considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of cycles and latent confounding, and when no variable set is sufficient for adjustment",
    "checked": true,
    "id": "3c5e09eeb9360cbeb8999f5ddde3f9bf0a9e57cd",
    "semantic_title": "towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion",
    "citation_count": 2,
    "authors": [
      "Charles K. Assaad"
    ]
  },
  "https://openreview.net/forum?id=ZrqLpXbXvA": {
    "title": "Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning",
    "volume": "main",
    "abstract": "Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the \"explanatory units\" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allow for arbitrary unmeasured confounding among features. We demonstrate the resulting graph can differentiate between interpretable features that causally influence model predictions versus those that are merely associated with model predictions due to confounding. Our approach is motivated by a counterfactual theory of causal explanation wherein good explanations point to factors that are \"difference-makers\" in an interventionist sense",
    "checked": false,
    "id": "6e2836fc572ef2f9d2965e64ef5ba17d7eb48d03",
    "semantic_title": "petrophysical prediction of oil reservoirs using explainable artificial intelligence",
    "citation_count": 0,
    "authors": [
      "Numair Sani",
      "Daniel Malinsky",
      "Ilya Shpitser"
    ]
  },
  "https://openreview.net/forum?id=z3JZzu9EA3": {
    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications",
    "checked": true,
    "id": "6bcd708d2e49b34f34f157daa6bf1c3e062f57c5",
    "semantic_title": "a survey on large language model acceleration based on kv cache management",
    "citation_count": 18,
    "authors": [
      "Haoyang LI",
      "Yiming Li",
      "Anxin Tian",
      "Tianhao Tang",
      "Zhanchao Xu",
      "Xuejia Chen",
      "Nicole HU",
      "Wei Dong",
      "Li Qing",
      "Lei Chen"
    ]
  },
  "https://openreview.net/forum?id=CkVlt2Qgdb": {
    "title": "Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity",
    "volume": "main",
    "abstract": "Machine learning (ML) algorithms can often exhibit discriminatory behavior, negatively affecting certain populations across protected groups. To address this, numerous debiasing methods, and consequently evaluation measures, have been proposed. Current evaluation measures for debiasing methods suffer from two main limitations: (1) they primarily provide a global estimate of unfairness, failing to provide a more fine-grained analysis, and (2) they predominantly analyze the model output on a specific task, failing to generalize the findings to other tasks. In this work, we introduce Pointwise Normalized Kernel Alignment (PNKA), a pointwise representational similarity measure that addresses these limitations by measuring how debiasing measures affect the intermediate representations of individuals. On tabular data, the use of PNKA reveals previously unknown insights: while group fairness predominantly influences a small subset of the population, maintaining high representational similarity for the majority, individual fairness constraints uniformly impact representations across the entire population, altering nearly every data point. We show that by evaluating representations using PNKA, we can reliably predict the behavior of ML models trained on these representations. Moreover, applying PNKA to language embeddings shows that existing debiasing methods may not perform as intended, failing to remove biases from stereotypical words and sentences. Our findings suggest that current evaluation measures for debiasing methods are insufficient, highlighting the need for a deeper understanding of the effects of debiasing methods, and show how pointwise representational similarity metrics can help with fairness audits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camila Kolling",
      "Till Speicher",
      "Vedant Nanda",
      "Mariya Toneva",
      "Krishna P. Gummadi"
    ]
  },
  "https://openreview.net/forum?id=ZgjhykPSdU": {
    "title": "Statistical Error Bounds for GANs with Nonlinear Objective Functionals",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) are unsupervised learning methods for training a generator distribution to produce samples that approximate those drawn from a target distribution. Many such methods can be formulated as minimization of a metric or divergence between probability distributions. Recent works have derived statistical error bounds for GANs that are based on integral probability metrics (IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. In general, IPMs are defined by optimizing a linear functional (difference of expectations) over a space of discriminators. A much larger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed using $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences) together with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz functions). These GANs have nonlinear objective functions, depending on the choice of $f$, and have been shown to exhibit improved performance in a number of applications. In this work we derive statistical error bounds for $(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of finite-sample concentration inequalities. These results prove the statistical consistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs in the appropriate limit. Our results use novel Rademacher complexity bounds which provide new insight into the performance of IPM-GANs for distributions with unbounded support and have application to statistical learning tasks beyond GANs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremiah Birrell"
    ]
  },
  "https://openreview.net/forum?id=JkMifr17wc": {
    "title": "Closed-Form Diffusion Models",
    "volume": "main",
    "abstract": "Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves competitive sampling times while running on consumer-grade CPUs",
    "checked": true,
    "id": "1242e11e676fe238157127ef2825b89c50d99d10",
    "semantic_title": "closed-form diffusion models",
    "citation_count": 11,
    "authors": [
      "Christopher Scarvelis",
      "Haitz Sáez de Ocáriz Borde",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=Oyueig10Ed": {
    "title": "Policy Optimization via Adv2: Adversarial Learning on Advantage Functions",
    "volume": "main",
    "abstract": "We revisit the reduction of learning in adversarial Markov decision processes [MDPs] to adversarial learning based on $Q$--values; this reduction has been considered in a number of recent articles as one building block to perform policy optimization. Namely, we first consider and extend this reduction in an ideal setting where an oracle provides value functions: it may involve any adversarial learning strategy (not just exponential weights) and it may be based indifferently on $Q$--values or on advantage functions. We then present two extensions: on the one hand, convergence of the last iterate for a vast class of adversarial learning strategies (again, not just exponential weights), satisfying a property called monotonicity of weights; on the other hand, stronger regret criteria for learning in MDPs, inherited from the stronger regret criteria of adversarial learning called strongly adaptive regret and tracking regret. Third, we demonstrate how adversarial learning, also referred to as aggregation of experts, relates to aggregation (orchestration) of expert policies: we obtain stronger forms of performance guarantees in this setting than existing ones, via yet another, simple reduction. Finally, we discuss the impact of the reduction of learning in adversarial MDPs to adversarial learning in the practical scenarios where transition kernels are unknown and value functions must be learned. In particular, we review the literature and note that many strategies for policy optimization feature a policy-improvement step based on exponential weights with estimated $Q$--values. Our main message is that this step may be replaced by the application of any adversarial learning strategy on estimated $Q$--values or on estimated advantage functions. We leave the empirical evaluation of these twists for future research",
    "checked": true,
    "id": "2c860df4131cb10fb94821cec993699fd2d46f50",
    "semantic_title": "policy optimization via adv2: adversarial learning on advantage functions",
    "citation_count": 2,
    "authors": [
      "Matthieu Jonckheere",
      "Chiara Mignacco",
      "Gilles Stoltz"
    ]
  },
  "https://openreview.net/forum?id=4xXJDO8Bvu": {
    "title": "Node Classification With Reject Option",
    "volume": "main",
    "abstract": "One of the key tasks in graph learning is node classification. While Graph neural networks have been used for various applications, their adaptivity to reject option settings has not been previously explored. In this paper, we propose NCwR, a novel approach to node classification in Graph Neural Networks (GNNs) with an integrated reject option. This allows the model to abstain from making predictions for samples with high uncertainty. We propose cost-based and coverage-based methods for classification with abstention in node classification settings using GNNs. We perform experiments using our method on standard citation network datasets Cora, CiteSeer, PubMed and ogbn-arxiv. We also model the Legal judgment prediction problem on the ILDC dataset as a node classification problem, where nodes represent legal cases and edges represent citations. We further interpret the model by analyzing the cases in which it abstains from predicting and visualizing which part of the input features influenced this decision",
    "checked": true,
    "id": "7be8668a9dfc33278b868b3fa4176f01e11185ae",
    "semantic_title": "node classification with reject option",
    "citation_count": 0,
    "authors": [
      "Uday Bhaskar Kuchipudi",
      "Jayadratha Gayen",
      "Charu Sharma",
      "Naresh Manwani"
    ]
  },
  "https://openreview.net/forum?id=xT8BEgXmVc": {
    "title": "Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models",
    "volume": "main",
    "abstract": "Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue across different number of agents in a centralized architecture, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Extensive results on Starcraft Multi-Agent Challenge (SMAC) and MAMujoco demonstrate superior sample efficiency and overall performance compared to strong model-free approaches and existing model-based methods",
    "checked": true,
    "id": "ba5420997282c19fa04db9cbe8d9e877ef4f73c4",
    "semantic_title": "decentralized transformers with centralized aggregation are sample-efficient multi-agent world models",
    "citation_count": 2,
    "authors": [
      "Yang Zhang",
      "Chenjia Bai",
      "Bin Zhao",
      "Junchi Yan",
      "Xiu Li",
      "Xuelong Li"
    ]
  },
  "https://openreview.net/forum?id=3cnpZ5SIjU": {
    "title": "Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse",
    "volume": "main",
    "abstract": "For a widely-studied data model and general loss and sample-hardening functions we prove that the losses of Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by representations that exhibit Neural-Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and UCL losses. In contrast to existing literature, our theoretical results for SCL do not require class-conditional independence of augmented views and work for a general loss function class that includes the widely used InfoNCE loss function. Moreover, our proofs are simpler, compact, and transparent. Similar to existing literature, our theoretical claims also hold for the practical scenario where batching is used for optimization. We empirically demonstrate, for the first time, that Adam optimization (with batching) of HSCL and HUCL losses with random initialization and suitable hardness levels can indeed converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard-negatives or feature normalization, however, the representations learned via Adam suffer from Dimensional-Collapse (DC) and fail to attain the NC-geometry. These results exemplify the role of hard-negative sampling in contrastive representation learning and we conclude with several open theoretical problems for future work. The code can be found at https://github.com/rjiang03/HCL/tree/main",
    "checked": true,
    "id": "d97fda96d01250760a192547e2dd1357f1c40d3a",
    "semantic_title": "hard-negative sampling for contrastive learning: optimal representation geometry and neural- vs dimensional-collapse",
    "citation_count": 2,
    "authors": [
      "Ruijie Jiang",
      "Thuan Nguyen",
      "Shuchin Aeron",
      "Prakash Ishwar"
    ]
  },
  "https://openreview.net/forum?id=Ckh17xN2R2": {
    "title": "Infrastructure for AI Agents",
    "volume": "main",
    "abstract": "\\textbf{AI agents} plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. To facilitate beneficial interactions and mitigate harmful ones, much research focuses on directly modifying agent behaviour. For example, developers can train agents to follow user instructions. This focus on direct modifications is useful, but insufficient. We will also need external protocols and systems that shape how agents interact with institutions and other actors. For instance, agents will need more efficient protocols to communicate with each other and form agreements. In addition, attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of \\textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents",
    "checked": true,
    "id": "9504e2f28fd2316124d45bdb216f58781e1b81b6",
    "semantic_title": "infrastructure for ai agents",
    "citation_count": 3,
    "authors": [
      "Alan Chan",
      "Kevin Wei",
      "Sihao Huang",
      "Nitarshan Rajkumar",
      "Elija Perrier",
      "Seth Lazar",
      "Gillian K Hadfield",
      "Markus Anderljung"
    ]
  },
  "https://openreview.net/forum?id=WADLPccB6o": {
    "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems",
    "volume": "main",
    "abstract": "In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems",
    "checked": true,
    "id": "15c5ada80de939768177c5d13614f2de8f4abe5b",
    "semantic_title": "conformal bounds on full-reference image quality for imaging inverse problems",
    "citation_count": 0,
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ]
  },
  "https://openreview.net/forum?id=k1eYngOvf0": {
    "title": "G-RepsNet: A Lightweight Construction of Equivariant Networks for Arbitrary Matrix Groups",
    "volume": "main",
    "abstract": "Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key insight in our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations leads to scalable equivariant networks. Further, these networks are universal approximators of functions equivariant to orthogonal groups. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as $O(5)$, $O(1, 3)$, and $O(3)$ with scalars, vectors, and second-order tensors as data types. On image classification tasks, we find that G-RepsNet using second-order representations is competitive and often even outperforms sophisticated state-of-the-art equivariant models such as GCNNs and $E(2)$-CNNs. To further illustrate the generality of our approach, we show that G-RepsNet is competitive to G-FNO and EGNN on N-body predictions and solving PDEs respectively, while being efficient",
    "checked": true,
    "id": "9f24687521c534aaa822970176d6ea7e3cd2b164",
    "semantic_title": "g-repsnet: a lightweight construction of equivariant networks for arbitrary matrix groups",
    "citation_count": 0,
    "authors": [
      "Sourya Basu",
      "Suhas Lohit",
      "Matthew Brand"
    ]
  },
  "https://openreview.net/forum?id=4uPJN6yfY1": {
    "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes",
    "volume": "main",
    "abstract": "Machine-learning from a disparate set of tables, a data lake, requires assembling features by merging and aggregating tables. Data discovery can extend autoML to data tables by automating these steps. We present an in-depth analysis of such automated table augmentation for machine learning tasks, analyzing different methods for the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. We use two data lakes: Open Data US, a well-referenced real data lake, and a novel semi-synthetic dataset, YADL (Yet Another Data Lake), which we developed as a tool for benchmarking this data discovery task. Systematic exploration on both lakes outlines 1) the importance of accurately retrieving join candidates, 2) the efficiency of simple merging methods, and 3) the resilience of tree-based learners to noisy conditions. Our experimental environment is easily reproducible and based on open data, to foster more research on feature engineering, autoML, and learning in data lakes",
    "checked": true,
    "id": "33c4cea9159564a7bc4239f183c7b0bf6ecc4fff",
    "semantic_title": "retrieve, merge, predict: augmenting tables with data lakes",
    "citation_count": 4,
    "authors": [
      "Riccardo Cappuzzo",
      "Aimee Coelho",
      "Félix Lefebvre",
      "Paolo Papotti",
      "Gaël Varoquaux"
    ]
  },
  "https://openreview.net/forum?id=55593xywWG": {
    "title": "Foundation Models Meet Federated Learning: A One-shot Feature-sharing Method with Privacy and Performance Guarantees",
    "volume": "main",
    "abstract": "Adapting foundation models for downstream tasks via Federated Learning (FL) is a promising strategy for protecting privacy while leveraging the capability of foundation models. However, FL's iterative training and model transmission result in high communication costs and GPU memory demands, making large foundation models impractical for FL. This paper introduces a one-shot FL method with a server-side performance bound to enable foundation models by reducing communication costs and GPU memory requirements. Our approach, FedPFT (FL with Parametric Feature Transfer), involves clients learning and transferring parametric models for features extracted from frozen foundation models in a single round. Parametric models are then used to generate synthetic features at the server to train a classifier head. We evaluate FedPFT across eight vision datasets using three vision foundation models. Our findings demonstrate that FedPFT is agnostic to data heterogeneity and network topology and it enhances the communication-accuracy frontier up to 7.8\\%. Finally, we show FedPFT's compatibility with differential privacy and its resilience against reconstruction attacks. Our work highlights the capability of private, feature-sharing methods for one-shot knowledge transfer using foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Beitollahi",
      "Alex Bie",
      "Sobhan Hemati",
      "Leo Maxime Brunswic",
      "Xu Li",
      "Xi Chen",
      "Guojun Zhang"
    ]
  },
  "https://openreview.net/forum?id=XHXAvACdgv": {
    "title": "NITO: Neural Implicit Fields for Resolution-free and Domain-Adaptable Topology Optimization",
    "volume": "main",
    "abstract": "Structural topology optimization plays a crucial role in engineering by determining the optimal material layout within a design space to maximize performance under given constraints. We introduce Neural Implicit Topology Optimization (NITO), a deep learning regression approach to accelerate topology optimization tasks. We demonstrate that, compared to state-of-the-art diffusion models, NITO generates structures that are under 15% as structurally sub-optimal and does so ten times faster. Furthermore, we show that NITO is entirely resolution-free and domain-agnostic, offering a more scalable solution than the current fixed-resolution and domain-specific diffusion models. To achieve this state-of-the-art performance, NITO combines three key innovations. First, we introduce the Boundary Point Order-Invariant MLP (BPOM), which represents loads and supports in a sparse and domain-agnostic manner, allowing NITO to train on variable conditioning, domain shapes, and mesh resolutions. Second, we adopt a neural implicit field representation, which allows NITO to synthesize topologies of any shape or resolution. Finally, we propose an inference-time refinement step using a few steps of gradient-based optimization to enable NITO to achieve results comparable to direct optimization methods. These three innovations empower NITO with a precision and versatility that is currently unparalleled among competing deep learning approaches for topology optimization. Code & Data: https://github.com/ahnobari/NITO_Public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Heyrani Nobari",
      "Lyle Regenwetter",
      "Giorgio Giannone",
      "Faez Ahmed"
    ]
  },
  "https://openreview.net/forum?id=tYjoHjShxF": {
    "title": "An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration",
    "volume": "main",
    "abstract": "In out-of-distribution (OOD) generalization tasks, fine-tuning pre-trained models has become a prevalent strategy. Different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training dataset size, and training strategies impact generalization and uncertainty calibration on downstream tasks. We evaluated 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 120,000 GPU hours. Our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving OOD accuracy over algorithm improvement alone. Additionally, we find that larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence, contrary to some prior studies that found modern deep networks to calibrate worse than classical shallow models. Our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration",
    "checked": true,
    "id": "37f729f59495f001cee0eff1637afdb92f1ab3e7",
    "semantic_title": "an empirical study of pre-trained model selection for out-of-distribution generalization and calibration",
    "citation_count": 3,
    "authors": [
      "Hiroki Naganuma",
      "Ryuichiro Hataya",
      "Kotaro Yoshida",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=Hy2KAldqAo": {
    "title": "Robust Offline Imitation Learning from Diverse Auxiliary Data",
    "volume": "main",
    "abstract": "Offline imitation learning enables learning a policy solely from a set of expert demonstrations, without any environment interaction. To alleviate the issue of distribution shift arising due to the small amount of expert data, recent works incorporate large numbers of auxiliary demonstrations alongside the expert data. However, the performance of these approaches rely on assumptions about the quality and composition of the auxiliary data, and they are rarely successful when those assumptions do not hold. To address this limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data (ROIDA). ROIDA first identifies high-quality transitions from the entire auxiliary dataset using a learned reward function. These high-reward samples are combined with the expert demonstrations for weighted behavioral cloning. For lower-quality samples, ROIDA applies temporal difference learning to steer the policy towards high-reward states, improving long-term returns. This two-pronged approach enables our framework to effectively leverage both high and low-quality data without any assumptions. Extensive experiments validate that ROIDA achieves robust and consistent performance across multiple auxiliary datasets with diverse ratios of expert and non-expert demonstrations. ROIDA effectively leverages unlabeled auxiliary data, outperforming prior methods reliant on specific data assumptions",
    "checked": true,
    "id": "a2fcaf0e11b80e431e2e8bbac2916c29702a4402",
    "semantic_title": "robust offline imitation learning from diverse auxiliary data",
    "citation_count": 1,
    "authors": [
      "Udita Ghosh",
      "Dripta S. Raychaudhuri",
      "Jiachen Li",
      "Konstantinos Karydis",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openreview.net/forum?id=7KkytYYhMv": {
    "title": "Rethinking the Value of Training-Free Structured Pruning of LLMs",
    "volume": "main",
    "abstract": "This paper investigates the effectiveness of training-free structured pruning techniques for Large Language Models (LLMs), with a particular focus on depth and width pruning strategies. Through an extensive empirical evaluation across a diverse range of tasks, datasets and modalities, we reveal critical limitations in current pruning methods. While some tasks exhibit minimal performance degradation, others face significant deterioration, even at low pruning rates, contradicting prior findings that often rely on selective benchmarks. Our analysis also finds that depth pruning, despite its simplicity, usually outperforms the more granular width pruning approaches in maintaining downstream task performance. Our findings highlight that existing evaluations of pruned LLMs often overstate their effectiveness due to incomplete or limited evaluation tasks, necessitating a critical reassessment of the true value of pruning and emphasizing the need to explore more robust pruning algorithms",
    "checked": true,
    "id": "b25aa26caaaf5b1a048dc12309bf1e0085e80e4e",
    "semantic_title": "rethinking the value of training-free structured pruning of llms",
    "citation_count": 0,
    "authors": [
      "Nahush Lele",
      "Arnav Chavan",
      "Aryamaan Thakur",
      "Deepak Gupta"
    ]
  },
  "https://openreview.net/forum?id=Qhfw5CUVd7": {
    "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through \\textbf{F}ine-\\textbf{G}rained \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{F}eedback (\\textbf{\\ours}), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters",
    "checked": true,
    "id": "fa84ef486184eb1c3d63949b700342bbcaf7b0c7",
    "semantic_title": "fgaif: aligning large vision-language models with fine-grained ai feedback",
    "citation_count": 17,
    "authors": [
      "Liqiang Jing",
      "Xinya Du"
    ]
  },
  "https://openreview.net/forum?id=OE4P1tW8iQ": {
    "title": "Noise-free Loss Gradients: A Surprisingly Effective Baseline for Coreset Selection",
    "volume": "main",
    "abstract": "The exponential rise in size and complexity of deep learning models and datasets have resulted in a considerable demand for computational resources. Coreset selection is one of the methods to alleviate this rising demand. The goal is to select a subset from a large dataset to train a model that performs almost at par with the one trained on the large dataset while reducing computational time and resource requirements. Existing approaches either attempt to identify remarkable samples (e.g., Forgetting, Adversarial Deepfool, EL2N, etc.) that stand out from the rest or solve complex optimization (e.g., submodular maximization, OMP) problems to compose the coresets. This paper proposes a novel and intuitive approach to efficiently select a coreset based on the similarity of loss gradients. Our method works on the hypothesis that gradients of samples belonging to a given class will point in similar directions during the early training phase. Samples with most neighbours that produce similar gradient directions, in other words, that produce noise-free gradients, will represent that class. Through extensive experimentation, we have demonstrated the effectiveness of our approach in out-performing state-of-the-art coreset selection algorithms on a range of benchmark datasets from CIFAR-10 to ImageNet with architectures of varied complexity (ResNet-18, ResNet-50, VGG-16, ViT).We have also demonstrated the effectiveness of our approach in Generative Modelling by implementing coreset selection to reduce training time for various GAN models (DCGAN, MSGAN, SAGAN, SNGAN) for different datasets (CIFAR-10, CIFAR-100, Tiny ImageNet) while not impacting the performance metrics significantly. Source code is provided at URL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saumyaranjan Mohanty",
      "Chimata Anudeep",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=BDPvuD5FTg": {
    "title": "Graph-based Confidence Calibration for Large Language Models",
    "volume": "main",
    "abstract": "Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases",
    "checked": true,
    "id": "e1536547084406d9f9864cc2dc08ca46add4a30b",
    "semantic_title": "graph-based confidence calibration for large language models",
    "citation_count": 2,
    "authors": [
      "Yukun Li",
      "Sijia Wang",
      "Lifu Huang",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=mjsoESaWDH": {
    "title": "Preferential Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "Preferential Bayesian optimization (PBO) is a framework for optimizing a decision-maker's latent preferences over available design choices. While real-world problems often involve multiple conflicting objectives, existing PBO methods assume that preferences can be encoded by a single objective function. For instance, in the customization of robotic assistive devices, technicians aim to maximize user comfort while minimizing energy consumption to extend battery life. Likewise, in autonomous driving policy design, stakeholders must evaluate safety and performance trade-offs before committing to a policy. To bridge this gap, we introduce the first framework for PBO with multiple objectives. Within this framework, we propose dueling scalarized Thompson sampling (DSTS), a multi-objective generalization of the popular dueling Thompson sampling algorithm, which may also be of independent interest beyond our setting. We evaluate DSTS across four synthetic test functions and two simulated tasks—exoskeleton personalization and driving policy design—demonstrating that it outperforms several benchmarks. Finally, we prove that DSTS is asymptotically consistent. Along the way, we provide, to our knowledge, the first convergence guarantee for dueling Thompson sampling in single-objective PBO",
    "checked": true,
    "id": "e6280a37ab1adb45fadd03e38bc00d9f0e3d0536",
    "semantic_title": "preferential multi-objective bayesian optimization",
    "citation_count": 2,
    "authors": [
      "Raul Astudillo",
      "Kejun Li",
      "Maegan Tucker",
      "Chu Xin Cheng",
      "Aaron Ames",
      "Yisong Yue"
    ]
  },
  "https://openreview.net/forum?id=xVEHiAZ7uR": {
    "title": "Adam-family Methods with Decoupled Weight Decay in Deep Learning",
    "volume": "main",
    "abstract": "In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by AdamW, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, compared to the existing results on the choices of the parameters for the moment terms in Adam, we show that our proposed framework provides more flexibility for these parameters. As a practical application of our proposed framework, we propose a novel Adam-family method named Adam with Decoupled Weight Decay (AdamD), and establish its convergence properties under mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and is comparable to AdamW, in the aspects of both generalization performance and efficiency",
    "checked": true,
    "id": "b4e7f304bfc61a4679961de80e37d1c6f53be90d",
    "semantic_title": "adam-family methods with decoupled weight decay in deep learning",
    "citation_count": 3,
    "authors": [
      "Kuangyu Ding",
      "Nachuan Xiao",
      "Kim-chuan Toh"
    ]
  },
  "https://openreview.net/forum?id=lmHh4FmPWZ": {
    "title": "Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements",
    "checked": true,
    "id": "7ebf1eb74d4dae3e9bf91e5a34e1bef1f68c0645",
    "semantic_title": "generalized compressed sensing for image reconstruction with diffusion probabilistic models",
    "citation_count": 0,
    "authors": [
      "Ling-Qi Zhang",
      "Zahra Kadkhodaie",
      "Eero P Simoncelli",
      "David H. Brainard"
    ]
  },
  "https://openreview.net/forum?id=0c6iG28rRl": {
    "title": "Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification",
    "volume": "main",
    "abstract": "Predicting simple function classes has been widely used as a testbed for developing theory and understanding of the trained Transformer's in-context learning (ICL) ability. In this paper, we revisit the training of Transformers on linear regression tasks, and different from all the existing literature, we consider a bi-objective prediction task of predicting both the conditional expectation $\\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This additional uncertainty quantification objective provides a handle to (i) better design out-of-distribution experiments to distinguish ICL from in-weight learning (IWL) and (ii) make a better separation between the algorithms with and without using the prior information of the training distribution. Theoretically, we show that the trained Transformer reaches near Bayes optimum, suggesting the usage of the information of the training distribution. Our method can be extended to other cases. Specifically, with the Transformer's context window $S$, we prove a generalization bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\min\\{S, T\\}/(n T)})$ on $n$ tasks with sequences of length $T$, providing sharper analysis compared to previous results of $\\tilde{\\mathcal{O}}(\\sqrt{1/n})$. Empirically, we illustrate that while the trained Transformer behaves as the Bayes-optimal solution as a natural consequence of supervised training in distribution, it does not necessarily perform a Bayesian inference when facing task shifts, in contrast to the \\textit{equivalence} between these two proposed in many existing literature. We also demonstrate the trained Transformer's ICL ability over covariate shift and prompt-length shift and interpret them as a generalization over a meta distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Liu",
      "Zhongze Cai",
      "Guanting Chen",
      "Xiaocheng Li"
    ]
  },
  "https://openreview.net/forum?id=qRAjZuf48S": {
    "title": "A Theoretical Study of Neural Network Expressive Power via Manifold Topology",
    "volume": "main",
    "abstract": "A prevalent assumption regarding real-world data is that it lies on or close to a low-dimensional manifold. When deploying a neural network on data manifolds, the required size, i.e., the number of neurons of the network, heavily depends on the intricacy of the underlying latent manifold. While significant advancements have been made in understanding the geometric attributes of manifolds, it's essential to recognize that topology, too, is a fundamental characteristic of manifolds. In this study, we investigate network expressive power in terms of the latent data manifold. Integrating both topological and geometric facets of the data manifold, we present a size upper bound of ReLU neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Yao",
      "Lingjie Yi",
      "Mayank Goswami",
      "Chao Chen"
    ]
  },
  "https://openreview.net/forum?id=Gl6dF9soQo": {
    "title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
    "volume": "main",
    "abstract": "Learning predictive world models is crucial for enhancing the planning capabilities of reinforcement learning (RL) agents. Recently, MuZero-style algorithms, leveraging the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains. However, these methods struggle to scale in heterogeneous scenarios with diverse dependencies and task variability. To overcome these limitations, we introduce UniZero, a novel approach that employs a transformer-based world model to effectively learn a shared latent space. By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in the latent space. We show that UniZero significantly outperforms existing baselines in benchmarks that require long-term memory. Additionally, UniZero demonstrates superior scalability in multitask learning experiments conducted on Atari benchmarks. In standard single-task RL settings, such as Atari and DMControl, UniZero matches or even surpasses the performance of current state-of-the-art methods. Finally, extensive ablation studies and visual analyses validate the effectiveness and scalability of UniZero's design choices. Our code is available at \\textcolor{magenta}{https://github.com/opendilab/LightZero}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Pu",
      "Yazhe Niu",
      "Zhenjie Yang",
      "Jiyuan Ren",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://openreview.net/forum?id=WvgoxpGpuU": {
    "title": "T2L: Efficient Zero-Shot Action Recognition with Temporal Token Learning",
    "volume": "main",
    "abstract": "Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the temporal aspects inherent to the video domain. In this study, we present Efficient Zero-Shot Action Recognition with Temporal Token Learning(T2L), a simple and efficient adaptation of CLIP that addresses these challenges. T2L leverages Temporal Token Learning (TTL) for seamless temporal adaptation, requiring no fundamental changes to the core CLIP architecture while preserving its remarkable generalization abilities. TTL relies on temporal feature diversity (TFD), a novel learning objective, which guides TTL to focus on capturing motion, thereby enhancing its learning capabilities from videos. We perform extensive experiments on nine different benchmark datasets, thoroughly evaluating T2L for zero-shot learning and base-to-novel video action recognition, and also demonstrating its potential for few-shot generalization. Impressively, with merely 5.2 million learnable parameters, T2L can be efficiently trained on a single GPU (with 25x less learnable parameters, 3x reduction in GFLOPs, and 4x improvement in throughput when compared with prior best model), outperforming existing approaches in several evaluations",
    "checked": true,
    "id": "14193486de7c29741863c2936d2b51846c3dad35",
    "semantic_title": "t2l: efficient zero-shot action recognition with temporal token learning",
    "citation_count": 0,
    "authors": [
      "Shahzad Ahmad",
      "Sukalpa Chanda",
      "Yogesh S Rawat"
    ]
  },
  "https://openreview.net/forum?id=BMGikHBjlx": {
    "title": "Ctrl-V: Higher Fidelity Autonomous Vehicle Video Generation with Bounding-Box Controlled Object Motion",
    "volume": "main",
    "abstract": "Controllable video generation has attracted significant attention, largely due to advances in video diffusion models. In domains such as autonomous driving, developing highly accurate predictions for object motions is essential. This paper addresses the key challenge of enabling fine-grained control over object motion in the context of driving video synthesis. To accomplish this, we 1) employ a distinct, specialized model to forecast the trajectories of object bounding boxes, 2) adapt and enhance a separate video diffusion network to create video content conditioned on these high-quality trajectory forecasts, and 3) we are able to exert precise control over object position/movements using bounding boxes in both 2D and 3D spaces. Our method, Ctrl-V, leverages modified and fine-tuned Stable Video Diffusion (SVD) models to solve both trajectory and video generation. Extensive experiments conducted on the KITTI, Virtual-KITTI 2, BDD100k, and nuScenes datasets validate the effectiveness of our approach in producing realistic and controllable video generation. Project page: \\url{https://oooolga.github.io/ctrl-v.github.io/}",
    "checked": true,
    "id": "b756d04e6e410aff8506677084636652ad4648fc",
    "semantic_title": "ctrl-v: higher fidelity autonomous vehicle video generation with bounding-box controlled object motion",
    "citation_count": 0,
    "authors": [
      "Ge Ya Luo",
      "ZhiHao Luo",
      "Anthony Gosselin",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=Teu1Blr2YJ": {
    "title": "Node Feature Forecasting in Temporal Graphs: an Interpretable Online Algorithm",
    "volume": "main",
    "abstract": "In this paper, we propose an online algorithm mspace for forecasting node features in temporal graphs, which captures spatial cross-correlation among different nodes as well as the temporal auto-correlation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Evaluations against various baselines, including temporal graph neural network (TGNN) models and classical Kalman filters, demonstrate that mspace performs comparably to the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent performance across datasets with varying training sizes, a notable advantage over TGNN models that require abundant training samples to effectively learn the spatiotemporal trends in the data. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoretical bounds on multi-step forecasting error of mspace and show that it scales linearly with the number of forecast steps $q$ as $\\mathcal{O}(q)$. For an asymptotically large number of nodes $n$, and timesteps $T$, the computational complexity of mspace grows linearly with both \\$n\\$ and \\$T\\$, i.e., $\\mathcal{O}(nT)$, while its space complexity remains constant $\\mathcal{O}(1)$. We compare the performance of various mspace variants against ten recent TGNN baselines and two classical baselines, ARIMA and the Kalman filter, across ten real-world datasets. Lastly, we have investigated the interpretability of different mspace variants by analyzing model parameters alongside dataset characteristics to jointly derive model-centric and data-centric insights",
    "checked": true,
    "id": "bc273c281f1df1aa246c3359d2656c2c0922b826",
    "semantic_title": "node feature forecasting in temporal graphs: an interpretable online algorithm",
    "citation_count": 0,
    "authors": [
      "Aniq Ur Rahman",
      "Justin Coon"
    ]
  },
  "https://openreview.net/forum?id=Reh1S8rxfh": {
    "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach",
    "volume": "main",
    "abstract": "In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for reasonable causal models reflecting the broad knowledge of domain experts, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge-based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. The experiments in this work have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. These experiments have also revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: https://github.com/mas-takayama/LLM-and-SCD",
    "checked": true,
    "id": "c213737923f58ad6c4cd18a8c17bca6522d7f4c6",
    "semantic_title": "integrating large language models in causal discovery: a statistical causal approach",
    "citation_count": 20,
    "authors": [
      "MASAYUKI TAKAYAMA",
      "Tadahisa OKUDA",
      "Thong Pham",
      "Tatsuyoshi Ikenoue",
      "Shingo Fukuma",
      "Shohei Shimizu",
      "Akiyoshi Sannai"
    ]
  },
  "https://openreview.net/forum?id=72YVabBErN": {
    "title": "Efficient Open Set Single Image Test Time Adaptation of Vision Language Models",
    "volume": "main",
    "abstract": "Adapting models to dynamic, real-world environments characterized by shifting data distributions and unseen test scenarios is a critical challenge in deep learning. In this paper, we consider a realistic and challenging Test-Time Adaptation setting, where a model must continuously adapt to test samples that arrive sequentially, one at a time, while distinguishing between known and unknown classes. Current Test-Time Adaptation methods operate under closed-set assumptions or batch processing, differing from the real-world open-set scenarios. We address this limitation by establishing a comprehensive benchmark for Open-set Single-image Test-Time Adaptation using Vision-Language Models. Furthermore, we propose ROSITA, a novel framework that leverages dynamically updated feature banks to identify reliable test samples and employs a contrastive learning objective to improve the separation between known and unknown classes. Our approach effectively adapts models to domain shifts for known classes while rejecting unfamiliar samples. Extensive experiments across diverse real-world benchmarks demonstrate that ROSITA sets a new state-of-the-art in open-set TTA, achieving both strong performance and computational efficiency for real-time deployment. The code is released at https://github.com/manogna-s/ROSITA.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=eQeYyup1tm": {
    "title": "Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks",
    "volume": "main",
    "abstract": "Grokking is an intriguing phenomenon of delayed generalization, where neural networks initially memorize training data with perfect accuracy but exhibit poor generalization, subsequently transitioning to a generalizing solution with continued training. While factors such as weight norms and sparsity have been proposed to explain this delayed generalization, the influence of network structure remains underexplored. In this work, we link the grokking phenomenon to the lottery ticket hypothesis to investigate the impact of internal network structures. We demonstrate that utilizing lottery tickets obtained during the generalizing phase (termed grokked tickets) significantly reduces delayed generalization across various tasks, including multiple modular arithmetic operations, polynomial regression, sparse parity, and MNIST classification. Through controlled experiments, we show that the mitigation of delayed generalization is not due solely to reduced weight norms or increased sparsity, but rather to the discovery of good subnetworks. Furthermore, we find that grokked tickets exhibit periodic weight patterns and undergo rapid structural changes that coincide with improvements in generalization. Additionally, pruning techniques like the edge-popup algorithm can identify these effective structures without modifying the weights, thereby transforming memorizing networks into generalizing ones. These results underscore the novel insight that structural exploration plays a pivotal role in understanding grokking",
    "checked": true,
    "id": "325e649380a0d6bc10f51dda6679e39a26851e68",
    "semantic_title": "bridging lottery ticket and grokking: understanding grokking from inner structure of networks",
    "citation_count": 1,
    "authors": [
      "Gouki Minegishi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://openreview.net/forum?id=6o3vVBWYis": {
    "title": "Language Models Are Good Tabular Learners",
    "volume": "main",
    "abstract": "Transformer-based language models have become the de facto standard in natural language processing. However, they underperform in the tabular data domain compared to traditional tree-based methods. We posit that current models fail to achieve the full potential of language models due to (i) heterogeneity of tabular data; and (ii) challenges faced by the model in interpreting numerical values. Based on this hypothesis, we propose the Tabular Domain Transformer (TDTransformer) framework. TDTransformer has distinct embedding processes for different types of columns. The alignment layers for different column-types transform these embeddings to a common space. Besides, TDTransformer adapts piece-wise linear encoding for numerical values for better performance. We test the proposed method on 76 real-world tabular classification datasets from the OpenML benchmark. Extensive experiments indicate that TDTransformer significantly improves the state-of-the-art methods",
    "checked": true,
    "id": "2517c035aeffdc8f5039a23f29322e890dec426c",
    "semantic_title": "language models are good tabular learners",
    "citation_count": 1,
    "authors": [
      "Zhenhan Huang",
      "Kavitha Srinivas",
      "Horst Samulowitz",
      "Niharika S. D'Souza",
      "Charu C. Aggarwal",
      "Pin-Yu Chen",
      "Jianxi Gao"
    ]
  },
  "https://openreview.net/forum?id=vc7poEYOFK": {
    "title": "Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching",
    "volume": "main",
    "abstract": "Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential (VAPO) Flow Bayes, a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VAPO learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VAPO, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks",
    "checked": true,
    "id": "ab8f30257748b2830dca3930d6a613c054de7ad9",
    "semantic_title": "learning energy-based generative models via potential flow: a variational principle approach to probability density homotopy matching",
    "citation_count": 0,
    "authors": [
      "Junn Yong Loo",
      "Leong Fang Yu",
      "Michelle Adeline",
      "Julia K. Lau",
      "Hwa Hui Tew",
      "Arghya Pal",
      "VISHNU MONN BASKARAN",
      "Chee-Ming Ting",
      "Raphael CW Phan"
    ]
  },
  "https://openreview.net/forum?id=lyxRBPmmnV": {
    "title": "Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations",
    "volume": "main",
    "abstract": "Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks",
    "checked": true,
    "id": "934152abcb942dadffb0c34aea1e55d947efc91d",
    "semantic_title": "neural slot interpreters: grounding object semantics in emergent slot representations",
    "citation_count": 1,
    "authors": [
      "Bhishma Dedhia",
      "Niraj Jha"
    ]
  },
  "https://openreview.net/forum?id=FEo55EIvGI": {
    "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective",
    "volume": "main",
    "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse compared to those trained with cross-entropy loss. Furthermore, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the impact of label smoothing on model performance and calibration. Then, leveraging the unconstrained feature model, we derive closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses. We show that models trained with label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs",
    "checked": true,
    "id": "08901f2f5b363f28660f401afafccc2e6c9373d5",
    "semantic_title": "cross entropy versus label smoothing: a neural collapse perspective",
    "citation_count": 9,
    "authors": [
      "Li Guo",
      "George Andriopoulos",
      "Zifan Zhao",
      "Zixuan Dong",
      "Shuyang Ling",
      "Keith W. Ross"
    ]
  },
  "https://openreview.net/forum?id=3Jm4dbrKGZ": {
    "title": "Lurie Networks with Robust Convergent Dynamics",
    "volume": "main",
    "abstract": "The Lurie network is a novel and unifying time-invariant neural ODE. Many existing continuous-time models, including recurrent neural networks and neural oscillators, are special cases of the Lurie network in this context. Mild constraints on the weights and biases of the Lurie network are derived to ensure a generalised concept of stability is guaranteed. This generalised stability measure is that of k-contraction which permits global convergence to a point, line or plane in the neural state-space. This includes global convergence to one of multiple equilibrium points or limit cycles as observed in many dynamical systems including associative and working memory. Weights and biases of the Lurie network, which satisfy the k-contraction constraints, are encoded through unconstrained parametrisations. The novel stability results and parametrisations provide a toolset for training over the space of k-contracting Lurie network's using standard optimisation algorithms. These results are also leveraged to construct and train a graph Lurie network satisfying the same convergence properties. Empirical results show the improvement in prediction accuracy, generalisation and robustness on a range of simulated dynamical systems, when the graph structure and k-contraction conditions are introduced. These results also compare favourably against other well known stability-constrained models and an unconstrained neural ODE",
    "checked": true,
    "id": "d7714d5e003d84b8fef1dc9734b0724c5ad7eac4",
    "semantic_title": "lurie networks with robust convergent dynamics",
    "citation_count": 0,
    "authors": [
      "Carl R Richardson",
      "Matthew C. Turner",
      "Steve R. Gunn"
    ]
  },
  "https://openreview.net/forum?id=hMPzJ3qKpf": {
    "title": "LocalFormer: Mitigating Over-Globalising in Transformers on Graphs with Localised Training",
    "volume": "main",
    "abstract": "As Transformers become more popular for graph machine learning, a significant issue has recently been observed. Their global attention mechanisms tend to overemphasize distant vertices, leading to the phenomenon of ``over-globalising.'' This phenomenon often results in the dilution of essential local information, particularly in graphs where local neighbourhoods carry significant predictive power. Existing methods often struggle with rigidity in their local processing, where tightly coupled operations limit flexibility and adaptability in diverse graph structures. Additionally, these methods can overlook critical structural nuances, resulting in an incomplete integration of local and global contexts. This paper addresses these issues by proposing LocalFormer, a novel framework, to effectively localise a transformer model by integrating a distinct local module and a complementary module that integrates global information. The local module focuses on capturing and preserving fine-grained, neighbourhood-specific patterns, ensuring that the model maintains sensitivity to critical local structures. In contrast, the complementary module dynamically integrates broader context without overshadowing the localised information, offering a balanced approach to feature aggregation across different scales of the graph. Through collaborative and warm-up training strategies, these modules work synergistically to mitigate the adverse effects of over-globalising, leading to improved empirical performance. Our experimental results demonstrate the effectiveness of LocalFormer compared to state-of-the-art baselines on vertex-classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naganand Yadati"
    ]
  },
  "https://openreview.net/forum?id=UcrVnXBdZI": {
    "title": "On the effectiveness of Rotation-Equivariance in U-Net: A Benchmark for Image Segmentation",
    "volume": "main",
    "abstract": "Numerous studies have recently focused on incorporating different variations of equivariance in Convolutional Neural Networks (CNNs). In particular, rotation-equivariance has gathered significant attention due to its relevance in many applications related to medical imaging, microscopic imaging, satellite imaging, industrial tasks, etc. While prior research has primarily focused on enhancing classification tasks with rotation equivariant CNNs, their impact on more complex architectures, such as U-Net for image segmentation, remains scarcely explored. Indeed, previous work interested in integrating rotation-equivariance into U-Net architecture have focused on solving specific applications with a limited scope. In contrast, this paper aims to provide a more exhaustive evaluation of rotation equivariant U-Net for image segmentation across a broader range of tasks. We benchmark their effectiveness against standard U-Net architectures, assessing improvements in terms of performance and sustainability (i.e., computational cost). Our evaluation focuses on datasets whose orientation of objects of interest is arbitrary in the image (e.g., Kvasir-SEG), but also on more standard segmentation datasets (such as COCO-Stuff) as to explore the wider applicability of rotation equivariance beyond tasks undoubtedly concerned by rotation equivariance. The main contribution of this work is to provide insights into the trade-offs and advantages of integrating rotation equivariance for segmentation tasks",
    "checked": true,
    "id": "898ad18bf8e4f858502177ee4465f0c6a9476720",
    "semantic_title": "on the effectiveness of rotation-equivariance in u-net: a benchmark for image segmentation",
    "citation_count": 0,
    "authors": [
      "Robin Ghyselinck",
      "Valentin Delchevalerie",
      "Bruno Dumas",
      "Benoit Frenay"
    ]
  },
  "https://openreview.net/forum?id=J6oxTJPOyN": {
    "title": "LEGO-Learn: Label-Efficient Graph Open-Set Learning",
    "volume": "main",
    "abstract": "How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to a large number of labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that addresses open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then selects highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the $C$ known ID classes and an additional class representing OOD nodes (hence, a $C+1$ classifier). This classifier utilizes a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, achieving up to a $6.62\\%$ improvement in ID classification accuracy and a $7.49\\%$ increase in AUROC for OOD detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyan Xu",
      "Kay Liu",
      "Zhengtao Yao",
      "Philip S. Yu",
      "Mengyuan Li",
      "Kaize Ding",
      "Yue Zhao"
    ]
  },
  "https://openreview.net/forum?id=VlwqIz41Hp": {
    "title": "Generalized Prediction Set with Bandit Feedback",
    "volume": "main",
    "abstract": "In high-stakes environments where uncertainties abound, set-valued prediction offers a cautious and robust mechanism by presenting multiple potential labels as the prediction for each test instance to mitigate the potential risk associated with prediction errors. Yet, integrating this paradigm with out-of-distribution (OOD) detection remains scarcely explored in such settings as online learning with bandit feedback. The bandit feedback mechanism informs the learner about the correctness of the pulled arm/action instead of the explicit ground truth label, leaving the true class label unknown when an incorrect action is taken. To address this challenge, we introduce BanditGPS which conducts set-valued prediction with OOD detection in the bandit feedback setting, using an estimation to the ground truth of class labels. BanditGPS achieves three objectives: render small/informative prediction sets, enhance the OOD detection performance, and control the recall for all normal classes to meet prescribed requirements. Our approach is characterized by the loss function, which trades off between high OOD detection and small prediction sets. Theoretically, we prove that the convergence rate of the regret is $\\tilde{\\mathcal{O}}(T^{-1/2})$. The empirical results further show that BanditGPS effectively controls the recalls with promising performances on OOD detection and informative prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://openreview.net/forum?id=mk1YIkVvTQ": {
    "title": "Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose \\emph{Concept2Concept}, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo. Warning: This paper contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salma Abdel Magid",
      "Weiwei Pan",
      "Simon Warchol",
      "Grace Guo",
      "Junsik Kim",
      "Mahia Rahman",
      "Hanspeter Pfister"
    ]
  },
  "https://openreview.net/forum?id=VuLEOyTiPO": {
    "title": "GeNIe: Generative Hard Negative Images Through Diffusion",
    "volume": "main",
    "abstract": "Data augmentation is crucial in training deep models, preventing them from overfitting to limited data. Recent advances in generative AI, e.g., diffusion models, have enabled more sophisticated augmentation techniques that produce data resembling natural images. We introduce $\\texttt{GeNIe}$ a novel augmentation method which leverages a latent diffusion model conditioned on a text prompt to combine two contrasting data points (an image from the source category and a text prompt from the target category) to generate challenging augmentations. To achieve this, we adjust the noise level (equivalently, number of diffusion iterations) to ensure the generated image retains low-level and background features from the source image while representing the target category, resulting in a hard negative sample for the source category. We further automate and enhance $\\texttt{GeNIe}$ by adaptively adjusting the noise level selection on a per image basis (coined as $\\texttt{GeNIe-Ada}$), leading to further performance improvements. Our extensive experiments, in both few-shot and long-tail distribution settings, demonstrate the effectiveness of our novel augmentation method and its superior performance over the prior art. Our code is available at https://github.com/UCDvision/GeNIe",
    "checked": true,
    "id": "7f0e374f1920d962359d7e9ccb7a7d7e2d18e6f4",
    "semantic_title": "genie: generative hard negative images through diffusion",
    "citation_count": 4,
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Anuj Singh",
      "Navaneet K L",
      "Hamed Pirsiavash",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=FFnRLvWefK": {
    "title": "System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of optimizing initial conditions and termination time in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and the state's value can not be measured in real-time but only with a delay while the measuring device processes the sample. To identify the optimal conditions in limited trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information. At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block. We further develop a two-stage BO framework to effectively incorporate search space constraints, enabling efficient optimization of both initial conditions and observation timings. We conduct extensive experiments showcasing SANODEP's potential for few-shot BO within dynamical systems. We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy",
    "checked": true,
    "id": "fcc3aee3d63369b46495f52c938226e7f6a37977",
    "semantic_title": "system-aware neural ode processes for few-shot bayesian optimization",
    "citation_count": 3,
    "authors": [
      "Jixiang Qing",
      "Rebecca D. Langdon",
      "Robert Matthew Lee",
      "Behrang Shafei",
      "Mark van der Wilk",
      "Calvin Tsay",
      "Ruth Misener"
    ]
  },
  "https://openreview.net/forum?id=x9VQFjtOPS": {
    "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks",
    "volume": "main",
    "abstract": "LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects—task definition, LLM profiling, and search procedures—making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. This survey aims to provide a comprehensive but integrated technical review on existing LIS frameworks. Specifically, we unify task definitions under Markov Decision Process (MDP) and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For ongoing paper updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Search",
    "checked": true,
    "id": "c52045988ba4b6cedabeea93ac4b03c5d7a79a85",
    "semantic_title": "a survey on llm test-time compute via search: tasks, llm profiling, search algorithms, and relevant frameworks",
    "citation_count": 6,
    "authors": [
      "Xinzhe Li"
    ]
  },
  "https://openreview.net/forum?id=bCmEP1Ltwq": {
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "volume": "main",
    "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted operations research techniques. In this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach matches or surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes",
    "checked": true,
    "id": "f0a50c531fff4a7fdd7715fe20d2ade10f438fb4",
    "semantic_title": "neural deconstruction search for vehicle routing problems",
    "citation_count": 1,
    "authors": [
      "André Hottung",
      "Paula Wong-Chung",
      "Kevin Tierney"
    ]
  },
  "https://openreview.net/forum?id=IbQTE24aZw": {
    "title": "Deflated Dynamics Value Iteration",
    "volume": "main",
    "abstract": "The Value Iteration (VI) algorithm is an iterative procedure to compute the value function of a Markov decision process, and is the basis of many reinforcement learning (RL) algorithms as well. As the error convergence rate of VI as a function of iteration $k$ is $O(\\gamma^k)$, it is slow when the discount factor $\\gamma$ is close to $1$. To accelerate the computation of the value function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses matrix splitting and matrix deflation techniques to effectively remove (deflate) the top $s$ dominant eigen-structure of the transition matrix $\\mathcal{P}^\\pi$. We prove that this leads to a $\\tilde{O}(\\gamma^k |\\lambda_{s+1}|^k)$ convergence rate, where $\\lambda_{s+1}$ is the $(s+1)$-th largest eigenvalue of the dynamics matrix. We also extend DDVI to the RL setting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We empirically show the effectiveness of the proposed algorithms",
    "checked": true,
    "id": "345355d4fa7424f81b30e10129f50de73216cdbb",
    "semantic_title": "deflated dynamics value iteration",
    "citation_count": 2,
    "authors": [
      "Jongmin Lee",
      "Amin Rakhsha",
      "Ernest K. Ryu",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=zVo6PfBa0K": {
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "volume": "main",
    "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality—a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domains, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform the current state-of-the-art methods on almost all competition-level planning tasks",
    "checked": true,
    "id": "4435bc3de88b489a7ad20374db6af7f05926d371",
    "semantic_title": "generating symbolic world models via test-time scaling of large language models",
    "citation_count": 3,
    "authors": [
      "Zhouliang Yu",
      "Yuhuan Yuan",
      "Tim Z. Xiao",
      "Fuxiang Frank Xia",
      "Jie Fu",
      "Ge Zhang",
      "Ge lin",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=JyjTJAG9yZ": {
    "title": "Personalized Layer Selection for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) combine node attributes over a fixed granularity of the local graph structure around a node to predict its label. However, different nodes may relate to a node-level property with a different granularity of its local neighborhood, and using the same level of smoothing for all nodes can be detrimental to their classification. In this work, we challenge the common fact that a single GNN layer can classify all nodes of a graph by training GNNs with a distinct personalized layer for each node. Inspired by metric learning, we propose a novel algorithm, MetSelect, to select the optimal representation layer to classify each node. In particular, we identify a prototype representation of each class in a transformed GNN layer and then, classify using the layer where the distance is smallest to a class prototype after normalizing with that layer's variance. Results on 10 datasets and 3 different GNNs show that we significantly improve the node classification accuracy of GNNs in a plug-and-play manner. We also find that using variable layers for prediction enables GNNs to be deeper and more robust to poisoning attacks. We hope this work can inspire future works to learn more adaptive and personalized graph representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Sharma",
      "Vineeth Rakesh",
      "Yingtong Dou",
      "Srijan Kumar",
      "Mahashweta Das"
    ]
  },
  "https://openreview.net/forum?id=WgJgIULL9Q": {
    "title": "HyperVQ: MLR-based Vector Quantization in Hyperbolic Space",
    "volume": "main",
    "abstract": "The success of models operating on tokenized data has heightened the need for effective tokenization methods, particularly in vision and auditory tasks where inputs are naturally continuous. A common solution is to employ Vector Quantization (VQ) within VQ Variational Autoencoders (VQVAEs), transforming inputs into discrete tokens by clustering embeddings in Euclidean space. However, Euclidean embeddings not only suffer from inefficient packing and limited separation—due to their polynomial volume growth—but are also prone to codebook collapse, where only a small subset of codebook vectors are effectively utilized. To address these limitations, we introduce HyperVQ, a novel approach that formulates VQ as a hyperbolic Multinomial Logistic Regression (MLR) problem, leveraging the exponential volume growth in hyperbolic space to mitigate collapse and improve cluster separability. Additionally, HyperVQ represents codebook vectors as geometric representatives of hyperbolic decision hyperplanes, encouraging disentangled and robust latent representations. Our experiments demonstrate that HyperVQ matches traditional VQ in generative and reconstruction tasks, while surpassing it in discriminative performance and yielding a more efficient and disentangled codebook",
    "checked": true,
    "id": "61613785fe09423e8df112ea726bb1195bc2575c",
    "semantic_title": "hypervq: mlr-based vector quantization in hyperbolic space",
    "citation_count": 4,
    "authors": [
      "Nabarun Goswami",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=1weZ9Wsajk": {
    "title": "Optimizing Cycle Life Prediction of Lithium-ion Batteries via a Physics-Informed Model",
    "volume": "main",
    "abstract": "Accurately measuring the cycle lifetime of commercial lithium-ion batteries is crucial for performance and technology development. We introduce a novel hybrid approach combining a physics-based equation with a self-attention model to predict the cycle lifetimes of commercial lithium iron phosphate graphite cells via early-cycle data. After fitting capacity loss curves to this physics-based equation, we then use a self-attention layer to reconstruct entire battery capacity loss curves. Our model exhibits comparable performances to existing models while predicting more information: the entire capacity loss curve instead of cycle life. This provides more robustness and interpretability: our model does not need to be retrained for a different notion of end-of-life and is backed by physical intuition",
    "checked": true,
    "id": "2ea1a0f132357e236e9ab3c199280eac5ba9caed",
    "semantic_title": "optimizing cycle life prediction of lithium-ion batteries via a physics-informed model",
    "citation_count": 0,
    "authors": [
      "Nathan Sun",
      "Daniel Nicolae",
      "Sara Sameer",
      "Karena Yan"
    ]
  },
  "https://openreview.net/forum?id=WOwQKguWT0": {
    "title": "When SNN meets ANN: Error-Free ANN-to-SNN Conversion for Extreme Edge Efficiency",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to convolutional neural networks (CNN), thanks to advanced ANN-to-SNN conversion techniques, all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. However, these conversion techniques incur a large number of time steps, and consequently, high spiking activity. In this paper, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the existing conversion approaches. Our framework modifies the standard integrate-and-fire (IF) neuron model used in SNNs with no change in computational complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To reduce spiking activity, we propose training the source ANN with a fine-grained $\\ell_1$ regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with low latency, low compute energy, thanks to the low time steps and high spike sparsity, and high test accuracy, for example, $75.12$% with only $4$ time steps on the ImageNet dataset. Codes will be made available. Code is available at https://github.com/godatta/SNN_meets_ANN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gourav Datta",
      "Zeyu Liu",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Peter Anthony Beerel"
    ]
  },
  "https://openreview.net/forum?id=fqSVqPcaVi": {
    "title": "ASTRA: A Scene-aware Transformer-based Model for Trajectory Prediction",
    "volume": "main",
    "abstract": "We present ASTRA (A Scene-aware Transformer-based model for trajectory prediction), a light-weight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Izzeddin Teeti",
      "Aniket Thomas",
      "Munish Monga",
      "Sachin Kumar Giroh",
      "Uddeshya Singh",
      "Andrew Bradley",
      "Biplab Banerjee",
      "Fabio Cuzzolin"
    ]
  },
  "https://openreview.net/forum?id=78N9tCL6Ly": {
    "title": "Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances",
    "checked": true,
    "id": "c7fe09b6c8f067e0ed97401fbac56242d77371ef",
    "semantic_title": "leveraging unlabeled data sharing through kernel function approximation in offline reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Yen Ru Lai",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ]
  },
  "https://openreview.net/forum?id=xu4ATNjcdy": {
    "title": "Variational Stochastic Gradient Descent for Deep Neural Networks",
    "volume": "main",
    "abstract": "Optimizing deep neural networks is one of the main tasks in successful deep learning. Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better modeling the uncertainty of the gradients. Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer. We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Kuzina",
      "Haotian Chen",
      "Babak Esmaeili",
      "Jakub M. Tomczak"
    ]
  },
  "https://openreview.net/forum?id=2e1aZZd88C": {
    "title": "Non-Myopic Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO",
    "checked": true,
    "id": "561b1857598dec8935322c0fde2da06bd240bf27",
    "semantic_title": "non-myopic multi-objective bayesian optimization",
    "citation_count": 0,
    "authors": [
      "Syrine Belakaria",
      "Alaleh Ahmadian",
      "Barbara E Engelhardt",
      "Stefano Ermon",
      "Jana Doppa"
    ]
  },
  "https://openreview.net/forum?id=aKjJoEVKgO": {
    "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
    "volume": "main",
    "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain that focuses on developing efficient and sustainable training strategies to adapt models to emerging knowledge and achieve robustness in dynamic environments. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge. Since existing works concentrate mostly on continual fine-tuning for a limited selection of downstream tasks or training domains, we introduce a new benchmark designed to measure the adaptability of LLMs to changing pretraining data landscapes. We further examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) continual pretraining consistently improves <1.5B models studied in this work and is also superior to domain adaptation, (ii) larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus, (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting, (iv) continual pretraining boosts downstream task performance of GPT-2 family, (v) continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity while randomizing training domains leads to better transfer and final performance otherwise. We posit that our research establishes a new benchmark for CL in LLMs, providing a more realistic evaluation of knowledge retention and transfer across diverse domains",
    "checked": true,
    "id": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6",
    "semantic_title": "investigating continual pretraining in large language models: insights and implications",
    "citation_count": 29,
    "authors": [
      "Çağatay Yıldız",
      "Nishaanth Kanna Ravichandran",
      "Nitin Sharma",
      "Matthias Bethge",
      "Beyza Ermis"
    ]
  },
  "https://openreview.net/forum?id=XofMHO5yVY": {
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
    "volume": "main",
    "abstract": "Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score'' — a numerical estimate of the expertise of a reviewer in reviewing a paper — and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms currently employed in computer science conferences and come up with recommendations for stakeholders. Our four main findings are: - All algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, thereby highlighting the vital need for more research on the similarity-computation problem. - Most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter2 algorithm performs best. - The classical TF-IDF algorithm which can use full texts of papers is on par with Specter2 that uses only titles and abstracts. - The performance of off-the-shelf LLMs is worse than the specialized algorithms. We encourage researchers to participate in our survey and contribute their data to the dataset here: https://forms.gle/SP1Rh8eivGz54xR37",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivan Stelmakh",
      "John Frederick Wieting",
      "Yang Xi",
      "Graham Neubig",
      "Nihar B Shah"
    ]
  },
  "https://openreview.net/forum?id=NUV8THrLZC": {
    "title": "Efficient Exploration in Multi-Agent Reinforcement Learning via Farsighted Self-Direction",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning faces greater challenges with efficient exploration compared to single-agent counterparts, primarily due to the exponential growth in state and action spaces. Methods based on intrinsic rewards have been proven to enhance exploration efficiency in multi-agent scenarios effectively. However, these methods are plagued by instability during training and biases in exploration direction. To address these challenges, we propose Farsighted Self-Direction (FSD), a novel model-free method that utilizes a long-term exploration bonus to achieve coordinated exploration. Since prediction error against individual Q-values indicates a potential bonus for committed exploration, it is taken into account in action selection to directly guide the coordinated exploration. Further, we also use clipped double Q-learning to reduce noise in prediction error. We validate the method on didactic examples and demonstrate the outperformance of our method on challenging StarCraft II micromanagement tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Lao",
      "Xudong Guo",
      "Mengge Liu",
      "Junjie Yu",
      "Yi Liu",
      "Wenhui Fan"
    ]
  },
  "https://openreview.net/forum?id=8otbGorZK2": {
    "title": "Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images",
    "volume": "main",
    "abstract": "Despite considerable progress in image classification tasks, classification models seem unaffected by the images that significantly deviate from those that appear natural to human eyes. Specifically, while human perception can easily identify abnormal appearances or compositions in images, classification models overlook any alterations in the arrangement of object parts as long as they are present in any order, even if unnatural. Hence, this work exposes the vulnerability of having semantic and syntactic discrepancy in images (SSDI) in the form of corruptions that remove or shuffle image patches or present images in the form of puzzles. To address this vulnerability, we propose the concept of \"image grammar\", comprising \"image semantics\" and \"image syntax\". Image semantics pertains to the interpretation of parts or patches within an image, whereas image syntax refers to the arrangement of these parts to form a coherent object. We present a semi-supervised two-stage method for learning the image grammar of visual elements and environments solely from natural images. While the first stage learns the semantic meaning of individual object parts, the second stage learns how their relative arrangement constitutes an entire object. The efficacy of the proposed approach is then demonstrated by achieving SSDI detection rates ranging from 70% to 90% on corruptions generated from CelebA and SUN-RGBD datasets. Code is publicly available at: https://github.com/ChunTao1999/SSDI/",
    "checked": true,
    "id": "e7d88f9a88db846629893e2606ca724f4847cd8d",
    "semantic_title": "semantic-syntactic discrepancy in images (ssdi): learning meaning and order of features from natural images",
    "citation_count": 1,
    "authors": [
      "Chun Tao",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=sSAp8ITBpC": {
    "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)",
    "volume": "main",
    "abstract": "Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems",
    "checked": true,
    "id": "9fa830e5c3a108f13cdb25c05a9e6107e365ad83",
    "semantic_title": "operationalizing a threat model for red-teaming large language models (llms)",
    "citation_count": 13,
    "authors": [
      "Apurv Verma",
      "Satyapriya Krishna",
      "Sebastian Gehrmann",
      "Madhavan Seshadri",
      "Anu Pradhan",
      "John A. Doucette",
      "David Rabinowitz",
      "Leslie Barrett",
      "Tom Ault",
      "Hai Phan"
    ]
  },
  "https://openreview.net/forum?id=bXUipBbZDA": {
    "title": "Reinforcement Learning from Bagged Reward",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), it is commonly assumed that an immediate reward signal is generated for each action taken by the agent, helping the agent maximize cumulative rewards to obtain the optimal policy. However, in many real-world scenarios, designing immediate reward signals is difficult; instead, agents receive a single reward that is contingent upon a partial sequence or a complete trajectory. In this work, we define this challenging problem as RL from Bagged Reward (RLBR), where sequences of data are treated as bags with non-Markovian bagged rewards, leading to the formulation of Bagged Reward Markov Decision Processes (BRMDPs). Theoretically, we demonstrate that RLBR can be addressed by solving a standard MDP with properly redistributed bagged rewards allocated to each instance within a bag. Empirically, we find that reward redistribution becomes more challenging as the bag length increases, due to reduced informational granularity. Existing reward redistribution methods are insufficient to address these challenges. Therefore, we propose a novel reward redistribution method equipped with a bidirectional attention mechanism, enabling the accurate interpretation of contextual nuances and temporal dependencies within each bag. We experimentally demonstrate that the proposed method consistently outperforms existing approaches",
    "checked": false,
    "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "semantic_title": "a survey of reinforcement learning from human feedback",
    "citation_count": 142,
    "authors": [
      "Yuting Tang",
      "Xin-Qiang Cai",
      "Yao-Xiang Ding",
      "Qiyu Wu",
      "Guoqing Liu",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=AcLlg4J52H": {
    "title": "RS-Reg: Probabilistic and Robust Certified Regression through Randomized Smoothing",
    "volume": "main",
    "abstract": "Randomized smoothing has shown promising certified robustness against adversaries in classification tasks. Despite such success with only zeroth-order access to base models, randomized smoothing has not been extended to a general form of regression. By defining robustness in regression tasks flexibly through probabilities, we demonstrate how to establish upper bounds on input data point perturbation (using the $\\ell_2$ norm) for a user-specified probability of observing valid outputs. Furthermore, we showcase the asymptotic property of a basic averaging function in scenarios where the regression model operates without any constraint. We then derive a certified upper bound of the input perturbations when dealing with a family of regression models where the outputs are bounded. Our simulations verify the validity of the theoretical results and reveal the advantages and limitations of simple smoothing functions, i.e., averaging, in regression tasks. The code is publicly available at \\url{https://github.com/arekavandi/Certified_Robust_Regression}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aref Miri Rekavandi",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ]
  },
  "https://openreview.net/forum?id=qahoztvThX": {
    "title": "A functional framework for nonsmooth autodiff with {\\it maxpooling} functions",
    "volume": "main",
    "abstract": "We make a comment on the recent work by Boustany, by showing that the Murat-TrombettiTheorem provides a simple and efficient mathematical framework for nonsmooth automatic differentiation of {\\it maxpooling} functions. In particular it gives a the chain rule formula which correctly defines the composition of Lipschitz-continuous functions which are piecewise $C^1$. The formalism is applied to four basic examples, with some tests in PyTorch. A self contained proof of an important Stampacchia formula is in the appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Després"
    ]
  },
  "https://openreview.net/forum?id=ubrOSWyTS8": {
    "title": "∇QDARTS: Quantization as an Elastic Dimension to Differentiable NAS",
    "volume": "main",
    "abstract": "Differentiable Neural Architecture Search methods efficiently find high-accuracy architectures using gradient-based optimization in a continuous domain, saving computational resources. Mixed-precision search helps optimize precision within a fixed architecture. However, applying it to a NAS-generated network does not assure optimal performance as the optimized quantized architecture may not emerge from a standalone NAS method. In light of these considerations, this paper introduces ∇QDARTS, a novel approach that combines differentiable NAS with mixed-precision search for both weight and activation. ∇QDARTS aims to identify the optimal mixed-precision neural architecture capable of achieving remarkable accuracy while operating with minimal computational requirements in a single-shot, end-to-end differentiable framework, obviating the need for pretraining and proxy methods. Compared to fp32, ∇QDARTS shows impressive performance on CIFAR10 with (2,4) bit precision, reducing bit operations by 160× with a slight 1.57% accuracy drop. Increasing the capacity enables ∇QDARTS to match fp32 accuracy while reducing bit operations by 18×. For the ImageNet dataset, with just (2,4) bit precision, ∇QDARTS outperforms state-of-the-art methods such as APQ, SPOS, OQA, and MNAS by 2.3%, 2.9%, 0.3%, and 2.7% in terms of accuracy. By incorporating (2,4,8) bit precision, ∇QDARTS further minimizes the accuracy drop to 1% compared to fp32, alongside a substantial reduction of 17× in required bit operations and 2.6× in memory footprint. In terms of bit-operation (memory footprint) ∇QDARTS excels over APQ, SPOS, OQA, and MNAS with similar accuracy by 2.3× (12×), 2.4× (3×), 13% (6.2×), 3.4× (37%), for bit-operation (memory footprint), respectively. ∇QDARTS enhances the overall search and training efficiency, achieving a 3.1× and 1.54× improvement over APQ and OQA, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Payman Behnam",
      "Uday Kamal",
      "Sanjana Vijay Ganesh",
      "Zhaoyi Li",
      "Michael Andrew Jurado",
      "Alind Khare",
      "Igor Fedorov",
      "Gaowen Liu",
      "Alexey Tumanov"
    ]
  },
  "https://openreview.net/forum?id=sTdVnDW0HX": {
    "title": "Piecewise Constant Spectral Graph Neural Network",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network(PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vahan Martirosyan",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=KQzJYI6eo0": {
    "title": "Global Graph Counterfactual Explanation: A Subgraph Mapping Approach",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been widely deployed in various real-world applications. However, most GNNs are black-box models that lack explanations. One strategy to explain GNNs is through counterfactual explanation, which aims to find minimum perturbations on input graphs that change the GNN predictions. Existing works on GNN counterfactual explanations primarily concentrate on the local-level perspective (i.e., generating counterfactuals for each individual graph), which suffers from information overload and lacks insights into the broader cross-graph relationships. To address such issues, we propose GlobalGCE, a novel global-level graph counterfactual explanation method. GlobalGCE aims to identify a collection of subgraph mapping rules as counterfactual explanations for the target GNN. According to these rules, substituting certain significant subgraphs with their counterfactual subgraphs will change the GNN prediction to the desired class for most graphs (i.e., maximum coverage). Methodologically, we design a significant subgraph generator and a counterfactual subgraph autoencoder in our GlobalGCE, where the subgraphs and the rules can be effectively generated. Extensive experiments demonstrate the superiority of our GlobalGCE compared to existing baselines. Our code can be found at \\url{https://github.com/YinhanHe123/GlobalGCE}",
    "checked": true,
    "id": "745e233c29fedbf002b31a3987ce614178106eb8",
    "semantic_title": "global graph counterfactual explanation: a subgraph mapping approach",
    "citation_count": 0,
    "authors": [
      "Yinhan He",
      "Wendy Zheng",
      "Yaochen Zhu",
      "Jing Ma",
      "Saumitra Mishra",
      "Natraj Raman",
      "Ninghao Liu",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=F6l3BBPElY": {
    "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers",
    "volume": "main",
    "abstract": "This work proposes a novel setup where a neural network is trained to predict multiple steps of the reverse diffusion process in an unrolled manner, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively denoises the input during the reverse process until the final layer estimates the original input, $x_0$. Additionally, we introduce a new learning target by using latent variables, rather than the conventional approach of predicting the original input $x_0$ or source error $\\epsilon_0$. In speech synthesis, using $x_0$ or $\\epsilon_0$ often leads to large prediction errors in the early stages of the denoising process, causing distortion in the recovered speech. Our method mitigates this issue and, through extensive evaluation, demonstrates the generation of high-fidelity speech in competitive time, outperforming current state-of-the-art techniques. Moreover, the proposed approach generalizes well to unseen speech. Sample audio is available at \\url{https://onexpeters.github.io/UDPNet/}",
    "checked": true,
    "id": "47df6a7118baa441598e69e1cb5a3bd41cec97f6",
    "semantic_title": "speech synthesis by unrolling diffusion process using neural network layers",
    "citation_count": 0,
    "authors": [
      "Peter Ochieng"
    ]
  },
  "https://openreview.net/forum?id=goe6fv6iSh": {
    "title": "Gaussian Pre-Activations in Neural Networks: Myth or Reality?",
    "volume": "main",
    "abstract": "The study of feature propagation at initialization in neural networks lies at the root of numerous initialization designs. A very common assumption is that the pre-activations are Gaussian. Although this convenient *Gaussian hypothesis* can be justified when the number of neurons per layer tends to infinity, it is challenged by both theoretical and experimental work for finite-width neural networks. Our main contribution is to construct a family of pairs of activation functions and initialization distributions that ensure that the pre-activations remain Gaussian throughout the network depth, even in narrow neural networks, under the assumption that the pre-activations are independent. In the process, we discover a set of constraints that a neural network should satisfy to ensure Gaussian pre-activations. In addition, we provide a critical review of the claims of the Edge of Chaos line of work and construct a non-asymptotic Edge of Chaos analysis. We also propose a unified view on the propagation of pre-activations, encompassing the framework of several well-known initialization procedures. More generally, our work provides a principled framework for addressing the much-debated question: is it desirable to initialize the training of a neural network whose pre-activations are guaranteed to be Gaussian? Our code is available on GitHub: https://github.com/p-wol/gaussian-preact/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Wolinski",
      "Julyan Arbel"
    ]
  },
  "https://openreview.net/forum?id=akumIxQjNN": {
    "title": "ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs",
    "volume": "main",
    "abstract": "The expansion of neural network sizes and the enhanced resolution of modern image sensors result in heightened memory and power demands to process modern computer vision models. In order to deploy these models in extremely resource-constrained edge devices, it is crucial to reduce their peak memory, which is the maximum memory consumed during the execution of a model. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision, including image classification and diffusion-based image generation. For image classification, our method yields 4x-5x theoretical peak memory reduction with less degradation in accuracy for most CNN-based architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods when applied to the same student network. The code is available at https://github.com/mengtang-lab/ReDistill",
    "checked": false,
    "id": "7012a749533b2c30c3c633844008134fe73968cf",
    "semantic_title": "redistill: residual encoded distillation for peak memory reduction",
    "citation_count": 1,
    "authors": [
      "Fang Chen",
      "Gourav Datta",
      "Mujahid Al Rafi",
      "Hyeran Jeon",
      "Meng Tang"
    ]
  },
  "https://openreview.net/forum?id=9fPinz1iH2": {
    "title": "Heterophily-informed Message Passing",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learnt embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g. generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks",
    "checked": true,
    "id": "a642b5644af6305dc3c274efc7064d655c40ae84",
    "semantic_title": "heterophily-informed message passing",
    "citation_count": 0,
    "authors": [
      "Haishan Wang",
      "Arno Solin",
      "Vikas K Garg"
    ]
  },
  "https://openreview.net/forum?id=AIby9MQXbu": {
    "title": "Robust Model Selection of Gaussian Graphical Models",
    "volume": "main",
    "abstract": "In Gaussian graphical model selection, noise-corrupted samples present significant challenges. It is known that even minimal amounts of noise can obscure the underlying structure, leading to fundamental identifiability issues. A recent line of work addressing this \"robust model selection\" problem narrows its focus to tree-structured graphical models. Even within this specific class of models, exact structure recovery is shown to be impossible. However, several algorithms have been developed that are known to provably recover the underlying tree-structure up to an (unavoidable) equivalence class. In this paper, we extend these results beyond tree-structured graphs. We first characterize the equivalence class up to which general graphs can be recovered in the presence of noise. Despite the inherent ambiguity (which we prove is unavoidable), the structure that can be recovered reveals local clustering information and global connectivity patterns in the underlying model. Such information is useful in a range of real-world problems, including power grids, social networks, protein-protein interactions, and neural structures. We then propose an algorithm which provably recovers the underlying graph up to the identified ambiguity. We further provide finite sample guarantees in the high-dimensional regime for our algorithm and validate our results through numerical simulations",
    "checked": false,
    "id": "6447ffa996c21d26af441f33d06bc6773e4002c6",
    "semantic_title": "a simple method for estimating gaussian graphical models",
    "citation_count": 0,
    "authors": [
      "Abrar Zahin",
      "Rajasekhar Anguluri",
      "Lalitha Sankar",
      "Oliver Kosut",
      "Gautam Dasarathy"
    ]
  },
  "https://openreview.net/forum?id=h434zx5SX0": {
    "title": "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
    "volume": "main",
    "abstract": "Causal discovery, the task of inferring causal structure from data, has the potential to uncover mechanistic insights from biological experiments, especially those involving perturbations. However, causal discovery algorithms over larger sets of variables tend to be brittle against misspecification or when data are limited. For example, single-cell transcriptomics measures thousands of genes, but the nature of their relationships is not known, and there may be as few as tens of cells per intervention setting. To mitigate these challenges, we propose a foundation model-inspired approach: a supervised model trained on large-scale, synthetic data to predict causal graphs from summary statistics — like the outputs of classical causal discovery algorithms run over subsets of variables and other statistical hints like inverse covariance. Our approach is enabled by the observation that typical errors in the outputs of a discovery algorithm remain comparable across datasets. Theoretically, we show that the model architecture is well-specified, in the sense that it can recover a causal graph consistent with graphs over subsets. Empirically, we train the model to be robust to misspecification and distribution shift using diverse datasets. Experiments on biological and synthetic data confirm that this model generalizes well beyond its training set, runs on graphs with hundreds of variables in seconds, and can be easily adapted to different underlying data assumptions",
    "checked": true,
    "id": "a8232bcaaff260e8b61e51253bf5cedbd08cc89a",
    "semantic_title": "sample, estimate, aggregate: a recipe for causal discovery foundation models",
    "citation_count": 7,
    "authors": [
      "Menghua Wu",
      "Yujia Bao",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=IizmQoF86Y": {
    "title": "A Learning-Based Framework for Fair and Scalable Solution Generation in Kidney Exchange Problems",
    "volume": "main",
    "abstract": "Reinforcement learning and Generative Flow Networks, known as GFlowNets, present an exciting possibility for neural networks to model distributions across various data structures. In this paper, we broaden their applicability to data structures consisting of optimal solutions for a combinatorial problem. Concretely, we propose using Q-learning and various policy gradient methods, as well as GFlowNets to learn the distribution of optimal solutions for kidney exchange problems (KEPs). This could provide a useful tool for decision-making authorities, policymakers and clinicians, as it offers them multiple optimal or near-optimal solutions, and provides a complementary landscape to their traditional integer programming-based toolbox for promoting fairness and societal benefits. Our reinforcement learning-based framework trained on KEP instances provides an effective addition to computationally expensive exact approaches, notably mixed-integer programming. Our experiments thoroughly evaluate the quality of the solution sets sampled from the trained neural networks in terms of optimality, their scalability when dealing with real-sized KEP instances, and their capability to generate a diverse pool of solutions. We also cover the use of their efficient solution generation capabilities to improve fairness and simulate the evolution of the KEP pool in a dynamic setting. Our contribution is thus: 1) methodological, as it introduces a novel setting for reinforcement learning in addition to GFlowNets, 2) implementational, as it delves beyond the theory and details how to use conditional information, and 3) of practical significance, as it considers a specific combinatorial problem in the healthcare domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William St-Arnaud",
      "Margarida Carvalho",
      "Golnoosh Farnadi"
    ]
  },
  "https://openreview.net/forum?id=HRvHCd03HM": {
    "title": "Double Horizon Model-Based Policy Optimization",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long ``distribution rollout'' (DR) and a short ``training rollout'' (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akihiro Kubo",
      "Paavo Parmas",
      "Shin Ishii"
    ]
  },
  "https://openreview.net/forum?id=vPVqQmjCy8": {
    "title": "LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling",
    "volume": "main",
    "abstract": "Time series~(TS) modeling is essential in dynamic systems like weather prediction and anomaly detection. Recent studies utilize Large Language Models (LLMs) for TS modeling, leveraging their powerful pattern recognition capabilities. These methods primarily position LLMs as the predictive backbone, often omitting the mathematical modeling within traditional TS models, such as periodicity. However, disregarding the potential of LLMs also overlooks their pattern recognition capabilities. To address this gap, we introduce \\textit{LLM-TS Integrator}, a novel framework that effectively integrates the capabilities of LLMs into traditional TS modeling. Central to this integration is our \\textit{mutual information} module. The core of this \\textit{mutual information} module is a traditional TS model enhanced with LLM-derived insights for improved predictive abilities. This enhancement is achieved by maximizing the mutual information between traditional model's TS representations and LLM's textual representation counterparts, bridging the two modalities. Moreover, we recognize that samples vary in importance for two losses: traditional prediction and mutual information maximization. To address this variability, we introduce the \\textit{sample reweighting} module to improve information utilization. This module assigns dual weights to each sample: one for prediction loss and another for mutual information loss, dynamically optimizing these weights via bi-level optimization. Our method achieves state-of-the-art or comparable performance across five mainstream TS tasks, including short-term and long-term forecasting, imputation, classification, and anomaly detection. Our code is available at: \\url{https://anonymous.4open.science/r/llm_ts_anonymous-F07D/README.MD}",
    "checked": true,
    "id": "013fefa7c76d4f07cd5b40bb12553cb17be9e98c",
    "semantic_title": "llm-ts integrator: integrating llm for enhanced time series modeling",
    "citation_count": 1,
    "authors": [
      "Can Chen",
      "Gabriel L. Oliveira",
      "Hossein Sharifi-Noghabi",
      "Tristan Sylvain"
    ]
  },
  "https://openreview.net/forum?id=beqSqPgE33": {
    "title": "Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees",
    "volume": "main",
    "abstract": "Graphical models are widely used in diverse application domains to model the conditional dependencies amongst a collection of random variables. In this paper, we consider settings where the graph structure is covariate-dependent, and investigate a deep neural network-based approach to estimate it. The method allows for flexible functional dependency on the covariate, and fits the data reasonably well in the absence of a Gaussianity assumption. Theoretical results with PAC guarantees are established for the method, under assumptions commonly used in an Empirical Risk Minimization framework. The performance of the proposed method is evaluated on several synthetic data settings and benchmarked against existing approaches. The method is further illustrated on real datasets involving data from neuroscience and finance, respectively, and produces interpretable results",
    "checked": true,
    "id": "f94e7f3658b928895c03be91b72d7b0e0a24f87b",
    "semantic_title": "covariate-dependent graphical model estimation via neural networks with statistical guarantees",
    "citation_count": 0,
    "authors": [
      "Jiahe Lin",
      "Yikai Zhang",
      "George Michailidis"
    ]
  },
  "https://openreview.net/forum?id=X6IY04Akw1": {
    "title": "Generalizable and Robust Spectral Method for Multi-view Representation Learning",
    "volume": "main",
    "abstract": "Multi-view representation learning (MvRL) has garnered substantial attention in recent years, driven by the increasing demand for applications that can effectively process and analyze data from multiple sources. In this context, graph Laplacian-based MvRL methods have demonstrated remarkable success in representing multi-view data. However, these methods often struggle with generalization to new data and face challenges with scalability. Moreover, in many practical scenarios, multi-view data is contaminated by noise or outliers. In such cases, modern deep-learning-based MvRL approaches that rely on alignment or contrastive objectives present degraded performance in downstream tasks, as they may impose incorrect consistency between clear and corrupted data sources. We introduce *SpecRaGE*, a novel fusion-based framework that integrates the strengths of graph Laplacian methods with the power of deep learning to overcome these challenges. SpecRage uses neural networks to learn parametric mapping that approximates a joint diagonalization of graph Laplacians. This solution bypasses the need for alignment while enabling generalizable and scalable learning of informative and meaningful representations. Moreover, it incorporates a meta-learning fusion module that dynamically adapts to data quality, ensuring robustness against outliers and noisy views. Our extensive experiments demonstrate that SpecRaGE outperforms state-of-the-art methods, particularly in scenarios with data contamination, paving the way for more reliable and efficient multi-view learning. Our code will be made publicly available upon acceptance",
    "checked": true,
    "id": "1faa5623932d4ee1b87780d086f87aa42583e402",
    "semantic_title": "generalizable and robust spectral method for multi-view representation learning",
    "citation_count": 1,
    "authors": [
      "Amitai Yacobi",
      "Ofir Lindenbaum",
      "Uri Shaham"
    ]
  },
  "https://openreview.net/forum?id=5qo8MF3QU1": {
    "title": "Out-of-Distribution Learning with Human Feedback",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) learning often relies on strong statistical assumptions or predefined OOD data distributions, limiting its effectiveness in real-world deployment for both OOD generalization and detection, especially when human inspection is minimal. This paper introduces a novel framework for OOD learning that integrates human feedback to enhance model adaptation and reliability. Our approach leverages freely available unlabeled data in the wild, which naturally captures environmental test-time OOD distributions under both covariate and semantic shifts. To effectively utilize such data, we propose selectively acquiring human feedback to label a small subset of informative samples. These labeled samples are then used to train both a multi-class classifier and an OOD detector. By incorporating human feedback, our method significantly improves model robustness and precision in handling OOD scenarios. We provide theoretical insights by establishing generalization error bounds for our algorithm. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a significant margin. Code is publicly available at https://github.com/HaoyueBaiZJU/ood-hf",
    "checked": true,
    "id": "04bf3cb0a104edd715d7ba639822174a86af7c11",
    "semantic_title": "out-of-distribution learning with human feedback",
    "citation_count": 4,
    "authors": [
      "Haoyue Bai",
      "Xuefeng Du",
      "Katie Rainey",
      "Shibin Parameswaran",
      "Yixuan Li"
    ]
  },
  "https://openreview.net/forum?id=B9BHjTN4z6": {
    "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning",
    "volume": "main",
    "abstract": "Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. Our documentation, examples, and source code are available at [https://github.com/RLE-Foundation/RLeXplore](https://github.com/RLE-Foundation/RLeXplore)",
    "checked": true,
    "id": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
    "semantic_title": "rlexplore: accelerating research in intrinsically-motivated reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Mingqi Yuan",
      "Roger Creus Castanyer",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng",
      "Glen Berseth"
    ]
  },
  "https://openreview.net/forum?id=hiiRCXmbAz": {
    "title": "Hyperparameters in Continual Learning: A Reality Check",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungmin Cha",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=Ucpfdn66k2": {
    "title": "When Are Bias-Free ReLU Networks Effectively Linear Networks?",
    "volume": "main",
    "abstract": "We investigate the implications of removing bias in ReLU networks regarding their expressivity and learning dynamics. We first show that two-layer bias-free ReLU networks have limited expressivity: the only odd function two-layer bias-free ReLU networks can express is a linear one. We then show that, under symmetry conditions on the data, these networks have the same learning dynamics as linear networks. This enables us to give analytical time-course solutions to certain two-layer bias-free (leaky) ReLU networks outside the lazy learning regime. While deep bias-free ReLU networks are more expressive than their two-layer counterparts, they still share a number of similarities with deep linear networks. These similarities enable us to leverage insights from linear networks to understand certain ReLU networks. Overall, our results show that some properties previously established for bias-free ReLU networks arise due to equivalence to linear networks",
    "checked": true,
    "id": "929e9a89b82c95f52eb23c7b932c1969487a5238",
    "semantic_title": "when are bias-free relu networks effectively linear networks?",
    "citation_count": 0,
    "authors": [
      "Yedi Zhang",
      "Andrew M Saxe",
      "Peter E. Latham"
    ]
  },
  "https://openreview.net/forum?id=nannw4SGfS": {
    "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics",
    "volume": "main",
    "abstract": "As learned image compression (LIC) methods become increasingly computationally demanding, enhancing their training efficiency is crucial. This paper takes a step forward in accelerating the training of LIC methods by modeling the neural training dynamics. We first propose a Sensitivity-aware True and Dummy Embedding Training mechanism (STDET) that clusters LIC model parameters into few separate modes where parameters are expressed as affine transformations of reference parameters within the same mode. By further utilizing the stable intra-mode correlations throughout training and parameter sensitivities, we gradually embed non-reference parameters, reducing the number of trainable parameters. Additionally, we incorporate a Sampling-then-Moving Average (SMA) technique, interpolating sampled weights from stochastic gradient descent (SGD) training to obtain the moving average weights, ensuring smooth temporal behavior and minimizing training state variances. Overall, our method significantly reduces training space dimensions and the number of trainable parameters without sacrificing model performance, thus accelerating model convergence. We also provide a theoretical analysis on the Noisy quadratic model, showing that the proposed method achieves a lower training variance than standard SGD. Our approach offers valuable insights for further developing efficient training methods for LICs",
    "checked": true,
    "id": "6abeb9b5b6ad6c3f6ea2b5fb6a0f0b2ddff59f49",
    "semantic_title": "accelerating learned image compression through modeling neural training dynamics",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ]
  },
  "https://openreview.net/forum?id=BaRD2Nfj41": {
    "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models",
    "volume": "main",
    "abstract": "Pretraining and finetuning models has become increasingly popular in decision-making. But there are still serious impediments in Imitation Learning from Observation (ILfO) with pretrained models. This study identifies two primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB). The EKB emerges due to the pretrained models' limitations in handling novel observations, which leads to inaccurate action inference. Conversely, the DKB stems from the reliance on limited demonstration datasets, restricting the model's adaptability across diverse scenarios. We propose separate solutions to overcome each barrier and apply them to Action Inference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new algorithm, AIME-NoB, integrates online interactions and a data-driven regulariser to mitigate the EKB. Additionally, it uses a surrogate reward function to broaden the policy's supported states, addressing the DKB. Our experiments on vision-based control tasks from the DeepMind Control Suite and MetaWorld benchmarks show that AIME-NoB significantly improves sample efficiency and converged performance, presenting a robust framework for overcoming the challenges in ILfO with pretrained models. Code available at https://github.com/IcarusWizard/AIME-NoB",
    "checked": true,
    "id": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
    "semantic_title": "overcoming knowledge barriers: online imitation learning from visual observation with pretrained world models",
    "citation_count": 0,
    "authors": [
      "Xingyuan Zhang",
      "Philip Becker-Ehmck",
      "Patrick van der Smagt",
      "Maximilian Karl"
    ]
  },
  "https://openreview.net/forum?id=yGGoOVpBVP": {
    "title": "Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods",
    "volume": "main",
    "abstract": "Recently, it has been observed that when training a deep neural net with SGD, the majority of the loss landscape's curvature quickly concentrates in a tiny *top* eigenspace of the loss Hessian, which remains largely stable thereafter. Independently, it has been shown that successful magnitude pruning masks for deep neural nets emerge early in training and remain stable thereafter. In this work, we study these two phenomena jointly and show that they are connected: We develop a methodology to measure the similarity between arbitrary parameter masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap* as the most useful such metric due to its interpretability and stability. To compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M parameters --an unprecedented scale by several orders of magnitude. Our experiments reveal an *overlap* between magnitude parameter masks and top Hessian eigenspaces consistently higher than chance-level, and that this effect gets accentuated for larger network sizes. This result indicates that *top Hessian eigenvectors tend to be concentrated around larger parameters*, or equivalently, that *larger parameters tend to align with directions of larger loss curvature*. Our work provides a methodology to approximate and analyze deep learning Hessians at scale, as well as a novel insight on the structure of their eigenspace",
    "checked": true,
    "id": "9ad03b95335af5b64e86fbe666f0516711b99524",
    "semantic_title": "connecting parameter magnitudes and hessian eigenspaces at scale using sketched methods",
    "citation_count": 0,
    "authors": [
      "Andres Fernandez",
      "Frank Schneider",
      "Maren Mahsereci",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=mvbZBaqSXo": {
    "title": "Dimension reduction via score ratio matching",
    "volume": "main",
    "abstract": "Gradient-based dimension reduction decreases the cost of Bayesian inference and probabilistic modeling by identifying maximally informative (and informed) low-dimensional projections of the data and parameters, allowing high-dimensional problems to be reformulated as cheaper low-dimensional problems. A broad family of such techniques identify these projections and provide error bounds on the resulting posterior approximations, via eigendecompositions of certain diagnostic matrices. Yet these matrices require gradients or even Hessians of the log-likelihood, excluding the purely data-driven setting and many problems of simulation-based inference. We propose a framework, derived from score-matching, to extend gradient-based dimension reduction to problems where gradients are unavailable. Specifically, we formulate an objective function to directly learn the score ratio function needed to compute the diagnostic matrices, propose a tailored parameterization for the score ratio network, and introduce regularization methods that capitalize on the hypothesized low-dimensional structure. We also introduce a novel algorithm to iteratively identify the low-dimensional reduced basis vectors more accurately with limited data based on eigenvalue deflation methods. We show that our approach outperforms standard score-matching for problems with low-dimensional structure, and demonstrate its effectiveness for PDE-constrained Bayesian inverse problems and conditional generative modeling",
    "checked": true,
    "id": "b9018ae0afe7f92707f5cdcb2b95e1b831d343a7",
    "semantic_title": "dimension reduction via score ratio matching",
    "citation_count": 1,
    "authors": [
      "Ricardo Baptista",
      "Michael Brennan",
      "Youssef Marzouk"
    ]
  },
  "https://openreview.net/forum?id=eIPwJgadfZ": {
    "title": "Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic Space",
    "volume": "main",
    "abstract": "Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts. However, learning in hyperbolic spaces poses significant challenges. In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem. Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions. In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima. From extensive empirical experiments, these methods are shown to achieve better classification accuracies than the projected gradient descent approach in most of the synthetic and real two-dimensional hyperbolic embedding dataset under the one-vs-rest multiclass-classification scheme",
    "checked": true,
    "id": "56ad3d35be28e983c6fd632fd7ef92a346e7241d",
    "semantic_title": "convex relaxation for solving large-margin classifiers in hyperbolic space",
    "citation_count": 0,
    "authors": [
      "Sheng Yang",
      "Peihan Liu",
      "Cengiz Pehlevan"
    ]
  },
  "https://openreview.net/forum?id=MbF1gYfIlY": {
    "title": "Can Kernel Methods Explain How the Data Affects Neural Collapse?",
    "volume": "main",
    "abstract": "A vast amount of literature has recently focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU)",
    "checked": true,
    "id": "c45d749646b4312a58564209172192e2ff5f7a4f",
    "semantic_title": "can kernel methods explain how the data affects neural collapse?",
    "citation_count": 3,
    "authors": [
      "Vignesh Kothapalli",
      "Tom Tirer"
    ]
  },
  "https://openreview.net/forum?id=CovLQwu611": {
    "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables creation of specialized language models for diverse tasks, resulting in numerous expert modules. In many practical use cases, these expert PEFT modules are integrated into a single model that answers arbitrary queries by routing queries to different experts. However, only a few experts can be kept in GPU memory due to memory constraints. Consequently, expert modules are frequently loaded and offloaded between CPU/GPU memory or disk storage. This frequent swapping dramatically increases communication overhead, leading unacceptable latency and degrading user experience. The large size of modern PEFT modules further exacerbates this latency. For example, QLoRA experts for 65B LLaMA are 3.2GB, making swapping a major communication bottleneck, particularly in memory-constrained environments. To address these issues, we present ComPEFT (compressed PEFT), a novel method for compressing fine-tuning residuals (task vectors) of PEFT models. Reducing expert PEFT module size effectively addresses both memory and communication limitations, facilitating faster swapping and enabling a higher density of experts within a given memory footprint. ComPEFT employs sparsification and ternary quantization to reduce PEFT module size without any additional training while preserving or enhancing model performance. Extensive evaluation across T5, T0, and LLaMA-based models with 200M − 65B parameters, ComPEFT achieves compression ratios of 8x − 50x. Specifically, we show that ComPEFT improves with scale – stronger models exhibit higher compressibility and better performance. We show ComPEFT applied to LLaMA − 65B outperforms QLoRA by 4.16% on MMLU with a 26x storage size reduction. Additionally, compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare ComPEFT with other PEFT methods, and test its efficacy for compressing full finetuning residual",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Leshem Choshen",
      "Colin Raffel",
      "Mohit Bansal"
    ]
  },
  "https://openreview.net/forum?id=322PpCGAX8": {
    "title": "MaxCutBench: Revisiting and Benchmarking Graph Neural Networks for Maximum Cut",
    "volume": "main",
    "abstract": "Recently, there has been much work on designing general heuristics for graph-based, combinatorial optimization problems via the incorporation of Graph Neural Networks (GNNs) to learn distribution-specific solution structures. However, there is a lack of consistency in evaluating these heuristics in terms of the baselines and instances chosen, making it difficult to assess the relative performance of the algorithms. In this paper, we introduce \\textbf{MaxCutBench}—an open-source benchmark suite dedicated to the NP-hard Maximum Cut problem. The suite offers a unified interface for $16$ algorithms, both traditional and machine-learning-based. Using our benchmark, we conduct an in-depth analysis of the implemented algorithms on a carefully selected set of hard instances from diverse graph datasets. Our main finding is that classical local search heuristics can outperform several highly cited learning-based approaches, including S2V-DQN (Khalil et al., 2017), ECO-DQN (Barrett et al., 2020), among others, in terms of objective value, generalization, inference time, and scalability. Additionally, we find that the performance of ECO-DQN either remains the same or improves when the GNN is replaced by simple linear regression. We hope our benchmark will contribute to the efforts of the community to standardize the evaluation of learned heuristics for combinatorial optimization. Code, data, and pre-trained models are available at: \\url{https://github.com/ankurnath/MaxCut-Bench}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankur Nath",
      "Alan Kuhnle"
    ]
  },
  "https://openreview.net/forum?id=YBPbMKJbLd": {
    "title": "Future-aware Safe Active Learning of Time Varying Systems using Gaussian Processes",
    "volume": "main",
    "abstract": "Experimental exploration of high-cost systems with safety constraints, common in engineering applications, is a challenging endeavor. Data-driven models offer a promising solution, but acquiring the requisite data remains expensive and is potentially unsafe. Safe active learning techniques prove essential, enabling the learning of high-quality models with minimal expensive data points and high safety. This paper introduces a safe active learning framework tailored for time-varying systems, addressing drift, seasonal changes, and complexities due to dynamic behavior. The proposed Time-aware Integrated Mean Squared Prediction Error (T-IMSPE) method minimizes posterior variance over current and future states, optimizing information gathering also in the time domain. Empirical results highlight T-IMSPE's advantages in model quality through synthetic and real-world examples. State of the art Gaussian processes are compatible with T-IMSPE. Our theoretical contributions include a clear delineation which Gaussian process kernels, domains, and weighting measures are suitable for T-IMSPE and even beyond for its non-time aware predecessor IMSPE",
    "checked": false,
    "id": "fdc69c043851bacf57ed840d930609bdfdfe09f5",
    "semantic_title": "future aware safe active learning of time varying systems using gaussian processes",
    "citation_count": 0,
    "authors": [
      "Markus Lange-Hegermann",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=CgWkVb2lHB": {
    "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
    "volume": "main",
    "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, focusing on key aspects of visual recognition, ranging from basic color and shape to semantic understanding. We introduce a dataset, LENS, to guide VLMs to follow the examination and check its readiness. Once the model is ready, we conduct the examination. We quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have the potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Hyeon-Woo",
      "Moon Ye-Bin",
      "Wonseok Choi",
      "Lee Hyun",
      "Tae-Hyun Oh"
    ]
  },
  "https://openreview.net/forum?id=jdvnaki7ZY": {
    "title": "Jet: A Modern Transformer-Based Normalizing Flow",
    "volume": "main",
    "abstract": "In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation, and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve a much simpler architecture that matches existing normalizing flow models and improves over them when paired with pretraining. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing the research frontier by serving as building components of more powerful generative models",
    "checked": true,
    "id": "0dc3ce7fa1ad45e39367da7118ce0947e69c5a0d",
    "semantic_title": "jet: a modern transformer-based normalizing flow",
    "citation_count": 3,
    "authors": [
      "Alexander Kolesnikov",
      "André Susano Pinto",
      "Michael Tschannen"
    ]
  },
  "https://openreview.net/forum?id=j6Rm6T2lFU": {
    "title": "Deep Koopman Learning using Noisy Data",
    "volume": "main",
    "abstract": "This paper proposes a data-driven framework to learn a finite-dimensional approximation of a Koopman operator for approximating the state evolution of a dynamical system under noisy observations. To this end, our proposed solution has two main advantages. First, the proposed method only requires the measurement noise to be bounded. Second, the proposed method modifies the existing deep Koopman operator formulations by characterizing the effect of the measurement noise on the Koopman operator learning and then mitigating it by updating the tunable parameter of the observable functions of the Koopman operator, making it easy to implement. The performance of the proposed method is demonstrated on several standard benchmarks. We then compare the presented method with similar methods proposed in the latest literature on Koopman learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjian Hao",
      "Devesh Upadhyay",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=DqPCWMiMU0": {
    "title": "CoDe: Blockwise Control for Denoising Diffusion Models",
    "volume": "main",
    "abstract": "Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines}. Our code is available at https://github.com/anujinho/code",
    "checked": true,
    "id": "72a2e7333f432ba2dde838d33f249d8a251a002c",
    "semantic_title": "code: blockwise control for denoising diffusion models",
    "citation_count": 5,
    "authors": [
      "Anuj Singh",
      "Sayak Mukherjee",
      "Ahmad Beirami",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=7rqV7Cb67L": {
    "title": "Fairness-Aware Dense Subgraph Discovery",
    "volume": "main",
    "abstract": "Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions",
    "checked": true,
    "id": "9b431a2c06e2d8414def6bbb3819e3a9506f7194",
    "semantic_title": "fairness-aware dense subgraph discovery",
    "citation_count": 0,
    "authors": [
      "Emmanouil Kariotakis",
      "Nicholas D Sidiropoulos",
      "Aritra Konar"
    ]
  },
  "https://openreview.net/forum?id=jXcx2oAIbw": {
    "title": "LLM-Guided Self-Supervised Tabular Learning With Task-Specific Pre-text Tasks",
    "volume": "main",
    "abstract": "One of the most common approaches for self-supervised representation learning is defining pre-text tasks to learn data representations. Existing works determine pre-text tasks in a \"task-agnostic'' way, without considering the forthcoming downstream tasks. This offers an advantage of broad applicability across tasks, but can also lead to a mismatch between task objectives, potentially degrading performance on downstream tasks. In this paper, we introduce TST-LLM, a framework that effectively reduces this mismatch when the natural language-based description of the downstream task is given without any ground-truth labels. TST-LLM instructs the LLM to use the downstream task's description and meta-information of data to discover features relevant to the target task. These discovered features are then treated as ground-truth labels to define \"target-specific'' pre-text tasks. TST-LLM consistently outperforms contemporary baselines, such as STUNT and LFR, with win ratios of 95% and 81%, when applied to 22 benchmark tabular datasets, including binary and multi-class classification, and regression tasks",
    "checked": false,
    "id": "b39b47ea594fe2365a055b6f0ca0a5915e88c6de",
    "semantic_title": "self-influence guided data reweighting for language model pre-training",
    "citation_count": 25,
    "authors": [
      "Sungwon Han",
      "Seungeon Lee",
      "Meeyoung Cha",
      "Sercan O Arik",
      "Jinsung Yoon"
    ]
  },
  "https://openreview.net/forum?id=9aiuB3kIjd": {
    "title": "FragFormer: A Fragment-based Representation Learning Framework for Molecular Property Prediction",
    "volume": "main",
    "abstract": "Molecular representation learning is central to molecular property prediction, which is a vital component in drug discovery. Existing methods, which mainly focus on the atom-level molecular graphs, often find it challenging to directly model the relation between fragment (substructure) and function of molecules, largely due to insufficient fragment priors. In this work, we propose a molecular self-supervised learning framework \\textbf{FragFormer}, which aims to learn the representation of fragments and their contextual relationships. Given the prior that an atom can be part of multiple functional groups, we develop $k$-\\textbf{D}egree \\textbf{Ove}rlapping fragmentation (\\textbf{DOVE}), which generates overlapping fragment graph by employing the iterative line graph. Besides, DOVE can preserve the connection information during the fragmentation phase compared to non-overlapping fragmentation. In the pre-training stage, we design a \\textit{nested masked fragment prediction} objective, to capture the hierarchical nature of fragments, namely that larger fragments can encompass multiple smaller ones. Based on FragFormer, we introduce a simple yet efficient \\textit{fragment-level} interpretation method \\textbf{FragCAM} for the molecular property prediction results with greater accuracy. Moreover, thanks to the fragment modeling, our model is more capable of processing large molecule, such as peptides, and capturing the long-range interactions inside molecules. Our approach achieves state-of-the-art (SOTA) performance on eight out of eleven molecular property prediction datasets on PharmaBench. On long-range biological benchmark with peptide data, FragFormer can beat strong baselines by a clear margin, which shows the model's potential to generalize to larger molecules. Finally, we demonstrate that our model can effectively identify decisive fragments for prediction results on a real-world dataset\\footnote{Our code is available at \\url{https://github.com/wjxts/FragFormer/}}",
    "checked": true,
    "id": "67f3d24314df085802b78a62526afd6afe7332c9",
    "semantic_title": "fragformer: a fragment-based representation learning framework for molecular property prediction",
    "citation_count": 0,
    "authors": [
      "Jiaxi Wang",
      "Yaosen Min",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://openreview.net/forum?id=spqbyeGyLR": {
    "title": "When resampling/reweighting improves feature learning in imbalanced classification? A toy-model study",
    "volume": "main",
    "abstract": "A toy model of binary classification is studied with the aim of clarifying the class-wise resampling/reweighting effect on the feature learning performance under the presence of class imbalance. In the analysis, a high-dimensional limit of the input space is taken while keeping the ratio of the dataset size against the input dimension finite and the non-rigorous replica method from statistical mechanics is employed. The result shows that there exists a case in which the no resampling/reweighting situation gives the best feature learning performance irrespectively of the choice of losses or classifiers, supporting recent findings in~\\citet{kang2019decoupling,cao2019learning}. It is also revealed that the key of the result is the symmetry of the loss and the problem setting. Inspired by this, we propose a further simplified model exhibiting the same property in the multiclass setting. These clarify when the class-wise resampling/reweighting becomes effective in imbalanced classification",
    "checked": false,
    "id": "f7b392ec3a91d64f448f25a61f5d143008d5d79f",
    "semantic_title": "when resampling/reweighting improves feature learning in imbalanced classification?: a toy-model study",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Obuchi",
      "Toshiyuki Tanaka"
    ]
  },
  "https://openreview.net/forum?id=Gdf4P7sEzE": {
    "title": "HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network",
    "volume": "main",
    "abstract": "In data science, hypergraphs are natural models for data exhibiting multi-way or group relationships in contrast to graphs which only model pairwise relationships. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important multi-way or group information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix — the magnetic Laplacian — which serves as the input to our proposed hypergraph neural network. We study $\\textit{HyperMagNet}$ for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatyana Benko",
      "Martin Buck",
      "Ilya Amburg",
      "Stephen J. Young",
      "Sinan Guven Aksoy"
    ]
  },
  "https://openreview.net/forum?id=TWOTKhwU5n": {
    "title": "ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting",
    "volume": "main",
    "abstract": "Addressing the challenges of irregularity and concept drift in streaming time series is crucial for real-world predictive modelling. Previous studies in time series continual learning often propose models that require buffering long sequences, potentially restricting the responsiveness of the inference system. Moreover, these models are typically designed for regularly sampled data, an unrealistic assumption in real-world scenarios. This paper introduces ODEStream, a novel buffer-free continual learning framework that incorporates a temporal isolation layer to capture temporal dependencies within the data. Simultaneously, it leverages the capability of neural ordinary differential equations to process irregular sequences and generate a continuous data representation, enabling seamless adaptation to changing dynamics in a data streaming scenario. Our approach focuses on learning how the dynamics and distribution of historical data change over time, facilitating direct processing of streaming sequences. Evaluations on benchmark real-world datasets demonstrate that ODEStream outperforms the state-of-the-art online learning and streaming analysis baseline models, providing accurate predictions over extended periods while minimising performance degradation over time by learning how the sequence dynamics change. The implementation of ODEStream is available at: \\url{https://github.com/FtoonAbushaqra/ODEStream.git}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Futoon M. Abushaqra",
      "Hao Xue",
      "Yongli Ren",
      "Flora D. Salim"
    ]
  },
  "https://openreview.net/forum?id=n4AaKOBWbB": {
    "title": "Amphibian: A Meta-Learning Framework for Rehearsal-Free, Fast Online Continual Learning",
    "volume": "main",
    "abstract": "Online continual learning is challenging as it requires fast adaptation over a stream of data in a non-stationary environment without forgetting the knowledge acquired in the past. To address this challenge, in this paper, we introduce Amphibian - a gradient-based meta-learner that learns to scale the direction of gradient descent to achieve the desired balance between fast learning and continual learning. For this purpose, using only the current batch of data, Amphibian minimizes a meta-objective that encourages alignments of gradients among given data samples along selected basis directions in the gradient space. From this objective, it learns a diagonal scale matrix in each layer that accumulates the history of such gradient alignments. Using these scale matrices Amphibian updates the model online only in the directions having positive cumulative gradient alignments among the data observed so far. With evaluation on standard continual image classification benchmarks, we show that such meta-learned scaled gradient descent in Amphibian achieves better accuracy in online continual learning than relevant baselines while enabling fast learning with less data and few-shot knowledge transfer to new tasks. We also introduce Amphibian-$\\beta$ a unified and principled framework for analyzing and understanding the fast learning and continual learning dynamics. Additionally, with loss landscape visualizations, we show such gradient updates incur minimum loss to the old task enabling fast continual learning in Amphibian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=qvJraN50DT": {
    "title": "Sample-efficient decoding of visual stimuli from fMRI through inter-individual functional alignment",
    "volume": "main",
    "abstract": "Deep learning is leading to major advances in the realm of brain decoding from functional Magnetic Resonance Imaging (fMRI). However, the large inter-individual variability in brain characteristics has constrained most studies to train models on one participant at a time. This limitation hampers the training of deep learning models, which typically requires very large datasets. Here, we propose to boost brain decoding of videos and static images across participants by aligning brain responses of training and left-out participants. Evaluated on a retrieval task, compared to the anatomically-aligned baseline, our method halves the median rank in out-of-subject setups. It also outperforms classical within-subject approaches when fewer than 100 minutes of data is available for the tested participant. Furthermore, we show that our alignment framework handles multiple subjects, which improves accuracy upon classical single-subject approaches. Finally, we show that this method aligns neural representations in accordance with brain anatomy. Overall, this study lays the foundations for leveraging extensive neuroimaging datasets and enhancing the decoding of individual brains when a limited amount of brain-imaging data is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Thual",
      "Yohann Benchetrit",
      "Felix Geilert",
      "Jérémy Rapin",
      "Iurii Makarov",
      "Stanislas Dehaene",
      "Bertrand Thirion",
      "Hubert Banville",
      "Jean-Remi King"
    ]
  },
  "https://openreview.net/forum?id=16f7ea1N3p": {
    "title": "LLM-Select: Feature Selection with Large Language Models",
    "volume": "main",
    "abstract": "In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., ``blood pressure'') in predicting an outcome of interest (e.g., ``heart failure''), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training \\textit{but also for deciding which features to collect in the first place}. This could potentially benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel P Jeong",
      "Zachary Chase Lipton",
      "Pradeep Kumar Ravikumar"
    ]
  },
  "https://openreview.net/forum?id=MTrhFmkC45": {
    "title": "Reproducibility Study of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
    "volume": "main",
    "abstract": "This paper presents a reproducibility study and extension of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We validate the original findings using a range of open-weight models (1.5B-70B parameters), GPT-4, and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems",
    "checked": false,
    "id": "9cf6d9d629937ba5c1fc88fb1e0035b68870601a",
    "semantic_title": "reproducibility study of cooperation, competition, and maliciousness: llm-stakeholders interactive negotiation",
    "citation_count": 0,
    "authors": [
      "Jose L. Garcia",
      "Karolina Hajkova",
      "Maria Marchenko",
      "Carlos Miguel Patiño"
    ]
  },
  "https://openreview.net/forum?id=DVeFqV56Iz": {
    "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model",
    "volume": "main",
    "abstract": "This manuscript studies the unsupervised change point detection problem in time series of graphs using a decoder-only latent space model. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that bridges the observed graphs and latent representations. The prior distributions of the latent spaces are learned from the observed data as empirical Bayes to assist change point detection. Specifically, the model parameters are estimated via maximum approximate likelihood, with a Group Fused Lasso regularization imposed on the prior parameters. The augmented Lagrangian is solved via Alternating Direction Method of Multipliers, and Langevin Dynamics are recruited for posterior inference. Simulation studies show good performance of the latent space model in supporting change point detection and real data experiments yield change points that align with significant events",
    "checked": true,
    "id": "fc1b8630b6cfc1a81c3c9669015a79c9a6f331bf",
    "semantic_title": "change point detection in dynamic graphs with decoder-only latent space model",
    "citation_count": 0,
    "authors": [
      "Yik Lun Kei",
      "Jialiang Li",
      "Hangjian Li",
      "Yanzhen Chen",
      "OSCAR HERNAN MADRID PADILLA"
    ]
  },
  "https://openreview.net/forum?id=OPFnpl7KiF": {
    "title": "Design Editing for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization} (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparamters, DEMO's score is competitive with the best previously reported scores in the literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yuan",
      "Youyuan Zhang",
      "Can Chen",
      "Haolun Wu",
      "Melody Zixuan Li",
      "Jianmo Li",
      "James J. Clark",
      "Xue Liu"
    ]
  },
  "https://openreview.net/forum?id=8L3khbpUJL": {
    "title": "Referential communication in heterogeneous communities of pre-trained visual deep networks",
    "volume": "main",
    "abstract": "As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of referential communication in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evidence that it is capturing high-level semantic features of objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matéo Mahaut",
      "Roberto Dessi",
      "Francesca Franzon",
      "Marco Baroni"
    ]
  },
  "https://openreview.net/forum?id=dghM7sOudh": {
    "title": "MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory",
    "volume": "main",
    "abstract": "While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation. The project repository is publicly available at: https://github.com/amodaresi/MemLLM",
    "checked": false,
    "id": "47c8f0d7232f52f1a48e933e32309dc35ad85f49",
    "semantic_title": "memllm: finetuning llms to use an explicit read-write memory",
    "citation_count": 10,
    "authors": [
      "Ali Modarressi",
      "Abdullatif Köksal",
      "Ayyoob Imani",
      "Mohsen Fayyaz",
      "Hinrich Schuetze"
    ]
  },
  "https://openreview.net/forum?id=UdcF3JbSKb": {
    "title": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For Uncertainty",
    "volume": "main",
    "abstract": "Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in NCGPs is prohibitively expensive for large datasets, thus requiring approximations in practice. The approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. We introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for NCGPs. As we demonstrate on large-scale classification problems, our method significantly accelerates posterior inference compared to competitive baselines by trading off reduced computation for increased uncertainty",
    "checked": true,
    "id": "cbd66e7f8f2db171eb108328d1a7bd5667a8ff12",
    "semantic_title": "accelerating non-conjugate gaussian processes by trading off computation for uncertainty",
    "citation_count": 3,
    "authors": [
      "Lukas Tatzel",
      "Jonathan Wenger",
      "Frank Schneider",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=B4SyciDyIh": {
    "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction",
    "volume": "main",
    "abstract": "Knowledge graph embedding (KGE) models encode the structural information of knowledge graphs to predicting new links. Effective training of these models requires distinguishing between positive and negative samples with high precision. Although prior research has shown that improving the quality of negative samples can significantly enhance model accuracy, identifying high-quality negative samples remains a challenging problem. This paper theoretically investigates the condition under which negative samples lead to optimal KG embedding and identifies a sufficient condition for an effective negative sample distribution. Based on this theoretical foundation, we propose \\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that \\emph{generates} negative samples satisfying this condition, in contrast to conventional methods that focus on \\emph{identifying} challenging negative samples within the training data. Importantly, the simplicity of \\textsc{EMU} ensures seamless integration with existing KGE models and negative sampling methods. To evaluate its efficacy, we conducted comprehensive experiments across multiple datasets. The results consistently demonstrate significant improvements in link prediction performance across various KGE models and negative sampling methods. Notably, \\textsc{EMU} enables performance improvements comparable to those achieved by models with embedding dimension five times larger. An implementation of the method and experiments are available at \\url{https://github.com/nec-research/EMU-KG}",
    "checked": true,
    "id": "a18c375afa74642140215fc95ec9a3eeb0006668",
    "semantic_title": "optimal embedding guided negative sample generation for knowledge graph link prediction",
    "citation_count": 0,
    "authors": [
      "Makoto Takamoto",
      "Daniel Onoro Rubio",
      "Wiem Ben Rim",
      "Takashi Maruyama",
      "Bhushan Kotnis"
    ]
  },
  "https://openreview.net/forum?id=muWEt1TOyo": {
    "title": "SE3Set: Harnessing Equivariant Hypergraph Neural Networks for Molecular Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network architecture tailored for advanced molecular representation learning. Hypergraphs are not merely an extension of traditional graphs; they are pivotal for modeling high-order relationships, a capability that conventional equivariant graph-based methods lack due to their inherent limitations in representing intricate many-body interactions. To achieve this, we first construct hypergraphs by proposing a new fragmentation method that considers both chemical and three-dimensional spatial information of the molecular system. We then design SE3Set, which incorporates equivariance into the hypergraph neural network. This ensures that the learned molecular representations are invariant to spatial transformations, thereby providing robustness essential for the accurate prediction of molecular properties. SE3Set has shown performance on par with state-of-the-art (SOTA) models for small molecule datasets like QM9 and MD17. It demonstrates outstanding performance on the MD22 dataset, achieving a remarkable ~20\\% improvement in accuracy across all molecules. Furthermore, on the OE62 dataset, SE3Set outperforms all short-range models. We also conducted a detailed analysis of OE62, highlighting the prevalence of complex many-body interactions in large molecules. This exceptional performance of SE3Set across diverse molecular structures underscores its transformative potential in computational chemistry, offering a route to more accurate and physically nuanced modeling. The code of this work is available at https://github.com/Navantock/SE3Set",
    "checked": true,
    "id": "0befffb1194cc1d76e8fd93ebc32e412167282b1",
    "semantic_title": "se3set: harnessing equivariant hypergraph neural networks for molecular representation learning",
    "citation_count": 2,
    "authors": [
      "Hongfei Wu",
      "Lijun Wu",
      "Guoqing Liu",
      "Zhirong Liu",
      "Bin Shao",
      "Zun Wang"
    ]
  },
  "https://openreview.net/forum?id=l4Qnj4tHBx": {
    "title": "Oblique Bayesian Additive Regression Trees",
    "volume": "main",
    "abstract": "Current implementations of Bayesian Additive Regression Trees (BART) are based on axis-aligned decision rules that recursively partition the feature space using a single feature at a time. Several authors have demonstrated that oblique trees, whose decision rules are based on linear combinations of features, can sometimes yield better predictions than axis-aligned trees and exhibit excellent theoretical properties. We develop an oblique version of BART that leverages a data-adaptive decision rule prior that recursively partitions the feature space along random hyperplanes. Using several synthetic and real-world benchmark datasets, we systematically compared our oblique BART implementation to axis-aligned BART and other tree ensemble methods, finding that oblique BART was competitive with --- and sometimes much better than --- those methods",
    "checked": true,
    "id": "5f0bcec2e82bce511c432018dd160079608e599a",
    "semantic_title": "oblique bayesian additive regression trees",
    "citation_count": 0,
    "authors": [
      "Paul-Hieu V. Nguyen",
      "Ryan Yee",
      "Sameer Deshpande"
    ]
  },
  "https://openreview.net/forum?id=FIWHRSuoos": {
    "title": "Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift",
    "volume": "main",
    "abstract": "Estimating the test performance of a model, possibly under distribution shift, without having access to the ground-truth labels is a challenging, yet very important problem for the safe deployment of machine learning algorithms in the wild. Existing works mostly rely on information from either the outputs or the extracted features of neural networks to estimate a score that correlates with the ground-truth test accuracy. In this paper, we investigate -- both empirically and theoretically -- how the information provided by the gradients can be predictive of the ground-truth test accuracy even under distribution shifts. More specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our intuition is that these gradients should be of higher magnitude when the model generalizes poorly. We provide the theoretical insights behind our approach and the key ingredients that ensure its empirical success. Extensive experiments conducted with various architectures on diverse distribution shifts demonstrate that our method significantly outperforms current state-of-the-art approaches. The code is available at \\url{https://github.com/Renchunzi-Xie/GdScore}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RENCHUNZI XIE",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Ievgen Redko",
      "Jianfeng Zhang",
      "Bo An"
    ]
  },
  "https://openreview.net/forum?id=PJUbMDkQVY": {
    "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
    "volume": "main",
    "abstract": "Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach FLP consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical \"emergent phase\". In our experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. Further, we present FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpus with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins",
    "checked": true,
    "id": "fa2a637f6532562a9eff1f5e9fef4438aae3f28b",
    "semantic_title": "scaling laws for predicting downstream performance in llms",
    "citation_count": 9,
    "authors": [
      "Yangyi Chen",
      "Binxuan Huang",
      "Yifan Gao",
      "Zhengyang Wang",
      "Jingfeng Yang",
      "Heng Ji"
    ]
  },
  "https://openreview.net/forum?id=c7vkDg558Z": {
    "title": "EDM-TTS: Efficient Dual-Stage Masked Modeling for Alignment-Free Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Tokenized speech modeling has significantly advanced zero-shot text-to-speech (TTS) capabilities. The most de facto approach involves a dual-stage process: text-to-semantic (T2S) followed by semantic-to-acoustic (S2A) generation. Several auto-regressive (AR) and non-autoregressive (NAR) methods have been explored in literature for both the stages. While AR models achieve state-of-the-art performance, its token-by-token generation causes inference inefficiencies, while NAR methods while being more efficient, require explicit alignment for upsampling intermediate representations, which constrains the model's capability for more natural prosody. To overcome these issues, we propose an **E**fficient **D**ual-stage **M**asked **TTS** (EDM-TTS) model that employs an alignment-free masked generative approach for the T2S stage that overcomes the constrains of an explicit aligner, while retaining the efficiency of NAR methods. For the S2A stage, we introduce an innovative NAR approach using a novel Injection Conformer architecture, that effectively models the conditional dependence among different acoustic quantization levels, optimized by a masked language modeling objective, enabling zero-shot speech generation. Our evaluations demonstrated not only the superior inference efficiency of EDM-TTS, but also its state-of-the-art high-quality zero-shot speech quality, naturalness and speaker similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabarun Goswami",
      "Hanqin Wang",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=B6y12Ot0cP": {
    "title": "Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure",
    "volume": "main",
    "abstract": "Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets",
    "checked": true,
    "id": "f94c8d22f66a1306595c5aa5dfe94da89dbf65b3",
    "semantic_title": "formal verification of graph convolutional networks with uncertain node features and uncertain graph structure",
    "citation_count": 0,
    "authors": [
      "Tobias Ladner",
      "Michael Eichelbeck",
      "Matthias Althoff"
    ]
  },
  "https://openreview.net/forum?id=J5IRyTKZ9s": {
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "volume": "main",
    "abstract": "Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Łucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian Tramèr",
      "Javier Rando"
    ]
  },
  "https://openreview.net/forum?id=GaUtrgXMHe": {
    "title": "Bayesian Transferability Assessment for Spiking Neural Networks",
    "volume": "main",
    "abstract": "Brain-inspired spiking neural networks (SNNs) attract broad interest in neuromorphic computing but suffer the problem of being difficult to optimize. Concurrently, pre-trained models (PTMs) have become a foundation for developing and applying artificial intelligence. Therefore, it is expected that pre-trained SNNs can alleviate the optimization difficulty of training from scratch. However, with a lot of PTMs available in the model hubs, effectively selecting the most appropriate PTM for a given task remains a significant challenge, often necessitating exhaustive fine-tuning and grid-searching. While several solutions to this challenge have been proposed for the mainstream artificial neural network (ANNs), aimed at developing efficient methods to assess the transferability of PTMs on target tasks, the realm of SNNs remains unexplored. The currently most used transferability assessment method for ANNs predicts transferability in a Bayesian perspective. Feature maps extracted by the PTM backbone on the target task are used to calculate the maximum model evidence as the indicator of transferability. However, ANNs and SNNs differ in architecture, rendering the existing Bayesian method incompatible with SNNs. To solve this problem, this paper introduces a novel approach to using the feature maps averaged over the time domain to calculate maximum evidence. Our proposed $\\textbf{M}$aximum $\\textbf{E}$vidence method with $\\textbf{A}$veraged $\\textbf{F}$eatures (MEAF) demonstrates effectiveness for SNNs. Additionally, the current algorithm calculates maximum evidence in an iterative way. To accelerate the selection of PTMs, an approximation method is proposed to avoid iteration in the calculation of maximum evidence, significantly reducing time consumption. It is shown through experiment that the proposed MEAF method is effective for the transferability assessment of SNNs. MEAF outperforms information theory-based assessment methods such as LEEP and NCE, which can directly adapt to SNNs on neuromorphic datasets, underscoring its potential to streamline PTM selection and application in the realm of SNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqing Hao",
      "Wenhui Wang"
    ]
  },
  "https://openreview.net/forum?id=vttqWoSJIW": {
    "title": "Relative Phase Equivariant Deep Neural Systems for Physical Layer Communications",
    "volume": "main",
    "abstract": "In the era of telecommunications, the increasing demand for complex and specialized communication systems has led to a focus on improving physical layer communications. Artificial intelligence (AI) has emerged as a promising solution avenue for doing so. Deep neural receivers have already shown significant promise in improving the performance of communications systems. However, a major challenge lies in developing deep neural receivers that match the energy efficiency and speed of traditional receivers. This work investigates the incorporation of inductive biases in the physical layer using group-equivariant deep learning to improve the parameter efficiency of deep neural receivers. We do so by constructing a deep neural receiver that is equivariant with respect to the phase of arrival. We show that the inclusion of relative phase equivariance significantly reduces the error rate of deep neural receivers at similar model sizes. Thus, we show the potential of group-equivariant deep learning in the domain of physical layer communications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Mark Hoogendoorn",
      "Rob van der Mei"
    ]
  },
  "https://openreview.net/forum?id=D2PjEPGXgh": {
    "title": "Multi-Bellman operator for convergence of $Q$-learning with linear function approximation",
    "volume": "main",
    "abstract": "We investigate the convergence of $Q$-learning with linear function approximation and introduce the multi-Bellman operator, an extension of the traditional Bellman operator. By analyzing the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes a contraction, yielding stronger fixed-point guarantees compared to the original Bellman operator. Building on these insights, we propose the multi-$Q$-learning algorithm, which achieves convergence and approximates the optimal solution with arbitrary precision. This contrasts with traditional $Q$-learning, which lacks such convergence guarantees. Finally, we empirically validate our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo S. Carvalho",
      "Pedro A. Santos",
      "Francisco S. Melo"
    ]
  },
  "https://openreview.net/forum?id=laPAh2hRFC": {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
    "volume": "main",
    "abstract": "Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, an algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SmoothLLM offers improved robustness against the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Robey",
      "Eric Wong",
      "Hamed Hassani",
      "George J. Pappas"
    ]
  },
  "https://openreview.net/forum?id=haP586YomL": {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "volume": "main",
    "abstract": "Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ]
  },
  "https://openreview.net/forum?id=sNzBi8rZTy": {
    "title": "Reinforcement Learning for Causal Discovery without Acyclicity Constraints",
    "volume": "main",
    "abstract": "Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets. Our implementation is provided at https://github.com/baosws/ALIAS",
    "checked": true,
    "id": "70073cba96d7fc4073951eb8b2c0e1b1dac22bd0",
    "semantic_title": "reinforcement learning for causal discovery without acyclicity constraints",
    "citation_count": 1,
    "authors": [
      "Bao Duong",
      "Hung Le",
      "Biwei Huang",
      "Thin Nguyen"
    ]
  },
  "https://openreview.net/forum?id=TR6iUG8i6Z": {
    "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
    "volume": "main",
    "abstract": "Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishan Gurumurthy",
      "Himanshu Pal",
      "Charu Sharma"
    ]
  },
  "https://openreview.net/forum?id=z37LCgSIzI": {
    "title": "ResiDual Transformer Alignment with Spectral Decomposition",
    "volume": "main",
    "abstract": "When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performance on different data distributions while modelling an extremely interpretable and parameter-efficient transformation, as we extensively show on 70 pre-trained network-dataset combinations (7 models, 10 datasets)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Basile",
      "Valentino Maiorca",
      "Luca Bortolussi",
      "Emanuele Rodolà",
      "Francesco Locatello"
    ]
  },
  "https://openreview.net/forum?id=uKZ0R4IQaO": {
    "title": "Dynamic Pricing in the Linear Valuation Model using Shape Constraints",
    "volume": "main",
    "abstract": "We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model eliminating the need for tuning parameters commonly required by existing methods. Previous works have addressed the challenge of unknown market noise distribution $F_0$ using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the assumption that $F_0$ satisfies Lipschitz (or stronger) conditions. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\\alpha$-H\\\"older continuous for some $\\alpha \\in (0,1]$, for which we derive a regret upper bound. Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains lower empirical regret in comparison to several existing methods in the literature while offering the advantage of being tuning-parameter free",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Bracale",
      "Moulinath Banerjee",
      "Yuekai Sun",
      "Salam Turki",
      "Kevin Stoll"
    ]
  },
  "https://openreview.net/forum?id=9Xj5w4DX0t": {
    "title": "Rank Suggestion in Non-negative Matrix Factorization: Residual Sensitivity to Initial Conditions (RSIC)",
    "volume": "main",
    "abstract": "Determining the appropriate rank in Non-negative Matrix Factorization (NMF) is a critical challenge that often requires extensive parameter tuning and domain-specific knowledge. Traditional methods for rank determination focus on identifying a single optimal rank, which may not capture the complex structure inherent in real-world datasets. In this study, we introduce a novel approach called Residual Sensitivity to Intial Conditions (RSIC) that suggests potentially multiple ranks of interest by analyzing the sensitivity of the relative residuals (e.g., relative reconstruction error) to different initializations. By computing the Mean Coordinatewise Interquartile Range (MCI) of the residuals across multiple random initializations, our method identifies regions where the NMF solutions are less sensitive to initial conditions and potentially more meaningful. We evaluate RSIC on a diverse set of datasets, including single-cell gene expression data, image data, and text data, and compare it against current state-of-the-art rank determination methods. Our experiments demonstrate that RSIC effectively identifies relevant ranks consistent with the underlying structure of the data, outperforming traditional methods in scenarios where they are computationally infeasible or less accurate. This approach provides a more scalable and generalizable solution for rank determination in NMF that does not rely on domain-specific knowledge or assumptions",
    "checked": true,
    "id": "ef655348d40b0fc09f1b126af8d822476bb07f3c",
    "semantic_title": "rank suggestion in non-negative matrix factorization: residual sensitivity to initial conditions (rsic)",
    "citation_count": 0,
    "authors": [
      "Marc A. Tunnell",
      "Zachary DeBruine",
      "Erin Carrier"
    ]
  },
  "https://openreview.net/forum?id=cFmmaxkD5A": {
    "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
    "volume": "main",
    "abstract": "Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at https://github.com/Alexiland/MLO-MAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Ramtin Hosseini",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Ranak Roy Chowdhury",
      "Rajesh K. Gupta",
      "Pengtao Xie"
    ]
  },
  "https://openreview.net/forum?id=ntGPYNUF3t": {
    "title": "Latte: Latent Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, \\textit{i.e.}, FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Gengyun Jia",
      "Ziwei Liu",
      "Yuan-Fang Li",
      "Cunjian Chen",
      "Yu Qiao"
    ]
  },
  "https://openreview.net/forum?id=LJHVPWNnV6": {
    "title": "Graph Potential Field Neural Network for Massive Agents Group-wise Path Planning",
    "volume": "main",
    "abstract": "Multi-agent path planning is important in both multi-agent path finding and multi-agent reinforcement learning areas. However, continual group-wise multi-agent path planning that requires the agents to perform as a team to pursue high team scores instead of individually is less studied. To address this problem, we propose a novel graph potential field-based neural network (GPFNN), which models a valid potential field map for path planning. Our GPFNN unfolds the T-step iterative optimization of the potential field maps as a T-layer feedforward neural network. Thus, a deeper GPFNN leads to more precise potential field maps without the over-smoothing issue. A potential field map inherently provides a monotonic potential flow from any source node to the target nodes to construct the optimal path (w.r.t. the potential decay), equipping our GPFNN with an elegant planning ability. Moreover, we incorporate dynamically updated boundary conditions into our GPFNN to address group-wise multi-agent path planning that supports both static targets and dynamic moving targets. Empirically, experiments on three different-sized mazes (up to $1025 \\times 1025$ sized mazes) with up to 1,000 agents demonstrate the planning ability of our GPFNN to handle both static and dynamic moving targets. Experiments on extensive graph node classification tasks on six graph datasets (up to millions of nodes) demonstrate the learning ability of our GPFNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming Lyu",
      "Xiaowei Zhou",
      "Xingrui Yu",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=JT2KMuo2BV": {
    "title": "Rethinking Patch Dependence for Masked Autoencoders",
    "volume": "main",
    "abstract": "In this work, we examine the impact of inter-patch dependencies in the decoder of masked autoencoders (MAE) on representation learning. We decompose the decoding mechanism for masked reconstruction into self-attention between mask tokens and cross-attention between masked and visible tokens. Our findings reveal that MAE reconstructs coherent images from visible patches not through interactions between patches in the decoder but by learning a global representation within the encoder. This discovery leads us to propose a simple visual pretraining framework: cross-attention masked autoencoders (CrossMAE). This framework employs only cross-attention in the decoder to independently read out reconstructions for a small subset of masked patches from encoder outputs. This approach achieves comparable or superior performance to traditional MAE across models ranging from ViT-S to ViT-H and significantly reduces computational requirements. By its design, CrossMAE challenges the necessity of interaction between mask tokens for effective masked pretraining. Code and models are publicly available: https://crossmae.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Fu",
      "Long Lian",
      "Renhao Wang",
      "Baifeng Shi",
      "XuDong Wang",
      "Adam Yala",
      "Trevor Darrell",
      "Alexei A Efros",
      "Ken Goldberg"
    ]
  },
  "https://openreview.net/forum?id=FkKBxp0FhR": {
    "title": "A Systematic Evaluation of the Planning and Scheduling Abilities of the Reasoning Model o1",
    "volume": "main",
    "abstract": "OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive Large Language Models (LLMs)–making it a new kind of model: a Large Reasoning Model (LRM)–and be generally capable of tackling procedural reasoning tasks. We present the first comprehensive evaluation of these models on the fundamental tasks of planning and scheduling. Previous research attempted to use LLMs' expressive generation capabilities to solve these problems, but met with only limited success. We fill in the gaps in this literature by testing a larger suite of state-of-the-art LLMs on a set of large benchmarks, and then use this as a baseline to evaluate o1-preview and o1-mini. We see that while they can offer significant accuracy improvements over LLMs, this single metric is misleading and incomplete, as LRM queries demand large and unpredictable costs and take significant amounts of time to complete. We provide a case study demonstrating that, at those same price points, other methods of inference time scaling can do just as well. We also show that, contrary to OpenAI's injunctions, o1's performance can be improved further by embedding it in compound systems that separately, but complementarily, scale inference time further. Finally, while the paper is focused on o1, we provide similar evaluations of a more recent (and open-weight) LRM -- DeepSeek R1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Atharva Gundawar",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=7bIfe2I7bK": {
    "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models",
    "volume": "main",
    "abstract": "The visual world is fundamentally compositional. Visual scenes are defined by the composition of objects and their relations. Hence, it is essential for computer vision systems to reflect and exploit this compositionality to achieve robust and generalizable scene understanding. While major strides have been made toward the development of general-purpose, multimodal generative models, including both text-to-image models and multimodal vision-language models, it remains unclear whether these systems are capable of accurately generating and interpreting scenes involving the composition of multiple objects and relations. In this work, we present an evaluation of the compositional visual processing capabilities in the current generation of text-to-image (DALL-E 3) and multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5, QWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these systems to human participants. The results suggest that these systems display some ability to solve compositional and relational tasks, showing notable improvements over the previous generation of multimodal models, but with performance nevertheless well below the level of human participants, particularly for more complex scenes involving many (>5) objects and multiple relations. These results highlight the need for further progress toward compositional understanding of visual scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhao Fu",
      "Andrew Jun Lee",
      "Yixin Anna Wang",
      "Ida Momennejad",
      "Trevor Bihl",
      "Hongjing Lu",
      "Taylor Whittington Webb"
    ]
  },
  "https://openreview.net/forum?id=3jdI0aEW3k": {
    "title": "Distributed and Secure Kernel-Based Quantum Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "764cfd1d64a18a869b41942b7bc64cc518cc8799",
    "semantic_title": "distributed and secure kernel-based quantum machine learning",
    "citation_count": 0,
    "authors": [
      "Arjhun Swaminathan",
      "Mete Akgün"
    ]
  },
  "https://openreview.net/forum?id=X3gSvQjShh": {
    "title": "An Embedding is Worth a Thousand Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "614d89e5c8839291270eb8ff0ed2e9bbadf49957",
    "semantic_title": "an embedding is worth a thousand noisy labels",
    "citation_count": 0,
    "authors": [
      "Francesco Di Salvo",
      "Sebastian Doerrich",
      "Ines Rieger",
      "Christian Ledig"
    ]
  },
  "https://openreview.net/forum?id=gxUp2d4JTw": {
    "title": "LTL-Constrained Policy Optimization with Cycle Experience Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameesh Shah",
      "Cameron Voloshin",
      "Chenxi Yang",
      "Abhinav Verma",
      "Swarat Chaudhuri",
      "Sanjit A. Seshia"
    ]
  },
  "https://openreview.net/forum?id=I1gALvbRxj": {
    "title": "Bézier Flow: a Surface-wise Gradient Descent Method for Multi-objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Sannai",
      "Yasunari Hikima",
      "Ken Kobayashi",
      "Akinori Tanaka",
      "Naoki Hamada"
    ]
  },
  "https://openreview.net/forum?id=SBM9yeNZz5": {
    "title": "Maximising the Utility of Validation Sets for Imbalanced Noisy-label Meta-learning",
    "volume": "main",
    "abstract": "Meta-learning is an effective method to handle imbalanced and noisy-label learning, but it generally depends on a clean validation set. Unfortunately, this validation set has poor scalability when the number of classes increases, as traditionally these samples need to be randomly selected, manually labelled and balanced-distributed. This problem therefore has motivated the development of meta-learning methods to automatically select validation samples that are likely to have clean labels and balanced class distribution. Unfortunately, a common missing point of existing meta-learning methods for noisy label learning is the lack of consideration for data informativeness when constructing the validation set. The construction of an informative validation set requires hard samples, i.e., samples that the model has low confident prediction, but these samples are more likely to be noisy, which can degrade the meta reweighting process. Therefore, the balance between sample informativeness and cleanness is an important criteria for validation set optimization. In this paper, we propose new criteria to characterise the utility of such meta-learning validation sets, based on: 1) sample informativeness; 2) balanced class distribution; and 3) label cleanliness. We also introduce a new imbalanced noisy-label meta-learning (INOLML) algorithm that auto- matically builds a validation set by maximising such utility criteria. The proposed method shows state-of-the-art (SOTA) results compared to previous meta-learning and noisy-label learning approaches on several noisy-label learning benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Anh Dung",
      "Cuong C. Nguyen",
      "Vasileios Belagiannis",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=sSOxuUjE2o": {
    "title": "Controlled Training Data Generation with Diffusion Models",
    "volume": "main",
    "abstract": "We present a method to control a text-to-image generative model to produce training data useful for supervised learning. Unlike previous works that employ an open-loop approach via pre-defined prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system that involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model to find adversarial prompts that result in generated images that maximize the model's loss and, consequently, expose its vulnerabilities. While these adversarial prompts generate training examples curated for improving the given model, they are not curated for a specific target distribution of interest, which can be inefficient. Therefore, we introduce the second feedback mechanism that can optionally guide the generation process towards a desirable target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. The proposed closed-loop system allows us to control the training data generation for a given model and target image distribution. We evaluate on different tasks, datasets, and architectures, with different types of distribution shifts (corruptions, spurious correlations, unseen domains) and illustrate the advantages of the proposed feedback mechanisms compared to open-loop approaches",
    "checked": true,
    "id": "45f9ca4f97fadf37fb7276f26a1d35905077be49",
    "semantic_title": "controlled training data generation with diffusion models",
    "citation_count": 6,
    "authors": [
      "Teresa Yeo",
      "Andrei Atanov",
      "Harold Luc Benoit",
      "Aleksandr Alekseev",
      "Ruchira Ray",
      "Pooya Esmaeil Akhoondi",
      "Amir Zamir"
    ]
  },
  "https://openreview.net/forum?id=Okxp1W8If0": {
    "title": "(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum",
    "volume": "main",
    "abstract": "Stochastic heavy ball momentum (SHB) is commonly used to train machine learning models, and often provides empirical improvements over stochastic gradient descent. By primarily focusing on strongly-convex quadratics, we aim to better understand the theoretical advantage of SHB and subsequently improve the method. For strongly-convex quadratics, Kidambi et al. (2018) show that SHB (with a mini-batch of size $1$) cannot attain accelerated convergence, and hence has no theoretical benefit over SGD. They conjecture that the practical gain of SHB is a by-product of using larger mini-batches. We first substantiate this claim by showing that SHB can attain an accelerated rate when the mini-batch size is larger than a threshold $b^*$ that depends on the condition number $\\kappa$. Specifically, we prove that with the same step-size and momentum parameters as in the deterministic setting, SHB with a sufficiently large mini-batch size results in an $O\\left(\\exp(-\\frac{T}{\\sqrt{\\kappa}}) + \\sigma \\right)$ convergence when measuring the distance to the optimal solution in the $\\ell_2$ norm, where $T$ is the number of iterations and $\\sigma^2$ is the variance in the stochastic gradients. We prove a lower-bound which demonstrates that a $\\kappa$ dependence in $b^*$ is necessary. To ensure convergence to the minimizer, we design a noise-adaptive multi-stage algorithm that results in an $O\\left(\\exp\\left(-\\frac{T}{\\sqrt{\\kappa}}\\right) + \\frac{\\sigma}{\\sqrt{T}}\\right)$ rate when measuring the distance to the optimal solution in the $\\ell_2$ norm. We also consider the general smooth, strongly-convex setting and propose the first noise-adaptive SHB variant that converges to the minimizer at an $O(\\exp(-\\frac{T}{\\kappa}) + \\frac{\\sigma^2}{T})$ rate when measuring the distance to the optimal solution in the squared $\\ell_2$ norm. We empirically demonstrate the effectiveness of the proposed algorithms",
    "checked": true,
    "id": "f340532ef3da3400b4310531ec549b1767bba953",
    "semantic_title": "(accelerated) noise-adaptive stochastic heavy-ball momentum",
    "citation_count": 0,
    "authors": [
      "Anh Quang Dang",
      "Reza Babanezhad Harikandeh",
      "Sharan Vaswani"
    ]
  },
  "https://openreview.net/forum?id=nWk5OtZ7ze": {
    "title": "Quantile Activation: Correcting a failure mode of traditional ML models",
    "volume": "main",
    "abstract": "Standard ML models fail to infer the context distribution and suitably adapt. For instance, the learning fails when the underlying distribution is actually a mixture of distributions with contradictory labels. Learning also fails if there is a shift between train and test distributions. Standard neural network architectures like MLPs or CNNs are not equipped to handle this. In this article, we propose a simple activation function, quantile activation (QAct), that addresses this problem without significantly increasing computational costs. The core idea is to \"adapt\" the outputs of each neuron to its context distribution. The proposed quantile activation (QAct) outputs the relative quantile position of neuron activations within their context distribution, diverging from the direct numerical outputs common in traditional networks. A specific case of the above failure mode is when there is an inherent distribution shift, i.e the test distribution differs slightly from the train distribution. We validate the proposed activation function under covariate shifts, using datasets designed to test robustness against distortions. Our results demonstrate significantly better generalisation across distortions compared to conventional classifiers and other adaptive methods, across various architectures. Although this paper presents a proof of concept, we find that this approach unexpectedly outperforms DINOv2 (small), despite DINOv2 being trained with a much larger network and dataset",
    "checked": false,
    "id": "451d57bf84f776b631fddaf05127169838e71051",
    "semantic_title": "quantile activation: correcting a failure mode of ml models",
    "citation_count": 0,
    "authors": [
      "Aditya Challa",
      "Sravan Danda",
      "Laurent Najman",
      "Snehanshu Saha"
    ]
  },
  "https://openreview.net/forum?id=hCyT4RsF27": {
    "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
    "volume": "main",
    "abstract": "Graphs are growing rapidly and so are the number of different categories associated with it. Applications like e-commerce, healthcare, recommendation systems, and various social media platforms are rapidly moving towards graph representation of data due to their ability to capture both structural and attribute information. One crucial task in graph analysis is node classification, where unlabeled nodes are categorized into predefined classes. In practice, novel classes appear incrementally sometimes with just a few labels (seen classes) or even without any labels (unseen classes), either because they are new or haven't been explored much. Traditional methods assume abundant labeled data for training, which isn't always feasible. We investigate a broader objective: Graph Class Incremental Learning under Weak Supervision (GCL), addressing this challenge by meta-training on base classes with limited labeled instances. During the incremental streams, novel classes can have few-shot or zero-shot representation. Our proposed framework GOTHAM efficiently accommodates these unlabeled nodes by finding the closest prototype representation, serving as class representatives in the attribute space. For Text-Attributed Graphs (TAGs), our framework additionally incorporates semantic information to enhance the representation. By employing teacher-student knowledge distillation to mitigate forgetting, GOTHAM achieves promising results across various tasks. Experiments on datasets such as Cora-ML, Amazon, and OBGN-Arxiv showcase the effectiveness of our approach in handling evolving graph data under limited supervision",
    "checked": true,
    "id": "4f4b5bcb828c5e8171092c48a56780c7d2113e90",
    "semantic_title": "gotham: graph class incremental learning framework under weak supervision",
    "citation_count": 0,
    "authors": [
      "Aditya Hemant Shahane",
      "Prathosh AP",
      "Sandeep Kumar"
    ]
  },
  "https://openreview.net/forum?id=dNWaTuKV9M": {
    "title": "Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning",
    "volume": "main",
    "abstract": "The primary objective of methods in continual learning is to learn tasks in a sequential manner over time (sometimes from a stream of data), while mitigating the detrimental phenomenon of catastrophic forgetting. This paper proposes a method to learn an effective representation between previous and newly encountered class prototypes. We propose a prototypical network with a Bayesian learning-driven contrastive loss (BLCL), tailored specifically for class-incremental learning scenarios. We introduce a contrastive loss that incorporates novel classes into the latent representation by reducing intra-class and increasing inter-class distance. Our approach dynamically adapts the balance between the cross-entropy and contrastive loss functions with a Bayesian learning technique. Experimental results conducted on the CIFAR-10, CIFAR-100, and ImageNet100 datasets for image classification and images of a GNSS-based dataset for interference classification validate the efficacy of our method, showcasing its superiority over existing state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha L. Raichur",
      "Lucas Heublein",
      "Tobias Feigl",
      "Alexander Rügamer",
      "Christopher Mutschler",
      "Felix Ott"
    ]
  },
  "https://openreview.net/forum?id=s1zfBJysbI": {
    "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. This approach ensures the temporal consistency in the edited videos while maintaining high fidelity to the editing text prompt. We further propose a recursive and ensembled refinement by revisiting the denoising step and guidance scale used in video diffusion process with Video-3DGS. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (×1.9, ×4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos",
    "checked": true,
    "id": "4437854b36a71d150cda9812c2e9f337f60f457d",
    "semantic_title": "enhancing temporal consistency in video editing by reconstructing videos with 3d gaussian splatting",
    "citation_count": 1,
    "authors": [
      "Inkyu Shin",
      "Qihang Yu",
      "Xiaohui Shen",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ]
  },
  "https://openreview.net/forum?id=GXlsrvOGIK": {
    "title": "On Learning Representations for Tabular Data Distillation",
    "volume": "main",
    "abstract": "Dataset distillation generates a small set of information-rich instances from a large dataset, resulting in reduced storage requirements, privacy or copyright risks, and computational costs for downstream modeling, though much of the research has focused on the image data modality. We study tabular data distillation, which brings in novel challenges such as the inherent feature heterogeneity and the common use of non-differentiable learning models (such as decision tree ensembles and nearest-neighbor predictors). To mitigate these challenges, we present $\\texttt{TDColER}$, a tabular data distillation framework via column embeddings-based representation learning. To evaluate this framework, we also present a tabular data distillation benchmark, ${{\\sf \\small TDBench}}$. Based on an elaborate evaluation on ${{\\sf \\small TDBench}}$, resulting in 226,200 distilled datasets and 541,980 models trained on them, we demonstrate that $\\texttt{TDColER}$ is able to boost the distilled data quality of off-the-shelf distillation schemes by 0.5-143% across 7 different tabular learning models. All of the code used in the experiments can be found in http://github.com/inwonakng/tdbench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inwon Kang",
      "Parikshit Ram",
      "Yi Zhou",
      "Horst Samulowitz",
      "Oshani Seneviratne"
    ]
  },
  "https://openreview.net/forum?id=baZLwdphqw": {
    "title": "Stabilizing the Kumaraswamy Distribution",
    "volume": "main",
    "abstract": "Large-scale latent variable models require expressive continuous distributions that support efficient sampling and low-variance differentiation, achievable through the reparameterization trick. The Kumaraswamy (KS) distribution is both expressive and supports the reparameterization trick with a simple closed-form inverse CDF. Yet, its adoption remains limited. We identify and resolve numerical instabilities in the log-pdf, CDF, and inverse CDF, exposing issues in libraries like PyTorch and TensorFlow. We then introduce simple and scalable latent variable models to address exploration-exploitation trade-offs in contextual multi-armed bandits and facilitate uncertainty quantification for link prediction with graph neural networks. We find these models to be most performant when paired with the stable KS. Our results support the stabilized KS distribution as a core component in scalable variational models for bounded latent variables",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Gonzalo Mateos"
    ]
  },
  "https://openreview.net/forum?id=AHTz2mTlKk": {
    "title": "Empirical Bayes Trend Filtering Through a Variational Inference Framework",
    "volume": "main",
    "abstract": "This paper introduces a novel framework for Bayesian trend filtering using an empirical Bayes approach and a variational inference algorithm. Trend filtering is a nonparametric regression technique that has gained popularity for its simple formulation and local adaptability. Bayesian adaptations of trend filtering have been proposed as an alternative method, while they often rely on computationally intensive sampling-based methods for posterior inference. We propose an empirical Bayes trend filtering (EBTF) that leverages shrinkage priors, estimated through an empirical Bayes procedure by maximizing the marginal likelihood. To address the computational challenges posed by large datasets, we implement a variational inference algorithm for posterior computation, ensuring scalability and efficiency. Our framework is flexible, allowing the incorporation of various shrinkage priors, and optimizes the level of smoothness directly from the data. We also discuss alternative formulations of the EBTF model, along with their pros and cons. We demonstrate the performance of our EBTF method through comprehensive simulations and real-world data applications, highlighting its ability to maintain computational efficiency while providing accurate trend estimation",
    "checked": true,
    "id": "3a66c8a6926a4e01e26a92a15c2205d031e76b53",
    "semantic_title": "empirical bayes trend filtering through a variational inference framework",
    "citation_count": 0,
    "authors": [
      "Dongyue Xie"
    ]
  },
  "https://openreview.net/forum?id=MJOKrHqiV1": {
    "title": "Multi-Output Distributional Fairness via Post-Processing",
    "volume": "main",
    "abstract": "The post-processing approaches are becoming prominent techniques to enhance machine learning models' fairness because of their intuitiveness, low computational cost, and excellent scalability. However, most existing post-processing methods are designed for task-specific fairness measures and are limited to single-output models. In this paper, we introduce a post-processing method for multi-output models, such as the ones used for multi-task/multi-class classification and representation learning, to enhance a model's distributional parity, a task-agnostic fairness measure. Existing methods for achieving distributional parity rely on the (inverse) cumulative density function of a model's output, restricting their applicability to single-output models. Extending previous works, we propose to employ optimal transport mappings to move a model's outputs across different groups towards their empirical Wasserstein barycenter. An approximation technique is applied to reduce the complexity of computing the exact barycenter and a kernel regression method is proposed to extend this process to out-of-sample data. Our empirical studies evaluate the proposed approach against various baselines on multi-task/multi-class classification and representation learning tasks, demonstrating the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li",
      "Qihang Lin",
      "Ayush Ghosh",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=uJELgNGiMW": {
    "title": "Meta-Learning to Teach Semantic Prompts for Open Domain Generalization in Vision-Language Models",
    "volume": "main",
    "abstract": "Open Domain Generalization (ODG) addresses the challenges posed by domain and category shifts between labeled training sources and unlabeled target domains. Current state-of-the-art methods struggle with the limitations of traditional CNN backbones, leading to reduced generalization and increased error rates in detecting target open samples without prior knowledge. Additionally, recent CLIP-based prompt learning approaches fail to distinguish between known and unknown classes effectively, resulting in suboptimal performance. To address these challenges, we propose MetaPrompt, which leverages the semantic strengths of the vision-language model CLIP and the ''learning-to-learn'' capabilities of Meta-Learning to achieve robust generalization across domain and category shifts. Our framework introduces three key innovations: First, we approach ODG as a multi-class classification problem that includes both known and novel categories, designing novel prompts capable of detecting unknown class samples across multiple domains. These prompts are trained using Meta-Learning with momentum updates, enabling smooth and accurate differentiation between known and unknown classes. Second, we introduce a novel domain-agnostic semantic attention-based prompt alongside domain-focused prompts to enhance robustness in classifying unknown classes across various domains. Finally, we incorporate an unsupervised contrastive loss during episodic Meta-Training, which reinforces the boundaries in the metric space between known and unknown classes, thereby enhancing ''unknown'' class awareness in the prompts. MetaPrompt has demonstrated its superiority through extensive testing on diverse datasets, excelling in both closed and open-set DG scenarios and consistently outperforming existing solutions",
    "checked": true,
    "id": "b75ff9f243a4be3714ed384946de6dc21401ff38",
    "semantic_title": "meta-learning to teach semantic prompts for open domain generalization in vision-language models",
    "citation_count": 0,
    "authors": [
      "Shirsha Bose",
      "Mainak Singha",
      "Ankit Jha",
      "Souradeep Mukhopadhyay",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=nay3Kvw8BD": {
    "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
    "volume": "main",
    "abstract": "Large-scale machine learning (ML) models are increasingly being used in critical domains like education, lending, recruitment, healthcare, criminal justice, etc. However, the training, deployment, and utilization of these models demand substantial computational resources. To decrease computation and memory costs, machine learning models with sparse weight matrices are widely used in the literature. Among sparse models, those with special sparse structures (e.g., models with block-wise sparse weight matrices) fit better with the hardware accelerators and can decrease the memory and computation costs during the inference. Unfortunately, while there are several efficient training methods, none of them are designed to train a block-wise sparse model efficiently. As a result, the current methods for training block-wise sparse models start with full and dense models leading to inefficient training. In this work, we focus on training models with \\textit{block-wise sparse matrices} and propose an efficient training algorithm to decrease both computation and memory costs during training and inference. In addition, we will show that our proposed method enables us to efficiently find the right block size for the sparsity pattern during the training process. Our extensive empirical and theoretical analyses show that our algorithms can decrease the computation and memory costs significantly without a performance drop compared to baselines",
    "checked": true,
    "id": "df5f3641603273efdeedbed9052fa558c383d2b2",
    "semantic_title": "an efficient training algorithm for models with block-wise sparsity",
    "citation_count": 0,
    "authors": [
      "Ding Zhu",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili"
    ]
  },
  "https://openreview.net/forum?id=OTwnNBxZFB": {
    "title": "Almost Sure Convergence of Stochastic Gradient Methods under Gradient Domination",
    "volume": "main",
    "abstract": "Stochastic gradient methods are among the most important algorithms in training machine learning problems. While classical assumptions such as strong convexity allow a simple analysis they are rarely satisfied in applications. In recent years, global and local gradient domination properties have shown to be a more realistic replacement of strong convexity. They were proved to hold in diverse settings such as (simple) policy gradient methods in reinforcement learning and training of deep neural networks with analytic activation functions. We prove almost sure convergence rates $f(X_n)-f^*\\in o\\big( n^{-\\frac{1}{4\\beta-1}+\\epsilon}\\big)$ of the last iterate for stochastic gradient descent (with and without momentum) under global and local $\\beta$-gradient domination assumptions. The almost sure rates get arbitrarily close to recent rates in expectation. Finally, we demonstrate how to apply our results to the training task in both supervised and reinforcement learning",
    "checked": true,
    "id": "466a489d9f21a64f331120660b6a5c8c456192cf",
    "semantic_title": "almost sure convergence of stochastic gradient methods under gradient domination",
    "citation_count": 2,
    "authors": [
      "Simon Weissmann",
      "Sara Klein",
      "Waïss Azizian",
      "Leif Döring"
    ]
  },
  "https://openreview.net/forum?id=WfAvMdwiE8": {
    "title": "Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot Class-Incremental Tuning of Foundation Models",
    "volume": "main",
    "abstract": "We propose Consistency-guided Asynchronous Contrastive Tuning (CoACT), a novel method for continuously tuning foundation models to learn new classes in few-shot settings. CoACT consists of three key components: (i) asynchronous contrastive tuning, which learns new classes by including LoRA modules in the pre-trained encoder while enforcing consistency between two asynchronous encoders; (ii) controlled fine-tuning, which facilitates effective tuning of a subset of the foundation model; and (iii) consistency-guided incremental tuning, which enforces additional regularization during later sessions to reduce forgetting of the learned classes. We evaluate our proposed solution on Few-Shot Class-Incremental Learning (FSCIL) as well as a new and more challenging setup called Few-Shot Class-Incremental Tuning (FSCIT), which facilitates the continual tuning of vision foundation models to learn new classes with only a few samples per class. Unlike traditional FSCIL, FSCIT does not require a large in-distribution base session for initial fully supervised training prior to the incremental few-shot sessions. We conduct extensive evaluations across 16 diverse datasets, demonstrating the effectiveness of CoACT in both FSCIL and FSCIT setups. CoACT outperforms existing methods by up to 5.02% in FSCIL and up to 12.51% in FSCIT for individual datasets, with an average improvement of 2.47%. Furthermore, CoACT exhibits reduced forgetting and enhanced robustness in low-shot experiments. Detailed ablation and sensitivity studies highlight the contribution of each component of CoACT. We make our code publicly available at https://github.com/ShuvenduRoy/CoACT-FSCIL",
    "checked": true,
    "id": "a30c3e02307682c3274036e8be41bada4442a163",
    "semantic_title": "consistency-guided asynchronous contrastive tuning for few-shot class-incremental tuning of foundation models",
    "citation_count": 1,
    "authors": [
      "Shuvendu Roy",
      "Elham Dolatabadi",
      "Arash Afkanpour",
      "Ali Etemad"
    ]
  },
  "https://openreview.net/forum?id=heeJqQXKg7": {
    "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
    "volume": "main",
    "abstract": "Literature reviews are an essential component of scientific research, but they remain time-intensive and challenging to write, especially due to the recent influx of research papers. This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract. We decompose the task into two components: (1) Retrieving related works given a query abstract and (2) Writing a literature review based on the retrieved results. We analyze how effective LLMs are for both components. For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base. Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods while providing insights into the LLM's decision-making process. In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review. To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation protocol to promote additional research and development in this regard. Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning. Particularly, we find that combining keyword-based and document-embedding-based search improves precision and recall during retrieval by 10% and 30%, respectively, compared to using either of the methods in isolation. Further, we demonstrate that our planning-based approach achieves higher-quality reviews by minimizing hallucinated references in the generated review by 18-26% compared to existing simpler LLM-based generation methods. Our project page including a demonstration system and toolkit can be accessed here: https://litllm.github.io",
    "checked": true,
    "id": "3bf29a0420b1042f5e0a319c27cb32d46d9cde3e",
    "semantic_title": "litllms, llms for literature review: are we there yet?",
    "citation_count": 0,
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy Dj Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=M62P7iOT7d": {
    "title": "DeformTime: capturing variable dependencies with deformable attention for time series forecasting",
    "volume": "main",
    "abstract": "In multivariable time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and often overlook the potential of using exogenous variables in enhancing the prediction of the target endogenous variable. To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy. It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB). Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB. We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables. The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 7.2% on average. Notably, performance gains remain consistent across longer forecasting horizons",
    "checked": true,
    "id": "6c5ad359f1ca77ebaca62cae8263c48864b786d4",
    "semantic_title": "deformtime: capturing variable dependencies with deformable attention for time series forecasting",
    "citation_count": 1,
    "authors": [
      "Yuxuan Shu",
      "Vasileios Lampos"
    ]
  },
  "https://openreview.net/forum?id=9kFlOyLwyf": {
    "title": "Latent Covariate Shift: Unlocking Partial Identifiability for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics. Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, which we refer to as the latent content variable. Within this new paradigm, we present an intricate causal generative model by introducing latent noises across domains, along with a latent content variable and a latent style variable to achieve more nuanced rendering of observational data. We demonstrate that the latent content variable can be identified up to block identifiability due to its versatile yet distinct causal structure. We anchor our theoretical insights into a novel MSDA method, which learns the label distribution conditioned on the identifiable latent content variable, thereby accommodating more substantial distribution shifts. The proposed approach showcases exceptional performance and efficacy on both simulated and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Anton van den Hengel",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=Wj8yFjIpom": {
    "title": "$f$-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
    "volume": "main",
    "abstract": "Independent learning is a straightforward solution for fully decentralized learning in cooperative multi-agent reinforcement learning (MARL). The study of independent learning has a history of decades, and the representatives, such as independent Q-learning and independent PPO, can achieve good performances on several benchmarks. However, most independent learning algorithms lack convergence guarantees or theoretical support. In this paper, we propose a general formulation of independent policy optimization, $f$-divergence policy optimization. We hope that a more general policy optimization formulation will provide deeper insights into fully decentralized learning. We demonstrate the generality of this formulation and analyze its limitations. Based on this formulation, we further propose a novel independent learning algorithm, TVPO, which theoretically guarantees convergence. Empirically, we demonstrate that TVPO outperforms state-of-the-art fully decentralized learning methods on three popular cooperative MARL benchmarks, thereby verifying the efficacy of TVPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=vQDKYYuqWA": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "main",
    "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chen",
      "Oier Mees",
      "Aviral Kumar",
      "Sergey Levine"
    ]
  },
  "https://openreview.net/forum?id=pKilnjQsb0": {
    "title": "Implicit Bias and Fast Convergence Rates for Self-attention",
    "volume": "main",
    "abstract": "We study the fundamental optimization principles of self-attention, the defining mechanism of transformers, by analyzing the implicit bias of gradient-based optimizers in training a self-attention layer with a linear decoder in binary classification. Building on prior studies in linear logistic regression, recent findings demonstrate that the key-query matrix $W_t$ from gradient-descent (GD) converges in direction towards $W_{mm}$, which maximizes the margin between optimal and non-optimal tokens across sequences. However, this convergence is local, dependent on initial conditions, only holds asymptotically as the number of iterations increases, and leaves questions about the potential benefits of adaptive step-size rules unaddressed. To bridge this gap, we first establish scenarios for which convergence is provably global. We then analyze two adaptive step-size strategies: normalized GD and Polyak step-size, demonstrating finite-time convergence rates for $W_t$ to $W_{mm}$, and quantifying the sparsification rate of the attention map. These findings not only show that these strategies can accelerate parameter convergence over standard GD in a non-convex setting but also deepen the understanding of the implicit bias in self-attention, linking it more closely to the phenomena observed in linear logistic regression despite its intricate non-convex nature",
    "checked": true,
    "id": "621fed685b47d70d2b3bf4ed6c7c7c623b5e715d",
    "semantic_title": "implicit bias and fast convergence rates for self-attention",
    "citation_count": 19,
    "authors": [
      "Bhavya Vasudeva",
      "Puneesh Deora",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=sXq3Wb3vef": {
    "title": "Decomposing The Dark Matter of Sparse Autoencoders",
    "volume": "main",
    "abstract": "Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in ``dark matter'': unexplained variance in activations. This work investigates dark matter as an object of study in its own right. Surprisingly, we find that much of SAE dark matter---about half of the error vector itself and $>90\\% $ of its norm---can be linearly predicted from the initial activation vector. Additionally, we find that the scaling behavior of SAE error norms at a per token level is remarkably predictable: larger SAEs mostly struggle to reconstruct the same contexts as smaller SAEs. We build on the linear representation hypothesis to propose models of activations that might lead to these observations, including postulating a new type of ``introduced error''; these insights imply that the part of the SAE error vector that cannot be linearly predicted (``nonlinear'' error) might be fundamentally different from the linearly predictable component. To validate this hypothesis, we empirically analyze nonlinear SAE error and show that 1) it contains fewer not yet learned features, 2) SAEs trained on it are quantitatively worse, 3) it helps predict SAE per-token scaling behavior, and 4) it is responsible for a proportional amount of the downstream increase in cross entropy loss when SAE activations are inserted into the model. Finally, we examine two methods to reduce nonlinear SAE error: inference time gradient pursuit, which leads to a very slight decrease in nonlinear error, and linear transformations from earlier layer SAE outputs, which leads to a larger reduction",
    "checked": true,
    "id": "74485409331b414368616c5acdcaced4f1b4506b",
    "semantic_title": "decomposing the dark matter of sparse autoencoders",
    "citation_count": 14,
    "authors": [
      "Joshua Engels",
      "Logan Riggs Smith",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=Mae23iEqPS": {
    "title": "Predicting sub-population specific viral evolution",
    "volume": "main",
    "abstract": "Forecasting the change in the distribution of viral variants is crucial for therapeutic design and disease surveillance. This task poses significant modeling challenges due to the sharp differences in virus distributions across sub-populations (e.g., countries) and their dynamic interactions. Existing machine learning approaches that model the variant distribution as a whole are incapable of making location-specific predictions and ignore transmissions that shape the viral landscape. In this paper, we propose a sub-population specific protein evolution model, which predicts the time-resolved distributions of viral proteins in different locations. The algorithm explicitly models the transmission rates between sub-populations and learns their interdependence from data. The change in protein distributions across all sub-populations is defined through a linear ordinary differential equation (ODE) parametrized by transmission rates. Solving this ODE yields the likelihood of a given protein occurring in particular sub-populations. Multi-year evaluation on both SARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms baselines in accurately predicting distributions of viral proteins across continents and countries. We also find that the transmission rates learned from data are consistent with the transmission pathways discovered by retrospective phylogenetic analysis",
    "checked": true,
    "id": "9e552f28fccc526cad55a876a3b8613b6a925804",
    "semantic_title": "predicting sub-population specific viral evolution",
    "citation_count": 0,
    "authors": [
      "Wenxian Shi",
      "Menghua Wu",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=SB7JzhDG45": {
    "title": "Simulation-based Bayesian Inference from Privacy Protected Data",
    "volume": "main",
    "abstract": "Many modern statistical analysis and machine learning applications require training models on sensitive user data. Under a formal definition of privacy protection, differentially private algorithms inject calibrated noise into the confidential data or during the data analysis process to produce privacy-protected datasets or queries. However, restricting access to only privatized data during statistical analysis makes it computationally challenging to make valid statistical inferences. In this work, we propose simulation-based inference methods from privacy-protected datasets. In addition to sequential Monte Carlo approximate Bayesian computation, we adopt neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and with ordinary linear regression models. Illustrating the privacy-utility trade-off, our experiments and analysis demonstrate the necessity and feasibility of designing valid statistical inference procedures to correct for biases introduced by the privacy-protection mechanisms",
    "checked": true,
    "id": "08cb14d5314dbe317fdb3e2b7dec0a7b7e151745",
    "semantic_title": "simulation-based bayesian inference from privacy protected data",
    "citation_count": 1,
    "authors": [
      "Yifei Xiong",
      "Nianqiao Ju",
      "Sanguo Zhang"
    ]
  },
  "https://openreview.net/forum?id=0AOUWC4ss8": {
    "title": "Illustrated Landmark Graphs for Long-horizon Policy Learning",
    "volume": "main",
    "abstract": "Applying learning-based approaches to long-horizon sequential decision-making tasks requires a human teacher to carefully craft reward functions or curate demonstrations to elicit desired behaviors. To simplify this, we first introduce an alternative form of task-specification, Illustrated Landmark Graph (ILG), that represents the task as a directed graph where each vertex corresponds to a region of the state space (a landmark), and each edge represents an easier to achieve sub-task. A landmark in the ILG is conveyed to the agent through a few illustrative examples grounded in the agent's observation space. Second, we propose ILG-Learn, a human in the loop algorithm that interleaves planning over the ILG and sub-task policy learning. ILG-Learn adaptively plans through the ILG by relying on the human teacher's feedback to estimate the success rates of learned policies. We conduct experiments on long-horizon block stacking and point maze navigation tasks, and find that our approach achieves considerably higher success rates (~ 50% improvement) compared to hierarchical reinforcement learning and imitation learning baselines. Additionally, we highlight how the flexibility of the ILG specification allows the agent to learn a sequence of sub-tasks that is better suited to its limited capabilities",
    "checked": true,
    "id": "0a653ecb7cab770a3eddf270864aabc1cc05dd47",
    "semantic_title": "illustrated landmark graphs for long-horizon policy learning",
    "citation_count": 1,
    "authors": [
      "Christopher Watson",
      "Arjun Krishna",
      "Rajeev Alur",
      "Dinesh Jayaraman"
    ]
  },
  "https://openreview.net/forum?id=Rwf31BYTAU": {
    "title": "Adaptive Incentive Design for Markov Decision Processes with Unknown Rewards",
    "volume": "main",
    "abstract": "Incentive design, also known as model design or environment design for Markov decision processes(MDPs), refers to a class of problems in which a leader can incentivize his follower by modifying the follower's reward function, in anticipation that the follower's optimal policy in the resulting MDP can be desirable for the leader's objective. In this work, we propose gradient-ascent algorithms to compute the leader's optimal incentive design, despite the lack of knowledge about the follower's reward function. First, we formulate the incentive design problem as a bi-level optimization problem and demonstrate that, by the softmax temporal consistency between the follower's policy and value function, the bi-level optimization problem can be reduced to single-level optimization, for which a gradient-based algorithm can be developed to optimize the leader's objective. We establish several key properties of incentive design in MDPs and prove the convergence of the proposed gradient-based method. Next, we show that the gradient terms can be estimated from observations of the follower's best response policy, enabling the use of a stochastic gradient-ascent algorithm to compute a locally optimal incentive design without knowing or learning the follower's reward function. Finally, we analyze the conditions under which an incentive design remains optimal for two different rewards which are policy invariant. The effectiveness of the proposed algorithm is demonstrated using a small probabilistic transition system and a stochastic gridworld",
    "checked": false,
    "id": "31c85aa0d48a8249b15876501f7c1e2105fe699a",
    "semantic_title": "adaptive incentive design for markov decision processes with unknown rewards ∗",
    "citation_count": 0,
    "authors": [
      "Haoxiang Ma",
      "Shuo Han",
      "Ahmed Hemida",
      "Charles A kamhoua",
      "Jie Fu"
    ]
  },
  "https://openreview.net/forum?id=tUnyInYbjK": {
    "title": "Influence Learning in Complex Systems",
    "volume": "main",
    "abstract": "High sample complexity hampers the successful application of reinforcement learning methods, especially in real-world problems where simulating complex dynamics is computationally demanding. Influence-based abstraction (IBA) was proposed to mitigate this issue by breaking down the global model of large-scale distributed systems, such as traffic control problems, into small local sub-models. Each local model includes only a few state variables and a representation of the influence exerted by the external portion of the system. This approach allows converting a complex simulator into local lightweight simulators, enabling more effective applications of planning and reinforcement learning methods. However, the effectiveness of IBA critically depends on the ability to accurately approximate the influence of each local model. While there are a few examples showing promising results in benchmark problems, the question of whether this approach is feasible in more practical scenarios remains open. In this work, we take steps towards addressing this question by conducting an extensive empirical study of learning models for influence approximations in various realistic domains, and evaluating how these models generalize over long horizons. We find that learning the influence is often a manageable learning task, even for complex and large systems. Additionally, we demonstrate the efficacy of the approximation models for long-horizon problems. By using short trajectories, we can learn accurate influence approximations for much longer horizons",
    "checked": true,
    "id": "3499e4f9ed483674c9b2c2b7170f05ce8d6773a8",
    "semantic_title": "influence learning in complex systems",
    "citation_count": 0,
    "authors": [
      "Elena Congeduti",
      "Roberto Rocchetta",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=t1utIThKHD": {
    "title": "An Information Theoretic Approach to Machine Unlearning",
    "volume": "main",
    "abstract": "To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. We explore unlearning from an information theoretic perspective, connecting the influence of a sample to the information gain a model receives by observing it. From this, we derive a simple but principled zero-shot unlearning method based on the geometry of the model. Our approach takes the form of minimising the gradient of a learned function with respect to a small neighbourhood around a target forget point. This induces a smoothing effect, causing forgetting by moving the boundary of the classifier. We explore the intuition behind why this approach can jointly unlearn forget samples while preserving general model performance through a series of low-dimensional experiments. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method is competitive with state-of-the-art performance under the strict constraints of zero-shot unlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Foster",
      "Kyle Fogarty",
      "Stefan Schoepf",
      "Zack Dugue",
      "Cengiz Oztireli",
      "Alexandra Brintrup"
    ]
  },
  "https://openreview.net/forum?id=JhYbGiFn3Y": {
    "title": "Emergent representations in networks trained with the Forward-Forward algorithm",
    "volume": "main",
    "abstract": "The Backpropagation algorithm has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, the recently introduced Forward-Forward algorithm replaces the forward and backward passes of Backpropagation with two forward passes. In this work, we show that the internal representations obtained by the Forward-Forward algorithm can organise into category-specific ensembles exhibiting high sparsity -- composed of a low number of active units. This situation is reminiscent of what has been observed in cortical sensory areas, where neuronal ensembles are suggested to serve as the functional building blocks for perception and action. Interestingly, while this sparse pattern does not typically arise in models trained with standard Backpropagation, it can emerge in networks trained with Backpropagation on the same objective proposed for the Forward-Forward algorithm",
    "checked": true,
    "id": "5c3de66ded77aefa17bf77d12f0130fc4a383628",
    "semantic_title": "emergent representations in networks trained with the forward-forward algorithm",
    "citation_count": 9,
    "authors": [
      "Niccolo Tosato",
      "Lorenzo Basile",
      "Emanuele Ballarin",
      "Giuseppe De Alteriis",
      "Alberto Cazzaniga",
      "Alessio ansuini"
    ]
  },
  "https://openreview.net/forum?id=ZfPbCFZQbx": {
    "title": "Robust Symbolic Regression for Dynamical System Identification",
    "volume": "main",
    "abstract": "Real-world complex systems often miss high-fidelity physical descriptions and are typically subject to partial observability. Learning the dynamics of such systems is a challenging and ubiquitous problem, encountered in diverse critical applications which require interpretability and qualitative guarantees.Our paper addresses this problem in the case of sparsely observed probability distribution flows, governed by ODEs. Specifically, we devise a {\\it white box} approach -dubbed Symbolic Distribution Flow Learner (\\texttt{SDFL})- leveraging symbolic search with a Wasserstein-based loss function, resulting in a robust model-recovery scheme which naturally lends itself to cope with partial observability. Additionally, we furnish the proposed framework with theoretical guarantees on the number of required {\\it snapshots} to achieve a certain level of fidelity in the model-discovery. We illustrate the performance of the proposed scheme on the prototypical problem of Kuramoto networks and a standard benchmark of single-cell RNA sequence trajectory data. The numerical experiments demonstrate the competitive performance of \\texttt{SDFL} in comparison to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramzi Dakhmouche",
      "Ivan Lunati",
      "Hossein Gorji"
    ]
  },
  "https://openreview.net/forum?id=0yPWtbR3MC": {
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "volume": "main",
    "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training. We examine the seemingly obvious question: \\emph{how to effectively prompt VLMs for semantic segmentation}. To that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the out-of-distribution MESS dataset collection. We introduce a scalable prompting scheme, \\emph{few-shot prompted semantic segmentation}, inspired by open-vocabulary segmentation and few-shot learning. It turns out that VLMs lag far behind specialist models trained for a specific segmentation task, by about 30\\% on average on the Intersection-over-Union metric. Moreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve. Our analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11\\% improvement in performance. Motivated by our findings, we propose PromptMatcher, a remarkably simple training-free baseline that combines both text and visual prompts, achieving state-of-the-art results outperforming the best text-prompted VLM by 2.5\\%, and the top visual-prompted VLM by 3.5\\% on few-shot prompted semantic segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niccolò Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "A. Cristiano I. Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ]
  },
  "https://openreview.net/forum?id=gangoPXSRw": {
    "title": "Probabilistic neural operators for functional uncertainty quantification",
    "volume": "main",
    "abstract": "Neural operators aim to approximate the solution operator of a system of differential equations purely from data. They have shown immense success in modeling complex dynamical systems across various domains. However, the occurrence of uncertainties inherent in both model and data has so far rarely been taken into account\\textemdash{}a critical limitation in complex, chaotic systems such as weather forecasting. In this paper, we introduce the probabilistic neural operator (PNO), a framework for learning probability distributions over the output function space of neural operators. PNO extends neural operators with generative modeling based on strictly proper scoring rules, integrating uncertainty information directly into the training process. We provide a theoretical justification for the approach and demonstrate improved performance in quantifying uncertainty across different domains and with respect to different baselines. Furthermore, PNO requires minimal adjustment to existing architectures, shows improved performance for most probabilistic prediction tasks, and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. Implementing our approach into large-scale models for physical applications can lead to improvements in corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bülte",
      "Philipp Scholl",
      "Gitta Kutyniok"
    ]
  },
  "https://openreview.net/forum?id=A6tOXkkE4Z": {
    "title": "Decision-Focused Surrogate Modeling for Mixed-Integer Linear Optimization",
    "volume": "main",
    "abstract": "Mixed-integer optimization is at the core of many online decision-making systems that demand frequent updates of decisions in real time. However, due to their combinatorial nature, mixed-integer linear programs (MILPs) can be difficult to solve, rendering them often unsuitable for time-critical online applications. To address this challenge, we develop a data-driven approach for constructing surrogate optimization models in the form of linear programs (LPs) that can be solved much more efficiently than the corresponding MILPs. We train these surrogate LPs in a decision-focused manner such that for different model inputs, they achieve the same or close to the same optimal solutions as the original MILPs. One key advantage of the proposed method is that it allows the incorporation of all of the original MILP's linear constraints, which significantly increases the likelihood of obtaining feasible predicted solutions. Results from two computational case studies indicate that this decision-focused surrogate modeling approach is highly data-efficient and provides very accurate predictions of the optimal solutions. In these examples, the resulting surrogate LPs outperform state-of-the-art neural-network-based optimization proxies",
    "checked": true,
    "id": "670b6e19dae39f03a0f1c91734502f1bddaf6fbd",
    "semantic_title": "decision-focused surrogate modeling for mixed-integer linear optimization",
    "citation_count": 0,
    "authors": [
      "Shivi Dixit",
      "Rishabh Gupta",
      "Qi Zhang"
    ]
  },
  "https://openreview.net/forum?id=4ZJjr9YbBw": {
    "title": "A Vector Bernstein Inequality for Self-Normalized Martingales",
    "volume": "main",
    "abstract": "We prove a Bernstein inequality for vector-valued self-normalized martingales. We first give an alternative perspective of the corresponding sub-Gaussian bound due to Abbasi-Yadkori et al. via a PAC-Bayesian argument with Gaussian priors. By instantiating this argument to priors drawn uniformly over well-chosen ellipsoids, we obtain a Bernstein bound",
    "checked": true,
    "id": "e595a287eeca558c7e40bdd036ca4f0f95fd2f3f",
    "semantic_title": "a vector bernstein inequality for self-normalized martingales",
    "citation_count": 1,
    "authors": [
      "Ingvar Ziemann"
    ]
  },
  "https://openreview.net/forum?id=Cw2xlg0e46": {
    "title": "Long-context LLMs Struggle with Long In-context Learning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs",
    "checked": true,
    "id": "2717e5c7384ec12cfd6cf9c34897c6adad3230ed",
    "semantic_title": "long-context llms struggle with long in-context learning",
    "citation_count": 181,
    "authors": [
      "Tianle Li",
      "Ge Zhang",
      "Quy Duc Do",
      "Xiang Yue",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=d9htascfP8": {
    "title": "Meta-learning Population-based Methods for Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) algorithms are highly sensitive to their hyperparameter settings. Recently, numerous methods have been proposed to dynamically optimize these hyperparameters. One prominent approach is Population-Based Bandits (PB2), which uses time-varying Gaussian processes (GP) to dynamically optimize hyperparameters with a population of parallel agents. Despite its strong overall performance, PB2 experiences slow starts due to the GP initially lacking sufficient information. To mitigate this issue, we propose four different methods that utilize meta-data from various environments. These approaches are novel in that they adapt meta-learning methods to accommodate the time-varying setting. Among these approaches, MultiTaskPB2, which uses meta-learning for the surrogate model, stands out as the most promising approach. It outperforms PB2 and other baselines in both anytime and final performance across two RL environment families",
    "checked": true,
    "id": "1090b0bd3c8734e571b126f8403128f2d8c7d9ef",
    "semantic_title": "meta-learning population-based methods for reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Johannes Hog",
      "Raghu Rajan",
      "André Biedenkapp",
      "Noor Awad",
      "Frank Hutter",
      "Vu Nguyen"
    ]
  },
  "https://openreview.net/forum?id=kd6CfmdPfX": {
    "title": "Posterior Sampling for Reinforcement Learning on Graphs",
    "volume": "main",
    "abstract": "Many Markov Decision Processes (MDPs) exhibit structure in their state and action spaces that is not exploited. We consider the case where the structure can be modelled using a directed acyclic graph (DAG) composed of nodes and edges. In this case, each node has a state, and the state transition dynamics are influenced by the states and actions at its parent nodes. We propose an MDP framework, \\emph{Directed Acyclic Markov Decision Process} (DAMDP) that formalises this problem, and we develop algorithms to perform planning and learning. Crucially, DAMDPs retain many of the benefits of MDPs, as we can show that Dynamic Programming can find the optimal policy in known DAMDPs. We also demonstrate how to perform Reinforcement Learning in DAMDPs when the transition probabilities and the reward function are unknown. To this end, we derive a posterior sampling-based algorithm that is able to leverage the graph structure to boost learning efficiency. Moreover, we obtain a theoretical bound on the Bayesian regret for this algorithm, which directly shows the efficiency gain from considering the graph structure. We then conclude by empirically demonstrating that by harnessing the DAMDP, our algorithm outperforms traditional posterior sampling for Reinforcement Learning in both a maximum flow problem and a real-world wind farm optimisation task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Robert",
      "Aldo A. Faisal",
      "Ciara Pike-Burke"
    ]
  },
  "https://openreview.net/forum?id=wPHVijYksq": {
    "title": "A limitation on black-box dynamics approaches to Reinforcement Learning",
    "volume": "main",
    "abstract": "We prove a fundamental limitation on the computational efficiency of a large class of Reinforcement Learning (RL) methods. This limitation applies to model-free RL methods as well as some model-based methods, such as AlphaZero. We provide a formalism that describes this class and present a family of RL problems provably intractable for these methods. Conversely, the problems in the family can be efficiently solved by toy methods. We identify several types of algorithms proposed in the literature that can avoid our limitation, including algorithms that construct an inverse dynamics model, and planning algorithms that leverage an explicit model of the dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brieuc Pinon",
      "Raphael Jungers",
      "Jean-Charles Delvenne"
    ]
  },
  "https://openreview.net/forum?id=w4nd5695sq": {
    "title": "Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors",
    "volume": "main",
    "abstract": "Learning with Errors (LWE) is a hard math problem underlying recently standardized post-quantum cryptography (PQC) systems for key exchange and digital signatures. Prior work proposed new machine learning (ML)-based attacks on LWE problems with small, sparse secrets, but these attacks require millions of LWE samples to train on and take days to recover secrets. We propose three key methods---better preprocessing, angular embeddings and model pre-training---to improve these attacks, speeding up preprocessing by $25\\times$ and improving model sample efficiency by $10\\times$. We demonstrate for the first time that pre-training improves and reduces the cost of ML attacks on LWE. Our architecture improvements enable scaling to larger-dimension LWE problems: this work is the first instance of ML attacks recovering sparse binary secrets in dimension $n=1024$, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed, albeit for larger modulus $q$. Our ML-based approach is the only attack which has successfully recovered secrets for these parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stevens",
      "Emily Wenger",
      "Cathy Yuanchen Li",
      "Niklas Nolte",
      "Eshika Saxena",
      "Francois Charton",
      "Kristin E. Lauter"
    ]
  },
  "https://openreview.net/forum?id=xBbj46Y2fN": {
    "title": "What's Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias",
    "volume": "main",
    "abstract": "Knowledge Distillation is a commonly used Deep Neural Network (DNN) compression method, which often maintains overall generalization performance. However, we show that even for balanced image classification datasets, such as CIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are statistically significantly affected by distillation when comparing class-wise accuracy (i.e. class bias) between a teacher/distilled student or distilled student/non-distilled student model. Changes in class bias are not necessarily an undesirable outcome when considered outside of the context of a model's usage. Using two common fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD) on models trained with the CelebA, Trifeature, and HateXplain datasets, our results suggest that increasing the distillation temperature improves the distilled student model's fairness, and the distilled student fairness can even surpass the fairness of the teacher model at high temperatures. Additionally, we examine individual fairness, ensuring similar instances receive similar predictions. Our results confirm that higher temperatures also improve the distilled student model's individual fairness. This study highlights the uneven effects of distillation on certain classes and its potentially significant role in fairness, emphasizing that caution is warranted when using distilled models for sensitive application domains",
    "checked": false,
    "id": "7235f2e446d470920d2063796baeaa2f0b5e1c74",
    "semantic_title": "what is left after distillation? how knowledge transfer impacts fairness and bias",
    "citation_count": 1,
    "authors": [
      "Aida Mohammadshahi",
      "Yani Ioannou"
    ]
  },
  "https://openreview.net/forum?id=CeNNIQ8GJf": {
    "title": "Efficient Multi-Agent Cooperation Learning through Teammate Lookahead",
    "volume": "main",
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) is a rapidly growing research field that has achieved outstanding results across a variety of challenging cooperation tasks. However, existing MARL algorithms typically overlook the concurrent updates of teammate agents. An agent always learns from the data that it cooperates with one set of (current) teammates, but then practices with another set of (updated) teammates. This phenomenon, termed as ``teammate delay'', leads to a discrepancy between the agent's learning objective and the actual evaluation scenario, which can degrade learning stability and efficiency. In this paper, we tackle this challenge by introducing a lookahead strategy that enables agents to learn to cooperate with predicted future teammates, allowing the explicit awareness of concurrent teammate updates. This lookahead strategy is designed to seamlessly integrate with existing policy-gradient-based MARL methods, enhancing their performance without significant modifications to their underlying structures. The extensive experiments demonstrate the effectiveness of this approach, showing that the lookahead strategy can enhance the cooperation learning efficiency and achieve superior performance over the state-of-the-art MARL algorithms",
    "checked": false,
    "id": "69b3592be220e5641eeeca96b70427615d428a93",
    "semantic_title": "efficient multi-agent cooperation learning through team-mate lookahead",
    "citation_count": 1,
    "authors": [
      "Feng Chen",
      "Xinwei Chen",
      "Rong-Jun Qin",
      "Cong Guan",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://openreview.net/forum?id=DcIW0idrg8": {
    "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement",
    "volume": "main",
    "abstract": "We propose a novel memory-modular learner for image classification that separates knowledge memorization from reasoning. Our model enables effective generalization to new classes by simply replacing the memory contents, without the need for model retraining. Unlike traditional models that encode both world knowledge and task-specific skills into their weights during training, our model stores knowledge in the external memory of web-crawled image and text data. At inference time, the model dynamically selects relevant content from the memory based on the input image, allowing it to adapt to arbitrary classes by simply replacing the memory contents. The key differentiator that our learner meta-learns to perform classification tasks with noisy web data from unseen classes, resulting in robust performance across various classification scenarios. Experimental results demonstrate the promising performance and versatility of our approach in handling diverse classification tasks, including zero-shot/few-shot classification of unseen classes, fine-grained classification, and class-incremental classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Ahmet Iscen",
      "Eunchan Jo",
      "Sua Choi",
      "Minsu Cho",
      "Cordelia Schmid"
    ]
  },
  "https://openreview.net/forum?id=HJbcwRbMQQ": {
    "title": "Efficient Training of Multi-task Neural Solver for Combinatorial Optimization",
    "volume": "main",
    "abstract": "Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. Naive application of conventional multi-task learning approaches often falls short in delivering a high-quality, unified neural solver. This deficiency primarily stems from the significant computational demands and a lack of adequate consideration for the complexities inherent in COPs. In this paper, we propose a general and efficient training paradigm to deliver a unified combinarotial multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. By employing theoretically grounded approximations, our method significantly enhances overall performance, regardless of whether it is within constrained training budgets, across equivalent training epochs, or in terms of generalization capabilities, when compared to conventional training schedules. On the real-world datasets of TSPLib and CVRPLib, our method also achieved the best results compared to single task learning and multi-task learning approaches. Additionally, the influence matrix provides empirical evidence supporting common practices in the field of learning to optimize, further substantiating the effectiveness of our approach. Our code is open-sourced and available at \\url{https://github.com/LOGO-CUHKSZ/MTL-COP}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenguang Wang",
      "Zhang-Hua Fu",
      "Pinyan Lu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=B1q9po4LPl": {
    "title": "Uncovering Strong Lottery Tickets in Graph Transformers: A Path to Memory Efficient and Robust Graph Learning",
    "volume": "main",
    "abstract": "Graph Transformers (GTs) have recently demonstrated strong capabilities for capturing complex relationships in graph-structured data using global self-attention mechanisms. On the other hand, their high memory requirements during inference remain a challenge for practical deployment. In this study, we investigate the existence of strong lottery tickets (SLTs) — subnetworks within randomly initialized neural networks that can attain competitive accuracy without weight training — in GTs. Previous studies have explored SLTs in message-passing neural networks (MPNNs), showing that SLTs not only exist in MPNNs but also help mitigate over-smoothing problems and improve robustness. However, the potential of SLTs in GTs remains unexplored. With GTs having 4.5$\\times$ more parameters than MPNNs, SLTs hold even greater application value in this context. We find that fixed random weights with a traditional SLT search method cannot adapt to imbalances of features in GTs, leading to highly biased attention that destabilizes model performance. To overcome this issue and efficiently search for SLTs, we introduce a novel approach called Adaptive Scaling. We empirically confirm the existence of SLTs within GTs and demonstrate their versatility through extensive experiments across different GT architectures, including NodeFormer, GRIT, and GraphGPS. Our findings demonstrate that SLTs achieve comparable accuracy while reducing memory usage by 2--32$\\times$, effectively generalize to out-of-distribution data, and enhance robustness against adversarial perturbations. This work highlights that SLTs offer a resource-efficient approach to improving the scalability, efficiency, and robustness of GTs, with broad implications for applications involving graph data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Ito",
      "Jiale Yan",
      "Hikari Otsuka",
      "Kazushi Kawamura",
      "Masato Motomura",
      "Thiem Van Chu",
      "Daichi Fujiki"
    ]
  },
  "https://openreview.net/forum?id=MKCwO34oIq": {
    "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting",
    "volume": "main",
    "abstract": "Text-to-image (T2I) diffusion models have demonstrated impressive capabilities in generating high-quality images given a text prompt. However, ensuring the prompt-image alignment remains a considerable challenge, i.e., generating images that faithfully align with the prompt's semantics. Recent works attempt to improve the faithfulness by optimizing the latent code, which potentially could cause the latent code to go out-of-distribution and thus produce unrealistic images. In this paper, we propose FRAP, a simple, yet effective approach based on adaptively adjusting the per-token prompt weights to improve prompt-image alignment and authenticity of the generated images. We design an online algorithm to adaptively update each token's weight coefficient, which is achieved by minimizing a unified objective function that encourages object presence and the binding of object-modifier pairs. Through extensive evaluations, we show FRAP generates images with significantly higher prompt-image alignment to prompts from complex datasets, while having a lower average latency compared to recent latent code optimization methods, e.g., 4 seconds faster than D&B on the COCO-Subject dataset. Furthermore, through visual comparisons and evaluation of the CLIP-IQA-Real metric, we show that FRAP not only improves prompt-image alignment but also generates more authentic images with realistic appearances. We also explore combining FRAP with prompt rewriting LLM to recover their degraded prompt-image alignment, where we observe improvements in both prompt-image alignment and image quality. We release the code at the following link: https://github.com/LiyaoJiang1998/FRAP/",
    "checked": true,
    "id": "62607e5ec6cd338f30b040a98d7eaf297a25f83c",
    "semantic_title": "frap: faithful and realistic text-to-image generation with adaptive prompt weighting",
    "citation_count": 2,
    "authors": [
      "Liyao Jiang",
      "Negar Hassanpour",
      "Mohammad Salameh",
      "Mohan Sai Singamsetti",
      "Fengyu Sun",
      "Wei Lu",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=4zGPT0ZwnU": {
    "title": "Theoretical Insights into Overparameterized Models in Multi-Task and Replay-Based Continual Learning",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) is a machine learning paradigm that aims to improve the generalization performance of a model on multiple related tasks by training it simultaneously on those tasks. Unlike MTL, where the model has instant access to the training data of all tasks, continual learning (CL) involves adapting to new sequentially arriving tasks over time without forgetting the previously acquired knowledge. Despite the wide practical adoption of CL and MTL and extensive literature on both areas, there remains a gap in the theoretical understanding of these methods when used with overparameterized models such as deep neural networks. This paper studies the overparameterized linear models as a proxy for more complex models. We develop theoretical results describing the effect of various system parameters on the model's performance in an MTL setup. Specifically, we study the impact of model size, dataset size, and task similarity on the generalization error and knowledge transfer. Additionally, we present theoretical results to characterize the performance of replay-based CL models. Our results reveal the impact of buffer size and model capacity on the forgetting rate in a CL setup and help shed light on some of the state-of-the-art CL methods. Finally, through extensive empirical evaluations, we demonstrate that our theoretical findings are also applicable to deep neural networks, offering valuable guidance for designing MTL and CL models in practice",
    "checked": true,
    "id": "684f55d1ce1c31d8ce6f9cf97b502341024c49f5",
    "semantic_title": "theoretical insights into overparameterized models in multi-task and replay-based continual learning",
    "citation_count": 4,
    "authors": [
      "Mohammadamin Banayeeanzade",
      "Mahdi Soltanolkotabi",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=a6WthNFhL2": {
    "title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a pivotal framework for the development of effective global models (global FL) or personalized models (personalized FL) across clients with heterogeneous, non-iid data distribution. A key challenge in FL is client drift, where data heterogeneity impedes the aggregation of scattered knowledge. Recent studies have tackled the client drift issue by identifying significant divergence in the last linear (classifier) layer. To mitigate this divergence, strategies such as freezing the classifier weights and aligning the feature extractor accordingly have proven effective. Although the local alignment between classifier and feature extractor has been studied as a crucial factor in FL, we observe that it may lead the model to overemphasize the observed classes and underestimate the unobserved classes within each client. Therefore, our goals are twofold: (1) improving local alignment and (2) maintaining the representation of unseen class samples, ensuring that the solution seamlessly incorporates knowledge from individual clients, thus enhancing performance in both global and personalized FL. To achieve this, we introduce a novel algorithm named FedDr+, which empowers local model alignment using dot-regression loss. FedDr+ freezes the classifier as a simplex ETF to align the features and improves aggregated global models by employing a feature distillation mechanism to retain information about unseen/missing classes. Our empirical results demonstrate that FedDr+ not only outperforms methods with a frozen classifier but also surpasses other state-of-the-art approaches, ensuring robust performance across diverse data distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongyoon Kim",
      "Minchan Jeong",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Sumyeong Ahn",
      "Se-Young Yun"
    ]
  },
  "https://openreview.net/forum?id=G1p0YwrX8X": {
    "title": "Sparsified State-Space Models are Efficient Highway Networks",
    "volume": "main",
    "abstract": "State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woomin Song",
      "Jihoon Tack",
      "Sangwoo Mo",
      "Seunghyuk Oh",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=BvKYsaOVEn": {
    "title": "Removing Structured Noise using Diffusion Models",
    "volume": "main",
    "abstract": "Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines using normalizing flows, adversarial networks and various posterior sampling methods for diffusion models. This opens up new opportunities and relevant practical applications of diffusion modeling for inverse problems in the context of non-Gaussian measurement models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tristan Stevens",
      "Hans van Gorp",
      "Faik C Meral",
      "Junseob Shin",
      "Jason Yu",
      "Jean-luc Robert",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=UaaT2fI9DC": {
    "title": "On Using Certified Training towards Empirical Robustness",
    "volume": "main",
    "abstract": "Adversarial training is arguably the most popular way to provide empirical robustness against specific adversarial examples. While variants based on multi-step attacks incur significant computational overhead, single-step variants are vulnerable to a failure mode known as catastrophic overfitting, which hinders their practical utility for large perturbations. A parallel line of work, certified training, has focused on producing networks amenable to formal guarantees of robustness against any possible attack. However, the wide gap between the best-performing empirical and certified defenses has severely limited the applicability of the latter. Inspired by recent developments in certified training, which rely on a combination of adversarial attacks with network over-approximations, and by the connections between local linearity and catastrophic overfitting, we present experimental evidence on the practical utility and limitations of using certified training towards empirical robustness. We show that, when tuned for the purpose, a recent certified training algorithm can prevent catastrophic overfitting on single-step attacks, and that it can bridge the gap to multi-step baselines under appropriate experimental settings. Finally, we present a conceptually simple regularizer for network over-approximations that can achieve similar effects while markedly reducing runtime",
    "checked": true,
    "id": "a9e7ac965872fab42f25fa6ac952818aa03bb0f9",
    "semantic_title": "on using certified training towards empirical robustness",
    "citation_count": 1,
    "authors": [
      "Alessandro De Palma",
      "Serge Durand",
      "Zakaria Chihani",
      "François Terrier",
      "Caterina Urban"
    ]
  },
  "https://openreview.net/forum?id=Nu6N69i8SB": {
    "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
    "volume": "main",
    "abstract": "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2% of the wall-clock time and text quality in 75.6% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs)",
    "checked": true,
    "id": "18ea06ae95cad35d3c79610d16dd2a3c9ee208a5",
    "semantic_title": "mixture-of-transformers: a sparse and scalable architecture for multi-modal foundation models",
    "citation_count": 9,
    "authors": [
      "Weixin Liang",
      "LILI YU",
      "Liang Luo",
      "Srini Iyer",
      "Ning Dong",
      "Chunting Zhou",
      "Gargi Ghosh",
      "Mike Lewis",
      "Wen-tau Yih",
      "Luke Zettlemoyer",
      "Xi Victoria Lin"
    ]
  },
  "https://openreview.net/forum?id=nMCJ8bFq4B": {
    "title": "Multiplayer Information Asymmetric Contextual Bandits",
    "volume": "main",
    "abstract": "Single-player contextual bandits are a well-studied problem in reinforcement learning that has seen applications in various fields such as advertising, healthcare, and finance. In light of the recent work on information asymmetric bandits, we propose a novel multiplayer information asymmetric contextual bandit framework where there are multiple players each with their own set of actions. At every round, they observe the same context vectors and simultaneously take an action from their own set of actions, giving rise to a joint action. However, upon taking this action the players are subjected to information asymmetry in (1) actions and/or (2) rewards. We designed an algorithm mLinUCB by modifying the classical single-player algorithm LinUCB in \\cite{chu2011contextual} to achieve the optimal regret $O(\\sqrt{T})$ when only one kind of asymmetry is present. We then propose a novel algorithm ETC that is built on explore-then-commit principles to achieve the same optimal regret when both types of asymmetry are present",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chang",
      "Yuanhao Lu"
    ]
  },
  "https://openreview.net/forum?id=pqZ6nOm3WF": {
    "title": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo-Line-Search Learning Rate",
    "volume": "main",
    "abstract": "While stochastic gradient descent (SGD) can use various learning rates, such as constant or diminishing rates, previous numerical results showed that SGD performs better than other deep-learning optimizers when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization indicating that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, is a convex function of the batch size; that is, there exists a critical batch size that minimizes the SFO complexity. Finally, we provide numerical results that support our theoretical results",
    "checked": false,
    "id": "e42975390268b311e6225f32b701a7cc6135b4ae",
    "semantic_title": "relationship between batch size and number of steps needed for nonconvex optimization of stochastic gradient descent using armijo line search",
    "citation_count": 0,
    "authors": [
      "Yuki Tsukada",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=OGCuDFab4b": {
    "title": "Daphne: Multi-Pass Compilation of Probabilistic Programs into Graphical Models and Neural Networks",
    "volume": "main",
    "abstract": "Daphne is a probabilistic programming system that provides an expressive syntax to denote a large, but restricted, class of probabilistic models. Programs written in the Daphne language can be compiled into a general graph data structure of a corresponding probabilistic graphical model with simple link functions that can easily be implemented in a wide range of programming environments. Alternatively Daphne can also further compile such a graphical model into understandable and vectorized PyTorch code that can be used to train neural networks for inference. The Daphne compiler is structured in a layered multi-pass compiler framework that allows independent and easy extension of the syntax by adding additional passes. It leverages extensive partial evaluation to reduce all syntax extensions to the graphical model at compile time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Dietrich Weilbach",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=ELtNtkGXoK": {
    "title": "Cluster Tree for Nearest Neighbor Search",
    "volume": "main",
    "abstract": "Tree-based algorithms are an important and widely used class of algorithms for Nearest Neighbor Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in spite of possessing theoretical guarantees and strong practical performance, a major drawback of the RP tree is its lack of adaptability to the input dataset. Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing *ClusterTree*, a new tree based algorithm. Our approach utilizes randomness as in RP trees while adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees and other tree based methods for NNS while maintaining efficient construction time. In addition, we show theoretically and empirically that *ClusterTree* finds partitions which are superior to those found by RP trees in preserving the cluster structure of the input dataset",
    "checked": true,
    "id": "e706a5a27815c9053f5dee5e79fff886ab6abc41",
    "semantic_title": "cluster tree for nearest neighbor search",
    "citation_count": 0,
    "authors": [
      "Dan Kushnir",
      "Sandeep Silwal"
    ]
  },
  "https://openreview.net/forum?id=UWNa9Pv6qA": {
    "title": "Neuron-based explanations of neural networks sacrifice completeness and interpretability",
    "volume": "main",
    "abstract": "High quality explanations of neural networks (NNs) should exhibit two key properties. Completeness ensures that they accurately reflect a network's function and interpretability makes them understandable to humans. Many existing methods provide explanations of individual neurons within a network. In this work we provide evidence that for AlexNet pretrained on ImageNet, neuron-based explanation methods sacrifice both completeness and interpretability compared to activation principal components. Neurons are a poor basis for AlexNet embeddings because they don't account for the distributed nature of these representations. By examining two quantitative measures of completeness and conducting a user study to measure interpretability, we show the most important principal components provide more complete and interpretable explanations than the most important neurons. Much of the activation variance may be explained by examining relatively few high-variance PCs, as opposed to studying every neuron. These principal components also strongly affect network function, and are significantly more interpretable than neurons. Our findings suggest that explanation methods for networks like AlexNet should avoid using neurons as a basis for embeddings and instead choose a basis, such as principal components, which accounts for the high dimensional and distributed nature of a network's internal representations. Interactive demo and code available at https://ndey96.github.io/neuron-explanations-sacrifice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nolan Simran Dey",
      "Eric Taylor",
      "Alexander Wong",
      "Bryan P. Tripp",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=vRYt8QLKqK": {
    "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
    "volume": "main",
    "abstract": "Semi-supervised object detection (SSOD) based on pseudo-labeling significantly reduces dependence on large labeled datasets by effectively leveraging both labeled and unlabeled data. However, real-world applications of SSOD often face critical challenges, including class imbalance, label noise, and labeling errors. We present an in-depth analysis of SSOD under real-world conditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs between label quality and quantity. Based on our findings, we propose four building blocks that can be seamlessly integrated into an SSOD framework. Rare Class Collage (RCC): a data augmentation method that enhances the representation of rare classes by creating collages of rare objects. Rare Class Focus (RCF): a stratified batch sampling strategy that ensures a more balanced representation of all classes during training. Ground Truth Label Correction (GLC): a label refinement method that identifies and corrects false, missing, and noisy ground truth labels by leveraging the consistency of teacher model predictions. Pseudo-Label Selection (PLS): a selection method for removing low-quality pseudo-labeled images, guided by a novel metric estimating the missing detection rate while accounting for class rarity. We validate our methods through comprehensive experiments on autonomous driving datasets, resulting in up to 6% increase in SSOD performance. Overall, our investigation and novel, data-centric, and broadly applicable building blocks enable robust and effective SSOD in complex, real-world scenarios. Code is available at https://mos-ks.github.io/publications",
    "checked": true,
    "id": "697452342af59669949f3f774513844426f1c8b3",
    "semantic_title": "building blocks for robust and effective semi-supervised real-world object detection",
    "citation_count": 0,
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein",
      "Azarm Nowzad",
      "Fikret Sivrikaya",
      "Sahin Albayrak"
    ]
  },
  "https://openreview.net/forum?id=msI02LXVJX": {
    "title": "Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation",
    "volume": "main",
    "abstract": "This work investigates whether time series of natural phenomena can be understood as being generated by sequences of latent states which are ordered in systematic and regular ways. We focus on clinical time series and ask whether clinical measurements can be interpreted as being generated by meaningful physiological states whose succession follows systematic principles. Uncovering the underlying compositional structure will allow us to create synthetic data to alleviate the notorious problem of sparse and low-resource data settings in clinical time series forecasting, and deepen our understanding of clinical data. We start by conceptualizing compositionality for time series as a property of the data generation process, and then study data-driven procedures that can reconstruct the elementary states and composition rules of this process. We evaluate the success of this methods using two empirical tests originating from a domain adaptation perspective. Both tests infer the similarity of the original time series distribution and the synthetic time series distribution from the similarity of expected risk of time series forecasting models trained and tested on original and synthesized data in specific ways. Our experimental results show that the test set performance achieved by training on compositionally synthesized data is comparable to training on original clinical time series data, and that evaluation of models on compositionally synthesized test data shows similar results to evaluating on original test data. In both experiments, performance based on compositionally synthesized data by far surpasses that based on synthetic data that were created by randomization-based data augmentation. An additional downstream evaluation of the prediction task of sequential organ failure assessment (SOFA) scores shows significant performance gains when model training is entirely based on compositionally synthesized data compared to training on original data, with improvements increasing with the size of the synthesized training set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hagmann",
      "Michael Staniek",
      "Stefan Riezler"
    ]
  },
  "https://openreview.net/forum?id=oAzu0gzUUb": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "main",
    "abstract": "In unsupervised domain adaptation (UDA), aligning source and target domains improves the predictive performance of learned models on the target domain. A common methodological improvement in alignment methods is to divide the domains and align sub-domains instead. These sub-domain-based algorithms have demonstrated great empirical success but lack theoretical support. In this work, we establish a rigorous theoretical understanding of the advantages of these methods that have the potential to enhance their overall impact on the field. Our theory uncovers that sub-domain-based methods optimize an error bound that is at least as strong as non-sub-domain-based error bounds and is empirically verified to be much stronger. Furthermore, our analysis indicates that when the marginal weights of sub-domains shift between source and target tasks, the performance of these methods may be compromised. We therefore implement an algorithm to robustify sub-domain alignment for domain adaptation under sub-domain shift, offering a valuable adaptation strategy for future sub-domain-based methods. Empirical experiments across various benchmarks validate our theoretical insights, prove the necessity for the proposed adaptation strategy, and demonstrate the algorithm's competitiveness in handling label shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Liu",
      "Juncheng Dong",
      "Ziyang Jiang",
      "Ahmed Aloui",
      "Keyu Li",
      "Michael Hunter Klein",
      "Vahid Tarokh",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=hDywd5AbIM": {
    "title": "SAFE-NID: Self-Attention with Normalizing-Flow Encodings for Network Intrusion Detection",
    "volume": "main",
    "abstract": "Machine learning models are increasingly adopted to monitor network traffic and detect intrusions. In this work, we introduce SAFE-NID, a novel machine learning approach for real-time packet-level traffic monitoring and intrusion detection that includes a safeguard to detect zero day attacks as out-of-distribution inputs. Unlike traditional models, which falter against zero-day attacks and concept drift, SAFE-NID leverages a lightweight encoder-only transformer architecture combined with a novel normalizing flows-based safeguard. This safeguard not only quantifies uncertainty but also identifies out-of-distribution (OOD) inputs, enabling robust performance in dynamic threat landscapes. Our generative model learns class-conditional representations of the internal features of the deep neural network. We demonstrate the effectiveness of our approach by converting publicly available network flow-level intrusion datasets into packet-level ones. We release the labeled packet-level versions of these datasets with over 50 million packets each and describe the challenges in creating these datasets. We withhold from the training data certain attack categories to simulate zero-day attacks. Existing deep learning models, which achieve an accuracy of over 99% when detecting known attacks, only correctly classify 1% of the novel attacks. Our proposed transformer architecture with normalizing flows model safeguard achieves an area under the receiver operating characteristic curve of over 0.97 in detecting these novel inputs, outperforming existing combinations of neural architectures and model safeguards. The additional latency in processing each packet by the safeguard is a small fraction of the overall inference task. This dramatic improvement in detecting zero-day attacks and distribution shifts emphasizes SAFE-NID's novelty and utility as a reliable and efficient safety monitoring tool for real-world network intrusion detection",
    "checked": true,
    "id": "70bac818c9e83d47950f52b285c09ac96a91d8aa",
    "semantic_title": "safe-nid: self-attention with normalizing-flow encodings for network intrusion detection",
    "citation_count": 0,
    "authors": [
      "Brian Matejek",
      "Ashish Gehani",
      "Nathaniel D. Bastian",
      "Daniel J Clouse",
      "Bradford J Kline",
      "Susmit Jha"
    ]
  },
  "https://openreview.net/forum?id=aPyJilTiIb": {
    "title": "A Unified View of Double-Weighting for Marginal Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José I. Segovia-Martín",
      "Santiago Mazuelas",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=qsipSdfWeV": {
    "title": "Distilling Datasets Into Less Than One Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asaf Shul",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=uxyWlXPuIg": {
    "title": "On Using Secure Aggregation in Differentially Private Federated Learning with Multiple Local Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. Heikkilä"
    ]
  },
  "https://openreview.net/forum?id=XVSQnnf7QT": {
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d19547be89a5b3acad6087ed65c62286d855e14a",
    "semantic_title": "which backbone to use: a resource-efficient domain specific comparison for computer vision",
    "citation_count": 5,
    "authors": [
      "Pranav Jeevan P",
      "Amit Sethi"
    ]
  },
  "https://openreview.net/forum?id=tIfS6jyO9f": {
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "volume": "main",
    "abstract": "The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is qualitatively compared to a classical Kalman filter and quantitatively to Seq2Seq and TrAISformer models. The Fréchet distance is introduced as the main evaluation metric for these comparisons. We show that it is possible to predict ship trajectories quite precisely up to 8 hours ahead with 30 minutes of context, using solely GNSS positions, without relying on any additional information such as speed, course, or external conditions — unlike many traditional methods. We demonstrate that this alternative works well enough to predict trajectories worldwide",
    "checked": true,
    "id": "5edac0ff5c13118de4d32272a08c3b8c7bd1d1b5",
    "semantic_title": "enhancing maritime trajectory forecasting via h3 index and causal language modelling (clm)",
    "citation_count": 2,
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aurélien Chateigner"
    ]
  },
  "https://openreview.net/forum?id=EoiuRII7MQ": {
    "title": "Lower Ricci Curvature for Efficient Community Detection",
    "volume": "main",
    "abstract": "This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and scale-free discrete curvature designed to enhance community detection in networks. Addressing the computational challenges posed by existing curvature-based methods, LRC offers a streamlined approach with linear computational complexity, which makes it well suited for large-scale network analysis. We further develop an LRC-based preprocessing method that effectively augments popular community detection algorithms. Through applications on multiple real-world datasets, including the NCAA football league network, the DBLP collaboration network, the Amazon product co-purchasing network, and the YouTube social network, we demonstrate the efficacy of our method in significantly improving the performance of various community detection algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Jin Park",
      "Didong Li"
    ]
  },
  "https://openreview.net/forum?id=uRbf9ANAns": {
    "title": "Meta-learning Optimizers for Communication-Efficient Learning",
    "volume": "main",
    "abstract": "Communication-efficient variants of SGD, specifically local SGD, have received a great deal of interest in recent years. These approaches compute multiple gradient steps locally on each worker, before averaging model parameters, helping relieve the critical communication bottleneck in distributed deep learning training. Although many variants of these approaches have been proposed, they can sometimes lag behind state-of-the-art adaptive optimizers for deep learning. In this work, we investigate if the recent progress in the emerging area of learned optimizers can potentially close this gap in homogeneous data and homogeneous device settings while remaining communication-efficient. Specifically, we meta-learn how to perform global updates given an update from local SGD iterations. Our results demonstrate that learned optimizers can substantially outperform local SGD and its sophisticated variants while maintaining their communication efficiency. Our learned optimizers can even generalize to unseen and much larger datasets and architectures, including ImageNet and ViTs, and to unseen modalities such as language modeling. We therefore show the potential of learned optimizers for improving communication-efficient distributed learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles-Étienne Joseph",
      "Benjamin Thérien",
      "Abhinav Moudgil",
      "Boris Knyazev",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=HTpMOl6xSI": {
    "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
    "volume": "main",
    "abstract": "Scaling large language models has driven remarkable advancements across various domains, yet the continual increase in model size presents significant challenges for real-world deployment. The Mixture of Experts (MoE) architecture offers a promising solution by dynamically selecting and activating only a subset of experts during inference, thus substantially reducing computational costs while preserving high performance. Despite these benefits, MoE introduces new inefficiencies, such as excessive parameters and communication overhead. In this work, we present a holistic study of compression techniques for Mixture of Experts to enhance both efficiency and scalability. While recent efforts have focused on Expert Trimming, which reduces the number of experts, these approaches still suffer from considerable communication and computational costs. To address this, we propose more aggressive strategies, such as Layer Drop, which removes entire MoE layers, and Block Drop, which eliminates transformer blocks. Surprisingly, these aggressive pruning techniques not only preserve model performance but also substantially improve computation and memory efficiency. Furthermore, beyond Expert Trimming, we also introduce Expert Slimming, which compresses individual experts to further boost performance and can be seamlessly integrated with Expert Trimming. Extensive experimental results demonstrate the effectiveness of our proposed methods—Layer Drop and Block Drop—along with the comprehensive recipe that integrates Expert Slimming and Expert Trimming, achieving a 6.05× speedup with 77.1% reduced memory usage while maintaining over 92% of performance on Mixtral-8×7B. Our code is released at https://github.com/CASE-Lab-UMD/Unified-MoE-Compression",
    "checked": true,
    "id": "4d0ee7bca07af07e319add08efcdd403e215abff",
    "semantic_title": "towards efficient mixture of experts: a holistic study of compression techniques",
    "citation_count": 7,
    "authors": [
      "Shwai He",
      "Daize Dong",
      "Liang Ding",
      "Ang Li"
    ]
  },
  "https://openreview.net/forum?id=MGdydNfWzQ": {
    "title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning",
    "volume": "main",
    "abstract": "We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks. This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems. We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem. We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators. We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions. Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning",
    "checked": true,
    "id": "c14ce2ad7c54b3f1c9c4ab53c5ae375d5a594f08",
    "semantic_title": "ensemble and mixture-of-experts deeponets for operator learning",
    "citation_count": 0,
    "authors": [
      "Ramansh Sharma",
      "Varun Shankar"
    ]
  },
  "https://openreview.net/forum?id=56EBglCFvx": {
    "title": "HARE: Human-in-the-Loop Algorithmic Recourse",
    "volume": "main",
    "abstract": "Machine learning models are seeing increasing use as decision making systems in domains such as education, finance and healthcare. It is desirable that these models are trustworthy to the end-user, by ensuring fairness, transparency and reliability of decisions. In this work, we consider a key aspect of responsible and transparent AI models -- actionable explanations, viz. the ability of such models to provide recourse to end users adversely affected by their decisions. While algorithmic recourse has seen a variety of efforts in recent years, there have been very few efforts on exploring personalized recourse for a given user. Two users with the same feature profile may prefer vastly different recourses. The limited work in this direction hitherto rely on one-time feature preferences provided by a user. Instead, we present a human-in-the-loop formulation of algorithmic recourse that can incorporate both relative and absolute human feedback for a given test instance. We show that our formulation can extend any existing recourse generating method, enabling the generation of recourses that are satisfactory to the user. We perform experiments on 3 benchmark datasets on top of 6 popular baseline recourse methods where we observe that our framework performs significantly better on simulated user preferences",
    "checked": true,
    "id": "23ff34777e3ae9a4ebffc473e4092b0e05bde183",
    "semantic_title": "hare: human-in-the-loop algorithmic recourse",
    "citation_count": 0,
    "authors": [
      "Sai Srinivas Kancheti",
      "Rahul Vigneswaran",
      "Bamdev Mishra",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=nNN1pPJRVL": {
    "title": "Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction",
    "volume": "main",
    "abstract": "This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on $60$~ second labeled sequences of $1$~Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of $10\\%$ and $8\\%$, respectively, over the baseline model. Most importantly, severe events are detected $60\\%$ of the time, against $20\\%$ for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach",
    "checked": true,
    "id": "e53b8fedc66a121e88a02b726613dd5770971af7",
    "semantic_title": "domain generalization for time series: enhancing drilling regression models for stick-slip index prediction",
    "citation_count": 0,
    "authors": [
      "Hana YAHIA",
      "Bruno Figliuzzi",
      "Florent Di Meglio",
      "Gerbaud",
      "Stephane Menand",
      "Mohamed MAHJOUB"
    ]
  },
  "https://openreview.net/forum?id=VNM6V1gi3k": {
    "title": "Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations",
    "volume": "main",
    "abstract": "This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. It is shown here that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in (Euclidean) norm and approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of the recently introduced neural correlation function. Additionally, this paper also studies the KKT points of the neural correlation function for feed-forward networks with (Leaky) ReLU and polynomial (Leaky) ReLU activations, deriving necessary and sufficient conditions for rank-one KKT points",
    "checked": true,
    "id": "d9e2b6247c0a53aef00fae47f698aa7489014b65",
    "semantic_title": "early directional convergence in deep homogeneous neural networks for small initializations",
    "citation_count": 4,
    "authors": [
      "Akshay Kumar",
      "Jarvis Haupt"
    ]
  },
  "https://openreview.net/forum?id=HaAg9RN7Hi": {
    "title": "Unlabelled Compressive Sensing under Sparse Permutation and Prior Information",
    "volume": "main",
    "abstract": "In this paper, we study the problem of unlabelled compressed sensing, where the correspondence between the measurement values and the rows of the sensing matrix is lost, the number of measurements is less than the dimension of the regression vector, and the regression vector is sparse in the identity basis. Additionally, motivated by practical situations, we assume that we accurately know a small number of correspondences between the rows of the measurement matrix and the measurement vector. We propose a tractable estimator, based on a modified form of the \\textsc{Lasso}, to estimate the regression vector, and we derive theoretical error bounds for the estimate. This is unlike previous approaches to unlabelled compressed sensing, which either do not produce theoretical bounds or which produce bounds for intractable estimators. We show that our algorithm outperforms a hard thresholding pursuit (\\textsc{Htp}) approach and an $\\ell_1$-norm estimator used to solve a similar problem across diverse regimes. We also propose a modified \\textsc{Htp} based estimator which has superior properties to the baseline \\textsc{Htp} estimator. Lastly, we show an application of unlabelled compressed sensing in image registration, demonstrating the utility of a few known point correspondences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garweet Sresth",
      "Satish Mulleti",
      "Ajit Rajwade"
    ]
  },
  "https://openreview.net/forum?id=osesw2V10u": {
    "title": "A unifying framework for generalised Bayesian online learning in non-stationary environments",
    "volume": "main",
    "abstract": "We propose a unifying framework for methods that perform probabilistic online learning in non-stationary environments. We call the framework BONE, which stands for generalised (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments. BONE provides a common structure to tackle a variety of problems, including online continual learning, prequential forecasting, and contextual bandits. The framework requires specifying three modelling choices: (i) a model for measurements (e.g., a neural network), (ii) an auxiliary process to model non-stationarity (e.g., the time since the last changepoint), and (iii) a conditional prior over model parameters (e.g., a multivariate Gaussian). The framework also requires two algorithmic choices, which we use to carry out approximate inference under this framework: (i) an algorithm to estimate beliefs (posterior distribution) about the model parameters given the auxiliary variable, and (ii) an algorithm to estimate beliefs about the auxiliary variable. We show how the modularity of our framework allows for many existing methods to be reinterpreted as instances of BONE, and it allows us to propose new methods. We compare experimentally existing methods with our proposed new method on several datasets, providing insights into the situations that make each method more suitable for a specific task. We provide a Jax open source library to facilitate the adoption of this framework",
    "checked": true,
    "id": "c4d64a922168e2918377fa6effcf81f593be9a60",
    "semantic_title": "a unifying framework for generalised bayesian online learning in non-stationary environments",
    "citation_count": 1,
    "authors": [
      "Gerardo Duran-Martin",
      "Leandro Sánchez-Betancourt",
      "Alex Shestopaloff",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=GEilvtsFNV": {
    "title": "Variational Neural Stochastic Differential Equations with Change Points",
    "volume": "main",
    "abstract": "In this work, we explore modeling change points in time-series data using neural stochastic differential equations (neural SDEs). We propose a novel model formulation and training procedure based on the variational autoencoder (VAE) framework for modeling time-series as a neural SDE. Unlike existing algorithms training neural SDEs as VAEs, our proposed algorithm only necessitates a Gaussian prior of the initial state of the latent stochastic process, rather than a Wiener process prior on the entire latent stochastic process. We develop two methodologies for modeling and estimating change points in time-series data with distribution shifts. Our iterative algorithm alternates between updating neural SDE parameters and updating the change points based on either a maximum likelihood-based approach or a change point detection algorithm using the sequential likelihood ratio test. We also discuss theoretical implications of the proposed change point detection scheme. Finally, we present an empirical evaluation that demonstrates the expressive power of our proposed model, showing that it can effectively model both classical parametric SDEs and some real datasets with distribution shifts",
    "checked": true,
    "id": "33e8721b7eaef68fd59847b3921b8467ef332097",
    "semantic_title": "variational neural stochastic differential equations with change points",
    "citation_count": 0,
    "authors": [
      "Yousef El-Laham",
      "Zhongchang Sun",
      "Haibei Zhu",
      "Tucker Balch",
      "Svitlana Vyetrenko"
    ]
  },
  "https://openreview.net/forum?id=y5Hf0otJLk": {
    "title": "Respecting the limit: Bayesian optimization with a bound on the optimal value",
    "volume": "main",
    "abstract": "In many real-world optimization problems, we have prior information about what objective function values are achievable. In this paper, we study the scenario that we have either exact knowledge of the minimum value or a, possibly inexact, lower bound on its value. We propose bound-aware Bayesian optimization (BABO), a Bayesian optimization method that uses a new surrogate model and acquisition function to utilize such prior information. We present SlogGP, a new surrogate model that incorporates bound information and adapts the Expected Improvement (EI) acquisition function accordingly. Empirical results on a variety of benchmarks demonstrate the benefit of taking prior information about the optimal value into account, and that the proposed approach significantly outperforms existing techniques. Furthermore, we notice that even in the absence of prior information on the bound, the proposed SlogGP surrogate model still performs better than the standard GP model in most cases, which we explain by its larger expressiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Juergen Branke",
      "Matthias Poloczek"
    ]
  },
  "https://openreview.net/forum?id=dg1tqNIWg3": {
    "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
    "volume": "main",
    "abstract": "In supervised machine learning, privileged information (PI) is information that is unavailable at inference, but is accessible during training time. Research on learning using privileged information (LUPI) aims to transfer the knowledge captured in PI onto a model that can perform inference without PI. It seems that this extra bit of information ought to make the resulting model better. However, finding conclusive theoretical or empirical evidence that supports the ability to transfer knowledge using PI has been challenging. In this paper, we critically examine the assumptions underlying existing theoretical analyses and argue that there is little theoretical justification for when LUPI should work. We analyze two main LUPI methods - generalized distillation and marginalization with weight sharing - and reveal that apparent improvements in empirical risk may not directly result from PI. Instead, these improvements often stem from dataset anomalies or modifications in model design misguidedly attributed to PI. Our experiments for a wide variety of application domains further demonstrate that state-of-the-art LUPI approaches fail to effectively transfer knowledge from PI. Thus, we advocate for practitioners to exercise caution when working with PI to avoid unintended inductive biases",
    "checked": true,
    "id": "c9a9a006dbfd58509fc80857d258868d3ddcbe6f",
    "semantic_title": "rethinking knowledge transfer in learning using privileged information",
    "citation_count": 0,
    "authors": [
      "Danil Provodin",
      "Bram van den Akker",
      "Christina Katsimerou",
      "Maurits Clemens Kaptein",
      "Mykola Pechenizkiy"
    ]
  },
  "https://openreview.net/forum?id=FecsgPCOHk": {
    "title": "Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning",
    "volume": "main",
    "abstract": "The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way---without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of Directed Acyclic Graphs (DAG), to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees under the Maximal Ancestral Graph (MAG) class. We leverage the idea of a superstructure---a set of learned or existing candidate hypotheses---to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces",
    "checked": true,
    "id": "a0c0df87cabaf2893a90dd45ef1e30c632a703ff",
    "semantic_title": "causal discovery over high-dimensional structured hypothesis spaces with causal graph partitioning",
    "citation_count": 1,
    "authors": [
      "Ashka Shah",
      "Adela Frances DePavia",
      "Nathaniel C Hudson",
      "Ian Foster",
      "Rick Stevens"
    ]
  },
  "https://openreview.net/forum?id=uafxqhImPM": {
    "title": "On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective",
    "volume": "main",
    "abstract": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a novel paradigm for function approximation by leveraging univariate spline-based decompositions inspired by the Kolmogorov–Arnold theorem. Despite their theoretical appeal---particularly the potential for inducing smoother decision boundaries and lower effective Lipschitz constants---their adversarial robustness remains largely unexplored. In this work, we conduct the first comprehensive evaluation of KAN robustness in adversarial settings, focusing on both fully connected (FCKANs) and convolutional (CKANs) instantiations for image classification tasks. Across a wide range of benchmark datasets (MNIST, FashionMNIST, KMNIST, CIFAR-10, SVHN, and a subset of ImageNet), we compare KANs against conventional architectures using an extensive suite of attacks, including white-box methods (FGSM, PGD, C\\&W, MIM), black-box approaches (Square Attack, SimBA, NES), and ensemble attacks (AutoAttack). Our experiments reveal that while small- and medium-scale KANs are not consistently more robust than their standard counterparts, large-scale KANs exhibit markedly enhanced resilience against adversarial perturbations. An ablation study further demonstrates that critical hyperparameters---such as number of knots and spline order---significantly influence robustness. Moreover, adversarial training experiments confirm the inherent safety advantages of KAN-based architectures. Overall, our findings provide novel insights into the adversarial behavior of KANs and lay a rigorous foundation for future research on robust, interpretable network designs",
    "checked": true,
    "id": "1c80805893b30d2d779ecfbe236c2a1c32290dd8",
    "semantic_title": "on the robustness of kolmogorov-arnold networks: an adversarial perspective",
    "citation_count": 6,
    "authors": [
      "Tal Alter",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=uA19Xo1o31": {
    "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
    "volume": "main",
    "abstract": "We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Faysse",
      "Patrick Fernandes",
      "Nuno M Guerreiro",
      "António Loison",
      "Duarte Miguel Alves",
      "Caio Corro",
      "Nicolas Boizard",
      "João Alves",
      "Ricardo Rei",
      "Pedro Henrique Martins",
      "Antoni Bigata Casademunt",
      "François Yvon",
      "Andre Martins",
      "Gautier Viaud",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=uPCvfyr2KP": {
    "title": "Reheated Gradient-based Discrete Sampling for Combinatorial Optimization",
    "volume": "main",
    "abstract": "Recently, gradient-based discrete sampling has emerged as a highly efficient, general-purpose solver for various combinatorial optimization (CO) problems, achieving performance comparable to or surpassing the popular data-driven approaches. However, we identify a critical issue in these methods, which we term ``wandering in contours''. This behavior refers to sampling new different solutions that share very similar objective values for a long time, leading to computational inefficiency and suboptimal exploration of potential solutions. In this paper, we introduce a novel reheating mechanism inspired by the concept of critical temperature and specific heat in physics, aimed at overcoming this limitation. Empirically, our method demonstrates superiority over existing sampling-based and data-driven algorithms across a diverse array of CO problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muheng Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=5zRs34Ls3C": {
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "volume": "main",
    "abstract": "Graph anomaly detection (GAD) is becoming increasingly crucial in various applications, ranging from financial fraud detection to fake news detection. However, current GAD methods largely overlook the fairness problem, which might result in discriminatory decisions skewed toward certain demographic groups defined on sensitive attributes (e.g., gender). This greatly limits the applicability of these methods in real-world scenarios in light of societal and ethical restrictions. To address this critical gap, we make the first attempt to integrate fairness with utility in GAD decision-making. Specifically, we devise a novel DisEntangle-based FairnEss-aware aNomaly Detection framework on the attributed graph, named DEFEND. DEFEND first introduces disentanglement in GNNs to capture informative yet sensitive-irrelevant node representations, effectively reducing bias inherent in graphrepresentation learning. Besides, to alleviate discriminatory bias in evaluating anomalies, DEFEND adopts a reconstruction-based method, which concentrates solely on node attributes and avoids incorporating biased graph topology. Additionally, given the inherent association between sensitive-relevant and -irrelevant attributes, DEFEND further constrains the correlation between the reconstruction error and predicted sensitive attributes. Empirical evaluations on real-world datasets reveal that DEFEND performs effectively in GAD and significantly enhances fairness compared to state-of-the-art baselines. Our code is available at https://github.com/AhaChang/DEFEND",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ]
  },
  "https://openreview.net/forum?id=Xv3ZrFayIO": {
    "title": "Attention Overlap Is Responsible for The Entity Missing Problem in Text-to-image Diffusion Models!",
    "volume": "main",
    "abstract": "Text-to-image diffusion models such as Stable Diffusion and DALL-E have exhibited impressive capabilities in producing high-quality, diverse, and realistic images based on textual prompts. Nevertheless, a common issue arises where these models encounter difficulties in faithfully generating every entity specified in the prompt, leading to a recognized challenge known as entity missing in visual compositional generation. While previous studies indicated that actively adjusting cross-attention maps during inference could potentially resolve the issue, there has been a lack of systematic investigation into the specific objective function required for this task. In this work, we thoroughly investigate three potential causes of entity missing from the perspective of cross-attention maps: insufficient attention intensity, excessive attention spread, and significant overlap between attention maps of different entities. Through comprehensive empirical analysis, we found that optimizing metrics that quantify the overlap between attention maps of entities is highly effective at mitigating entity missing. We hypothesize that during the denoising process, entity-related tokens engage in a form of competition for attention toward specific regions through the cross-attention mechanism. This competition may result in the attention of a spatial location being divided among multiple tokens, leading to difficulties in accurately generating the entities associated with those tokens. Building on this insight, we propose four overlap-based loss functions that can be used to implicitly manipulate the latent embeddings of the diffusion model during inference: Intersection over union (IoU), center-of-mass (CoM) distance, Kullback–Leibler (KL) divergence, and clustering compactness (CC). Extensive experiments on a diverse set of prompts demonstrate that our proposed training-free methods substantially outperform previous approaches on a range of compositional alignment metrics, including visual question-answering, captioning score, CLIP similarity, and human evaluation. Notably, our method outperforms the best baseline by $9\\%$ in human evaluation",
    "checked": true,
    "id": "074dad74f5089af7186cabe9bf7fd91f96d9d7a4",
    "semantic_title": "attention overlap is responsible for the entity missing problem in text-to-image diffusion models!",
    "citation_count": 2,
    "authors": [
      "Arash Mari Oriyad",
      "Mohammadali Banayeeanzade",
      "Reza Abbasi",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openreview.net/forum?id=iHYCdTAOqF": {
    "title": "The Time-Energy Model: Selective Time-Series Forecasting Using Energy-Based Models",
    "volume": "main",
    "abstract": "Time-series forecasting is an important task in many domains, including finance, weather prediction, and energy consumption forecasting, and deep learning methods have emerged as the best-performing time-series forecasting methods over the last few years. However, most proposed time-series forecasting models are deterministic and are prone to errors when deployed in production, potentially causing significant losses and penalties when making predictions with low confidence. In this paper, we propose the Time-Energy Model (TEM), a framework that introduces so-called selective time-series forecasting using energy-based models. Selective forecasting estimates model confidence and allows the end-user to selectively reject forecasts while maintaining a desired target coverage. TEM is model-agnostic and can be used to improve forecasting accuracy of any encoder-decoder deterministic time-series forecasting model. TEM is trained using a combination of supervised and self-supervised learning, leveraging excellent single-point prediction accuracy while maintaining the ability to reject forecasts based on model confidence. Experimental results indicate that TEM generalizes well across 5 state-of-the-art deterministic time-series forecasting models and 5 benchmark time-series forecasting datasets. Using selective forecasting, TEM reduces prediction error by up to $49.1\\%$ over 5 state-of-the-art deterministic models. Furthermore, TEM has up to $87.0\\%$ lower error than selected baseline EBM models, and achieves significantly better performance than state-of-the-art selective deep learning models. Code for the proposed TEM framework is available at https://github.com/JonasBrusokas/Time-Energy-Model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Brusokas",
      "Seshu Tirupathi",
      "Dalin Zhang",
      "Torben Bach Pedersen"
    ]
  },
  "https://openreview.net/forum?id=Is9APiPg4V": {
    "title": "Characterizing the Convergence of Game Dynamics via Potentialness",
    "volume": "main",
    "abstract": "Understanding the convergence landscape of multi-agent learning is a fundamental problem of great practical relevance in many applications of artificial intelligence and machine learning. While it is known that learning dynamics converge to Nash equilibrium in potential games, the behavior of dynamics in many important classes of games that do not admit a potential is poorly understood. To measure how ``close'' a game is to being potential, we consider a distance function, that we call ``potentialness'', and which relies on a strategic decomposition of games introduced by Candogan et al. (2011). We introduce a numerical framework enabling the computation of this metric, which we use to calculate the degree of ``potentialness'' in generic matrix games, as well as (non-generic) games that are important in economic applications, namely auctions and contests. Understanding learning in the latter games has become increasingly important due to the wide-spread automation of bidding and pricing with no-regret learning algorithms. We empirically show that potentialness decreases and concentrates with an increasing number of agents or actions; in addition, potentialness turns out to be a good predictor for the existence of pure Nash equilibria and the convergence of no-regret learning algorithms in matrix games. In particular, we observe that potentialness is very low for complete-information models of the all-pay auction where no pure Nash equilibrium exists, and much higher for Tullock contests, first-, and second-price auctions, explaining the success of learning in the latter. In the incomplete-information version of the all-pay auction, a pure Bayes-Nash equilibrium exists and it can be learned with gradient-based algorithms. Potentialness nicely characterizes these differences to the complete-information version",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Bichler",
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Matthias Oberlechner",
      "Bary Pradelski"
    ]
  },
  "https://openreview.net/forum?id=OGifiton47": {
    "title": "Active Diffusion Subsampling",
    "volume": "main",
    "abstract": "Subsampling is commonly used to mitigate costs associated with data acquisition, such as time or energy requirements, motivating the development of algorithms for estimating the fully-sampled signal of interest $x$ from partially observed measurements $y$. In maximum- entropy sampling, one selects measurement locations that are expected to have the highest entropy, so as to minimize uncertainty about $x$. This approach relies on an accurate model of the posterior distribution over future measurements, given the measurements observed so far. Recently, diffusion models have been shown to produce high-quality posterior samples of high-dimensional signals using guided diffusion. In this work, we propose Active Diffusion Subsampling (ADS), a method for designing intelligent subsampling masks using guided dif- fusion in which the model tracks a distribution of beliefs over the true state of $x$ throughout the reverse diffusion process, progressively decreasing its uncertainty by actively choosing to acquire measurements with maximum expected entropy, ultimately producing the pos- terior distribution $p(x | y)$. ADS can be applied using pre-trained diffusion models for any subsampling rate, and does not require task-specific retraining – just the specification of a measurement model. Furthermore, the maximum entropy sampling policy employed by ADS is interpretable, enhancing transparency relative to existing methods using black-box policies. Experimentally, we show that through designing informative subsampling masks, ADS significantly improves reconstruction quality compared to fixed sampling strategies on the MNIST and CelebA datasets, as measured by standard image quality metrics, includ- ing PSNR, SSIM, and LPIPS. Furthermore, on the task of Magnetic Resonance Imaging acceleration, we find that ADS performs competitively with existing supervised methods in reconstruction quality while using a more interpretable acquisition scheme design procedure. Code is available at https://active-diffusion-subsampling.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oisín Nolan",
      "Tristan Stevens",
      "Wessel L. van Nierop",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=XosdLS7KVE": {
    "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for Transformer Pretraining",
    "volume": "main",
    "abstract": "Large language models (LLMs) have made significant strides in complex tasks, yet their widespread adoption is impeded by substantial computational demands. With hundreds of billion parameters, transformer-based LLMs necessitate months of pretraining across a high-end GPU cluster. However, this paper reveals a compelling finding: transformers exhibit considerable redundancy in pretraining computations, which motivates our proposed solution, Mixed Sparsity Training (MST), an efficient pretraining method that can reduce about $75$% of Floating Point Operations (FLOPs) while maintaining performance. MST integrates dynamic sparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention (HSA) during pretraining, involving three distinct phases: warm-up, ultra-sparsification, and restoration. The warm-up phase transforms the dense model into a sparse one, and the restoration phase reinstates connections. Throughout these phases, the model is trained with a dynamically evolving sparse topology and an HSA mechanism to maintain performance and minimize training FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction of $4\\times$ without compromising performance",
    "checked": false,
    "id": "65631c9294f6a1c2b7562133c08342ea7428853a",
    "semantic_title": "mixed sparsity training: achieving 4⨉ flop reduction for transformer pretraining",
    "citation_count": 0,
    "authors": [
      "Pihe Hu",
      "Shaolong Li",
      "Xun Wang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LDzvZEVl5H": {
    "title": "Online Control-Informed Learning",
    "volume": "main",
    "abstract": "This paper proposes an Online Control-Informed Learning (OCIL) framework, which employs the well-established optimal control and state estimation techniques in the field of control to solve a broad class of learning tasks in an online fashion. This novel integration effectively handles practical issues in machine learning such as noisy measurement data, online learning, and data efficiency. By considering any robot as a tunable optimal control system, we propose an online parameter estimator based on extended Kalman filter (EKF) to incrementally tune the system in an online fashion, enabling it to complete designated learning or control tasks. The proposed method also improves the robustness in learning by effectively managing noise in the data. Theoretical analysis is provided to demonstrate the convergence of OCIL. Three learning modes of OCIL, i.e. Online Imitation Learning, Online System Identification, and Policy Tuning On-the-fly, are investigated via experiments, which validate their effectiveness",
    "checked": true,
    "id": "4ce0358920dd394751a1bae16ac6f1b005e619c2",
    "semantic_title": "online control-informed learning",
    "citation_count": 1,
    "authors": [
      "Zihao Liang",
      "Tianyu Zhou",
      "Zehui Lu",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=D3DA7pgpvn": {
    "title": "Visual Privacy Auditing with Diffusion Models",
    "volume": "main",
    "abstract": "Data reconstruction attacks on machine learning models pose a substantial threat to privacy, potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) provides theoretical guarantees, determining appropriate DP parameters remains challenging. Current formal guarantees on the success of data reconstruction suffer from overly stringent assumptions regarding adversary knowledge about the target data, particularly in the image domain, raising questions about their real-world applicability. In this work, we empirically investigate this discrepancy by introducing a reconstruction attack based on diffusion models (DMs) that only assumes adversary access to real-world image priors and specifically targets the DP defense. We find that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as heuristic auditing tools for visualizing privacy leakage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristian Schwethelm",
      "Johannes Kaiser",
      "Moritz Knolle",
      "Sarah Lockfisch",
      "Daniel Rueckert",
      "Alexander Ziller"
    ]
  },
  "https://openreview.net/forum?id=nuIUTHGlM5": {
    "title": "Calibrated Probabilistic Forecasts for Arbitrary Sequences",
    "volume": "main",
    "abstract": "Real-world data streams can change unpredictably due to distribution shifts, feedback loops and adversarial actors, which challenges the validity of forecasts. We present a forecasting framework ensuring valid uncertainty estimates regardless of how data evolves. Leveraging the concept of Blackwell approachability from game theory, we introduce a forecasting framework that guarantees calibrated uncertainties for outcomes in any compact space (e.g., classification or bounded regression). We extend this framework to recalibrate existing forecasters, guaranteeing calibration without sacrificing predictive performance. We implement both general-purpose gradient-based algorithms and algorithms optimized for popular special cases of our framework. Empirically, our algorithms improve calibration and downstream decision-making for energy systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Marx",
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ]
  },
  "https://openreview.net/forum?id=QlBaDKb370": {
    "title": "State space models can express $n$-gram languages",
    "volume": "main",
    "abstract": "Recent advancements in recurrent neural networks (RNNs) have reinvigorated interest in their application to natural language processing tasks, particularly with the development of more efficient and parallelizable variants known as state space models (SSMs), which have shown competitive performance against transformer models while maintaining a lower memory footprint. While RNNs and SSMs (e.g., Mamba) have been empirically more successful than rule-based systems based on $n$-gram models, a rigorous theoretical explanation for this success has not yet been developed, as it is unclear how these models encode the combinatorial rules that govern the next-word prediction task. In this paper, we construct state space language models that can solve the next-word prediction task for languages generated from $n$-gram rules, thereby showing that the former are more expressive. Our proof shows how SSMs can encode $n$-gram rules using new theoretical results on their memorization capacity, and demonstrates how their context window can be controlled by restricting the spectrum of the state transition matrix. We conduct experiments with a small dataset generated from $n$-gram rules to show how our framework can be applied to SSMs and RNNs obtained through gradient-based optimization",
    "checked": false,
    "id": "7417f274a7740b2f608ae2beccb857096eb71afb",
    "semantic_title": "state space models can express n-gram languages",
    "citation_count": 1,
    "authors": [
      "Vinoth Nandakumar",
      "Qiang Qu",
      "Peng Mi",
      "Tongliang Liu"
    ]
  },
  "https://openreview.net/forum?id=VxC4PZ71Ym": {
    "title": "Unlearning Personal Data from a Single Image",
    "volume": "main",
    "abstract": "Machine unlearning aims to erase data from a model as if the latter never saw them during training. While existing approaches unlearn information from complete or partial access to the training data, this access can be limited over time due to privacy regulations. Currently, no setting or benchmark exists to probe the effectiveness of unlearning methods in such scenarios. To fill this gap, we propose a novel task we call One-Shot Unlearning of Personal Identities (1-SHUI) that evaluates unlearning models when the training data is not available. We focus on unlearning identity data, which is specifically relevant due to current regulations requiring personal data deletion after training. To cope with data absence, we expect users to provide a portraiting picture to aid unlearning. We design requests on CelebA, CelebA-HQ, and MUFAC with different unlearning set sizes to evaluate applicable methods in 1-SHUI. Moreover, we propose MetaUnlearn, an effective method that meta-learns to forget identities from a single image. Our findings indicate that existing approaches struggle when data availability is limited, especially when there is a dissimilarity between the provided samples and the training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas De Min",
      "Massimiliano Mancini",
      "Stéphane Lathuilière",
      "Subhankar Roy",
      "Elisa Ricci"
    ]
  },
  "https://openreview.net/forum?id=pF2ukh7HxA": {
    "title": "FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness",
    "volume": "main",
    "abstract": "Optimizing deep learning algorithms currently requires slow, manual derivation, potentially leaving much performance untapped. Methods like FlashAttention have achieved a x6 performance improvement over native PyTorch by avoiding unnecessary data transfers, but required three iterations over three years to be developed. Automated compiled methods have consistently lagged behind. This paper extends Neural Circuit Diagrams for deep learning models to consider resource usage and the distribution of tasks across a GPU hierarchy. We show how diagrams can use simple relabellings to derive high-level streaming and tiling optimization strategies along with performance models. We show how this high-level performance model allows the effects of quantization and multi-level GPU hierarchies to be readily considered. We develop a methodology for representing intermediate-level pseudocode with diagrams, allowing hardware-aware algorithms to be derived step-by-step. Finally, we show how our methodology can be used to better understand existing techniques like FlashAttention. This work uses a theoretical framework to link assumptions about GPU behaviour to claims about performance. We aim to lay the groundwork for a scientific approach to GPU optimization where experiments can address clear hypotheses rather than post-hoc rationalizations",
    "checked": true,
    "id": "724431c5c3fbc8566802a5b5d8e93cb1720a9a5c",
    "semantic_title": "flashattention on a napkin: a diagrammatic approach to deep learning io-awareness",
    "citation_count": 2,
    "authors": [
      "Vincent Abbott",
      "Gioele Zardini"
    ]
  },
  "https://openreview.net/forum?id=EEeVYfXor5": {
    "title": "Out of Spuriousity: Improving Robustness to Spurious Correlations without Group Annotations",
    "volume": "main",
    "abstract": "Machine learning models are known to learn spurious correlations, i.e., features that have strong correlations with class labels but no causal relationship. Relying on these correlations leads to poor performance in data groups that do not contain these correlations, and poor generalization. Approaches to mitigate spurious correlations either rely on the availability of group annotations or require access to different model checkpoints to approximate these group annotations. We propose PruSC, a method for extracting a spurious-free subnetwork from a dense network. PruSC does not require prior knowledge of the spurious correlations and is able to mitigate the effect of multiple spurious attributes. Specifically, we observe that ERM training leads to clusters in representation space that are induced by spurious correlations. We then define a supervised contrastive loss to extract a subnetwork that distorts such clusters, forcing the model to learn only class-specific clusters, rather than attribute-class specific clusters. Our method outperforms all annotation-free methods, achieves worst-group accuracy competitive with methods that require annotations and can mitigate the effect of multiple spurious correlations. Our results show that in a fully trained dense network, there exists a subnetwork that uses only invariant features in classification tasks, thereby eliminating the influence of spurious features",
    "checked": true,
    "id": "472870875e5140d96e70b924c3bd54b18d117025",
    "semantic_title": "out of spuriousity: improving robustness to spurious correlations without group annotations",
    "citation_count": 2,
    "authors": [
      "Phuong Quynh Le",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  "https://openreview.net/forum?id=5PPbvCExZs": {
    "title": "No Need for Ad-hoc Substitutes: The Expected Cost is a Principled All-purpose Classification Metric",
    "volume": "main",
    "abstract": "The expected cost (EC) is one of the main classification metrics introduced in statistical and machine learning books. It is based on the assumption that, for a given application of interest, each decision made by the system has a corresponding cost which depends on the true class of the sample. An evaluation metric can then be defined by taking the expectation of the cost over the data. Two special cases of the EC are widely used in the machine learning literature: the error rate (one minus the accuracy) and the balanced error rate (one minus the balanced accuracy or unweighted average recall). Other instances of the EC can be useful for applications in which some types of errors are more severe than others, or when the prior probabilities of the classes differ between the evaluation data and the use-case scenario. Surprisingly, the general form for the EC is rarely used in the machine learning literature. Instead, alternative ad-hoc metrics like the F-beta score and the Matthews correlation coefficient (MCC) are used for many applications. In this work, we argue that the EC is superior to these alternative metrics, being more general, interpretable, and adaptable to any application scenario. We provide both theoretically-motivated discussions as well as examples to illustrate the behavior of the different metrics",
    "checked": true,
    "id": "7374b213570dba10ee5374673ff69a3b92d50739",
    "semantic_title": "no need for ad-hoc substitutes: the expected cost is a principled all-purpose classification metric",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer"
    ]
  },
  "https://openreview.net/forum?id=HOnL5hjaIt": {
    "title": "Generalized Tangent Kernel: A Unified Geometric Foundation for Natural Gradient and Standard Gradient",
    "volume": "main",
    "abstract": "Natural gradients have been widely studied from both theoretical and empirical perspectives, and it is commonly believed that natural gradients have advantages over standard (Euclidean) gradients in capturing the intrinsic geometric structure of the underlying function space and being invariant under reparameterization. However, for function optimization, a fundamental theoretical issue regarding the existence of natural gradients on the function space remains underexplored. We address this issue by providing a geometric perspective and mathematical framework for studying both natural gradient and standard gradient that is more complete than existing studies. The key tool that unifies natural gradient and standard gradient is a generalized form of the Neural Tangent Kernel (NTK), which we name the Generalized Tangent Kernel (GTK). Using a novel orthonormality property of GTK, we show that for a fixed parameterization, GTK determines a Riemannian metric on the entire function space which makes the standard gradient as \"natural\" as the natural gradient in capturing the intrinsic structure of the parameterized function space. Many aspects of this approach relate to RKHS theory. For the practical side of this theory paper, we showcase that our framework motivates new solutions to the non-immersion/degenerate case of natural gradient and leads to new families of natural/standard gradient descent methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Wei Xu"
    ]
  },
  "https://openreview.net/forum?id=Yk7GUlJwGa": {
    "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
    "volume": "main",
    "abstract": "We introduce a novel approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model's focus toward regions with higher geometric complexity. This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks. Our method also presents a feature-level knowledge distillation technique designed to guide the prediction of geometric complexity, which utilizes a comprehensive context from feature-level information. Extensive experiments confirm our method's superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, segmentation, and few-shot tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David OSOWIECHI",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openreview.net/forum?id=FoQK84nwY3": {
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "volume": "main",
    "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Han Zhong",
      "Zhihan Liu",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan Awadalla",
      "Zhaoran Wang"
    ]
  },
  "https://openreview.net/forum?id=RXoSmiyObR": {
    "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
    "volume": "main",
    "abstract": "Counterfactual fairness is a fundamental principle in machine learning that allows the analysis of the effects of sensitive attributes in each individual decision by integrating the knowledge of causal graphs. An issue in dealing with counterfactual fairness is that unfair causal effects are often context-specific, influenced by religious, cultural, and national differences, making it difficult to create a universally applicable model. This leads to the challenge of dealing with frequent adaptation to changes in fairness assessments when localizing a model. Thus, applicability across a variety of models and efficiency becomes necessary to meet this challenge. We propose the first efficient post-process approach to achieve path-specific counterfactual fairness by adjusting a model's outputs based on a given causal graph. This approach is model-agnostic, prioritizing on flexibility and generalizability to deliver robust results across various domains and model architectures. By means of the mathematical tools in cooperative game, the Möbius inversion formula and dividends, we demonstrate that our post-process approach can be executed efficiently. We empirically show that proposed algorithm outperforms existing in-process approaches for path-specific counterfactual fairness and a post-process approach for counterfactual fairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisuke Hatano",
      "Satoshi Hara",
      "Hiromi Arai"
    ]
  },
  "https://openreview.net/forum?id=03UB1MCAMr": {
    "title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning",
    "volume": "main",
    "abstract": "In recent years, Graph Neural Networks (GNNs) have become the de facto tool for learning node and graph representations. Most GNNs typically consist of a sequence of neighborhood aggregation (a.k.a., message-passing) layers, within which the representation of each node is updated based on those of its neighbors. The most expressive message-passing GNNs can be obtained through the use of the sum aggregator and of MLPs for feature transformation, thanks to their universal approximation capabilities. However, the limitations of MLPs recently motivated the introduction of another family of universal approximators, called Kolmogorov-Arnold Networks (KANs) which rely on a different representation theorem. In this work, we compare the performance of KANs against that of MLPs on graph learning tasks. We implement three new KAN-based GNN layers, inspired respectively by the GCN, GAT and GIN layers. We evaluate two different implementations of KANs using two distinct base families of functions, namely B-splines and radial basis functions. We perform extensive experiments on node classification, link prediction, graph classification and graph regression datasets. Our results indicate that KANs are on-par with or better than MLPs on all tasks studied in this paper. We also show that the size and training speed of RBF-based KANs is only marginally higher than for MLPs, making them viable alternatives. Code available at https://github.com/RomanBresson/KAGNN",
    "checked": true,
    "id": "78167a0578e995148dac629768e9495113c8babd",
    "semantic_title": "kagnns: kolmogorov-arnold networks meet graph learning",
    "citation_count": 49,
    "authors": [
      "Roman Bresson",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Michail Chatzianastasis",
      "Jun Pang",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=Xz5IcOizQ6": {
    "title": "Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "main",
    "abstract": "Continual Federated Learning (CFL) is essential for enabling real-world applications where multiple decentralized clients adaptively learn from continuous data streams. A significant challenge in CFL is mitigating catastrophic forgetting, where models lose previously acquired knowledge when learning new information. Existing approaches often face difficulties due to the constraints of device storage capacities and the heterogeneous nature of data distributions among clients. While some CFL algorithms have addressed these challenges, they frequently rely on unrealistic assumptions about the availability of task boundaries (i.e., knowing when new tasks begin). To address these limitations, we introduce Fed-A-GEM, a federated adaptation of the A-GEM method, which employs a buffer-based gradient projection approach. Fed-A-GEM alleviates catastrophic forgetting by leveraging local buffer samples and aggregated buffer gradients, thus preserving knowledge across multiple clients. Our method is combined with existing CFL techniques, enhancing their performance in the CFL context. Our experiments on standard benchmarks show consistent performance improvements across diverse scenarios. For example, in a task-incremental learning scenario using the CIFAR-100 dataset, our method can increase the accuracy by up to 27%. Our code is available at https://github.com/shenghongdai/Fed-A-GEM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghong Dai",
      "Jy-yong Sohn",
      "Yicong Chen",
      "S M Iftekharul Alam",
      "Ravikumar Balakrishnan",
      "Suman Banerjee",
      "Nageen Himayat",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=38cwP8xVxD": {
    "title": "The 2024 Foundation Model Transparency Index",
    "volume": "main",
    "abstract": "Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, we conduct a follow-up study (v1.1) after 6 months: we score 14 developers against the same 100 indicators. While in v1.0 we searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. We find that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. We observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. We publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to us via developers. Our findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=dczXe0S1oL": {
    "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
    "volume": "main",
    "abstract": "Many real-world applications require machine-learning models to be able to deal with non-stationary data distributions and thus learn autonomously over an extended period of time, often in an online setting. One of the main challenges in this scenario is the so-called catastrophic forgetting (CF) for which the learning model tends to focus on the most recent tasks while experiencing predictive degradation on older ones. In the online setting, the most effective solutions employ a fixed-size memory buffer to store old samples used for replay when training on new tasks. Many approaches have been presented to tackle this problem and conflicting strategies are proposed to populate the memory. Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? Furthermore, it is not clear how predictive uncertainty information for memory management can be leveraged in the most effective manner. Starting from the intuition that predictive uncertainty provides an idea of the samples' location in the decision space, this work presents an in-depth analysis of different uncertainty estimates and strategies for populating the memory. The investigation provides a better understanding of the characteristics data points should have for alleviating CF. Then, we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood. Finally, we demonstrate that the use of predictive uncertainty measures helps in reducing CF in different settings",
    "checked": true,
    "id": "05a91c7f3dfe394ffc0501f6ee8ce2747c752043",
    "semantic_title": "how to leverage predictive uncertainty estimates for reducing catastrophic forgetting in online continual learning",
    "citation_count": 2,
    "authors": [
      "Giuseppe Serra",
      "Ben Werner",
      "Florian Buettner"
    ]
  },
  "https://openreview.net/forum?id=ZInwrlkQ3f": {
    "title": "An elementary concentration bound for Gibbs measures arising in statistical learning theory",
    "volume": "main",
    "abstract": "We present an elementary concentration bound for Gibbs measures whose log-likelihood is a function of the empirical risk. This bound controls the distance between samples from the (random) Gibbs measure and the minimizers of the population risk function. This bound is a generalization of a recent inequality developed by Ramsay et al., 2024. As a corollary, we obtain sample complexity bounds and bounds on the inverse temperature so that the samples are within a prescribed error of the population value. The latter bound on the inverse temperature is essentially sharp. We demonstrate our work on three canonical classes of examples: classification of two component mixture models, robust regression, and spiked matrix and tensor models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Ramsay",
      "Aukosh Jagannath",
      "Shojaeddin Chenouri"
    ]
  },
  "https://openreview.net/forum?id=tSFpsfndE7": {
    "title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation",
    "volume": "main",
    "abstract": "Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Bernecker",
      "Ghalia Rehawi",
      "Francesco Paolo Casale",
      "Janine Knauer-Arloth",
      "Annalisa Marsico"
    ]
  },
  "https://openreview.net/forum?id=N28FdYO2sH": {
    "title": "Learning Linear Polytree Structural Equation Model",
    "volume": "main",
    "abstract": "We are interested in the problem of learning the directed acyclic graph (DAG) when data are generated from a linear structural equation model (SEM) and the causal structure can be characterized by a polytree. Under the Gaussian polytree models, we study sufficient conditions on the sample sizes for the well-known Chow-Liu algorithm to exactly recover both the skeleton and the equivalence class of the polytree, which is uniquely represented by a CPDAG. On the other hand, necessary conditions on the required sample sizes for both skeleton and CPDAG recovery are also derived in terms of information-theoretic lower bounds, which match the respective sufficient conditions and thereby give a sharp characterization of the difficulty of these tasks. We also consider the problem of inverse correlation matrix estimation under the linear polytree models, and establish the estimation error bound in terms of the dimension and the total number of v-structures. We also consider an extension of group linear polytree models, in which each node represents a group of variables. Our theoretical findings are illustrated by comprehensive numerical simulations, and experiments on benchmark data also demonstrate the robustness of polytree learning when the true graphical structures can only be approximated by polytrees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingmei Lou",
      "Yu Hu",
      "Xiaodong Li"
    ]
  },
  "https://openreview.net/forum?id=6nBIweDYzZ": {
    "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators",
    "volume": "main",
    "abstract": "Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision signals. Recently, pre-trained Large Language Models (LLMs) -- even those at the scale of GPT-2 -- have demonstrated great potential in synthesizing tabular data. However, their applications under DP constraints remain largely unexplored. In this work, we address this gap by applying DP techniques to the generation of synthetic tabular data. Our findings shows that LLMs face difficulties in generating coherent text when fine-tuned with DP, as privacy budgets are inefficiently allocated to non-private elements like table structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning framework for differentially private tabular data generation. The first stage involves non-private fine-tuning on a pseudo dataset, followed by DP fine-tuning on a private dataset. Our empirical results show that this approach improves performance across various settings and metrics compared to directly fine-tuned LLMs in DP contexts. We release our code and setup at https://github.com/tejuafonja/DP-2Stage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejumade Afonja",
      "Hui-Po Wang",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=BPDVZajOW5": {
    "title": "Optimizing Estimators of Squared Calibration Errors in Classification",
    "volume": "main",
    "abstract": "In this work, we propose a mean-squared error-based risk that enables the comparison and optimization of estimators of squared calibration errors in practical settings. Improving the calibration of classifiers is crucial for enhancing the trustworthiness and interpretability of machine learning models, especially in sensitive decision-making scenarios. Although various calibration (error) estimators exist in the current literature, there is a lack of guidance on selecting the appropriate estimator and tuning its hyperparameters. By leveraging the bilinear structure of squared calibration errors, we reformulate calibration estimation as a regression problem with independent and identically distributed (i.i.d.) input pairs. This reformulation allows us to quantify the performance of different estimators even for the most challenging calibration criterion, known as canonical calibration. Our approach advocates for a training-validation-testing pipeline when estimating a calibration error on an evaluation dataset. We demonstrate the effectiveness of our pipeline by optimizing existing calibration estimators and comparing them with novel kernel ridge regression-based estimators on standard image classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gregor Gruber",
      "Francis R. Bach"
    ]
  },
  "https://openreview.net/forum?id=ZdMIXltJzK": {
    "title": "Reset-free Reinforcement Learning with World Models",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) is an appealing paradigm for training intelligent agents, enabling policy acquisition from the agent's own autonomously acquired experience. However, the training process of RL is far from automatic, requiring extensive human effort to reset the agent and environments. To tackle the challenging reset-free setting, we first demonstrate the superiority of model-based (MB) RL methods in such setting, showing that a straightforward adaptation of MBRL can outperform all the prior state-of-the-art methods while requiring less supervision. We then identify limitations inherent to this direct extension and propose a solution called model-based reset-free (MoReFree) agent, which further enhances the performance. MoReFree adapts two key mechanisms, exploration and policy learning, to handle reset-free tasks by prioritizing task-relevant states. It exhibits superior data-efficiency across various reset-free tasks without access to environmental reward or demonstrations while significantly outperforming privileged baselines that require supervision. Our findings suggest model-based methods hold significant promise for reducing human effort in RL. Website: https://yangzhao-666.github.io/morefree",
    "checked": true,
    "id": "783045d6f5577c715b9096a798b0e13b27d98033",
    "semantic_title": "reset-free reinforcement learning with world models",
    "citation_count": 0,
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat",
      "Edward S. Hu"
    ]
  },
  "https://openreview.net/forum?id=QIzRdjIWnS": {
    "title": "Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance",
    "volume": "main",
    "abstract": "This paper provides the first tight convergence analyses for RMSProp and Adam for non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. RMSProp is firstly analyzed, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to the dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and the first-order momentum. We develop a new upper bound on the first-order term in the descent lemma, which is also a function of the gradient norm. We show that Adam with proper hyperparameters converges to an $\\epsilon$-stationary point with an iteration complexity of $\\mathcal O(\\epsilon^{-4})$. Our complexity results for both RMSProp and Adam match with the complexity lower bound established in Arjevani et al. (2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yi Zhou",
      "Shaofeng Zou"
    ]
  },
  "https://openreview.net/forum?id=UV58hNygne": {
    "title": "HoSNNs: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
    "volume": "main",
    "abstract": "While spiking neural networks (SNNs) offer a promising neurally-inspired model of computation, they are vulnerable to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to design a threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model and utilize TA-LIF neurons to construct the adversarially robust homeostatic SNNs (HoSNNs) for improved robustness. The TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, offering a local feedback control solution to the minimization of each neuron's membrane potential error caused by adversarial disturbance. Theoretical analysis demonstrates favorable dynamic properties of TA-LIF neurons in terms of the bounded-input bounded-output stability and suppressed time growth of membrane potential error, underscoring their superior robustness compared with the standard LIF neurons. When trained with weak FGSM attacks (\\(\\epsilon = 2/255\\)), our HoSNNs significantly outperform conventionally trained LIF-based SNNs across multiple datasets. Furthermore, under significantly stronger PGD7 attacks (\\(\\epsilon = 8/255\\)), HoSNN achieves notable improvements in accuracy, increasing from 30.90% to 74.91% on FashionMNIST, 0.44% to 36.82% on SVHN, 0.54% to 43.33% on CIFAR10, and 0.04% to 16.66% on CIFAR100",
    "checked": false,
    "id": "9025ffd1fd220cbbf4a91aa758662d17716dd345",
    "semantic_title": "hosnn: adversarially-robust homeostatic spiking neural networks with adaptive firing thresholds",
    "citation_count": 3,
    "authors": [
      "Hejia Geng",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=8Q4qxe9a9Z": {
    "title": "A Self-Explainable Heterogeneous GNN for Relational Deep Learning",
    "volume": "main",
    "abstract": "Recently, significant attention has been given to the idea of viewing relational databases as heterogeneous graphs, enabling the application of graph neural network (GNN) technology for predictive tasks. However, existing GNN methods struggle with the complexity of the heterogeneous graphs induced by databases with numerous tables and relations. Traditional approaches either consider all possible relational meta-paths, thus failing to scale with the number of relations, or rely on domain experts to identify relevant meta-paths. A recent solution does manage to learn informative meta-paths without expert supervision, but assumes that a node's class depends solely on the existence of a meta-path occurrence. In this work, we present a self-explainable heterogeneous GNN for relational data, that supports models in which class membership depends on aggregate information obtained from multiple occurrences of a meta-path. Experimental results show that in the context of relational databases, our approach effectively identifies informative meta-paths that faithfully capture the model's reasoning mechanisms. It significantly outperforms existing methods in both synthetic and real-world scenarios",
    "checked": true,
    "id": "c253896381b6763981e83dc67a726b6e5e1b6d8f",
    "semantic_title": "a self-explainable heterogeneous gnn for relational deep learning",
    "citation_count": 1,
    "authors": [
      "Francesco Ferrini",
      "Antonio Longa",
      "Andrea Passerini",
      "Manfred Jaeger"
    ]
  },
  "https://openreview.net/forum?id=9NVJ0ZgEfT": {
    "title": "Long Short-Term Imputer: Handling Consecutive Missing Values in Time Series",
    "volume": "main",
    "abstract": "Encountered frequently in time series data, missing values can significantly impede time-series analysis. With the progression of deep learning, advanced imputation models delve into the temporal dependencies inherent in time series data, showcasing remarkable performance. This positions them as intuitive selections for time series imputation tasks which assume ``Miss Completely at Random''. Nonetheless, long-interval consecutive missing values may obstruct the model's ability to grasp long-term temporal dependencies, consequently hampering the efficacy of imputation performance. To tackle this challenge, we propose Long Short-term Imputer (LSTI) to impute consecutive missing values with different length of intervals. Long-term Imputer is designed using the idea of bi-directional autoregression. A forward prediction model and a backward prediction model are trained with a consistency regularization, which is designed to capture long-time dependency and can adapt to long-interval consecutive missing values. Short-term Imputer is designed to capture short-time dependency and can impute the short-interval consecutive missing values effectively. A meta-weighting network is then proposed to take advantage of the strengths of two imputers. As a result, LSTI can impute consecutive missing values with different intervals effectively. Experiments demonstrate that our approach, on average, reduces the error by 57.4% compared to state-of-the-art deep models across five datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng You",
      "Xinyang Chen",
      "Yu Sun",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openreview.net/forum?id=58gPkcVbFL": {
    "title": "Evolution of Discriminator and Generator Gradients in GAN Training: From Fitting to Collapse",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) are powerful generative models but often suffer from mode mixture and mode collapse. We propose a perspective that views GAN training as a two-phase progression from fitting to collapse, where mode mixture and mode collapse are treated as inter-connected. Inspired by the particle model interpretation of GANs, we leverage the discriminator gradient to analyze particle movement and the generator gradient, specifically \"steepness,\" to quantify the severity of mode mixture by measuring the generator's sensitivity to changes in the latent space. Using these theoretical insights into evolution of gradients, we design a specialized metric that integrates both gradients to detect the transition from fitting to collapse. This metric forms the basis of an early stopping algorithm, which stops training at a point that retains sample quality and diversity. Experiments on synthetic and real-world datasets, including MNIST, Fashion MNIST, and CIFAR-10, validate our theoretical findings and demonstrate the effectiveness of the proposed algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ]
  },
  "https://openreview.net/forum?id=GkYOcbNLaW": {
    "title": "Cycle Conditioning for Robust Representation Learning from Categorical Data",
    "volume": "main",
    "abstract": "This paper introduces a novel diffusion-based method for learning representations from categorical data. Conditional diffusion models have demonstrated their potential to extract meaningful representations from input samples. However, they often struggle to yield versatile, general-purpose information, limiting their adaptability to unforeseen tasks. To address this, we propose a cycle conditioning approach for diffusion models, designed to capture expressive information from conditioning samples. However, cycle conditioning alone can be insufficient. Diffusion models may ignore conditioning samples that vary across training iterations, an issue that occurs within cycle conditioning. To counter this limitation, we introduce additional \"spelling\" information to guide the conditioning process, ensuring that the conditioning sample remains influential during denoising. While this supervision enhances the generalizability of extracted representations, it is constrained by the sparse nature of spelling information in categorical data, leading to sparse latent conditions. This sparsity reduces the robustness of the extracted representations for downstream tasks or as effective guidance in the diffusion process. To overcome this challenge, we propose a linear navigation strategy within the latent space of conditioning samples, allowing dense representations to be extracted even with sparse supervision. Our experiments demonstrate that our method achieves at least a 1.42\\% improvement in AUROC and a 4.12\\% improvement in AUCPR over the best results from existing state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Tabejamaat",
      "Farzaneh Etminani",
      "Mattias Ohlsson"
    ]
  },
  "https://openreview.net/forum?id=CrKMqRAhBo": {
    "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems",
    "volume": "main",
    "abstract": "Using AI to write formal proofs for mathematical problems is a challenging task that has seen some advancements in recent years. Automated systems such as Lean can verify the correctness of proofs written in formal language, yet writing the proofs in formal language can be challenging for humans and machines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal proofs are available only for 6 of these problems (3 of which are only written by mathematicians). The model with best accuracy can only prove 2 of these 20 IMO problems, from 1950s and 60s, while its training set is a secret. In this work, we write complete, original formal proofs for the remaining IMO problems in Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands the availability of proof currently in the public domain by creating 5,880 lines of Lean proof. The goal of the paper is to pave the way for developing AI models that can automatically write the formal proofs for all the IMO problems in miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we devise a method to decompose the proofs of these problems into their building blocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean code. These lemmas are not trivial, yet they are approachable, providing the opportunity to evaluate and diagnose the failures and successes of AI models. We evaluate the ability of the SOTA LLMs on our dataset and analyze their success and failure modes from different perspectives. Our dataset and code is available at: https://github.com/roozbeh-yz/IMO-Steps",
    "checked": true,
    "id": "616739238dbd02e4e748025a2d52044ad7865d36",
    "semantic_title": "a lean dataset for international math olympiad: small steps towards writing math proofs for hard problems",
    "citation_count": 2,
    "authors": [
      "Roozbeh Yousefzadeh",
      "Xuenan Cao"
    ]
  },
  "https://openreview.net/forum?id=HkmymFPODz": {
    "title": "Deep Active Learning in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9708fd28cd47d29bb5787c2fc0ec9714b17683c8",
    "semantic_title": "deep active learning in the open world",
    "citation_count": 2,
    "authors": [
      "Tian Xie",
      "Jifan Zhang",
      "Haoyue Bai",
      "Robert D Nowak"
    ]
  },
  "https://openreview.net/forum?id=J7cY9Jr9WM": {
    "title": "A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amadou Siaka SANGARE",
      "Nicolas Dunou",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=bqMJToTkvT": {
    "title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Kong",
      "Hangyu Mao",
      "Zhao Qi",
      "Bin Zhang",
      "Jingqing Ruan",
      "Li Shen",
      "Yongzhe Chang",
      "Xueqian Wang",
      "Rui Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=jRbKsQ3sYO": {
    "title": "Combating Inter-Task Confusion and Catastrophic Forgetting by Metric Learning and Re-Using a Past Trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayedmoslem Shokrolahi",
      "IL MIN KIM"
    ]
  },
  "https://openreview.net/forum?id=prVLANCshF": {
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
    "volume": "main",
    "abstract": "This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at https://github.com/UCSC-VLAA/AttnGCG-attack",
    "checked": true,
    "id": "0d57a75e910ff4161da920f49a99ae703f947c61",
    "semantic_title": "attngcg: enhancing jailbreaking attacks on llms with attention manipulation",
    "citation_count": 8,
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Jieru Mei",
      "Bingchen Zhao",
      "Yisen Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=IaUh7CSD3k": {
    "title": "Metalearning Continual Learning Algorithms",
    "volume": "main",
    "abstract": "General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF), i.e., previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to metalearn their own in-context continual (meta)learning algorithms. ACL encodes continual learning (CL) desiderata---good performance on both old and new tasks---into its metalearning objectives. Our experiments demonstrate that ACL effectively resolves \"in-context catastrophic forgetting,\" a problem that naive in-context learning algorithms suffer from; ACL learned algorithms outperform both hand-crafted learning algorithms and popular meta-continual learning methods on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple standard image classification datasets. We also discuss the current limitations of in-context CL by comparing ACL with state-of-the-art CL methods that leverage pre-trained models. Overall, we bring several novel perspectives into the long-standing problem of CL",
    "checked": true,
    "id": "087bca53cc05ccdba9b481cbfb11569ca80f32f4",
    "semantic_title": "metalearning continual learning algorithms",
    "citation_count": 3,
    "authors": [
      "Kazuki Irie",
      "Róbert Csordás",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=CAkt3DsAZs": {
    "title": "Meta-Learning for Graphs with Heterogeneous Node Attribute Spaces for Few-Shot Edge Predictions",
    "volume": "main",
    "abstract": "Prediction of edges between nodes in graph data is useful for many applications, such as social network analysis and knowledge graph completion. Existing graph neural network-based approaches have achieved notable advancements, but encounter significant difficulty in building an effective model when there is an insufficient number of known edges in graphs. Although some meta-learning approaches were introduced to solve this problem, having an assumption that the nodes of training graphs and test graphs are in homogeneous attribute spaces, which limits the flexibility of applications. In this paper, we proposed a meta-learning method for edge prediction that can learn from graphs with nodes in heterogeneous attribute spaces. The proposed model consists of attribute-wise message-passing networks that transform information between connected nodes for each attribute, resulting in attribute-specific node embeddings. The node embeddings are obtained by calculating the mean of the attribute-specific node embeddings.The encoding operation can be repeated multiple times to capture complex patterns. The attribute-wise message-passing networks are shared across all graphs, allowing knowledge transfer between different graphs.The probabilities of edges are estimated by the Euclidian distance between node embeddings. Experimental results on 14 real-world data sets demonstrate that the proposed method outperforms existing methods in edge prediction problems with sparse edge information",
    "checked": true,
    "id": "3eeb187ff516fb8b1dd6b13fd79ec2f0382bf4b9",
    "semantic_title": "meta-learning for graphs with heterogeneous node attribute spaces for few-shot edge predictions",
    "citation_count": 0,
    "authors": [
      "Zhong Chuang",
      "Yusuke Tanaka",
      "Tomoharu Iwata"
    ]
  },
  "https://openreview.net/forum?id=jJOVpnNrEp": {
    "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
    "volume": "main",
    "abstract": "Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minttu Alakuijala",
      "Reginald McLean",
      "Isaac Woungang",
      "Nariman Farsad",
      "Samuel Kaski",
      "Pekka Marttinen",
      "Kai Yuan"
    ]
  },
  "https://openreview.net/forum?id=2Zan4ATYsh": {
    "title": "DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization",
    "volume": "main",
    "abstract": "Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL",
    "checked": true,
    "id": "d410f37fcb58d4b2712aeffc02a876a0e1d04aaa",
    "semantic_title": "divil: unveiling and addressing over-invariance for out-of- distribution generalization",
    "citation_count": 1,
    "authors": [
      "Jiaqi WANG",
      "Yuhang Zhou",
      "Zhixiong Zhang",
      "Qiguang Chen",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=k4AxEwTaHq": {
    "title": "FaAlGrad: Fairness through Alignment of Gradients across Different Subpopulations",
    "volume": "main",
    "abstract": "The growing deployment of Machine Learning systems has increased interest in systems optimized for other important criteria along with the expected task performance. For instance, machine learning models often exhibit biases that lead to unfair outcomes for certain protected subpopulations. This work aims to handle the bias in machine learning models and enhance their fairness by aligning the loss gradients. Specifically, leveraging the meta-learning technique, we propose a novel training framework that aligns the gradients computed across different subpopulations for learning fair classifiers. Aligning the gradients enables our framework to regularize the training process, thereby prioritizing fairness over predictive accuracy. Our experiments on multiple benchmark datasets demonstrate significant improvements in fairness metrics without having any exclusive regularizers for fairness. Thus our work contributes to developing fairer machine learning models with broader societal benefits",
    "checked": true,
    "id": "3980bd94e9e1f54c0366a4ef003b4866cb2b44f0",
    "semantic_title": "faalgrad: fairness through alignment of gradients across different subpopulations",
    "citation_count": 1,
    "authors": [
      "Nikita Malik",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=xXs2GKXPnH": {
    "title": "Faster Diffusion Through Temporal Attention Decomposition",
    "volume": "main",
    "abstract": "We explore the role of the attention mechanism during inference in text-conditional diffusion models. Empirical observations suggest that cross-attention outputs converge to a fixed point after several inference steps. The convergence time naturally divides the entire inference process into two phases: an initial phase for planning text-oriented visual semantics, which are then translated into images in a subsequent fidelity-improving phase. Cross-attention is essential in the initial phase but almost irrelevant thereafter. Self-attention, however, initially plays a minor role but becomes increasingly important in the second phase. These findings yield a simple and training-free method called TGATE which efficiently generates images by caching and reusing attention outputs at scheduled time steps. Experiments show TGATE's broad applicability to various existing text-conditional diffusion models which it speeds up by 10-50%. The code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Wentian Zhang",
      "Jinheng Xie",
      "Francesco Faccio",
      "Mengmeng Xu",
      "Tao Xiang",
      "Mike Zheng Shou",
      "Juan-Manuel Perez-Rua",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=Za9Tm07fig": {
    "title": "TACO Vision Models Can Be Efficiently Specialized via Few-Shot Task-Aware Compression",
    "volume": "main",
    "abstract": "Recent vision architectures and self-supervised training methods have enabled training computer vision models that are extremely accurate, but come with massive computational costs. In settings such as identifying species in camera traps in the field, users have limited resources, and may fine-tune a pretrained model on (often limited) data from a small set of specific categories of interest. Such users may still wish to make use of highly-accurate large models, but are often constrained by the computational cost. To address this, we ask: can we quickly compress generalist models into accurate and efficient specialists given a small amount of data? Towards this goal, we propose a simple and versatile technique, which we call Few-Shot Task-Aware COmpression (TACO). Given a general-purpose model pretrained on a broad task, such as classification on ImageNet or iNaturalist datasets with thousands of categories, TACO produces a much smaller model that is accurate on specialized tasks, such as classifying across vehicle types or animal species, based only on a few examples from each target class. The method is based on two key insights - 1) a powerful specialization effect for data-aware compression, which we showcase for the first time; 2) a dedicated finetuning procedure with knowledge distillation, which prevents overfitting even in scenarios where data is very scarce. Specifically, TACO is applied in few-shot fashion, i.e. only a few task-specific samples are used for compression, and the procedure has low computational overhead. We validate this approach experimentally using highly-accurate ResNet, ViT/DeiT, and ConvNeXt models, originally trained on ImageNet and iNaturalist datasets, which we specialize and compress to a diverse set of ``downstream'' subtasks, with notable computational speedups on both CPU and GPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kuznedelev",
      "Soroush Tabesh",
      "Kimia Noorbakhsh",
      "Elias Frantar",
      "Sara Beery",
      "Eldar Kurtic",
      "Dan Alistarh"
    ]
  },
  "https://openreview.net/forum?id=XPREcQlAM0": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "main",
    "abstract": "In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-homogeneous, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan V. Truong"
    ]
  },
  "https://openreview.net/forum?id=ZckLMG00sO": {
    "title": "Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators",
    "volume": "main",
    "abstract": "Machine learning force fields (MLFFs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations, limiting their ability to model phenomena occurring over longer timescales and compromising the quality of estimated observables. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which leverages joint supervision from reference quantum-mechanical calculations and system observables. StABlE Training iteratively runs many MD simulations in parallel to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. We achieve efficient end-to-end automatic differentiation through MD simulations using our Boltzmann Estimator, a generalization of implicit differentiation techniques to a broader class of stochastic algorithms. Unlike existing techniques based on active learning, our approach requires no additional ab-initio energy and forces calculations to correct instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, using three modern MLFF architectures. StABlE-trained models achieve significant improvements in simulation stability, data efficiency, and agreement with reference observables. Crucially, the stability improvements cannot be matched by simply reducing the simulation timestep, meaning that StABlE Training effectively allows for larger timesteps in MD simulations. By incorporating observables into the training process alongside first-principles calculations, StABlE Training can be viewed as a general semi-empirical framework applicable across MLFF architectures and systems. This makes it a powerful tool for training stable and accurate MLFFs, particularly in the absence of large reference datasets. Our code is publicly available at https://github.com/ASK-Berkeley/StABlE-Training",
    "checked": true,
    "id": "93d6ac8afaf3ff7832e0011511aacc15a980e6e4",
    "semantic_title": "stability-aware training of machine learning force fields with differentiable boltzmann estimators",
    "citation_count": 4,
    "authors": [
      "Sanjeev Raja",
      "Ishan Amin",
      "Fabian Pedregosa",
      "Aditi S. Krishnapriyan"
    ]
  },
  "https://openreview.net/forum?id=6jTQrr3APY": {
    "title": "Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA",
    "volume": "main",
    "abstract": "In this paper we propose a new iterative algorithm to solve the fair PCA (FPCA) problem. We start with the max-min fair PCA formulation originally proposed in \\cite{samadi1} and derive a simple and efficient iterative algorithm which is based on the minorization-maximization (MM) approach. The proposed algorithm relies on the relaxation of a semi-orthogonality constraint which is proved to be tight at every iteration of the algorithm. The vanilla version of the proposed algorithm requires solving a semi-definite program (SDP) at every iteration, which can be further simplified to a quadratic program by formulating the dual of the surrogate maximization problem. We also propose two important reformulations of the fair PCA problem: a) fair robust PCA - which can handle outliers in the data, and b) fair sparse PCA - which can enforce sparsity on the estimated fair principal components. The proposed algorithms are computationally efficient and monotonically increase their respective design objectives at every iteration. An added feature of the proposed algorithms is that they do not require the selection of any hyperparameter (except for the fair sparse PCA case where a penalty parameter that controls the sparsity has to be chosen by the user). We numerically compare the performance of the proposed methods with two of the state-of-the-art approaches on synthetic data sets and real-life data sets",
    "checked": true,
    "id": "001301f34e79cca46f56bc203f80889fd54757df",
    "semantic_title": "fair principal component analysis (pca): minorization-maximization algorithms for fair pca, fair robust pca and fair sparse pca",
    "citation_count": 5,
    "authors": [
      "Prabhu babu",
      "Petre Stoica",
      "Astha Saini"
    ]
  },
  "https://openreview.net/forum?id=EWT4GxjGDS": {
    "title": "Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems",
    "volume": "main",
    "abstract": "Online platforms such as YouTube, Instagram heavily rely on recommender systems to decide what content to present to users. Producers, in turn, often create content that is likely to be recommended to users and have users engage with it. To do so, producers try to align their content with the preferences of their targeted user base. In this work, we explore the equilibrium behavior of producers who are interested in maximizing user engagement. We study two variants of the content-serving rule for the platform's recommender system, and provide a structural characterization of producer behavior at equilibrium: namely, each producer chooses to focus on a single embedded feature. We further show that specialization, defined as different producers optimizing for distinct types of content, naturally emerges from the competition among producers trying to maximize user engagement. We provide a heuristic for computing equilibria of our engagement game, and evaluate it experimentally. We highlight i) the performance and convergence of our heuristic, ii) the degree of producer specialization, and iii) the impact of the content-serving rule on producer and user utilities at equilibrium and provide guidance on how to set the content-serving rule",
    "checked": true,
    "id": "647a71fe805d06f96f510a2653aa96edb459c849",
    "semantic_title": "producers equilibria and dynamics in engagement-driven recommender systems",
    "citation_count": 3,
    "authors": [
      "Krishna Acharya",
      "Juba Ziani",
      "Jingyan Wang",
      "Varun Vangala"
    ]
  },
  "https://openreview.net/forum?id=dvRysCqmYQ": {
    "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a robust framework for various generative tasks, including tabular data synthesis. However, current tabular diffusion models tend to inherit bias in the training dataset and generate biased synthetic data, which may influence discriminatory actions. In this research, we introduce a novel tabular diffusion model that incorporates sensitive guidance to generate fair synthetic data with balanced joint distributions of the target label and sensitive attributes, such as sex and race. The empirical results demonstrate that our method effectively mitigates bias in training data while maintaining the quality of the generated samples. Furthermore, we provide evidence that our approach outperforms existing methods for synthesizing tabular data on fairness metrics such as demographic parity ratio and equalized odds ratio, achieving improvements of over $10\\%$. Our implementation is available at https://github.com/comp-well-org/fair-tab-diffusion",
    "checked": true,
    "id": "3e8add053702c5b12a337a16af9a81631e310082",
    "semantic_title": "balanced mixed-type tabular data synthesis with diffusion models",
    "citation_count": 9,
    "authors": [
      "Zeyu Yang",
      "Han Yu",
      "Peikun Guo",
      "Khadija Zanna",
      "Xiaoxue Yang",
      "Akane Sano"
    ]
  },
  "https://openreview.net/forum?id=zSK81A2hxQ": {
    "title": "A Neural Material Point Method for Particle-based Emulation",
    "volume": "main",
    "abstract": "Mesh-free Lagrangian methods are widely used for simulating fluids, solids, and their complex interactions due to their ability to handle large deformations and topological changes. These physics simulators, however, require substantial computational resources for accurate simulations. To address these issues, deep learning emulators promise faster and scalable simulations, yet they often remain expensive and difficult to train, limiting their practical use. Inspired by the Material Point Method (MPM), we present NeuralMPM, a neural framework for particle-based emulation. NeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes updates on grid nodes using image-to-image neural networks, and interpolates back to the particles. Similarly to MPM, NeuralMPM benefits from the regular voxelized representation to simplify the computation of the state dynamics, while avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the advantages of NeuralMPM on 6 datasets, including fluid dynamics and fluid-solid interactions simulated with MPM and Smoothed Particles Hydrodynamics (SPH). Compared to GNS and DMCF, NeuralMPM reduces training time from 10 days to 15 hours, memory consumption by 10x-100x, and increases inference speed by 5x-10x, while achieving comparable or superior long-term accuracy, making it a promising approach for practical forward and inverse problems. A project page is available at https://neuralmpm.isach.be/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Rochman-Sharabi",
      "Sacha Lewin",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=0RJvZY0h6O": {
    "title": "Lognormal Mutations and their Use in Detecting Surreptitious Fake Images",
    "volume": "main",
    "abstract": "In many cases, adversarial attacks against fake detectors employ algorithms specifically crafted for automatic image classifiers. These algorithms perform well, thanks to an excellent ad hoc distribution of initial attacks. However, these attacks are easily detected due to their specific initial distribution. Consequently, we explore alternative black-box attacks inspired by generic black-box optimization tools, particularly focusing on the \\lognormal{} algorithm that we successfully extend to attack fake detectors. Moreover, we demonstrate that this attack evades detection by neural networks trained to flag classical adversarial examples. Therefore, we train more general models capable of identifying a broader spectrum of attacks, including classical black-box attacks designed for images, black-box attacks driven by classical optimization, and no-box attacks. By integrating these attack detection capabilities with fake detectors, we develop more robust and effective fake detection systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Teytaud",
      "Mariia Zameshina",
      "Tom Sander",
      "Pierre Fernandez",
      "Furong Ye",
      "Laurent Najman",
      "Thomas Bäck",
      "Ismail Labiad"
    ]
  },
  "https://openreview.net/forum?id=k3Ab6RuJE9": {
    "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
    "volume": "main",
    "abstract": "Motivated by the progress of large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning (ML) models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical ML problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why an update is performed. We empirically verify the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability",
    "checked": true,
    "id": "3c4de61a9414e22d1588ceae367a3f8ef75fb60a",
    "semantic_title": "verbalized machine learning: revisiting machine learning with language models",
    "citation_count": 9,
    "authors": [
      "Tim Z. Xiao",
      "Robert Bamler",
      "Bernhard Schölkopf",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=fC4bh1PmZr": {
    "title": "Counterfactual Learning of Stochastic Policies with Continuous Actions",
    "volume": "main",
    "abstract": "Counterfactual reasoning from logged data has become increasingly important for many applications such as web advertising or healthcare. In this paper, we address the problem of learning stochastic policies with continuous actions from the viewpoint of counterfactual risk minimization (CRM). While the CRM framework is appealing and well studied for discrete actions, the continuous action case raises new challenges about modelization, optimization, and~offline model selection with real data which turns out to be particularly challenging. Our paper contributes to these three aspects of the CRM estimation pipeline. First, we introduce a modelling strategy based on a joint kernel embedding of contexts and actions, which overcomes the shortcomings of previous discretization approaches. Second, we empirically show that the optimization aspect of counterfactual learning is important, and we demonstrate the benefits of proximal point algorithms and smooth estimators. Finally, we propose an evaluation protocol for offline policies in real-world logged systems, which is challenging since policies cannot be replayed on test data, and we release a new large-scale dataset along with multiple synthetic, yet realistic, evaluation setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houssam Zenati",
      "Alberto Bietti",
      "Matthieu Martin",
      "Eustache Diemert",
      "Pierre Gaillard",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Vwgjk5ysWn": {
    "title": "Why is constrained neural language generation particularly challenging?",
    "volume": "main",
    "abstract": "Recent advances in deep neural language models combined with the capacity of large scale datasets have accelerated the development of natural language generation systems that produce fluent and coherent texts (to various degrees of success) in a multitude of tasks and application contexts. However, controlling the output of these models for desired user and task needs is still an open challenge. This is crucial not only to customizing the content and style of the generated language, but also to their safe and reliable deployment in the real world. We present an extensive survey on the emerging topic of constrained neural language generation in which we formally define and categorize the problems of natural language generation by distinguishing between conditions and constraints (the latter being testable conditions on the output text instead of the input), present constrained text generation tasks, and review existing methods and evaluation metrics for constrained text generation. Our aim is to highlight recent progress and trends in this emerging field, informing on the most promising directions and limitations towards advancing the state-of-the-art of constrained neural language generation research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Garbacea",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=K6CvWPtF62": {
    "title": "Provable Quantum Algorithm Advantage for Gaussian Process Quadrature",
    "volume": "main",
    "abstract": "The aim of this paper is to develop novel quantum algorithms for Gaussian process quadrature methods. Gaussian process quadratures are numerical integration methods where Gaussian processes are used as functional priors for the integrands to capture the uncertainty arising from the sparse function evaluations. Quantum computers have emerged as potential replacements for classical computers, offering exponential reductions in the computational complexity of machine learning tasks. In this paper, we combine Gaussian process quadrature and quantum computing by proposing a quantum low-rank Gaussian process quadrature method based on a Hilbert space approximation of the Gaussian process kernel and enhancing the quadrature using a quantum circuit. The method combines the quantum phase estimation algorithm with the quantum principal component analysis technique to extract information up to a desired rank. Then, Hadamard and SWAP tests are implemented to find the expected value and variance that determines the quadrature. We use numerical simulations of a quantum computer to demonstrate the effectiveness of the method. Furthermore, we provide a theoretical complexity analysis that shows a polynomial advantage over classical Gaussian process quadrature methods. The code is available at https://github.com/cagalvisf/Quantum_HSGPQ",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristian A. Galvis-Florez",
      "Ahmad Farooq",
      "Simo Särkkä"
    ]
  },
  "https://openreview.net/forum?id=ojeCoOKwWp": {
    "title": "Differentially Private Source-Target Clustering",
    "volume": "main",
    "abstract": "We consider a new private variant of the Source-Target Clustering (STC) setting, which was introduced by de Mathelin et al. (2022). In STC, there is a target dataset that needs to be clustered by selecting centers, in addition to centers that are already provided in a separate source dataset. The goal is to select centers from the target, such that the target clustering cost given the additional source centers is minimized. We consider private STC, in which the source dataset is private and should only be used under the constraint of differential privacy. This is motivated by scenarios in which the existing centers are private, for instance because they represent individuals in a social network. We derive lower bounds for the private STC objective, illustrating the theoretical limitations on worst-case guarantees for this setting. We then present a differentially private algorithm with asymptotically advantageous results under a data-dependent analysis, in which the guarantee depends on properties of the dataset, as well as more practical variants. We demonstrate in experiments the reduction in clustering cost that is obtained by our practical algorithms compared to baseline approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Schnapp",
      "Sivan Sabato"
    ]
  },
  "https://openreview.net/forum?id=WxHTSPS2pi": {
    "title": "Uncertainty-Based Experience Replay for Task-Agnostic Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "Model-based reinforcement learning uses a learned dynamics model to imagine actions and select those with the best expected outcomes. An experience replay buffer collects the outcomes of all actions executed in the environment, which is then used to iteratively train the dynamics model. However, as the complexity and scale of tasks increase, training times and memory requirements can grow drastically without necessarily retaining useful experiences. Continual learning proposes a more realistic scenario where tasks are learned in sequence, and the replay buffer can help mitigate catastrophic forgetting. However, it is not realistic to expect the buffer to infinitely grow as the sequence advances. Furthermore, storing every single experience executed in the environment does not necessarily provide a more accurate model. We argue that the replay buffer needs to have the minimal necessary size to retain relevant experiences that cover both common and rare states. Therefore, we propose using an uncertainty-based replay buffer filtering to enable an effective implementation of continual learning agents using model-based reinforcement learning. We show that the combination of the proposed strategies leads to reduced training times, smaller replay buffer size, and less catastrophic forgetting, all while maintaining performance",
    "checked": true,
    "id": "f69f38c22d38f913e96ac316006d8f74917c98e3",
    "semantic_title": "uncertainty-based experience replay for task-agnostic continual reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Adrian Remonda",
      "Cole Corbitt Terrell",
      "Eduardo E. Veas",
      "Marc Masana"
    ]
  },
  "https://openreview.net/forum?id=E2zKNuwNDc": {
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "volume": "main",
    "abstract": "Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, the empirical evidence suggests that DPO typically assigns implicit rewards that overfit, and trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and use distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM such that its induced implicit reward, i.e., the scaled log-likelihood ratio of the model to the reference model, matches an explicit reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO",
    "checked": true,
    "id": "5c9eec060bd9b7af1b8f71a18c0402de3dc98388",
    "semantic_title": "robust preference optimization through reward model distillation",
    "citation_count": 30,
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Peter Shaw",
      "Jonathan Berant"
    ]
  },
  "https://openreview.net/forum?id=6LO1y8ZE0F": {
    "title": "SimPLR: A Simple and Plain Transformer for Efficient Object Detection and Segmentation",
    "volume": "main",
    "abstract": "The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and pyramid designs remain a key factor for their empirical success. In this paper, we show that shifting the multiscale inductive bias into the attention mechanism can work well, resulting in a plain detector ‘SimPLR' whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple architecture, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation, as well as panoptic segmentation. Code is released at \\url{https://github.com/kienduynguyen/SimPLR}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Kien Nguyen",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openreview.net/forum?id=TnT59yz7lc": {
    "title": "Exploiting Benford's Law for Weight Regularization of Deep Neural Networks",
    "volume": "main",
    "abstract": "Stochastic learning of Deep Neural Network (DNN) parameters is highly sensitive to training strategy, hyperparameters, and available training data. Many state-of-the-art solutions use weight regularization to adjust parameter distributions, prevent overfitting, and support generalization of DNNs. None of the existing regularization techniques have ever exploited a typical distribution of numerical datasets with respect to the first non-zero (or significant) digit, called Benford's Law (BL). In this paper, we show that the deviation of the significant digit distribution of the DNN weights from BL is closely related to the generalization of the DNN. In particular, when the DNN is presented with limited training data. To take advantage of this finding, we use BL to target the weight regularization of DNNs. Extensive experiments are performed on image, table, and speech data, considering convolutional (CNN) and Transformer-based neural network architectures with varying numbers of parameters. We show that the performance of DNNs is improved by minimizing the distance between the significant digit distributions of the DNN weights and the BL distribution along with L2 regularization. The improvements depend on the network architecture and how it deals with limited data. However, the proposed penalty term improves consistently and some CNN-based architectures gain up to $15\\%$ test accuracy over the default training scheme with L2 regularization on subsets of CIFAR 100",
    "checked": true,
    "id": "6a3ca23d87b7864b02616f6ce13c7cc976e4c6d6",
    "semantic_title": "exploiting benford's law for weight regularization of deep neural networks",
    "citation_count": 0,
    "authors": [
      "Julius Ott",
      "Huawei Sun",
      "Enrico Rinaldi",
      "Gianfranco Mauro",
      "Lorenzo Servadei",
      "Robert Wille"
    ]
  },
  "https://openreview.net/forum?id=rWSiBknwQa": {
    "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
    "volume": "main",
    "abstract": "The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLMs, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, potentially ignoring the superior generation capabilities of contemporary LLMs. To investigate the robustness of LLMs while using their generation ability, we propose a novel rational evaluation pipeline that leverages reward models as diagnostic tools to evaluate the long conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters.Our extensive empirical experiments demonstrate that TREvaL provides an identification for the lack of robustness of nowadays LLMs.Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted, calling for more attention on the robustness during alignment process",
    "checked": true,
    "id": "57207b935fc3484d175f5e9e2980d73ca793f994",
    "semantic_title": "are large language models really robust to word-level perturbations?",
    "citation_count": 24,
    "authors": [
      "Haoyu Wang",
      "Guozheng Ma",
      "Cong Yu",
      "Ning Gui",
      "Linrui Zhang",
      "Zhiqi Huang",
      "Suwei Ma",
      "Yongzhe Chang",
      "Sen Zhang",
      "Li Shen",
      "Xueqian Wang",
      "Peilin Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=42v6I5Ut9a": {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "volume": "main",
    "abstract": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just a forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks",
    "checked": true,
    "id": "57404457c52bbaead08adef52eba2fc7e161b492",
    "semantic_title": "single-pass detection of jailbreaking input in large language models",
    "citation_count": 0,
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pSk5qyt1ob": {
    "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
    "volume": "main",
    "abstract": "Estimating the expectation of a Bernoulli random variable based on $N$ independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks—such as certifying the statistical safety of dynamical systems—can be formulated as BPCI problems. Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as training-conditional CP was recently employed to tackle the problem of safety certification. In this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification",
    "checked": true,
    "id": "96eda85cd91da0eb1f3d08dadb53196ca0e4a3e4",
    "semantic_title": "on training-conditional conformal prediction and binomial proportion confidence intervals",
    "citation_count": 0,
    "authors": [
      "Rudi Coppola",
      "Manuel Mazo Espinosa"
    ]
  },
  "https://openreview.net/forum?id=EcMVskXo1n": {
    "title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM",
    "checked": true,
    "id": "b700791efdb5e8202f1d05d1cf6269e56618d4f3",
    "semantic_title": "generative risk minimization for out-of-distribution generalization on graphs",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Yaochen Zhu",
      "Chuxu Zhang",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=69RntSRF5K": {
    "title": "An Analytical Model for Overparameterized Learning Under Class Imbalance",
    "volume": "main",
    "abstract": "We study class-imbalanced linear classification in a high-dimensional Gaussian mixture model. We develop a tight, closed form approximation for the test error of several practical learning methods, including logit adjustment and class dependent temperature. Our approximation allows us to analytically tune and compare these methods, highlighting how and when they overcome the pitfalls of standard cross-entropy minimization. We test our theoretical findings on simulated data and imbalanced CIFAR10, MNIST and FashionMNIST datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliav Mor",
      "Yair Carmon"
    ]
  },
  "https://openreview.net/forum?id=t5cy5v9wph": {
    "title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, there is debate on the extent to which they are performing general abstract reasoning versus employing shortcuts or other non-robust processes, such as ones that overly rely on similarity to what has been seen in their training data. Here we investigate the robustness of analogy-making abilities previously claimed for LLMs on three of four domains studied by Webb et al. (2023): letter-string analogies, digit matrices, and story analogies. For each of these domains we test humans and GPT models on robustness to variants of the original analogy problems—versions that test the same abstract reasoning abilities but that are likely dissimilar from tasks in the pre-training data. The performance of a system that uses robust abstract reasoning should not decline substantially on these variants. On simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply. This pattern is less pronounced as the complexity of these analogy problems is increased, as both humans and GPT models perform poorly on both the original and variant problems requiring more complex analogies. On digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. Lastly, we assess the robustness of humans and GPT models on story-based analogy problems, finding that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing. This work provides evidence that, despite previously reported successes of LLMs on zero-shot analogical reasoning, these models often lack the robustness of zero-shot human analogy- making, exhibiting brittleness on most of the variations we tested. More generally, this work points to the importance of carefully evaluating AI systems not only for accuracy but also robustness when testing their cognitive capabilities. Code, data, and results for all experiments is available at https://github.com/marthaflinderslewis/robust-analogy",
    "checked": true,
    "id": "be8f1cdd0365a2b50ada35219592bcc7abc00009",
    "semantic_title": "evaluating the robustness of analogical reasoning in large language models",
    "citation_count": 10,
    "authors": [
      "Martha Lewis",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=adhsMqURI1": {
    "title": "Comparing the information content of probabilistic representation spaces",
    "volume": "main",
    "abstract": "Probabilistic representation spaces convey information about a dataset and are shaped by factors such as the training data, network architecture, and loss function. Comparing the information content of such spaces is crucial for understanding the learning process, yet most existing methods assume point-based representations, neglecting the distributional nature of probabilistic spaces. To address this gap, we propose two information-theoretic measures to compare general probabilistic representation spaces by extending classic methods to compare the information content of hard clustering assignments. Additionally, we introduce a lightweight method of estimation that is based on fingerprinting a representation space with a sample of the dataset, designed for scenarios where the communicated information is limited to a few bits. We demonstrate the utility of these measures in three case studies. First, in the context of unsupervised disentanglement, we identify recurring information fragments within individual latent dimensions of VAE and InfoGAN ensembles. Second, we compare the full latent spaces of models and reveal consistent information content across datasets and methods, despite variability during training. Finally, we leverage the differentiability of our measures to perform model fusion, synthesizing the information content of weak learners into a single, coherent representation. Across these applications, the direct comparison of information content offers a natural basis for characterizing the processing of information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kieran A. Murphy",
      "Sam Dillavou",
      "Danielle Bassett"
    ]
  },
  "https://openreview.net/forum?id=jAHEBivObO": {
    "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Owing to the growing concerns about privacy and regulatory compliance, it is desirable to regulate the output of generative models. To that end, the objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained Generative Adversarial Network (GAN) where the underlying training data set is inaccessible. Our approach is inspired by the observation that the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed two-stage method, known as 'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also maintaining the quality of generated samples. In the initial stage, we adapt a pre-trained GAN on a set of negative samples (containing undesired features) provided by the user. Subsequently, we train the original pre-trained GAN using positive samples, along with a repulsion regularizer. This regularizer encourages the learned model parameters to move away from the parameters of the adapted model (first stage) while not degrading the generation quality. We provide theoretical insights into the proposed method. To the best of our knowledge, our approach stands as the first method addressing unlearning within the realm of high-fidelity GANs (such as StyleGAN). We validate the effectiveness of our method through comprehensive experiments, encompassing both class-level unlearning on the MNIST and AFHQ dataset and feature-level unlearning tasks on the CelebA-HQ dataset. Our code and implementation is available at: https://github.com/atriguha/Adapt_Unlearn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyush Tiwary",
      "Atri Guha",
      "Subhodip Panda",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=D3DBqvSDbj": {
    "title": "On Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training-data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models",
    "checked": true,
    "id": "122a7e217fe70d5a1a44a6e2b67e859d1fc8e28d",
    "semantic_title": "on memorization in diffusion models",
    "citation_count": 50,
    "authors": [
      "Xiangming Gu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan Li",
      "Min Lin",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=dNJmJ8bh1M": {
    "title": "The Sparse Matrix-Based Random Projection: A Study of Binary and Ternary Quantization",
    "volume": "main",
    "abstract": "Random projection is a simple yet effective technique for dimension reduction, widely used in various machine learning tasks. Following the projection step, quantization is often applied to further reduce the complexity of projected data. In general, quantized projections are expected to approximately preserve the pairwise distances between the original data points, to avoid significant performance degradation in subsequent tasks. While this distance preservation property has been investigated for Gaussian matrices, our work further extends the analysis to hardware-friendly $\\{0,1\\}$-binary matrices, particularly focusing on cases where the projections are quantized into two types of low bit-width codes: $\\{0,1\\}$-binary codes and $\\{0,\\pm1\\}$-ternary codes. It is found that the distance preservation property tends to be better maintained, when the binary projection matrices exhibit sparse structures. This is validated through classification and clustering experiments, where extremely sparse binary matrices, with only one nonzero entry per column, achieve superior or comparable performance to other denser binary matrices and Gaussian matrices. This presents an opportunity to significantly reduce the computational and storage complexity of the quantized random projection model, without compromising, and potentially even improving its performance",
    "checked": true,
    "id": "47b3005a2f8b52c74739bb3e6ea4c6e0f6816b40",
    "semantic_title": "the sparse matrix-based random projection: a study of binary and ternary quantization",
    "citation_count": 0,
    "authors": [
      "Weizhi Lu",
      "Zhongzheng Li",
      "Mingrui Chen",
      "Weiyu Li"
    ]
  },
  "https://openreview.net/forum?id=Sx1khIIi95": {
    "title": "Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning",
    "volume": "main",
    "abstract": "We consider gradient-based optimisation of wide, shallow neural networks, where the output of each hidden node is scaled by a positive parameter. The scaling parameters are non-identical, differing from the classical Neural Tangent Kernel (NTK) parameterisation. We prove that for large such neural networks, with high probability, gradient flow and gradient descent converge to a global minimum and can learn features in some sense, unlike in the NTK parameterisation. We perform experiments illustrating our theoretical results and discuss the benefits of such scaling in terms of prunability and transfer learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Caron",
      "Fadhel Ayed",
      "Paul Jung",
      "Hoil Lee",
      "Juho Lee",
      "Hongseok Yang"
    ]
  },
  "https://openreview.net/forum?id=rfPns0WJyg": {
    "title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability",
    "volume": "main",
    "abstract": "Optimal decision-making under partial observability requires reasoning about the uncertainty of the environment's hidden state. However, most reinforcement learning architectures handle partial observability with sequence models that have no internal mechanism to incorporate uncertainty in their hidden state representation, such as recurrent neural networks, deterministic state-space models and transformers. Inspired by advances in probabilistic world models for reinforcement learning, we propose a standalone Kalman filter layer that performs closed-form Gaussian inference in linear state-space models and train it end-to-end within a model-free architecture to maximize returns. Similar to efficient linear recurrent layers, the Kalman filter layer processes sequential data using a parallel scan, which scales logarithmically with the sequence length. By design, Kalman filter layers are a drop-in replacement for other recurrent layers in standard model-free architectures, but importantly they include an explicit mechanism for probabilistic filtering of the latent state representation. Experiments in a wide variety of tasks with partial observability show that Kalman filter layers excel in problems where uncertainty reasoning is key for decision-making, outperforming other stateful models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos E. Luis",
      "Alessandro Giacomo Bottero",
      "Julia Vinogradska",
      "Felix Berkenkamp",
      "Jan Peters"
    ]
  },
  "https://openreview.net/forum?id=hCxtlfvL22": {
    "title": "Latent Space Energy-based Neural ODEs",
    "volume": "main",
    "abstract": "This paper introduces novel deep dynamical models designed to represent continuous-time sequences. Our approach employs a neural emission model to generate each data point in the time series through a non-linear transformation of a latent state vector. The evolution of these latent states is implicitly defined by a neural ordinary differential equation (ODE), with the initial state drawn from an informative prior distribution parameterized by an Energy-based model (EBM). This framework is extended to disentangle dynamic states from underlying static factors of variation, represented as time-invariant variables in the latent space. We train the model using maximum likelihood estimation with Markov chain Monte Carlo (MCMC) in an end-to-end manner. Experimental results on oscillating systems, videos and real-world state sequences (MuJoCo) demonstrate that our model with the learnable energy-based prior outperforms existing counterparts, and can generalize to new dynamic parameterization, enabling long-horizon predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Deqian Kong",
      "Jianwen Xie",
      "Kookjin Lee",
      "Ying Nian Wu",
      "Yezhou Yang"
    ]
  },
  "https://openreview.net/forum?id=U8EMkndyq4": {
    "title": "Using representation balancing to learn conditional-average dose responses from clustered data",
    "volume": "main",
    "abstract": "Estimating the response to an intervention with an associated dose conditional on a unit's covariates, the \"conditional-average dose response\" (CADR), is a relevant task in a variety of domains, from healthcare to business, economics, and beyond. Estimating such a response is challenging for several reasons: Firstly, it typically needs to be estimated from observational data, which can be confounded and negatively affect the performance of intervention response estimators used for counterfactual inference. Secondly, the continuity of the dose prevents the adoption of approaches used to estimate responses to binary-valued interventions. That is why the machine learning (ML) community has proposed several tailored CADR estimators. Yet, the proposal of most of these methods requires strong assumptions on the distribution of data and the assignment of interventions, which go beyond the standard assumptions in causal inference. Whereas previous works have so far focused on smooth shifts in covariate distributions across doses, in this work, we will study estimating CADR from clustered data and where different doses are assigned to different segments of a population. On a novel benchmarking dataset, we show the impacts of clustered data on model performance. Additionally, we propose an estimator, CBRNet, that enables the application of representation balancing for CADR estimation through clustering the covariate space and a novel loss function. CBRNet learns cluster-agnostic and hence dose-agnostic covariate representations through representation balancing for unbiased CADR inference. We run extensive experiments to illustrate the workings of our method and compare it with the state of the art in ML for CADR estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bockel-Rickermann",
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Tim Verdonck",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=TRKwzPnXWQ": {
    "title": "ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "This paper presents a new self-supervised video representation learning framework \\textbf{ARVideo}, which \\textit{autoregressively} predict the next video token in a tailored sequence order. Two key designs are included. First, we organize autoregressive video tokens into clusters that span both \\textit{spatially} and \\textit{temporally}, thereby enabling a richer aggregation of contextual information compared to the standard spatial-only or temporal-only clusters. Second, we adopt a randomized spatiotemporal prediction order to facilitate learning from multi-dimensional data, addressing the limitations of a handcrafted spatial-first or temporal-first sequence order. Extensive experiments establish ARVideo as an effective paradigm for self-supervised video representation learning. For example, when trained with the ViT-B backbone, ARVideo competitively attains 81.2\\% on Kinetics-400 and 70.9\\% on Something-Something V2, which are on par with the strong benchmark set by VideoMAE. Importantly, ARVideo also demonstrates higher training efficiency, \\ie, it trains 14\\% faster and requires 58\\% less GPU memory compared to VideoMAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Hongru Zhu",
      "Chen Wei",
      "Yijiang Li",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=GGHk5ukO6t": {
    "title": "Dynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "main",
    "abstract": "Protein-protein interaction (PPI) represents a central challenge within the biology field, and accurately predicting the consequences of mutations in this context is crucial for drug design and protein engineering. Deep learning (DL) has shown promise in forecasting the effects of such mutations but is hindered by two primary constraints. First, the structures of mutant proteins are often elusive to acquire. Secondly, PPI takes place dynamically, which is rarely integrated into the DL architecture design. To address these obstacles, we present a novel framework named Refine-PPI with two key enhancements. First, we introduce a structure refinement module trained by a mask mutation modeling (MMM) task on available wild-type structures, which is then transferred to hallucinate the inaccessible mutant structures. Second, we employ a new kind of geometric network, called the probability density cloud network (PDC-Net), to capture 3D dynamic variations and encode the atomic uncertainty associated with PPI. Comprehensive experiments on SKEMPI.v2 substantiate the superiority of Refine-PPI over all existing tools for predicting free energy change. These findings underscore the effectiveness of our hallucination strategy and the PDC module in addressing the absence of mutant protein structure and modeling geometric uncertainty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=h751wl9xiR": {
    "title": "ALTA: Compiler-Based Analysis of Transformers",
    "volume": "main",
    "abstract": "We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework --- language specification, symbolic interpreter, and weight compiler --- available to the community to enable further applications and insights",
    "checked": true,
    "id": "845f2d82f11c192b956c0053b4f49db90f2a8a48",
    "semantic_title": "alta: compiler-based analysis of transformers",
    "citation_count": 2,
    "authors": [
      "Peter Shaw",
      "James Cohan",
      "Jacob Eisenstein",
      "Kenton Lee",
      "Jonathan Berant",
      "Kristina Toutanova"
    ]
  },
  "https://openreview.net/forum?id=BLDtWlFKhn": {
    "title": "Density of states in neural networks: an in-depth exploration of learning in parameter space",
    "volume": "main",
    "abstract": "Learning in neural networks critically hinges on the intricate geometry of the loss landscape associated with a given task. Traditionally, most research has focused on finding specific weight configurations that minimize the loss. In this work, born from the cross-fertilization of machine learning and theoretical soft matter physics, we introduce a novel approach to examine the weight space across all loss values. Employing the Wang-Landau enhanced sampling algorithm, we explore the neural network density of states -- the number of network parameter configurations that produce a given loss value -- and analyze how it depends on specific features of the training set. Using both real-world and synthetic data, we quantitatively elucidate the relation between data structure and network density of states across different sizes and depths of binary-state networks. This work presents and illustrates a novel, informative analysis method that aims at paving the way for a better understanding of the interplay between structured data and the networks that process, learn, and generate them",
    "checked": true,
    "id": "e5ec33e0a0a443ac9f63eda091b12a87fd4c61e7",
    "semantic_title": "density of states in neural networks: an in-depth exploration of learning in parameter space",
    "citation_count": 0,
    "authors": [
      "Margherita Mele",
      "Roberto Menichetti",
      "Alessandro Ingrosso",
      "Raffaello Potestio"
    ]
  },
  "https://openreview.net/forum?id=HjpD5kpfa3": {
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "volume": "main",
    "abstract": "The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph's spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning",
    "checked": true,
    "id": "4ea0b877368675f030a63f2a8b82477e0a2073b8",
    "semantic_title": "rethinking spectral augmentation for contrast-based graph self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chaolong Ying",
      "Yimu Wang",
      "Yaoyao Xu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=JQ0agisXny": {
    "title": "A Strong Baseline for Molecular Few-Shot Learning",
    "volume": "main",
    "abstract": "Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods",
    "checked": true,
    "id": "dcdf4443c4e17e9e1d76a70adf4619c5aad3d736",
    "semantic_title": "a strong baseline for molecular few-shot learning",
    "citation_count": 0,
    "authors": [
      "Philippe Formont",
      "Hugo Jeannin",
      "Pablo Piantanida",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=u9EHndbiVw": {
    "title": "PROXI: Challenging the GNNs for Link Prediction",
    "volume": "main",
    "abstract": "Over the past decade, Graph Neural Networks (GNNs) have transformed graph representation learning. In the widely adopted message-passing GNN framework, nodes refine their representations by aggregating information from neighboring nodes iteratively. While GNNs excel in various domains, recent theoretical studies have raised concerns about their capabilities. GNNs aim to address various graph-related tasks by utilizing such node representations, however, this one-size-fits-all approach proves suboptimal for diverse tasks. Motivated by these observations, we conduct empirical tests to compare the performance of current GNN models with more conventional and direct methods in link prediction tasks. Introducing our model, PROXI, which leverages proximity information of node pairs in both graph and attribute spaces, we find that standard machine learning (ML) models perform competitively, even outperforming cutting-edge GNN models when applied to these proximity metrics derived from node neighborhoods and attributes. This holds true across both homophilic and heterophilic networks, as well as small and large benchmark datasets, including those from the Open Graph Benchmark (OGB). Moreover, we show that augmenting traditional GNNs with PROXI significantly boosts their link prediction performance. Our empirical findings corroborate the previously mentioned theoretical observations and imply that there exists ample room for enhancement in current GNN models to reach their potential",
    "checked": true,
    "id": "9e534ab6e0e6594cf616ffdd476ee18ddef578f9",
    "semantic_title": "proxi: challenging the gnns for link prediction",
    "citation_count": 0,
    "authors": [
      "Astrit Tola",
      "Jack Myrick",
      "Baris Coskunuzer"
    ]
  },
  "https://openreview.net/forum?id=RfFqBXLDQk": {
    "title": "On Space Folds of ReLU Neural Networks",
    "volume": "main",
    "abstract": "Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information",
    "checked": true,
    "id": "f306125cf30d0a33bc65c6a1ab7498371fdfb0ea",
    "semantic_title": "on space folds of relu neural networks",
    "citation_count": 1,
    "authors": [
      "Michal Lewandowski",
      "Hamid Eghbalzadeh",
      "Bernhard Heinzl",
      "Raphael Pisoni",
      "Bernhard A. Moser"
    ]
  },
  "https://openreview.net/forum?id=asiBW1bB9b": {
    "title": "Improving Consistency in Large Language Models through Chain of Guidance",
    "volume": "main",
    "abstract": "Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on \\textbf{Chain of Guidance} (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q\\&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent {\\it and} correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process. Code is available at \\url{https://github.com/vijilAI/chain_of_guidance}",
    "checked": true,
    "id": "a19ce065f9f0a0eef3efa9f85859b1f4fc1091c3",
    "semantic_title": "improving consistency in large language models through chain of guidance",
    "citation_count": 4,
    "authors": [
      "Harsh Raj",
      "Vipul Gupta",
      "Domenic Rosati",
      "Subhabrata Majumdar"
    ]
  },
  "https://openreview.net/forum?id=H4S4ETc8c9": {
    "title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment",
    "volume": "main",
    "abstract": "Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Since the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value can lead to a compromise of its performance on the true objective. Previous work proposes Regularized BoN sampling (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates reward hacking and empirically (Jinnai et al., 2024). However, Jinnai et al. (2024) introduce RBoN based on a heuristic and they lack the analysis of why such regularization strategy improves the performance of BoN sampling. The aim of this study is to analyze the effect of BoN sampling on regularization strategies. Using the regularization strategies corresponds to robust optimization, which maximizes the worst case over a set of possible perturbations in the proxy reward. Although the theoretical guarantees are not directly applicable to RBoN, RBoN corresponds to a practical implementation. This paper proposes an extension of the RBoN framework, called Stochastic RBoN sampling (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN in proxy reward. We then perform an empirical evaluation using the AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward. In addition, we also propose another simple RBoN method, the Sentence Length Regularized BoN, which has a better performance in the experiment as compared to the previous methods",
    "checked": true,
    "id": "aa2e3c52b4c6e847f6703c84f5640cae42dbc73d",
    "semantic_title": "evaluation of best-of-n sampling strategies for language model alignment",
    "citation_count": 4,
    "authors": [
      "Yuki Ichihara",
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Kenshi Abe",
      "Kaito Ariu",
      "Mitsuki Sakamoto",
      "Eiji Uchibe"
    ]
  },
  "https://openreview.net/forum?id=ScEv13W2f1": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "main",
    "abstract": "We study inferring 3D object-centric scene representations from a single image. While recent methods have shown potential in unsupervised 3D object discovery, they are limited in generalizing to unseen spatial configurations. This limitation stems from the lack of translation invariance in their 3D object representations. Previous 3D object discovery methods entangle objects' intrinsic attributes like shape and appearance with their 3D locations. This entanglement hinders learning generalizable 3D object representations. To tackle this bottleneck, we propose the unsupervised discovery of Object-Centric neural Fields (uOCF), which integrates translation invariance into the object representation. To allow learning object-centric representations from limited real-world images, we further introduce an object prior learning method that transfers object-centric prior knowledge from a synthetic dataset. To evaluate our approach, we collect four new datasets, including two real kitchen environments. Extensive experiments show that our approach significantly improves generalization and sample efficiency and enables unsupervised 3D object discovery in real scenes. Notably, uOCF demonstrates zero-shot generalization to unseen objects from a single real image. We attach our code in the supplementary file, and the project page is available at https://red-fairy.github.io/uOCF/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Hong-Xing Yu",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Wt6Iz5XNIO": {
    "title": "Understanding LLM Embeddings for Regression",
    "volume": "main",
    "abstract": "With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. In this paper, we provide one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, we quantify the contribution of different model effects, most notably model size and language understanding, which we find surprisingly do not always improve regression performance",
    "checked": true,
    "id": "36f1a2c9621a0f25fd35bf6c6fbb3f99341b4965",
    "semantic_title": "understanding llm embeddings for regression",
    "citation_count": 6,
    "authors": [
      "Eric Tang",
      "Bangding Yang",
      "Xingyou Song"
    ]
  },
  "https://openreview.net/forum?id=5qKI2dkrjL": {
    "title": "APR-CNN: Convolutional Neural Networks for the Adaptive Particle Representation of Large Microscopy Images",
    "volume": "main",
    "abstract": "We present APR-CNN, a novel class of convolutional neural networks designed for efficient and scalable three-dimensional microscopy image analysis. APR-CNNs operate natively on a sparse, multi-resolution image representation known as the Adaptive Particle Representation (APR). This significantly reduces memory and compute requirements compared to traditional pixel-based CNNs. We introduce APR-native layers for convolution, pooling, and upsampling, along with hybrid architectures that combine APR and pixel layers to balance accuracy and computational efficiency. We show in benchmarks that APR-CNNs achieve comparable segmentation accuracy to pixel-based CNNs while drastically reducing memory usage and inference time. We further showcase the potential of APR-CNNs in large-scale volumetric image analysis, reducing inference times from weeks to days. This opens up new avenues for applying deep learning to large, high-resolution, three-dimensional biomedical datasets with constrained computational resources",
    "checked": true,
    "id": "881cd10de5d261248c70ae22dab09ac590e367d5",
    "semantic_title": "apr-cnn: convolutional neural networks for the adaptive particle representation of large microscopy images",
    "citation_count": 0,
    "authors": [
      "Joel Jonsson",
      "Bevan Leslie Cheeseman",
      "Ivo Sbalzarini"
    ]
  },
  "https://openreview.net/forum?id=zjxKrb4ehr": {
    "title": "On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates",
    "volume": "main",
    "abstract": "We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions avoiding any Lipschitzness assumption on the score function. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm. Beyond the motivating example and in order to allow for the use of a diverse range of stochastic optimizers, we present our results using an $L^2$-accurate score estimation assumption, which crucially is formed under an expectation with respect to the stochastic optimizer and our novel auxiliary process that uses only known information. This approach yields the best known convergence rate for our sampling algorithm",
    "checked": true,
    "id": "93e13d702f6ffbd31b05051504041bf4b3b28c9e",
    "semantic_title": "on diffusion-based generative models and their error bounds: the log-concave case with full convergence estimates",
    "citation_count": 4,
    "authors": [
      "Stefano Bruno",
      "Ying Zhang",
      "Dongyoung Lim",
      "Omer Deniz Akyildiz",
      "Sotirios Sabanis"
    ]
  },
  "https://openreview.net/forum?id=A1R1cQ93Cb": {
    "title": "Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e111c488f0bfddd0d196d6ba932d97b52f44c2a5",
    "semantic_title": "relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization",
    "citation_count": 0,
    "authors": [
      "Sara Venturini",
      "Marianna De Santis",
      "Jordan Patracone",
      "Martin Schmidt",
      "Francesco Rinaldi",
      "Saverio Salzo"
    ]
  },
  "https://openreview.net/forum?id=nmBleuFzaN": {
    "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0638e1ecf313099120106a78bd23fd92863b9e75",
    "semantic_title": "maxwell's demon at work: efficient pruning by leveraging saturation of neurons",
    "citation_count": 1,
    "authors": [
      "Simon Dufort-Labbé",
      "Pierluca D'Oro",
      "Evgenii Nikishin",
      "Irina Rish",
      "Pierre-Luc Bacon",
      "Razvan Pascanu",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=ZMliWjMCor": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahrokh Ghoddousi Boroujeni",
      "Andreas Krause",
      "Giancarlo Ferrari-Trecate"
    ]
  },
  "https://openreview.net/forum?id=DrMCDS88IL": {
    "title": "Wasserstein Coreset via Sinkhorn Loss",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "eaa11aee30abe5267490cde64c0825004369e758",
    "semantic_title": "wasserstein coreset via sinkhorn loss",
    "citation_count": 0,
    "authors": [
      "Haoyun Yin",
      "Yixuan Qiu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=tzW948kU6x": {
    "title": "Diffusion on Graph: Augmentation of Graph Structure for Node Classification",
    "volume": "main",
    "abstract": "Graph diffusion models have recently been proposed to synthesize entire graphs, such as molecule graphs. Although existing methods have shown great performance in generating entire graphs for graph-level learning tasks, no graph diffusion models have been developed to generate synthetic graph structures, that is, synthetic nodes and associated edges within a given graph, for node-level learning tasks. Inspired by the research in the computer vision literature using synthetic data for enhanced performance, we propose Diffusion on Graph (DoG), which generates synthetic graph structures to boost the performance of GNNs. The synthetic graph structures generated by DoG are combined with the original graph to form an augmented graph for the training of node-level learning tasks, such as node classification and graph contrastive learning (GCL). To improve the efficiency of the generation process, a Bi-Level Neighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse effect of the noise introduced by the synthetic graph structures, a low-rank regularization method is proposed for the training of graph neural networks (GNNs) on the augmented graphs. Extensive experiments on various graph datasets for semi-supervised node classification and graph contrastive learning have been conducted to demonstrate the effectiveness of DoG with low-rank regularization. The code of DoG is available at \\url{https://github.com/Statistical-Deep-Learning/DoG}",
    "checked": true,
    "id": "3f05797344d43e54daa8e98664c4920ccd21e37f",
    "semantic_title": "diffusion on graph: augmentation of graph structure for node classification",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Changyu Liu",
      "Yingzhen Yang"
    ]
  },
  "https://openreview.net/forum?id=8rxtL0kZnX": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "main",
    "abstract": "Most of the current learning methodologies and benchmarking datasets in the hypergraph realm are obtained by \\emph{lifting} procedures from their graph analogs, leading to overshadowing specific characteristics of hypergraphs. This paper attempts to confront some pending questions in that regard: Q1 Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HNNs)? Q2 How do models that employ unique characteristics of higher-order networks perform compared to lifted models? Q3 Do well-established hypergraph datasets provide a meaningful benchmark for HNNs? To address them, we first introduce a novel conceptualization of homophily in higher-order networks based on a Message Passing (MP) scheme, unifying both the analytical examination and the modeling of higher-order networks. Further, we investigate some natural strategies for processing higher-order structures within HNNs (such as keeping hyperedge-dependent node representations or performing node/hyperedge stochastic samplings), leading us to the most general MP formulation up to date --MultiSet. Finally, we conduct an extensive set of experiments that contextualize our proposals",
    "checked": true,
    "id": "f1cb37f9a8846b8627465d460b20fc2e8f62fca2",
    "semantic_title": "hypergraph neural networks through the lens of message passing: a common perspective to homophily and architecture design",
    "citation_count": 10,
    "authors": [
      "Lev Telyatnikov",
      "Maria Sofia Bucarelli",
      "Guillermo Bernardez",
      "Olga Zaghen",
      "Simone Scardapane",
      "Pietro Lio"
    ]
  },
  "https://openreview.net/forum?id=sbmp55k6iE": {
    "title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network. In this paper, we present theoretical analyses of mini-batch SGD with four schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler. We show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikaru Umeda",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=JEHIVfjmOf": {
    "title": "JoIN: Joint GANs Inversion for Intrinsic Image Decomposition",
    "volume": "main",
    "abstract": "Intrinsic Image Decomposition (IID) is a challenging inverse problem that seeks to decompose a natural image into its underlying intrinsic components such as albedo and shading. While recent image decomposition methods rely on learning-based priors on these components, they often suffer from component cross-contamination owing to joint training of priors; or from Sim-to-Real gap since the priors trained on synthetic data are kept frozen during the inference on real images. In this work, we propose to solve the intrinsic image decomposition problem using a bank of Generative Adversarial Networks (GANs) as priors where each GAN is independently trained only on a single intrinsic component, providing stronger and more disentangled priors. At the core of our approach is the idea that the latent space of a GAN is a well-suited optimization domain to solve inverse problems. Given an input image, we propose to jointly invert the latent codes of a set of GANs and combine their outputs to reproduce the input. Contrary to all existing GAN inversion methods that are limited to inverting only a single GAN, our proposed approach, JoIN, is able to jointly invert multiple GANs using only a single image as supervision while still maintaining distribution priors of each intrinsic component. We show that our approach is modular, allowing various forward imaging models, and that it can successfully decompose both synthetic and real images. Further, taking inspiration from existing GAN inversion approaches, we allow for careful fine-tuning of the generator priors during the inference on real images. This way, our method is able to achieve excellent generalization on real images even though it uses only synthetic data to train the GAN priors. We demonstrate the success of our approach through exhaustive qualitative and quantitative evaluations and ablation studies on various datasets",
    "checked": true,
    "id": "db139e9df8ef1c0d2aeba88169394508c9a6fb94",
    "semantic_title": "join: joint gans inversion for intrinsic image decomposition",
    "citation_count": 2,
    "authors": [
      "Viraj Shah",
      "Svetlana Lazebnik",
      "Julien Philip"
    ]
  },
  "https://openreview.net/forum?id=1QeI99nH9k": {
    "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
    "volume": "main",
    "abstract": "Robust statistics aims to compute quantities to represent data where a fraction of it may be arbitrarily corrupted. The most essential statistic is the mean, and in recent years, there has been a flurry of theoretical advancement for efficiently estimating the mean in high dimensions on corrupted data. While several algorithms have been proposed that achieve near-optimal error, they all rely on large data size requirements as a function of dimension. In this paper, we perform an extensive experimentation over various mean estimation techniques where data size might not meet this requirement due to the high-dimensional setting. For data with inliers generated from a Gaussian with known covariance, we find experimentally that several robust mean estimation techniques can practically improve upon the sample mean, with the quantum entropy scaling approach from Dong \\etal (NeurIPS 2019) performing consistently the best. However, this consistent improvement is conditioned on a couple of simple modifications to how the steps to prune outliers work in the high-dimension low-data setting, and when the inliers deviate significantly from Gaussianity. In fact, with these modifications, they are typically able to achieve roughly the same error as taking the sample mean of the uncorrupted inlier data, even with very low data size. In addition to controlled experiments on synthetic data, we also explore these methods on large language models, deep pretrained image models, and non-contextual word embedding models that do not necessarily have an inherent Gaussian distribution. Yet, in these settings, a mean point of a set of embedded objects is a desirable quantity to learn, and the data exhibits the high-dimension low-data setting studied in this paper. We show both the challenges of achieving this goal, and that our updated robust mean estimation methods can provide significant improvement over using just the sample mean. We additionally publish a library of Python implementations of robust mean estimation algorithms, allowing practitioners and researchers to apply these techniques and to perform further experimentation",
    "checked": true,
    "id": "29bf80275de8817de20484f57d012d7d2be50083",
    "semantic_title": "robust high-dimensional mean estimation with low data size, an empirical study",
    "citation_count": 0,
    "authors": [
      "Cullen Anderson",
      "Jeff M. Phillips"
    ]
  },
  "https://openreview.net/forum?id=7CUluLpLxV": {
    "title": "Explaining Explainability: Recommendations for Effective Use of Concept Activation Vectors",
    "volume": "main",
    "abstract": "Concept-based explanations translate the internal representations of deep learning models into a language that humans are familiar with: concepts. One popular method for finding concepts is Concept Activation Vectors (CAVs), which are learnt using a probe dataset of concept exemplars. In this work, we investigate three properties of CAVs: (1) inconsistency across layers, (2) entanglement with other concepts, and (3) spatial dependency. Each property provides both challenges and opportunities in interpreting models. We introduce tools designed to detect the presence of these properties, provide insight into how each property can lead to misleading explanations, and provide recommendations to mitigate their impact. To demonstrate practical applications, we apply our recommendations to a melanoma classification task, showing how entanglement can lead to uninterpretable results and that the choice of negative probe set can have a substantial impact on the meaning of a CAV. Further, we show that understanding these properties can be used to our advantage. For example, we introduce spatially dependent CAVs to test if a model is translation invariant with respect to a specific concept and class. Our experiments are performed on natural images (ImageNet), skin lesions (ISIC 2019), and a new synthetic dataset, Elements. Elements is designed to capture a known ground truth relationship between concepts and classes. We release this dataset to facilitate further research in understanding and evaluating interpretability methods",
    "checked": true,
    "id": "6309f33979d4384ed2092088cafb1bb15563c6cc",
    "semantic_title": "explaining explainability: recommendations for effective use of concept activation vectors",
    "citation_count": 2,
    "authors": [
      "Angus Nicolson",
      "Lisa Schut",
      "Alison Noble",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=IrBYuh9W3T": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "main",
    "abstract": "ImageNet was famously created by querying several image search engines such as Flickr. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in the community that ImageNet images are stereotypical, unnatural, and overly simple representations of the class category. At the same time, it provides a simple and actionable takeaway for future dataset creation efforts",
    "checked": true,
    "id": "0609864d2aee4ed04671304b9de7e94db603fe1a",
    "semantic_title": "what makes imagenet look unlike laion",
    "citation_count": 10,
    "authors": [
      "Ali Shirali",
      "Moritz Hardt"
    ]
  },
  "https://openreview.net/forum?id=Bmy82p2eez": {
    "title": "Continual Learning from Simulated Interactions via Multitask Prospective Rehearsal for Bionic Limb Behavior Modeling",
    "volume": "main",
    "abstract": "Lower limb amputations and neuromuscular impairments severely restrict mobility, necessitating advancements beyond conventional prosthetics. While motorized bionic limbs show promise, their effectiveness depends on replicating the dynamic coordination of human movement across diverse environments. In this paper, we introduce a model for human behavior in the context of bionic prosthesis control. Our approach leverages human locomotion demonstrations to learn the synergistic coupling of the lower limbs, enabling the prediction of the kinematic behavior of a missing limb during tasks such as walking, climbing inclines, and stairs. We propose a multitasking, continually adaptive model that anticipates and refines movements over time. At the core of our method is a technique which we call the multitask prospective rehearsal, that anticipates and synthesizes future movements based on the previous prediction and employs a corrective mechanism for subsequent predictions. Our evolving architecture merges lightweight, task-specific modules on a shared backbone, ensuring both specificity and scalability. We validate our model through experiments on real-world human gait datasets, including transtibial amputees, across a wide range of locomotion tasks. Results demonstrate that our approach consistently outperforms baseline models, particularly in scenarios with distributional shifts, adversarial perturbations, and noise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharmita Dey",
      "Benjamin Paassen",
      "Sarath Ravindran Nair",
      "Sabri Boughorbel",
      "Arndt F. Schilling"
    ]
  },
  "https://openreview.net/forum?id=DYCSRf3vby": {
    "title": "Geometry-Aware visualization of high dimensional Symmetric Positive Definite matrices",
    "volume": "main",
    "abstract": "Symmetric Positive Definite (SPD) matrices are pervasive in machine learning, from data features (such as covariance matrices) to optimization process.These matrices induce a Riemannian structure, where the curvature plays a critical role in the success of approaches based on those geometries. Yet, for ML practitioners wanting to visualize SPD matrices, the existing (flat) Euclidean approaches will hide the curvature of the manifold. To overcome this lack of expressivity in the existing algorithms, we introduce Riemannian versions of two state-of-the-art techniques, namely t-SNE and Multidimensional Scaling. Therefore, we are able to reduce a set of $c \\times c$ SPD matrices into a set of $2 \\times 2$ SPD matrices in order to capture the curvature information and avoid any distortion induced by flattening the representation in an Euclidean setup. Moreover, our approaches pave the way for targeting more general dimensionality reduction applications while preserving the geometry of the data. We performed experiments on controlled synthetic dataset to ensure that the low-dimensional representation preserves the geometric properties of both SPD Gaussians and geodesics. We also conduct experiments on various real datasets, such as video, anomaly detection, brain signal and others",
    "checked": true,
    "id": "0f84816eedf3c092cf74e654eb9cfa4a04f560d5",
    "semantic_title": "geometry-aware visualization of high dimensional symmetric positive definite matrices",
    "citation_count": 1,
    "authors": [
      "Thibault de Surrel",
      "Sylvain Chevallier",
      "Fabien Lotte",
      "Florian Yger"
    ]
  },
  "https://openreview.net/forum?id=VM8bNd5A09": {
    "title": "CNN Interpretability with Multivector Tucker Saliency Maps for Self-Supervised Models",
    "volume": "main",
    "abstract": "Interpreting the decisions of Convolutional Neural Networks (CNNs) is essential for understanding their behavior, yet it remains a significant challenge, particularly for self-supervised models. Most existing methods for generating saliency maps rely on reference labels, restricting their use to supervised tasks. EigenCAM is the only notable label-independent alternative, leveraging Singular Value Decomposition to generate saliency maps applicable across CNN models, but it does not fully exploit the tensorial structure of feature maps. In this work, we introduce the Tucker Saliency Map (TSM) method, which applies Tucker tensor decomposition to better capture the inherent structure of feature maps, producing more accurate singular vectors and values. These are used to generate high-fidelity saliency maps, effectively highlighting objects of interest in the input. We further extend EigenCAM and TSM into multivector variants—Multivec-EigenCAM and Multivector Tucker Saliency Maps (MTSM)—which utilize all singular vectors and values, further improving saliency map quality. Quantitative evaluations on supervised classification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve competitive performance with label-dependent methods. Moreover, TSM enhances interpretability by approximately $50\\%$ over EigenCAM for both supervised and self-supervised models. Multivec-EigenCAM and MTSM further advance state-of-the-art interpretability performance on self-supervised models, with MTSM achieving the best results",
    "checked": true,
    "id": "3917390c0b121fce89eaf486bef35b134859336e",
    "semantic_title": "cnn interpretability with multivector tucker saliency maps for self-supervised models",
    "citation_count": 0,
    "authors": [
      "Aymene Mohammed Bouayed",
      "Samuel Deslauriers-gauthier",
      "Adrian IACOVELLI",
      "David Naccache"
    ]
  },
  "https://openreview.net/forum?id=YxXyRSlZ4b": {
    "title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach",
    "volume": "main",
    "abstract": "Lattice reduction is a combinatorial optimization problem aimed at finding the most orthogonal basis in a given lattice. The Lenstra–Lenstra–Lovász (LLL) algorithm is the best algorithm in the literature for solving this problem. In light of recent research on algorithm discovery, in this work, we would like to answer this question: is it possible to parametrize the algorithm space for lattice reduction problem with neural networks and find an algorithm without supervised data? Our strategy is to use equivariant and invariant parametrizations and train in a self-supervised way. We design a deep neural model outputting factorized unimodular matrices and train it in a self-supervised manner by penalizing non-orthogonal lattice bases. We incorporate the symmetries of lattice reduction into the model by making it invariant to isometries and scaling of the ambient space and equivariant with respect to the hyperocrahedral group permuting and flipping the lattice basis elements. We show that this approach yields an algorithm with comparable complexity and performance to the LLL algorithm on a set of benchmarks. Additionally, motivated by certain applications for wireless communication, we extend our method to a convolutional architecture which performs joint reduction of spatially-correlated lattices arranged in a grid, thereby amortizing its cost over multiple lattices",
    "checked": true,
    "id": "7e0276853f5475f8b62125d025e40f2bbdd08119",
    "semantic_title": "neural lattice reduction: a self-supervised geometric deep learning approach",
    "citation_count": 2,
    "authors": [
      "Giovanni Luca Marchetti",
      "Gabriele Cesa",
      "Kumar Pratik",
      "Arash Behboodi"
    ]
  },
  "https://openreview.net/forum?id=tzFjcVqmxw": {
    "title": "Enhancing Remaining Useful Life Prediction with Ensemble Multi-Term Fourier Graph Neural Networks",
    "volume": "main",
    "abstract": "Remaining useful life (RUL) prediction is crucial in predictive maintenance. Recently, deep learning forecasting methods, especially Spatio-Temporal Graph Neural Networks (ST-GNNs), have achieved remarkable performance in RUL prediction. Most existing ST-GNNs require searching for the graph structure before utilizing GNNs to learn spatial graph representation, and they necessitate a temporal model such as LSTM to leverage the temporal dependencies in a fixed lookback window. However, such an approach has several limitations. Firstly, it demands substantial computational resources to learn graph structures for the time series data. Secondly, independently learning spatial and temporal information disregards their inherent correlation, and thirdly, capturing information within a fixed lookback window ignores long-term dependencies across the entire time series. To mitigate the issues above, instead of treating the data within the lookback window as a sequence of graphs in ST-GNN methods, we regard it as a complete graph and employ a Fourier Graph Neural Network (FGN) to learn the spatiotemporal information within this graph in the frequency space. Additionally, we create training and test graphs with varying sizes of lookback windows, enabling the model to learn both short-term and long-term dependencies and provide multiple predictions for ensemble averaging. We also consider scenarios where sensor signals exhibit multiple operation conditions and design a sequence decomposition plugin to denoise input signals, aiming to enhance the performance of FGN. We evaluate the proposed model on two benchmark datasets, demonstrating its superior performance on the RUL prediction task compared to state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Song",
      "Laurens Bliek",
      "Yaoxin Wu",
      "Yingqian Zhang"
    ]
  },
  "https://openreview.net/forum?id=Wnd0XY0twh": {
    "title": "Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "main",
    "abstract": "Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines. Code is available at this repository: \\href{https://github.com/azencot-group/TSAA}{https://github.com/azencot-group/TSAA}",
    "checked": true,
    "id": "233d8d32faecb7a7e5544edaf437fe3f968d8cae",
    "semantic_title": "data augmentation policy search for long-term forecasting",
    "citation_count": 5,
    "authors": [
      "Liran Nochumsohn",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=M3SkSMfWcP": {
    "title": "Adaptive Multi-step Refinement Network for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "Point Cloud Registration (PCR) estimates the relative rigid transformation between two point clouds of the same scene. Despite significant progress with learning-based approaches, existing methods still face challenges when the overlapping region between the two point clouds is small. In this paper, we propose an adaptive multi-step refinement network that refines the registration quality at each step by leveraging the information from the preceding step. To achieve this, we introduce a training procedure and a refinement network. Firstly, to adapt the network to the current step, we utilize a generalized one-way attention mechanism, which prioritizes the last step's estimated overlapping region, and we condition the network on step indices. Secondly, instead of training the network to map either random transformations or a fixed pre-trained model's estimations to the ground truth, we train it on transformations with varying registration qualities, ranging from accurate to inaccurate, thereby enhancing the network's adaptiveness and robustness. Despite its conceptual simplicity, our method achieves state-of-the-art performance on both the 3DMatch/3DLoMatch and KITTI benchmarks. Notably, on 3DLoMatch, our method reaches 80.4% recall rate, with an absolute improvement of 1.2%",
    "checked": true,
    "id": "681d3d3ddc1319bed1f515197708a98467b3cd0c",
    "semantic_title": "adaptive multi-step refinement network for robust point cloud registration",
    "citation_count": 3,
    "authors": [
      "Zhi Chen",
      "Yufan Ren",
      "Tong Zhang",
      "Zheng Dang",
      "Wenbing Tao",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=zKv8qULV6n": {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "volume": "main",
    "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos",
    "checked": true,
    "id": "1a71f7b216b710b936da666027014adb83af8e7a",
    "semantic_title": "llava-onevision: easy visual task transfer",
    "citation_count": 754,
    "authors": [
      "Bo Li",
      "Yuanhan Zhang",
      "Dong Guo",
      "Renrui Zhang",
      "Feng Li",
      "Hao Zhang",
      "Kaichen Zhang",
      "Peiyuan Zhang",
      "Yanwei Li",
      "Ziwei Liu",
      "Chunyuan Li"
    ]
  },
  "https://openreview.net/forum?id=F5ALCh3GWG": {
    "title": "On the Regularization of Learnable Embeddings for Time Series Forecasting",
    "volume": "main",
    "abstract": "In forecasting multiple time series, accounting for the individual features of each sequence can be challenging. To address this, modern deep learning methods for time series analysis combine a shared (global) model with local layers, specific to each time series, often implemented as learnable embeddings. Ideally, these local embeddings should encode meaningful representations of the unique dynamics of each sequence. However, when these are learned end-to-end as parameters of a forecasting model, they may end up acting as mere sequence identifiers. Shared processing blocks may then become reliant on such identifiers, limiting their transferability to new contexts. In this paper, we address this issue by investigating methods to regularize the learning of local learnable embeddings for time series processing. Specifically, we perform the first extensive empirical study on the subject and show how such regularizations consistently improve performance in widely adopted architectures. Furthermore, we show that methods attempting to prevent the co-adaptation of local and global parameters by means of embeddings perturbation are particularly effective in this context. In this regard, we include in the comparison several perturbation-based regularization methods, going as far as periodically resetting the embeddings during training. The obtained results provide an important contribution to understanding the interplay between learnable local parameters and shared processing layers: a key challenge in modern time series processing models and a step toward developing effective foundation models for time series",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Butera",
      "Giovanni De Felice",
      "Andrea Cini",
      "Cesare Alippi"
    ]
  },
  "https://openreview.net/forum?id=JQGmbVK4Fr": {
    "title": "Towards context and domain-aware algorithms for scene analysis",
    "volume": "main",
    "abstract": "Interpersonal interactions and social situations in multimedia content encompass a rich blend of visual, textual, audio and contextual cues as well. However, contextual data integration in multimodal scene analysis research has often been overlooked, leading to incomplete interpretations. For instance, recognizing that two combatants in a video are positioned within a designated ring with a dedicated referee drastically alters the perception from a simple scuffle to a structured martial arts contest. This paper presents an innovative approach to scene analysis in video content, which not only incorporates contextual data but also emphasizes the most significant features during training. Additionally, we introduce a methodology for integrating domain knowledge into our framework. We evaluate our proposed methodology using two comprehensive datasets, demonstrating promising results compared to a baseline study using one of the datasets. These findings underscore the importance of integrating contextual data into multimodal video analysis, while also recognizing the challenges associated with their utilization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Serouis",
      "Florence Sèdes"
    ]
  },
  "https://openreview.net/forum?id=P5y82LKGbY": {
    "title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation",
    "volume": "main",
    "abstract": "Graph domain adaptation has recently enabled knowledge transfer across different graphs. However, without the semantic information on target graphs, the performance on target graphs is still far from satisfactory. To address the issue, we study the problem of active graph domain adaptation, which selects a small quantitative of informative nodes on the target graph for extra annotation. This problem is highly challenging due to the complicated topological relationships and the distribution discrepancy across graphs. In this paper, we propose a novel approach named Dual Consistency Delving with Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA consists of an edge-oriented graph subnetwork and a path-oriented graph subnetwork, which can explore topological semantics from complementary perspectives. In particular, our edge-oriented graph subnetwork utilizes the message passing mechanism to learn neighborhood information, while our path-oriented graph subnetwork explores high-order relationships from substructures. To jointly learn from two subnetworks, we roughly select informative candidate nodes with the consideration of consistency across two subnetworks. Then, we aggregate local semantics from its K-hop subgraph based on node degrees for topological uncertainty estimation. To overcome potential distribution shifts, we compare target nodes and their corresponding source nodes for discrepancy scores as an additional component for fine selection. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms various state-of-the-art approaches. The code implementation of DELTA is available at https://github.com/goose315/DELTA",
    "checked": true,
    "id": "fcc2cf3c39721b1b3d70d2dffeffb3bdb76a1c75",
    "semantic_title": "delta: dual consistency delving with topological uncertainty for active graph domain adaptation",
    "citation_count": 1,
    "authors": [
      "Pengyun Wang",
      "Yadi Cao",
      "Chris Russell",
      "Yanxin Shen",
      "Junyu Luo",
      "Ming Zhang",
      "Siyu Heng",
      "Xiao Luo"
    ]
  },
  "https://openreview.net/forum?id=gwXfZ3xkUq": {
    "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training",
    "volume": "main",
    "abstract": "Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. Despite its limitations, BFloat16 remains desirable for its computational efficiency, particularly given the substantial memory overhead required to extend the context window. To improve long-context training under BFloat16, we develop AnchorAttention, a plug-and-play attention method that enhances long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qian Liu",
      "Chao Du",
      "Tongyao Zhu",
      "Cunxiao Du",
      "Kenji Kawaguchi",
      "Tianyu Pang"
    ]
  },
  "https://openreview.net/forum?id=UYXPt7HUdl": {
    "title": "Score-Based Denoising Diffusion Models for Photon-Starved Image Restoration Problems",
    "volume": "main",
    "abstract": "Score-based denoising diffusion models have recently emerged as a powerful strategy to solve image restoration problems. Early diffusion models required problem-specific training. However, modern approaches can combine a likelihood function that is specified during test-time with a foundational pretrained diffusion model, which is used as an implicit prior in a Plug-and-Play (PnP) manner. This approach has been shown to deliver state-of-the-art performance in a wide range of image restoration problems involving Gaussian and mild Poisson noise. With extreme computer vision applications in mind, this paper presents the first PnP denoising diffusion method for photon-starved imaging problems. These problems arise in new quantum-enhanced imaging systems that exploit the particle nature of light to exceed the limitations of classical imaging. The problems involve highly challenging noise statistics, such as binomial, geometric, and low-intensity Poisson noise, which are difficult because of high uncertainty about the solution and because the models exhibit poor regularity properties (e.g., exploding scores, constraints). The proposed method is demonstrated on a series of challenging photon-starved imaging experiments with as little as 1 photon per pixel, where it delivers remarkably accurate solutions and outperforms alternative strategies from the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savvas Melidonis",
      "Yiming Xi",
      "Konstantinos C. Zygalakis",
      "Yoann Altmann",
      "Marcelo Pereyra"
    ]
  },
  "https://openreview.net/forum?id=W50i7r3DHE": {
    "title": "Instance-Aware Graph Prompt Learning",
    "volume": "main",
    "abstract": "Graph neural networks stand as the predominant technique for graph representation learning owing to their strong expressive power, yet the performance highly depends on the availability of high-quality labels in an end-to-end manner. Thus the pretraining and fine-tuning paradigm has been proposed to mitigate the label cost issue. Subsequently, the gap between the pretext tasks and downstream tasks has spurred the development of graph prompt learning which inserts a set of graph prompts into the original graph data with minimal parameters while preserving competitive performance. However, the current exploratory works are still limited since they all concentrate on learning fixed task-specific prompts which may not generalize well across the diverse instances that the task comprises. To tackle this challenge, we introduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming to generate distinct prompts tailored to different input instances. The process involves generating intermediate prompts for each instance using a lightweight architecture, quantizing these prompts through trainable codebook vectors, and employing the exponential moving average technique to ensure stable training. Extensive experiments conducted on multiple datasets and settings showcase the superior performance of IA-GPL compared to state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Jundong Li",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=xpnPYfufhz": {
    "title": "Partially Frozen Random Networks Contain Compact Strong Lottery Tickets",
    "volume": "main",
    "abstract": "Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning—strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated that SLTs could also be found within a randomly pruned source network. This phenomenon can be exploited to further compress the small memory size required by SLTs. However, their method is limited to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method for reducing the SLT memory size without restricting the sparsity of the SLTs that can be found. A random subset of the initial weights is frozen by either permanently pruning them or locking them as a fixed part of the SLT, resulting in a smaller model size. Experimental results show that Edge-Popup (Ramanujan et al., 2020; Sreenivasan et al., 2022) finds SLTs with better accuracy-to-model size trade-off within frozen networks than within dense or randomly pruned source networks. In particular, freezing $70\\%$ of a ResNet on ImageNet provides $3.3\\times$ compression compared to the SLT found within a dense counterpart, raises accuracy by up to $14.12$ points compared to the SLT found within a randomly pruned counterpart, and offers a better accuracy-model size trade-off than both",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Daiki Chijiwa",
      "Ángel López García-Arias",
      "Yasuyuki Okoshi",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Susumu Takeuchi",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=c7AAHdEYz5": {
    "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "main",
    "abstract": "Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of trained models when encountering input distribution shifts at test time. However, existing TTA methods often suffer significant performance drops when facing additional class distribution shifts. We first analyze TTA methods under label distribution shifts and identify the presence of class-wise confusion patterns commonly observed across different covariate shifts. Based on this observation, we introduce label Distribution shift-Aware prediction Refinement for Test-time adaptation (DART), a novel TTA method that refines the predictions by focusing on class-wise confusion patterns. DART trains a prediction refinement module during an intermediate time by exposing it to several batches with diverse class distributions using the training dataset. This module is then used during test time to detect and correct class distribution shifts, significantly improving pseudo-label accuracy for test data. Our method exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C, without any performance degradation when there is no label distribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DART's ability to correct inaccurate predictions caused by test-time distribution shifts. This improvement leads to enhanced performance in existing TTA methods, making DART a valuable plug-in tool",
    "checked": true,
    "id": "695573eff1cc003ed97e6b97b366a4e8d0e3c178",
    "semantic_title": "label distribution shift-aware prediction refinement for test-time adaptation",
    "citation_count": 0,
    "authors": [
      "Minguk Jang",
      "Hye Won Chung"
    ]
  },
  "https://openreview.net/forum?id=Q7aXOnEGgU": {
    "title": "On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing",
    "volume": "main",
    "abstract": "Weight sharing, equivariance, and local filters, as in convolutional neural networks, are believed to contribute to the sample efficiency of neural networks. However, it is not clear how each one of these design choices contributes to the generalization error. Through the lens of statistical learning theory, we aim to provide insight into this question by characterizing the relative impact of each choice on the sample complexity. We obtain lower and upper sample complexity bounds for a class of single hidden layer networks. For a large class of activation functions, the bounds depend merely on the norm of filters and are dimension-independent. We also provide bounds for max-pooling and an extension to multi-layer networks, both with mild dimension dependence. We provide a few takeaways from the theoretical results. It can be shown that depending on the weight-sharing mechanism, the non-equivariant weight-sharing can yield a similar generalization bound as the equivariant one. We show that locality has generalization benefits, however the uncertainty principle implies a trade-off between locality and expressivity. We conduct extensive experiments and highlight some consistent trends for these models",
    "checked": true,
    "id": "36526d1c7a4adaf8b5e276261903d7464dc8f89d",
    "semantic_title": "on the sample complexity of one hidden layer networks with equivariance, locality and weight sharing",
    "citation_count": 0,
    "authors": [
      "Arash Behboodi",
      "Gabriele Cesa"
    ]
  },
  "https://openreview.net/forum?id=2wgnepQjyF": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "main",
    "abstract": "Selective Prediction is the task of rejecting inputs a model would predict incorrectly on. This involves a trade-off between input space coverage (how many data points are accepted) and model utility (how good is the performance on accepted data points). Current methods for selective prediction typically impose constraints on either the model architecture or the optimization objective; this inhibits their usage in practice and introduces unknown interactions with pre-existing loss functions. In contrast to prior work, we show that state-of-the-art se- lective prediction performance can be attained solely from studying the (discretized) training dynamics of a model. We propose a general framework that, given a test input, monitors metrics capturing the instability of predictions from intermediate models (i.e., checkpoints) obtained during training w.r.t. the final model's prediction. In particular, we reject data points exhibiting too much disagreement with the final prediction at late stages in training. The proposed rejection mechanism is domain-agnostic (i.e., it works for both discrete and real-valued prediction) and can be flexibly combined with existing selective prediction approaches as it does not require any train-time modifications. Our experimental evaluation on image classification, regression, and time series problems shows that our method beats past state-of-the-art accuracy/utility trade-offs on typical selective prediction benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Israfil Bahceci",
      "Akram Bin Sediq",
      "HAMZA SOKUN",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=pxxmUKKgel": {
    "title": "How Does Code Pretraining Affect Language Model Task Performance?",
    "volume": "main",
    "abstract": "Large language models are increasingly trained on corpora containing both natural language and non-linguistic data like source code. Aside from aiding programming-related tasks, anecdotal evidence suggests that including code in pretraining corpora may improve performance on other, unrelated tasks, yet to date no work has been able to establish a causal connection by controlling between language and code data. Here we do just this. We pretrain language models on datasets which interleave natural language and code in two different settings: competitive, in which the total volume of data seen during pretraining is held constant; and additive, in which the volume of language data is held constant. We study how the pretraining mixture affects performance on (a) compositionality, measured by generalization accuracy on semantic parsing and syntactic transformation tasks, and more broadly on (b) downstream non-code-related objectives, measured by performance on tasks from the BigBench benchmark. We find that pretraining on higher proportions of code improves performance on compositional tasks involving structured output (like semantic parsing), and mathematics. Conversely, increase code mixture can harm performance on other tasks, including on tasks that requires sensitivity to linguistic structure such as syntax or morphology, and tasks measuring real-world knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Petty",
      "Sjoerd van Steenkiste",
      "Tal Linzen"
    ]
  },
  "https://openreview.net/forum?id=uDRzORdPT7": {
    "title": "DeepRRTime: Robust Time-series Forecasting with a Regularized INR Basis",
    "volume": "main",
    "abstract": "This work presents a simple, inexpensive, theoretically motivated regularization term to enhance the robustness of deep time-index models for time-series forecasting. Recently, DeepTime demonstrated that this class of models can rival state-of-the-art deep historical-value models on the long time-series forecasting (LTSF) benchmarks. The DeepTime framework comprises two key components: (1) a time-indexed basis parameterized as an implicit neural representation (INR), and (2) a meta-learning formulation that fits observed data to this basis via ridge regression, then extrapolates the result to generate forecasts. Our regularization term encourages the time-indexed basis elements to be more unit standardized and less mutually correlated, intended to enable more robust ridge regression. The regularized variant matches or outperforms DeepTime on all LTSF benchmarks. Moreover, it is significantly more resilient to missing values in the lookback window at test time, enhances forecast accuracy when applied to higher-frequency data than it was trained on, and boosts performance when trained on smaller datasets. Overall, we conclude that our regularized approach sets a new state-of-the-art for deep time-index models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Mahdi Gilany",
      "Kry Yik-Chau Lui",
      "Martin Magill",
      "Alexander Pashevich"
    ]
  },
  "https://openreview.net/forum?id=Lt2H8Bd8jF": {
    "title": "Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "The vast majority of Reinforcement Learning methods is largely impacted by the computation effort and data requirements needed to obtain effective estimates of action-value functions, which in turn determine the quality of the overall performance and the sample-efficiency of the learning procedure. Typically, action-value functions are estimated through an iterative scheme that alternates the application of an empirical approximation of the Bellman operator and a subsequent projection step onto a considered function space. It has been observed that this scheme can be potentially generalized to carry out multiple iterations of the Bellman operator at once, benefiting the underlying learning algorithm. However, until now, it has been challenging to effectively implement this idea, especially in high-dimensional problems. In this paper, we introduce iterated $Q$-Network (i-QN), a novel principled approach that enables multiple consecutive Bellman updates by learning a tailored sequence of action-value functions where each serves as the target for the next. We show that i-QN is theoretically grounded and that it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate the advantages of i-QN in Atari $2600$ games and MuJoCo continuous control problems",
    "checked": true,
    "id": "46233f3c5a0df20b9d7089586dfb54459aa148d1",
    "semantic_title": "iterated $q$-network: beyond one-step bellman updates in deep reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Théo Vincent",
      "Daniel Palenicek",
      "Boris Belousov",
      "Jan Peters",
      "Carlo D'Eramo"
    ]
  },
  "https://openreview.net/forum?id=jZBAVFGUUo": {
    "title": "Towards Measuring Predictability: To which extent data-driven approaches can extract deterministic relations from data exemplified with time series prediction and classification",
    "volume": "main",
    "abstract": "Minimizing loss functions is one important ingredient for machine learning to fit parameters such that the machine learning models extract relations hidden in the data. The smaller the loss function value on various splittings of a dataset, the better the machine learning model is assumed to perform. However, datasets are usually generated by dynamics consisting of deterministic components, where relations are clearly defined and consequently learnable, as well as stochastic parts where outcomes are random and thus not predictable. Depending on the amplitude of the deterministic and stochastic processes, the best achievable loss function value varies and is usually not known in real data science scenarios. In this research, a statistical framework is developed that provides measures to address the predictability of a target given the available input data and, after training a machine learning model, how much of the deterministic relations have been missed by the model. Consequently, the presented framework allows to differentiate model errors into unpredictable parts regarding the given input and a systematic miss of deterministic relations. The work extends the definition of model success or failure as well as the convergence of a training process. Moreover, it is demonstrated how such measures can enrich the procedure of model training. The framework is showcased with time series data on different synthetic and real-world datasets. The code is available at https://github.com/Saleh-Gholam-Zadeh/predictability_measure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh GHOLAM ZADEH",
      "Vaisakh Shaj",
      "Patrick Jahnke",
      "Gerhard Neumann",
      "Tim Breitenbach"
    ]
  },
  "https://openreview.net/forum?id=wIgRV336hC": {
    "title": "Minimax Lower Bounds for Estimating Distributions on Low-dimensional Spaces",
    "volume": "main",
    "abstract": "Recent statistical analyses of Generative Adversarial Networks (GAN) suggest that the error in estimating the target distribution in terms of the $\\beta$-H\\\"{o}lder Integral Probability Metric (IPM) scales as $\\mathcal{O}\\left(n^{-\\frac{\\beta}{\\overline{d}_{\\mathbb{M}}+\\delta}} \\vee n^{-1/2} \\log n \\right)$. Here $\\overline{d}_{\\mathbb{M}}$ is the upper Minkowski dimension of the corresponding support $\\mathbb{M}$ of the data distribution and $\\delta$ is a positive constant. It is, however, unknown as to whether this rate is minimax optimal, i.e. whether there are estimators that achieve a better test-error rate. This paper demonstrates that the minimax rate for estimating unknown distributions in the $\\beta$-H\\\"{o}lder IPM on $\\mathbb{M}$ scales as $\\Omega\\left(n^{-\\frac{\\beta}{\\underline{d}_{\\mathbb{M}}-\\delta}} \\vee n^{-1/2}\\right)$, where $\\underline{d}_{\\mathbb{M}}$ is the lower Minkowski dimension of $\\mathbb{M}$. Thus if the low-dimensional structure $\\mathbb{M}$ is regular in the Minkowski sense, i.e. $\\overline{d}_{\\mathbb{M}} = \\underline{d}_{\\mathbb{M}}$, GANs are roughly minimax optimal in estimating distributions on $\\mathbb{M}$. Further, the paper shows that the minimax estimation rate in the $p$-Wasserstein metric scales as $\\Omega\\left(n^{-\\frac{1}{\\underline{d}_{\\mathbb{M}}-\\delta}} \\vee n^{-1/(2p)}\\right)$",
    "checked": true,
    "id": "d9e8f6923d10eed08cb28aaadcd7c835791f2055",
    "semantic_title": "minimax lower bounds for estimating distributions on low-dimensional spaces",
    "citation_count": 0,
    "authors": [
      "Saptarshi Chakraborty"
    ]
  },
  "https://openreview.net/forum?id=XWAXcxNg4n": {
    "title": "Test-Time Adaptation with Source Based Auxiliary Tasks",
    "volume": "main",
    "abstract": "This work tackles a key challenge in Test Time Adaptation~(TTA): adapting on limited data. This challenge arises naturally from two scenarios. (i) Current TTA methods are limited by the bandwidth with which the stream reveals data, since conducting several adaptation steps on each revealed batch from the stream will lead to overfitting. (ii) In many realistic scenarios, the stream reveals insufficient data for the model to fully adapt to a given distribution shift. We tackle the first scenario problem with auxiliary tasks where we leverage unlabeled data from the training distribution. In particular, we propose distilling the predictions of an originally pretrained model on clean data during adaptation. We found that our proposed auxiliary task significantly accelerates the adaptation to distribution shifts. We report a performance improvement over the state of the art by 1.5\\% and 6\\% on average across all corruptions on ImageNet-C under episodic and continual evaluation, respectively. To combat the second scenario of limited data, we analyze the effectiveness of combining federated adaptation with our proposed auxiliary task across different models even when different clients observe different distribution shifts. We find that not only federated averaging enhances adaptation, but combining it with our auxiliary task provides a notable 6\\% performance gains over previous TTA methods",
    "checked": false,
    "id": "bbb65fbccd15c48847b996338bfdd119c9630214",
    "semantic_title": "test time adaptation for blind image quality assessment",
    "citation_count": 17,
    "authors": [
      "Motasem Alfarra",
      "Alvaro Correia",
      "Bernard Ghanem",
      "Christos Louizos"
    ]
  },
  "https://openreview.net/forum?id=HlzjI2fn2T": {
    "title": "On the stability of gradient descent with second order dynamics for time-varying cost functions",
    "volume": "main",
    "abstract": "Gradient based optimization algorithms deployed in Machine Learning (ML) applications are often analyzed and compared by their convergence rates or regret bounds. While these rates and bounds convey valuable information they don't always directly translate to stability guarantees. Stability and similar concepts, like robustness, will become ever more important as we move towards deploying models in real-time and safety critical systems. In this work we build upon the results in Gaudio et al. 2021 and Moreu & Annaswamy 2022 for gradient descent with second order dynamics when applied to explicitly time varying cost functions and provide more general stability guarantees. These more general results can aid in the design and certification of these optimization schemes so as to help ensure safe and reliable deployment for real-time learning applications. We also hope that the techniques provided here will stimulate and cross-fertilize the analysis that occurs on the same algorithms from the online learning and stochastic optimization communities",
    "checked": true,
    "id": "6b777bc1ac928c5bb66730b9b836df361cd0413b",
    "semantic_title": "on the stability of gradient descent with second order dynamics for time-varying cost functions",
    "citation_count": 0,
    "authors": [
      "Travis E Gibson",
      "Sawal Acharya",
      "Anjali Parashar",
      "Joseph Emilio Gaudio",
      "Anuradha Annaswamy"
    ]
  },
  "https://openreview.net/forum?id=O4CQ5AM5yP": {
    "title": "REX: GPU-Accelerated Sim2Real Framework with Delay and Dynamics Estimation",
    "volume": "main",
    "abstract": "Sim2real, the transfer of control policies from simulation to the real world, is crucial for efficiently solving robotic tasks without the risks associated with real-world learning. However, discrepancies between simulated and real environments, especially due to unmodeled dynamics and latencies, significantly impact the performance of these transferred policies. In this paper, we address the challenges of sim2real transfer caused by latency and asynchronous dynamics in real-world robotic systems. Our approach involves developing a novel framework, REX (Robotic Environments with jaX), that uses a graph-based simulation model to incorporate latency effects while optimizing for parallelization on accelerator hardware. Our framework simulates the asynchronous, hierarchical nature of real-world systems, while simultaneously estimating system dynamics and delays from real-world data and implementing delay compensation strategies to minimize the sim2real gap. We validate our approach on two real-world systems, demonstrating its effectiveness in improving sim2real performance by accurately modeling both system dynamics and delays. Our results show that the proposed framework supports both accelerated simulation and real-time processing, making it valuable for robot learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Jens Kober",
      "Robert Babuska",
      "Laura Ferranti"
    ]
  },
  "https://openreview.net/forum?id=udVkqIDYSM": {
    "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
    "volume": "main",
    "abstract": "We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM) framework for executing high-level robotic planning in a zero-shot regime. In our context, zero-shot high-level planning means that for a novel environment, we provide a VLLM with an image of the robot's surroundings and a task description, and the VLLM outputs the sequence of actions necessary for the robot to complete the task. Unlike previous methods for high-level visual planning for robotic manipulation, our method uses VLLMs for the entire planning process, enabling a more tightly integrated loop between perception, control, and planning. As a result, Wonderful Team's performance on real-world semantic and physical planning tasks often exceeds methods that rely on separate vision systems. For example, we see an average 40% success rate improvement on VimaBench over prior methods such as NLaP, an average 30% improvement over Trajectory Generators on tasks from the Trajectory Generator paper, including drawing and wiping a plate, and an average 70% improvement over Trajectory Generators on a new set of semantic reasoning tasks including environment rearrangement with implicit linguistic constraints. We hope these results highlight the rapid improvements of VLLMs in the past year, and motivate the community to consider VLLMs as an option for some high-level robotic planning problems in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidan Wang",
      "Rui Shen",
      "Bradly C. Stadie"
    ]
  },
  "https://openreview.net/forum?id=v47f4DwYZb": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "main",
    "abstract": "Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal y from the latent representation of a context signal x. JEPAs bypass the need for negative and positive samples, traditionally required by contrastive learning while avoiding the overfitting issues associated with generative pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm by proposing a Graph Joint-Embedding Predictive Architecture (Graph-JEPA). In particular, we employ masked modeling and focus on predicting the latent representations of masked subgraphs starting from the latent representation of a context subgraph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative prediction objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane. Through multiple experimental evaluations, we show that Graph-JEPA can learn highly semantic and expressive representations, as shown by the downstream performance in graph classification, regression, and distinguishing non-isomorphic graphs. The code is available at https://github.com/geriskenderi/graph-jepa",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geri Skenderi",
      "Hang Li",
      "Jiliang Tang",
      "Marco Cristani"
    ]
  },
  "https://openreview.net/forum?id=L7sQ8CW2FY": {
    "title": "Conformalized Credal Regions for Classification with Ambiguous Ground Truth",
    "volume": "main",
    "abstract": "An open question in Imprecise Probabilistic Machine Learning is how to empirically derive a credal region (i.e., a closed and convex family of probabilities on the output space) from the available data, without any prior knowledge or assumption. In classification problems, credal regions are a tool that is able to provide provable guarantees under realistic assumptions by characterizing the uncertainty about the distribution of the labels. Building on previous work, we show that credal regions can be directly constructed using conformal methods. This allows us to provide a novel extension of classical conformal prediction to problems with ambiguous ground truth, that is, when the exact labels for given inputs are not exactly known. The resulting construction enjoys desirable practical and theoretical properties: (i) conformal coverage guarantees, (ii) smaller prediction sets (compared to classical conformal prediction regions) and (iii) disentanglement of uncertainty sources (epistemic, aleatoric). We empirically verify our findings on both synthetic and real datasets",
    "checked": true,
    "id": "33dc60f595b7c84be7637680883c500e9c30ed24",
    "semantic_title": "conformalized credal regions for classification with ambiguous ground truth",
    "citation_count": 5,
    "authors": [
      "Michele Caprio",
      "David Stutz",
      "Shuo Li",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=2nRcWy3RLM": {
    "title": "Bridging Causality, Individual Fairness, and Adversarial Robustness in the Absence of Structural Causal Model",
    "volume": "main",
    "abstract": "Despite the essential need for comprehensive considerations in responsible AI, factors such as robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data and were unable to reflect counterfactual proximity. To address this, our paper introduces a \\emph{causal fair metric} formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the applications of the causal fair metric in classifiers. Empirical evaluation of real-world and synthetic datasets illustrates the effectiveness of our proposed metric in achieving an accurate classifier with fairness, resilience to adversarial perturbations, and a nuanced understanding of causal relationships",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ]
  },
  "https://openreview.net/forum?id=8tMMCf4YYn": {
    "title": "Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity",
    "volume": "main",
    "abstract": "We consider a partially personalized formulation of Federated Learning (FL) that strikes a balance between the flexibility of personalization and cooperativeness of global training. In our framework, we split the variables into global parameters, which are shared across all clients, and individual local parameters, which are kept private. We prove that under the right split of parameters, it is possible to find global parameters that allow each client to fit their data perfectly, and refer to the obtained problem as overpersonalized. For instance, the shared global parameters can be used to learn good data representations, whereas the personalized layers are fine-tuned for a specific client. Moreover, we present a simple algorithm for the partially personalized formulation that offers significant benefits to all clients. In particular, it breaks the curse of data heterogeneity in several settings, such as training with local steps, asynchronous training, and Byzantine-robust training",
    "checked": true,
    "id": "89de9757f408b499d45b741d8b91db08c181dada",
    "semantic_title": "partially personalized federated learning: breaking the curse of data heterogeneity",
    "citation_count": 11,
    "authors": [
      "Konstantin Mishchenko",
      "Rustem Islamov",
      "Eduard Gorbunov",
      "Samuel Horváth"
    ]
  },
  "https://openreview.net/forum?id=gLQ801ewwp": {
    "title": "Identifying Axiomatic Mathematical Transformation Steps using Tree-Structured Pointer Networks",
    "volume": "main",
    "abstract": "The classification of mathematical relations has become a new area of research in deep learning. A major focus lies on determining mathematical equivalence. While previous work has simply approached the task as a binary classification without providing further insight into the underlying decision, we aim to iteratively find a sequence of necessary steps to transform a mathematical expression into an arbitrary equivalent form. Each step in this sequence is specified by an axiom together with its position of application. We denote this task as Stepwise Equation Transformation Identification (SETI) task. To solve the task efficiently, we further propose TreePointerNet, a novel architecture which exploits the inherent tree structure of mathematical equations and consists of three key building blocks: (i) a transformer model tailored to work on hierarchically tree-structured equations, making use of (ii) a copy-pointer mechanism to extract the exact location of a transformation in the tree and finally (iii) custom embeddings that map distinguishable occurrences of the same token type to a common embedding. In addition, we introduce new datasets of equations for the SETI task. We benchmark our model against various baselines and perform an ablation study to quantify the influence of our custom embeddings and the copy-pointer component. Furthermore, we test the robustness of our model on data of unseen complexity. Our results clearly show that incorporating the hierarchical structure, embeddings and copy-pointer into a single model is highly beneficial for solving the SETI task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Wankerl",
      "Jan Pfister",
      "Andrzej Dulny",
      "Gerhard Götz",
      "Andreas Hotho"
    ]
  },
  "https://openreview.net/forum?id=p9KSFrTLx0": {
    "title": "Mixture Degree-Corrected Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "main",
    "abstract": "Multiplex graphs have emerged as a powerful tool for modeling complex data structures due to their ability to handle multiple relational layers. Clustering within a multiplex graph can involve merging vertices into communities that are consistent across all layers, grouping similar layers into clusters, or creating overlapping clusters among vertices and layers. However, a multiplex graph may exhibit distinct vertex communities based on the specific layers to which a vertex is connected. This scenario, termed multi-group community detection, significantly enhances the accuracy of clustering processes and aids in the interpretation of partitions. To date, the current literature on state-of-the-art community detection has not extensively addressed this modeling approach. In this paper, we introduce a novel methodology referred to as the \"Mixture Degree-Corrected Stochastic Block Model.\" This generative model, an extension of the widely utilized Degree-Corrected Stochastic Block Model (DCSBM), is designed to cluster similar layers by their community structures while simultaneously identifying communities within each layer's group. We provide a rigorous definition of the model and utilize an iterative technique to perform inference computations. Furthermore, we assess the identifiability of our proposed model and demonstrate the consistency of the maximum likelihood function through analytical analysis. The effectiveness of our method is evaluated using both real-word data sets and synthetic graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noureddine Henka",
      "Mohamad Assaad",
      "Sami Tazi"
    ]
  },
  "https://openreview.net/forum?id=x1dXvvElVd": {
    "title": "Interpreting Neurons in Deep Vision Networks with Language Models",
    "volume": "main",
    "abstract": "In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2× as likely to be selected as the best explanation for a neuron than the best baseline. Finally, we present a use case providing critical insights into land cover prediction models for sustainability applications",
    "checked": true,
    "id": "29c8873ec974145a3a2f847b105b5cff0d8dd85b",
    "semantic_title": "interpreting neurons in deep vision networks with language models",
    "citation_count": 6,
    "authors": [
      "Nicholas Bai",
      "Rahul Ajay Iyer",
      "Tuomas Oikarinen",
      "Akshay R. Kulkarni",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=f6yMdmrD2g": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "main",
    "abstract": "Training large scale Graph Neural Networks (GNNs) requires significant computational resources, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE, i.e., cores and/or GPUs) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work as batch sizes increase. Hence, it is favorable for processors equipped with a fast interconnect to work on a large minibatch together as a single larger processor, instead of working on separate smaller minibatches, even though global batch size is identical. We also show how to take advantage of the same phenomenon in serial execution by generating dependent consecutive minibatches. Our experimental evaluations show up to 4x bandwidth savings for fetching vertex embeddings, by simply increasing this dependency without harming model convergence. Combining our proposed approaches, we achieve up to 64\\% speedup over Independent Minibatching on single-node multi GPU systems, using same resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed Fatih Balin",
      "Dominique LaSalle",
      "Umit Catalyurek"
    ]
  },
  "https://openreview.net/forum?id=Ss9MTTN7OL": {
    "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains. However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine-tuning these models on private data. This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training. Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs. This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Miranda",
      "Elena Sofia Ruzzetti",
      "Andrea Santilli",
      "Fabio Massimo Zanzotto",
      "Sébastien Bratières",
      "Emanuele Rodolà"
    ]
  },
  "https://openreview.net/forum?id=OGaTF9iOxi": {
    "title": "Maximum Mean Discrepancy on Exponential Windows for Online Change Detection",
    "volume": "main",
    "abstract": "Detecting changes is of fundamental importance when analyzing data streams and has many applications, e.g., in predictive maintenance, fraud detection, or medicine. A principled approach to detect changes is to compare the distributions of observations within the stream to each other via hypothesis testing. Maximum mean discrepancy (MMD), a (semi-)metric on the space of probability distributions, provides powerful non-parametric two-sample tests on kernel-enriched domains. In particular, MMD is able to detect any disparity between distributions under mild conditions. However, classical MMD estimators suffer from a quadratic runtime complexity, which renders their direct use for change detection in data streams impractical. In this article, we propose a new change detection algorithm, called Maximum Mean Discrepancy on Exponential Windows (MMDEW), that combines the benefits of MMD with an efficient computation based on exponential windows. We prove that MMDEW enjoys polylogarithmic runtime and logarithmic memory complexity and show empirically that it outperforms the state of the art on benchmark data streams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Kalinke",
      "Marco Heyden",
      "Georg Gntuni",
      "Edouard Fouché",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=aWRMvXTvPf": {
    "title": "Shapley Values of Structured Additive Regression Models and Application to RKHS Weightings of Functions",
    "volume": "main",
    "abstract": "Shapley values are widely used in machine learning to interpret model predictions. However, they have an important drawback in their computational time, which is exponential in the number of variables in the data. Recent work has yielded algorithms that can efficiently and exactly calculate the Shapley values of specific model families, such as Decision Trees and Generalized Additive Models (GAMs). Unfortunately, these model families are fairly restricted. Consequently, we present STAR-SHAP, an algorithm for efficiently calculating the Shapley values of Structured Additive Regression (STAR) models, a generalization of GAMs which allow any number of variable interactions. While the computational cost of STAR-SHAP scales exponentially in the size of these interactions, it is independent of the total number of variables. This allows the interpretation of more complex and flexible models. As long as the variable interactions are moderately-sized, the computation of the Shapley values will be fast, even on high-dimensional datasets. Since STAR models with more than pairwise interactions (e.g. GA2Ms) are seldom used in practice, we also present a new class of STAR models built on the RKHS Weightings of Functions paradigm. More precisely, we introduce a new RKHS Weighting instantiation, and show how to transform it and other RKHS Weightings into STAR models. We therefore introduce a new family of STAR models, as well as the means to interpret their outputs in a timely manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Dubé",
      "Mario Marchand"
    ]
  },
  "https://openreview.net/forum?id=WppTEs4Kkn": {
    "title": "On the effects of similarity metrics in decentralized deep learning under distribution shift",
    "volume": "main",
    "abstract": "Decentralized Learning (DL) enables privacy-preserving collaboration among organizations or users to enhance the performance of local deep learning models. However, model aggregation becomes challenging when client data is heterogeneous, and identifying compatible collaborators without direct data exchange remains a pressing issue. In this paper, we investigate the effectiveness of various similarity metrics in DL for identifying peers for model merging, conducting an empirical analysis across multiple datasets with distribution shifts. Our research provides insights into the performance of these metrics, examining their role in facilitating effective collaboration. By exploring the strengths and limitations of these metrics, we contribute to the development of robust DL methods",
    "checked": false,
    "id": "2b6a6dc4011201400a2b2b173c53cee950511e73",
    "semantic_title": "on the effects of similarity metrics in decentralized deep learning under distributional shift",
    "citation_count": 0,
    "authors": [
      "Edvin Listo Zec",
      "Tom Hagander",
      "Eric Ihre-Thomason",
      "Sarunas Girdzijauskas"
    ]
  },
  "https://openreview.net/forum?id=VmfWywWuYQ": {
    "title": "Interactive Task Planning with Language Models",
    "volume": "main",
    "abstract": "An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals or distinct tasks, even during execution. However, most traditional methods require predefined module design, which makes it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models by incorporating both high-level planning and low-level skill execution through function calling, leveraging pretrained vision models to ground the scene in language. We verify the robustness of our system on the real world task of making milk tea drinks. Our system is able to generate novel high-level instructions for unseen objectives and successfully accomplishes user tasks. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Our approach is easy to adapt to different tasks by merely substituting the task guidelines, without the need for additional complex prompt engineering",
    "checked": true,
    "id": "50b59143bf3469f082b2308fa394bb6d55091a41",
    "semantic_title": "interactive task planning with language models",
    "citation_count": 37,
    "authors": [
      "Boyi Li",
      "Philipp Wu",
      "Pieter Abbeel",
      "Jitendra Malik"
    ]
  },
  "https://openreview.net/forum?id=4o8lIFkpn2": {
    "title": "\\copyright Plug-in Authorization for Human Copyright Protection in Text-to-Image Model",
    "volume": "main",
    "abstract": "This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities. State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community and model providers. To mitigate this, we propose the ©Plug-in Authorization framework, introducing three operations: addition, extraction, and combination. Addition involves training a ©plug-in for specific copyright, facilitating proper credit attribution. The extraction allows creators to reclaim copyright from infringing models, and the combination enables users to merge different ©plug-ins. These operations act as permits, incentivizing fair use and providing flexibility in authorization. We present innovative approaches, ``Reverse LoRA'' for extraction and ``EasyMerge'' for seamless combination. Experiments in artist-style replication and cartoon IP recreation demonstrate ©plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs. The code is available at \\url{https://github.com/zc1023/-Plug-in-Authorization.git}",
    "checked": false,
    "id": "17e7b58f99d5dbf6d7a117102f68d01aa84335b3",
    "semantic_title": "\\copyright plug-in authorization for human content copyright protection in text-to-image model",
    "citation_count": 2,
    "authors": [
      "Chao Zhou",
      "Huishuai Zhang",
      "Jiang Bian",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openreview.net/forum?id=3Y3o0yFZfu": {
    "title": "Private Fine-tuning of Large Language Models with Zeroth-order Optimization",
    "volume": "main",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) allows models to be trained in a privacy-preserving manner, but has proven difficult to scale to the era of foundation models. We introduce DP-ZO, a private fine-tuning method for large language models by privatizing zeroth order optimization methods. A key insight into the design of our method is that the direction of the gradient in the zeroth-order optimization we use is random and the only information from training data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO provides a strong privacy-utility trade-off across different tasks, and model sizes that are comparable to DP-SGD in $(\\varepsilon,\\delta)$-DP. Notably, DP-ZO possesses significant advantages over DP-SGD in memory efficiency, and obtains higher utility in $\\varepsilon$-DP when using the Laplace mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tang",
      "Ashwinee Panda",
      "Milad Nasr",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ]
  },
  "https://openreview.net/forum?id=bZzXgheUSD": {
    "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
    "volume": "main",
    "abstract": "The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer- based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ∼ 40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ∼ 1% of the number of parameters",
    "checked": true,
    "id": "4d143d446b0206d87f043bcdd34b37be182386bc",
    "semantic_title": "adapt to robustify prompt tuning vision transformers",
    "citation_count": 0,
    "authors": [
      "Masih Eskandar",
      "Tooba Imtiaz",
      "Zifeng Wang",
      "Jennifer Dy"
    ]
  },
  "https://openreview.net/forum?id=MO1slfU9xy": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "main",
    "abstract": "The performance of machine learning models on new data is critical for their success in real-world applications. Current methods to detect shifts in the input or output data distributions have limitations in identifying model behaviour changes when no labelled data is available. In this paper, we define \\emph{explanation shift} as the statistical comparison between how predictions from training data are explained and how predictions on new data are explained. We propose explanation shift as a key indicator to investigate the interaction between distribution shifts and learned models. We introduce an Explanation Shift Detector that operates on the explanation distributions, providing more sensitive and explainable changes in interactions between distribution shifts and learned models. We compare explanation shifts with other methods that are based on distribution shifts, showing that monitoring for explanation shifts results in more sensitive indicators for varying model behavior. We provide theoretical and experimental evidence and demonstrate the effectiveness of our approach on synthetic and real data. Additionally, we release an open-source Python package, \\texttt{skshift}, which implements our method and provides usage tutorials for further reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Mougan",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ]
  },
  "https://openreview.net/forum?id=zg0hPlABfY": {
    "title": "Enhancing Parameter Efficiency and Generalization in Large Models: A Regularized and Masked Low-Rank Adaptation Approach",
    "volume": "main",
    "abstract": "Large pre-trained models, such as large language models (LLMs), present significant resource challenges for fine-tuning due to their extensive parameter sizes, especially for applications in mobile systems. To address this, Low-Rank Adaptation (LoRA) has been developed to reduce resource consumption while maintaining satisfactory fine-tuning results. Despite its effectiveness, the original LoRA method faces the challenge of suboptimal performance. This paper investigates the intrinsic dimension of the matrix updates approximated by the LoRA method and reveals the performance benefits of increasing this intrinsic dimension. By employing regularization and a gradient masking method that encourages higher intrinsic dimension, the proposed method, termed Regularized and Masked LoRA (RM-LoRA), achieves superior generalization performance with the same or lower trainable parameter budget compared to the original LoRA and its latest variants across various open-source vision and language datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Mao",
      "Zihao Zhao",
      "Siqi Ping",
      "Yang Liu",
      "Wenbo Ding"
    ]
  },
  "https://openreview.net/forum?id=nu1SjVgSuy": {
    "title": "SPFormer: Enhancing Vision Transformer with Superpixel Representation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1e63c422fa2ee6cea9a37d9bff985a6fc968e1b5",
    "semantic_title": "spformer: enhancing vision transformer with superpixel representation",
    "citation_count": 4,
    "authors": [
      "Jieru Mei",
      "Liang-Chieh Chen",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=pWSrm3oP8b": {
    "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "29c826fe501cc1f5e672cbcc7af7c7b297c57e15",
    "semantic_title": "bridging the training-inference gap in llms by leveraging self-generated tokens",
    "citation_count": 3,
    "authors": [
      "Zhepeng Cen",
      "Yao Liu",
      "Siliang Zeng",
      "Pratik Chaudhari",
      "Huzefa Rangwala",
      "George Karypis",
      "Rasool Fakoor"
    ]
  },
  "https://openreview.net/forum?id=oeg2ncuSPz": {
    "title": "Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a6daf68e04beea2946bc83c1cdd2176651bf9c1",
    "semantic_title": "approximation rates and vc-dimension bounds for (p)relu mlp mixture of experts",
    "citation_count": 1,
    "authors": [
      "Anastasis Kratsios",
      "Haitz Sáez de Ocáriz Borde",
      "Takashi Furuya",
      "Marc T. Law"
    ]
  },
  "https://openreview.net/forum?id=hJHf7PCuVt": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Ling",
      "Zhimeng Jiang",
      "Na Zou",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=uF9ZdAwrCT": {
    "title": "In-distribution adversarial attacks on object recognition models using gradient-free search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spandan Madan",
      "Tomotake Sasaki",
      "Hanspeter Pfister",
      "Tzu-Mao Li",
      "Xavier Boix"
    ]
  },
  "https://openreview.net/forum?id=KbteA50cni": {
    "title": "Distributed Quasi-Newton Method for Fair and Fast Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi",
      "Linfeng Ye"
    ]
  },
  "https://openreview.net/forum?id=hMO8sT9qaD": {
    "title": "Making Reliable and Flexible Decisions in Long-tailed Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolian Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=dzQCRHKRdC": {
    "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Derezinski"
    ]
  },
  "https://openreview.net/forum?id=GDN5cFTNaL": {
    "title": "Adjacency Search Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meher Chaitanya",
      "Kshitijaa Jaglan",
      "Ulrik Brandes"
    ]
  },
  "https://openreview.net/forum?id=4Xz0WBAiX4": {
    "title": "ExCeL: Combined Extreme and Collective Logit Information for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen Karunanayake",
      "Suranga Seneviratne",
      "Sanjay Chawla"
    ]
  },
  "https://openreview.net/forum?id=8C8LJIqF4y": {
    "title": "Time Series Domain Adaptation via Channel-Selective Representation Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nauman Ahad",
      "Mark A. Davenport",
      "Eva L Dyer"
    ]
  },
  "https://openreview.net/forum?id=OOgsAZdFOt": {
    "title": "Can AI-Generated Text be Reliably Detected? Stress Testing AI Text Detectors Under Various Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinu Sankar Sadasivan",
      "Aounon Kumar",
      "Sriram Balasubramanian",
      "Wenxiao Wang",
      "Soheil Feizi"
    ]
  },
  "https://openreview.net/forum?id=aiOHc1LGpD": {
    "title": "Differentially Private Gradient Flow based on the Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilana Sebag",
      "Muni Sreenivas Pydi",
      "Jean-Yves Franceschi",
      "Alain Rakotomamonjy",
      "Mike Gartrell",
      "Jamal Atif",
      "Alexandre Allauzen"
    ]
  },
  "https://openreview.net/forum?id=dbaGuiYsTl": {
    "title": "Wasserstein Modality Alignment Makes Your Multimodal Transformer More Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhuo zhi",
      "Yuxuan Sun",
      "Qiangqiang Wu",
      "Ziquan Liu",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=AFxEdJwQcp": {
    "title": "A thorough reproduction and evaluation of $\\mu$P",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Vlassis",
      "David Belius",
      "Volodymyr Fomichov"
    ]
  },
  "https://openreview.net/forum?id=avDr56QjSI": {
    "title": "Semantic Alignment for Prompt-Tuning in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hari Chandana Kuchibhotla",
      "Sai Srinivas Kancheti",
      "Abbavaram Gowtham Reddy",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=Utjw2z1ale": {
    "title": "Identifying Spurious Correlations using Counterfactual Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Paul Cohen",
      "Louis Blankemeier",
      "Akshay S Chaudhari"
    ]
  },
  "https://openreview.net/forum?id=LVQ8BEL5n3": {
    "title": "Numerically Robust Fixed-Point Smoothing Without State Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Krämer"
    ]
  },
  "https://openreview.net/forum?id=r8UFp9olQ0": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Majellaro",
      "Jonathan Collu",
      "Aske Plaat",
      "Thomas M. Moerland"
    ]
  },
  "https://openreview.net/forum?id=Gb4HBGG9re": {
    "title": "Enhanced Federated Optimization: Adaptive Unbiased Client Sampling with Reduced Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dun Zeng",
      "Zenglin Xu",
      "Yu Pan",
      "Xu Luo",
      "Qifan Wang",
      "Xiaoying Tang"
    ]
  },
  "https://openreview.net/forum?id=vmmgFW3ztz": {
    "title": "Leveraging a Simulator for Learning Causal Representations from Post-Treatment Covariates for CATE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Nagalapatti",
      "Pranava Singhal",
      "Avishek Ghosh",
      "Sunita Sarawagi"
    ]
  },
  "https://openreview.net/forum?id=INijCSPtbQ": {
    "title": "Preventing Conflicting Gradients in Neural Marked Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=LZ9FmeFeLV": {
    "title": "Towards LifeSpan Cognitive Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Chi Han",
      "Tongtong Wu",
      "Xiaoxin He",
      "Wangchunshu Zhou",
      "Nafis Sadeq",
      "Xiusi Chen",
      "Zexue He",
      "Wei Wang",
      "Gholamreza Haffari",
      "Heng Ji",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=IIVr4Hu3Oi": {
    "title": "Distributed Multi-Agent Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prithviraj Tarale",
      "Edward Rietman",
      "Hava T Siegelmann"
    ]
  },
  "https://openreview.net/forum?id=T5OuTgPxHS": {
    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://openreview.net/forum?id=0mGho8wrv5": {
    "title": "SelfEval: Leveraging discriminative nature of generative models for evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Saketh Rambhatla",
      "Ishan Misra"
    ]
  },
  "https://openreview.net/forum?id=dHljjaNHh1": {
    "title": "Fairness Through Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ]
  },
  "https://openreview.net/forum?id=V2SD2uVKEE": {
    "title": "Zero-shot CLIP Class Forgetting via Text-image Space Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Kravets",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openreview.net/forum?id=lTX4bYREAZ": {
    "title": "A Scalable Approach for Mapper via Efficient Spatial Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Simi"
    ]
  },
  "https://openreview.net/forum?id=VIkycTWDWo": {
    "title": "Doubly Robust Conditional VAE via Decoder Calibration: An Implicit KL Annealing Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhui Liu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=IZrt6hB2sI": {
    "title": "Improving CLIP Counting Accuracy via Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisu Zhang",
      "Yicong Chen",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=ccu0M3nmlF": {
    "title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koki Okajima",
      "Tomoyuki Obuchi"
    ]
  },
  "https://openreview.net/forum?id=qbrE0LR7fF": {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer",
      "Daniel Ramos"
    ]
  },
  "https://openreview.net/forum?id=LdflD41Gn8": {
    "title": "On the Properties and Estimation of Pointwise Mutual Information Profiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paweł Czyż",
      "Frederic Grabowski",
      "Julia E Vogt",
      "Niko Beerenwinkel",
      "Alexander Marx"
    ]
  },
  "https://openreview.net/forum?id=BlYIPa0Fx1": {
    "title": "An analysis of the noise schedule for score-based generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislas Strasman",
      "Antonio Ocello",
      "Claire Boyer",
      "Sylvain Le Corff",
      "Vincent Lemaire"
    ]
  },
  "https://openreview.net/forum?id=PtD2gVmb3J": {
    "title": "Global Safe Sequential Learning via Efficient Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cen-You Li",
      "Olaf Dünnbier",
      "Marc Toussaint",
      "Barbara Rakitsch",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=QQE5j2OsLW": {
    "title": "Can Optimization Trajectories Explain Multi-Task Transfer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mueller",
      "Mark Dredze",
      "Nicholas Andrews"
    ]
  },
  "https://openreview.net/forum?id=yzbAFf8vd5": {
    "title": "A comparison between humans and AI at recognizing objects in unusual poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Netta Ollikka",
      "Amro Kamal Mohamed Abbas",
      "Andrea Perin",
      "Markku Kilpeläinen",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=V6ia5hWIMD": {
    "title": "νSAM: Memory-Efficient Sharpness-Aware Minimization via Nuclear Norm Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Parameswaran Raman",
      "Lenon Minorics",
      "Mingyi Hong",
      "Shoham Sabach",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WEYMCLu8u7": {
    "title": "Event-Triggered Time-Varying Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Brunzema",
      "Alexander von Rohr",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=PTTa3U29NR": {
    "title": "Optimization Dynamics of Equivariant and Augmented Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oskar Nordenfors",
      "Fredrik Ohlsson",
      "Axel Flinth"
    ]
  },
  "https://openreview.net/forum?id=QplBL2pV4Z": {
    "title": "Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Yin Huang",
      "Ruinan Jin",
      "Can Zhao",
      "Daguang Xu",
      "Xiaoxiao Li"
    ]
  },
  "https://openreview.net/forum?id=XL1N6iLr0G": {
    "title": "An Attribute-based Method for Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Reiss",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=8mgX3Uw2Ea": {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Zhu",
      "Sheng Liu",
      "Carlos Fernandez-Granda",
      "Narges Razavian"
    ]
  },
  "https://openreview.net/forum?id=tRpWaK3pWh": {
    "title": "A Generalization Bound for Nearly-Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Golikov"
    ]
  },
  "https://openreview.net/forum?id=Qq4ge9Qe31": {
    "title": "Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Perini",
      "Maja Rudolph",
      "Sabrina Schmedding",
      "Chen Qiu"
    ]
  },
  "https://openreview.net/forum?id=BhOJreYmur": {
    "title": "MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Guerra-Manzanares",
      "Farah Shamout"
    ]
  },
  "https://openreview.net/forum?id=b68QOenPWy": {
    "title": "Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leah Bar",
      "Boaz Lerner",
      "Nir Darshan",
      "Rami Ben-Ari"
    ]
  },
  "https://openreview.net/forum?id=I4IAwVOZrM": {
    "title": "Lifelong Learning in StyleGAN through Latent Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Kappiyath",
      "ANMOL GARG",
      "Ramya Hebbalaguppe",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=bwRxXiGO9A": {
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Boizard",
      "Kevin El Haddad",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=CTkABQvnkm": {
    "title": "Decoupled Sequence and Structure Generation for Realistic Antibody Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nayoung Kim",
      "Minsu Kim",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://openreview.net/forum?id=pxdSm7PW5Q": {
    "title": "Reviving Life on the Edge: Joint Score-Based Graph Generation of Rich Edge Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrod Berman",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=60Gi1w6hte": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc T. Law",
      "Karsten Kreis",
      "Haggai Maron"
    ]
  },
  "https://openreview.net/forum?id=eakh1Edffd": {
    "title": "Reinforcement learning with non-ergodic reward increments: robustness via ergodicity transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Erfaun Noorani",
      "James Price",
      "Ole Peters",
      "Colm Connaughton",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=bHdEtW5E7O": {
    "title": "Federated Learning with Efficient Local Adaptation for Realized Volatility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhao",
      "Lin Cai",
      "Wu-Sheng Lu"
    ]
  },
  "https://openreview.net/forum?id=MvYddudHuE": {
    "title": "Reweighting Improves Conditional Risk Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Jiahe Lin",
      "Fengpei Li",
      "Songzhu Zheng",
      "Anant Raj",
      "Anderson Schneider",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://openreview.net/forum?id=vZGZIIgcG4": {
    "title": "Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Rahbar",
      "Niklas Åkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=DqWvxSQ1TK": {
    "title": "From Promise to Practice: A Study of Common Pitfalls Behind the Generalization Gap in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeideh Ghanbari Azar",
      "Lorenzo Tronchin",
      "Attila Simkó",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=3mJZfL77WM": {
    "title": "Highway Graph to Accelerate Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidu Yin",
      "Zhen Zhang",
      "Dong Gong",
      "Stefano V Albrecht",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=DCAeXwLenB": {
    "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Fred Maurice NGOLE MBOULA",
      "Antoine Souloumiac"
    ]
  },
  "https://openreview.net/forum?id=gpHOtQQPJG": {
    "title": "Optimization and Generalization Guarantees for Weight Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Cisneros-Velarde",
      "Zhijie Chen",
      "Sanmi Koyejo",
      "Arindam Banerjee"
    ]
  },
  "https://openreview.net/forum?id=LzmsvRTqaJ": {
    "title": "Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative model for Quasar spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidhi Lalchand",
      "Anna-Christina Eilers"
    ]
  },
  "https://openreview.net/forum?id=fqkq1MgONB": {
    "title": "BM$^2$: Coupled Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Peluchetti"
    ]
  },
  "https://openreview.net/forum?id=wS1fD0ofay": {
    "title": "Partial-Label Learning with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fuchs",
      "Florian Kalinke",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=34vtRA3Nvu": {
    "title": "PRIMO: Private Regression in Multiple Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seth Neel"
    ]
  },
  "https://openreview.net/forum?id=ytKFKoCpyK": {
    "title": "ODNet: Opinion Dynamics-Inspired Neural Message Passing for Graphs and Hypergraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Zhou",
      "Outongyi Lv",
      "Jing Wang",
      "Xiang Xiao",
      "Weishu Zhao"
    ]
  },
  "https://openreview.net/forum?id=0u7pWfjri5": {
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=UrSgGSTM2J": {
    "title": "Minimax Posterior Contraction Rates for Unconstrained Distribution Estimation on $[0,1]^d$ under Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Matthew Jacobs",
      "Lekha Patel",
      "Anirban Bhattacharya",
      "Debdeep Pati"
    ]
  },
  "https://openreview.net/forum?id=9CWU8Oi86d": {
    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Yuzheng Hu",
      "Yong Lin",
      "Tong Zhang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=yawWz4qWkF": {
    "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Alina Fastowski",
      "Felix Pfeiffer",
      "Gjergji Kasneci"
    ]
  },
  "https://openreview.net/forum?id=CNaiJRcX84": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Paul E. Apolinario",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=iVV7IzI55V": {
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=AjJTg5M0r8": {
    "title": "Slicing Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Bonet",
      "Kimia Nadjahi",
      "Thibault Sejourne",
      "Kilian FATRAS",
      "Nicolas Courty"
    ]
  },
  "https://openreview.net/forum?id=yBgTVWccIx": {
    "title": "DafnyBench: A Benchmark for Formal Software Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chloe R Loughridge",
      "Qinyi Sun",
      "Seth Ahrenbach",
      "Federico Cassano",
      "Chuyue Sun",
      "Ying Sheng",
      "Anish Mudide",
      "Md Rakib Hossain Misu",
      "Nada Amin",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=x8wscCAJ2m": {
    "title": "Sparse Neural Architectures via Deterministic Ramanujan Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suryam Arnav Kalra",
      "Arindam Biswas",
      "Pabitra Mitra",
      "BISWAJIT BASU"
    ]
  },
  "https://openreview.net/forum?id=JHxrh00W1j": {
    "title": "Masked Capsule Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=gqh0yzPYdo": {
    "title": "No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu Gaur",
      "Darshan Singh S",
      "Makarand Tapaswi"
    ]
  },
  "https://openreview.net/forum?id=Conma3qnaT": {
    "title": "Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Verma",
      "Gantavya Bhatt",
      "Avi Schwarzschild",
      "Soumye Singhal",
      "Arnav Mohanty Das",
      "Chirag Shah",
      "John P Dickerson",
      "Pin-Yu Chen",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=XDbY3qhM42": {
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang ZHANG",
      "Joshua M. Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ]
  },
  "https://openreview.net/forum?id=oYP2Pd5aQt": {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Gao",
      "Chuanyang Zheng",
      "Enze Xie",
      "Han Shi",
      "Tianyang Hu",
      "Yu Li",
      "Michael Ng",
      "Zhenguo Li",
      "Zhaoqiang Liu"
    ]
  },
  "https://openreview.net/forum?id=Og3VxBFhwj": {
    "title": "Linear Convergence of Decentralized FedAvg for PL Objectives: The Interpolation Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti P Maralappanavar",
      "Prashant Khanduri",
      "Bharath B N"
    ]
  },
  "https://openreview.net/forum?id=XxbQAsxrRC": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franka Bause",
      "Fabian Jogl",
      "Patrick Indri",
      "Tamara Drucks",
      "David Penz",
      "Nils Morten Kriege",
      "Thomas Gärtner",
      "Pascal Welke",
      "Maximilian Thiessen"
    ]
  },
  "https://openreview.net/forum?id=aV6dCg1VFV": {
    "title": "Investigating the impact of missing value handling on Boosted trees and Deep learning for Tabular data: A Claim Reserving case study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Larionov",
      "Niall M. Adams",
      "Kevin N. Webster"
    ]
  },
  "https://openreview.net/forum?id=IK2cR89z45": {
    "title": "Personalized Privacy Amplification via Importance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fay",
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=bwyHf5eery": {
    "title": "A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data and Overparameterization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Johannes Zenn",
      "Robert Bamler"
    ]
  },
  "https://openreview.net/forum?id=kzPNHQ8ByY": {
    "title": "Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihong Yu",
      "Manav Mishra",
      "Alec Koppel",
      "Carl Busart",
      "Priya Narayan",
      "Dinesh Manocha",
      "Amrit Singh Bedi",
      "Pratap Tokekar"
    ]
  },
  "https://openreview.net/forum?id=Vq0wMFBjo2": {
    "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zang",
      "Tian Yun",
      "Hao Tan",
      "Trung Bui",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=o58uy91V2V": {
    "title": "On the Detection of Reviewer-Author Collusion Rings From Paper Bidding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Jecmen",
      "Nihar B Shah",
      "Fei Fang",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=ZA7D4nQuQF": {
    "title": "Transformers in Uniform TC$^0$",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=SeGNvJJjbs": {
    "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo"
    ]
  },
  "https://openreview.net/forum?id=KqRnsEMYLx": {
    "title": "Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madison Cooley",
      "Varun Shankar",
      "Mike Kirby",
      "Shandian Zhe"
    ]
  },
  "https://openreview.net/forum?id=nxQtoHHcj9": {
    "title": "An Analysis of Model Robustness across Concurrent Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongho Jeon",
      "Suhwan Choi",
      "Hyoje Lee",
      "Teresa Yeo"
    ]
  },
  "https://openreview.net/forum?id=ZnWqtPhHM7": {
    "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarshi Brahma",
      "Anuska Roy",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=JN7iNWaPTe": {
    "title": "Mental Modelling of Reinforcement Learning Agents by Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Lu",
      "Xufeng Zhao",
      "Josua Spisak",
      "Jae Hee Lee",
      "Stefan Wermter"
    ]
  },
  "https://openreview.net/forum?id=PzmaWLqK0e": {
    "title": "Reward-based Autonomous Online Learning Framework for Resilient Cooperative Target Monitoring using a Swarm of Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhankar Gupta",
      "Saksham Sharma",
      "Suresh Sundaram"
    ]
  },
  "https://openreview.net/forum?id=edULLIVnoc": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Cohen-Wang",
      "Joshua Vendrow",
      "Aleksander Madry"
    ]
  },
  "https://openreview.net/forum?id=xQbRFHfgGL": {
    "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivanshu Shekhar",
      "Shreyas Singh",
      "Tong Zhang"
    ]
  },
  "https://openreview.net/forum?id=Y8EspxaksH": {
    "title": "Faithful Interpretation for Graph Neural Networks",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Lu Yu",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ]
  },
  "https://openreview.net/forum?id=mAiMKnr9r5": {
    "title": "Random Policy Enables In-Context Reinforcement Learning within Trust Horizons",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqin Chen",
      "Santiago Paternain"
    ]
  },
  "https://openreview.net/forum?id=1p9hQTbjgo": {
    "title": "MiniFold: Simple, Fast, and Accurate Protein Structure Prediction",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Wohlwend",
      "Mateo Reveiz",
      "Matt McPartlon",
      "Axel Feldmann",
      "Wengong Jin",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=ssXSrZ94sR": {
    "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kay",
      "Timm Haucke",
      "Suzanne Stathatos",
      "Siqi Deng",
      "Erik Young",
      "Pietro Perona",
      "Sara Beery",
      "Grant Van Horn"
    ]
  },
  "https://openreview.net/forum?id=x6fXnsM9Ez": {
    "title": "The 2023 Foundation Model Transparency Index",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Betty Xiong",
      "Daniel Zhang",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=1Avb4jYjLb": {
    "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Nikhil Anand",
      "Nikhil Vyas",
      "Eran Malach",
      "Sham M. Kakade"
    ]
  },
  "https://openreview.net/forum?id=Y7dRmpGiHj": {
    "title": "What is the Relationship between Tensor Factorizations and Circuits (and How Can We Exploit it)?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Loconte",
      "Antonio Mari",
      "Gennaro Gala",
      "Robert Peharz",
      "Cassio de Campos",
      "Erik Quaeghebeur",
      "Gennaro Vessio",
      "Antonio Vergari"
    ]
  },
  "https://openreview.net/forum?id=YCt8lsIDwA": {
    "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yu",
      "Chenchen Zhu",
      "Sean Culatana",
      "Raghuraman Krishnamoorthi",
      "Fanyi Xiao",
      "Yong Jae Lee"
    ]
  },
  "https://openreview.net/forum?id=vKUPXuEzj8": {
    "title": "Reproducibility Study of 'SLICE: Stabilized LIME for Consistent Explanations for Image Classification",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Bandyopadhyay",
      "Chiranjeev Bindra",
      "Roan van Blanken",
      "Arijit Ghosh"
    ]
  },
  "https://openreview.net/forum?id=TJRyDi7mwH": {
    "title": "NeoBERT: A Next Generation BERT",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lola Le Breton",
      "Quentin Fournier",
      "Mariam El Mezouar",
      "Sarath Chandar"
    ]
  },
  "https://openreview.net/forum?id=855yo1Ubt2": {
    "title": "An Expanded Benchmark that Rediscovers and Affirms the Edge of Uncertainty Sampling for Active Learning in Tabular Datasets",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Po-Yi Lu",
      "Yi-Jie Cheng",
      "Chun-Liang Li",
      "Hsuan-Tien Lin"
    ]
  },
  "https://openreview.net/forum?id=H6DtMcZf5s": {
    "title": "Remembering to Be Fair Again: Reproducing Non-Markovian Fairness in Sequential Decision Making",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Domonkos Nagy",
      "Lohithsai Yadala Chanchu",
      "Krystof Bobek",
      "Xin Zhou",
      "Jacobus Smit"
    ]
  },
  "https://openreview.net/forum?id=zLfLTHOdZW": {
    "title": "[RE] GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyme Chatupanyachotikul",
      "Leonard Horns",
      "Matei Nastase"
    ]
  },
  "https://openreview.net/forum?id=l9rATNBB8Y": {
    "title": "Privacy Awareness for Information-Sharing Assistants: A Case-study on Form-filling with Contextual Integrity",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahra Ghalebikesabi",
      "Eugene Bagdasarian",
      "Ren Yi",
      "Itay Yona",
      "Ilia Shumailov",
      "Aneesh Pappu",
      "Chongyang Shi",
      "Laura Weidinger",
      "Robert Stanforth",
      "Leonard Berrada",
      "Pushmeet Kohli",
      "Po-Sen Huang",
      "Borja Balle"
    ]
  },
  "https://openreview.net/forum?id=sXr1fRjs1N": {
    "title": "Contextualized Messages Boost Graph Representations",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Godwin Lim",
      "Galvin Brice Sy Lim",
      "Renzo Roel Tan",
      "Kazushi Ikeda"
    ]
  },
  "https://openreview.net/forum?id=yeITEuhv4Q": {
    "title": "Revisiting Deep Hybrid Models for Out-of-Distribution Detection",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Ruben Schlumbom",
      "Eibe Frank"
    ]
  },
  "https://openreview.net/forum?id=mSoDRZXsqj": {
    "title": "Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Billy Joe Franks",
      "Moshe Eliasof",
      "Semih Cantürk",
      "Guy Wolf",
      "Carola-Bibiane Schönlieb",
      "Sophie Fellenz",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=rKAkp1f3R7": {
    "title": "Shedding Light on Problems with Hyperbolic Graph Learning",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isay Katsman",
      "Anna Gilbert"
    ]
  },
  "https://openreview.net/forum?id=IPmzyQSiQE": {
    "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zach Nussbaum",
      "John Xavier Morris",
      "Andriy Mulyar",
      "Brandon Duderstadt"
    ]
  },
  "https://openreview.net/forum?id=wcxrJcJ7vq": {
    "title": "The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgi Ganev",
      "Meenatchi Sundaram Muthu Selva Annamalai",
      "Emiliano De Cristofaro"
    ]
  },
  "https://openreview.net/forum?id=wF3ZtSlOcT": {
    "title": "Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Sidiropoulos",
      "Samarth Bhargav",
      "Panagiotis Eustratiadis",
      "Evangelos Kanoulas"
    ]
  },
  "https://openreview.net/forum?id=knv4lQFVoE": {
    "title": "A general framework of Riemannian adaptive optimization methods with a convergence analysis",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Sakai",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=tf6A9EYMo6": {
    "title": "Personalization of Large Language Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Ryan A. Rossi",
      "Branislav Kveton",
      "Yijia Shao",
      "Diyi Yang",
      "Hamed Zamani",
      "Franck Dernoncourt",
      "Joe Barrow",
      "Tong Yu",
      "Sungchul Kim",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Tyler Derr",
      "Hongjie Chen",
      "Junda Wu",
      "Xiang Chen",
      "Zichao Wang",
      "Subrata Mitra",
      "Nedim Lipka",
      "Nesreen K. Ahmed",
      "Yu Wang"
    ]
  },
  "https://openreview.net/forum?id=wHECkBOwyt": {
    "title": "Efficient Diffusion Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Shen",
      "Jingxuan Zhang",
      "Boning Xiong",
      "Rui Hu",
      "Shoufa Chen",
      "Zhongwei Wan",
      "Xin Wang",
      "Yu Zhang",
      "Zixuan Gong",
      "Guangyin Bao",
      "Chaofan Tao",
      "Yongfeng Huang",
      "Ye Yuan",
      "Mi Zhang"
    ]
  },
  "https://openreview.net/forum?id=ewwNKwh6SK": {
    "title": "Conditional Image Synthesis with Diffusion Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhan",
      "Defang Chen",
      "Jian-Ping Mei",
      "Zhenghe Zhao",
      "Jiawei Chen",
      "Chun Chen",
      "Siwei Lyu",
      "Can Wang"
    ]
  },
  "https://openreview.net/forum?id=ZiJYahyXLU": {
    "title": "Machine Learning with Physics Knowledge for Prediction: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Watson",
      "Chen Song",
      "Oliver Weeger",
      "Theo Gruner",
      "An Thai Le",
      "Kay Hansel",
      "Ahmed Hendawy",
      "Oleg Arenz",
      "Will Trojak",
      "Miles Cranmer",
      "Carlo D'Eramo",
      "Fabian Buelow",
      "Tanmay Goyal",
      "Jan Peters",
      "Martin W Hoffmann"
    ]
  },
  "https://openreview.net/forum?id=CsoSWpR5xC": {
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiachong Feng",
      "Longxu Dou",
      "Minzhi Li",
      "Qinghao Wang",
      "Yu Guo",
      "Haochuan Wang",
      "Chang Ma",
      "Lingpeng Kong"
    ]
  },
  "https://openreview.net/forum?id=D1PPuk8ZBI": {
    "title": "When Should Reinforcement Learning Use Causal Reasoning?",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Schulte",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=fHf4jbIfex": {
    "title": "Graph Theory-Based Deep Graph Similarity Learning: A Unified Survey of Pipeline, Techniques, and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouyang LIU",
      "Ning Liu",
      "Yixin Chen",
      "Ziqing Wen",
      "Jiezhong He",
      "Dongsheng Li"
    ]
  },
  "https://openreview.net/forum?id=u0azVc9Y0y": {
    "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Colin Raffel",
      "Mohammed Muqeeth",
      "Lucas Caccia",
      "Haokun Liu",
      "Tianlong Chen",
      "Mohit Bansal",
      "Leshem Choshen",
      "Alessandro Sordoni"
    ]
  },
  "https://openreview.net/forum?id=1BqXkjNEGP": {
    "title": "Autoregressive Models in Vision: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Xiong",
      "Gongye Liu",
      "Lun Huang",
      "Chengyue Wu",
      "Taiqiang Wu",
      "Yao Mu",
      "Yuan Yao",
      "Hui Shen",
      "Zhongwei Wan",
      "Jinfa Huang",
      "Chaofan Tao",
      "Shen Yan",
      "Huaxiu Yao",
      "Lingpeng Kong",
      "Hongxia Yang",
      "Mi Zhang",
      "Guillermo Sapiro",
      "Jiebo Luo",
      "Ping Luo",
      "Ngai Wong"
    ]
  },
  "https://openreview.net/forum?id=M7Lhr2anjg": {
    "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofiane ENNADIR",
      "Gabriela Zarzar Gandler",
      "Filip Cornell",
      "Lele Cao",
      "Oleg Smirnov",
      "Tianze Wang",
      "Levente Zólyomi",
      "Björn Brinne",
      "Sahar Asadi"
    ]
  },
  "https://openreview.net/forum?id=vz5P1Kbt6t": {
    "title": "Adaptive Physics-informed Neural Networks: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edgar Torres",
      "Mathias Niepert"
    ]
  },
  "https://openreview.net/forum?id=1nO4qFMiS0": {
    "title": "Open Problems in Technical AI Governance",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anka Reuel",
      "Benjamin Bucknall",
      "Stephen Casper",
      "Timothy Fist",
      "Lisa Soder",
      "Onni Aarne",
      "Lewis Hammond",
      "Lujain Ibrahim",
      "Alan Chan",
      "Peter Wills",
      "Markus Anderljung",
      "Ben Garfinkel",
      "Lennart Heim",
      "Andrew Trask",
      "Gabriel Mukobi",
      "Rylan Schaeffer",
      "Mauricio Baker",
      "Sara Hooker",
      "Irene Solaiman",
      "Sasha Luccioni",
      "Nitarshan Rajkumar",
      "Nicolas Moës",
      "Jeffrey Ladish",
      "David Bau",
      "Paul Bricman",
      "Neel Guha",
      "Jessica Newman",
      "Yoshua Bengio",
      "Tobin South",
      "Alex Pentland",
      "Sanmi Koyejo",
      "Mykel Kochenderfer",
      "Robert Trager"
    ]
  },
  "https://openreview.net/forum?id=FJgtVfUxLQ": {
    "title": "A Survey on the Honesty of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ]
  },
  "https://openreview.net/forum?id=QTsJXSvAI2": {
    "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amer Essakine",
      "Yanqi Cheng",
      "Chun-Wun Cheng",
      "Lipei Zhang",
      "Zhongying Deng",
      "Lei Zhu",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=sZdtTJInUg": {
    "title": "Class Incremental Learning from First Principles: A Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Ashtekar",
      "Jingxi Zhu",
      "Vasant G Honavar"
    ]
  },
  "https://openreview.net/forum?id=ukLxqA8zXj": {
    "title": "Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Hedström",
      "Philine Lou Bommer",
      "Thomas F Burns",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=RGsdAwWuu6": {
    "title": "Unified Risk Analysis for Weakly Supervised Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=YxKJihRcby": {
    "title": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SUBBA REDDY OOTA",
      "Zijiao Chen",
      "Manish Gupta",
      "Bapi Raju Surampudi",
      "Gael Jobard",
      "Frederic Alexandre",
      "Xavier Hinaut"
    ]
  },
  "https://openreview.net/forum?id=WUQsBiJqyP": {
    "title": "A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiliang Liu",
      "Sheng Xu",
      "Shicheng Liu",
      "Ashish Gaurav",
      "Sriram Ganapathi Subramanian",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=wZLWuFHxt5": {
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "XIAOYU XU",
      "Xiaobao Wu",
      "Jie Fu",
      "Feng Yichao",
      "Fengjun Pan",
      "Anh Tuan Luu"
    ]
  },
  "https://openreview.net/forum?id=RJT1baPhdV": {
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ]
  },
  "https://openreview.net/forum?id=ON7dtdEHVQ": {
    "title": "(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Kirsch"
    ]
  },
  "https://openreview.net/forum?id=7A96yteeF9": {
    "title": "Latent mixed-effect models for high-dimensional longitudinal data",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priscilla Ong",
      "Manuel Haussmann",
      "Otto Lönnroth",
      "Harri Lähdesmäki"
    ]
  },
  "https://openreview.net/forum?id=9L0B5N5hUX": {
    "title": "Investigating Generalization Behaviours of Generative Flow Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lazar Atanackovic",
      "Emmanuel Bengio"
    ]
  },
  "https://openreview.net/forum?id=xdWP1d8BxI": {
    "title": "Sparse Decomposition of Graph Neural Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Hu",
      "Mai Zeng",
      "Ge Zhang",
      "Pavel Rumiantsev",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=5298fKGmv3": {
    "title": "The BrowserGym Ecosystem for Web Agent Research",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Le Sellier de Chezelles",
      "Maxime Gasse",
      "Alexandre Lacoste",
      "Massimo Caccia",
      "Alexandre Drouin",
      "Léo Boisvert",
      "Megh Thakkar",
      "Tom Marty",
      "Rim Assouel",
      "Sahar Omidi Shayegan",
      "Lawrence Keunho Jang",
      "Xing Han Lù",
      "Ori Yoran",
      "Dehan Kong",
      "Frank F. Xu",
      "Siva Reddy",
      "Graham Neubig",
      "Quentin Cappart",
      "Russ Salakhutdinov",
      "Nicolas Chapados"
    ]
  },
  "https://openreview.net/forum?id=SbGt90dxdp": {
    "title": "Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Rumiantsev",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=FcyHZ6Q4k0": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=jrUUk5Fskm": {
    "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonios Valkanas",
      "Yuening Wang",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=hGaWq5Buj7": {
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussein Mozannar",
      "Valerie Chen",
      "Mohammed Alsobay",
      "Subhro Das",
      "Sebastian Zhao",
      "Dennis Wei",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Ameet Talwalkar",
      "David Sontag"
    ]
  }
}