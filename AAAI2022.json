{
  "https://ojs.aaai.org/index.php/AAAI/article/view/19873": {
    "title": "Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b836524d1d86e8612dff9fa5108000c6f78728fb",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19874": {
    "title": "Optimized Potential Initialization for Low-Latency Spiking Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f791cf3dbd0a1c97d340869988370fe086bfca03",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19875": {
    "title": "Planning with Biological Neurons and Synapses",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5f6312b44cc80bf7f6b51b5430f46a22d21d318b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19876": {
    "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "09897d26367a644811dc3be2109fe48778300d0a",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19877": {
    "title": "VECA: A New Benchmark and Toolkit for General Cognitive Development",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4bc58f0754829a24da39de6cdfda17a2bb3507fc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19878": {
    "title": "Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b31502cc80b808da383bcb3fdc6ee7c5079bdabc",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19879": {
    "title": "Multi-Sacle Dynamic Coding Improved Spiking Actor Network for Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7b8e746ef2c6c7d0fd1262a6905bf99acc5a1498",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19880": {
    "title": "Joint Human Pose Estimation and Instance Segmentation with PosePlusSeg",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ed50d44540518b7f3b96a39b84e2d5d8d3dbb649",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19881": {
    "title": "Logic Rule Guided Attribution with Dynamic Ablation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "24420650c05f8260a315c156a20de8e79e152e19",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19882": {
    "title": "Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fc4742f32fd906a2627201d6170c33ed73965908",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19883": {
    "title": "Deformable Part Region Learning for Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "60c2529f36bf43c1eef90fbb4d4496c263b0e2e1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19884": {
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cb09a44988c8f5ef7ff8bcfe1eb4509f9384cbbb",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19885": {
    "title": "Handwritten Mathematical Expression Recognition via Attention Aggregation Based Bi-directional Mutual Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bfa08d26a484df40b5de6dcade2565af91418174",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19886": {
    "title": "ADD: Frequency Attention and Multi-View Based Knowledge Distillation to Detect Low-Quality Compressed Deepfake Images",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c233de0f1bf308019b52920bae654043579491e",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19887": {
    "title": "LUNA: Localizing Unfamiliarity Near Acquaintance for Open-Set Long-Tailed Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b0a38249dbf08536c95844f3c716a273a97194b7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19888": {
    "title": "Prior Gradient Mask Guided Pruning-Aware Fine-Tuning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4cbfa720b09abc0e1832cdb72ce67d1f10416c89",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19889": {
    "title": "Context-Aware Transfer Attacks for Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "415e98ed66f9a0badc8f7dcafffdda1630557c2c",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19890": {
    "title": "OoDHDR-Codec: Out-of-Distribution Generalization for HDR Image Compression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cfbfe331d4438c080b5dc5b8308d2ac633967baf",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19891": {
    "title": "Visual Consensus Modeling for Video-Text Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "72acaae80826d27ad8eeed33c23dbae369aa14ea",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19892": {
    "title": "Proximal PanNet: A Model-Based Deep Network for Pansharpening",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "04788b110c555bc63a49f0166bfe572ba1edb828",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19893": {
    "title": "CF-DETR: Coarse-to-Fine Transformers for End-to-End Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f47ccff51c9153959fcba2863dbf7fdd04283665",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19894": {
    "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "317909e30c6c614fda97c1060c3a6297de8cffa1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19895": {
    "title": "Texture Generation Using Dual-Domain Feature Flow with Multi-View Hallucinations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "918d9522d57b4ed2bcf0996d22d610eb7605e09f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19896": {
    "title": "Resistance Training Using Prior Bias: Toward Unbiased Scene Graph Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "75ccd6dd8bbb466ecfdf3e2bf7691de32ea7e87c",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19897": {
    "title": "SASA: Semantics-Augmented Set Abstraction for Point-Based 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6c7df45080cc5ad3a8c3dea35a4c95191ce4cfe0",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19898": {
    "title": "Comprehensive Regularization in a Bi-directional Predictive Network for Video Anomaly Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e97a764c2ac3d0dc588dc363ea77721b300c0790",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19899": {
    "title": "Keypoint Message Passing for Video-Based Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d741ab50f814f0ef218a519c316d227acf7e28ba",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19900": {
    "title": "DCAN: Improving Temporal Action Detection via Dual Context Aggregation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fa85c6c09bec52f11feb6cb5b96e854d87a13ed9",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19901": {
    "title": "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c2957eb6f9d26dabafff1653f2bb09d6b6d7253a",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19902": {
    "title": "Explore Inter-contrast between Videos via Composition for Weakly Supervised Temporal Sentence Grounding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d9927d77914f67645d55178707bc7ef0999d919b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19903": {
    "title": "Adaptive Image-to-Video Scene Graph Generation via Knowledge Reasoning and Adversarial Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "700f19964b9a9cc164dd815f840466685ef4bb53",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19904": {
    "title": "Text Gestalt: Stroke-Aware Scene Text Image Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "103b86754729c532db733602810bc4030e7c9f78",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19905": {
    "title": "Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e2cd959758141683cef915540292e839e7edca0e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19906": {
    "title": "ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3360cb3348d0db8251a4d5d70f56b6ee37ec8ff7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19907": {
    "title": "Attacking Video Recognition Models with Bullet-Screen Comments",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3474b9eb70e8b0feb8ef72e7b8c222b8bd5209d6",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19908": {
    "title": "VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4113c49ff42b92c8a1192bc58412d7b646a9aeda",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19909": {
    "title": "TransZero: Attribute-Guided Transformer for Zero-Shot Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e54cb48aac19d8c7714fdc1af4c328dca49bb7f3",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19910": {
    "title": "Structured Semantic Transfer for Multi-Label Recognition with Partial Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1fe030dd8c8f0dcb35d74f2255d1e2baa166e812",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19911": {
    "title": "SJDL-Vehicle: Semi-supervised Joint Defogging Learning for Foggy Vehicle Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c67010cd7bc5021926332cfd0064f201668d0a6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19912": {
    "title": "Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ec17b999b4c61309e68442a751043239b5599cc8",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19913": {
    "title": "Guide Local Feature Matching by Overlap Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d2859386b9a6de476c35358ca2c707895d90ac7f",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19914": {
    "title": "Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "09bf883a92c4744c2615b978a1c62c1bb053a6f7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19915": {
    "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fa1aa090d09dc5d420d0cce37cea72c21e99d36b",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19916": {
    "title": "Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c2793f9d9a555b6bbfa22d2b15d127ff01efd7d2",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19917": {
    "title": "DeTarNet: Decoupling Translation and Rotation by Siamese Network for Point Cloud Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ad488e939cb9d31a25435d98987f570ef1f6a9c0",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19918": {
    "title": "LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a522ddc865ea6ba15923c3ad84882c1a3bf92f8f",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19919": {
    "title": "Efficient Virtual View Selection for 3D Hand Pose Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "85d5d0d7a48092243d383b7af7aa12d8c9357e34",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19920": {
    "title": "Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ff689f3d74f843a118cab38deac2c670e7f9d06f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19921": {
    "title": "PureGaze: Purifying Gaze Feature for Generalizable Gaze Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9beb13adf7c97dea56b37dead91c0665cc5c40bd",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19922": {
    "title": "(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f959acbc5b554801800a50c9a6eacdf896119920",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19923": {
    "title": "Event-Image Fusion Stereo Using Cross-Modality Feature Propagation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "405771deaaef87ba5b5ea9c3be0667f014abdf05",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19924": {
    "title": "Style-Guided and Disentangled Representation for Robust Image-to-Image Translation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e13310fcdd5ee12342a48ed880e7bf8c5abda3bc",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19925": {
    "title": "Denoised Maximum Classiﬁer Discrepancy for Source-Free Unsupervised Domain Adaptation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ea00bc6d6926a0be5d0e191e48c32dbf083e59b1",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19926": {
    "title": "Model-Based Image Signal Processors via Learnable Dictionaries",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bf7b34376518140379c84cd0dd8e71a4b8d888a6",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19927": {
    "title": "MMA: Multi-Camera Based Global Motion Averaging",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "479b97b03381ebf827a644746879797e33d7889c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19928": {
    "title": "GenCo: Generative Co-training for Generative Adversarial Networks with Limited Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3fe1eb2c788a6fbc2642f6b43e7915fe93fb318b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19929": {
    "title": "Unbiased IoU for Spherical Image Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "612b01f46729c3449add53b4c8ae2bdbbdd56510",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19930": {
    "title": "InsCLR: Improving Instance Retrieval with Self-Supervision",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8aa0161174f5a6c9b4d6eb019bf306a4cceb7f62",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19931": {
    "title": "Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "37486b4948d9b31632c2ca030ca6d9d268af4423",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19932": {
    "title": "Construct Effective Geometry Aware Feature Pyramid Network for Multi-Scale Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "032e0be06d5c52ac228fa0cb8aa1764c447f2d9e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19933": {
    "title": "Complementary Attention Gated Network for Pedestrian Trajectory Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d269ee705c57c8576cb5c29858087764287f8603",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19934": {
    "title": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "684088badaa01b7de3afc214ca2839a4bc212f52",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19935": {
    "title": "Backdoor Attacks on the DNN Interpretation System",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "51ebe054d93bcde2de3f53c913216d891917b846",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19936": {
    "title": "Learning to Learn Transferable Attack",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "11b981f8c34c848ebf67ccdfcebbd87e45ee2d8e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19937": {
    "title": "Perceptual Quality Assessment of Omnidirectional Images",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cb5bcf48443c4809c578c5e3d489f61eb16f1d6b",
    "citation_count": 66
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19938": {
    "title": "PatchUp: A Feature-Space Block-Level Regularization Technique for Convolutional Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5573c5f2e903b243d2cda1cdb43785873f13155d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19939": {
    "title": "DuMLP-Pin: A Dual-MLP-Dot-Product Permutation-Invariant Network for Set Feature Extraction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "12066543704c24062d056236c5b8e3ca5f70148b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19940": {
    "title": "Attention-Aligned Transformer for Image Captioning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "977f49f96ec7b532c29576e468adbd140c502810",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19941": {
    "title": "Model Doctor: A Simple Gradient Aggregation Strategy for Diagnosing and Treating CNN Classifiers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "caf543516034e4daf875efd37152145972dbb5b6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19942": {
    "title": "OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7eebdc034ef539c55257e09c65f6cb8b94c07a99",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19943": {
    "title": "DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d503245e82984c17e36801259b9cee485c258437",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19944": {
    "title": "Unsupervised Underwater Image Restoration: From a Homology Perspective",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b70c70e99449b199575917e8940f6b4abb65ed82",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19945": {
    "title": "Playing Lottery Tickets with Vision and Language",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "22299b440277b4bc887168a669408d5547c1461a",
    "citation_count": 24
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19946": {
    "title": "Feature Distillation Interaction Weighting Network for Lightweight Image Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c0d3173b7b74913f6b1895788501f51096aa183",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19947": {
    "title": "Weakly-Supervised Salient Object Detection Using Point Supervision",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6df10a508e679c03f713a347190bbf5cab07f3ef",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19948": {
    "title": "Latent Space Explanation by Intervention",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4799ea215e65e66a9a170303861763662d04e8b1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19949": {
    "title": "Lifelong Person Re-identification by Pseudo Task Knowledge Preservation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4f0ba24e1793859b222a77209cf1276b45df279e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19950": {
    "title": "Adversarial Robustness in Multi-Task Learning: Promises and Illusions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "940bfecc757404e10b837867ec0c531c96c61d85",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19951": {
    "title": "Deep Confidence Guided Distance for 3D Partial Shape Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8ae3477ed2f33449d62397839b96a9a58ec761fb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19952": {
    "title": "Predicting Physical World Destinations for Commands Given to Self-Driving Cars",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2ad60ff300a91f0e312a77f70c52ccc09fc945e7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19953": {
    "title": "Towards Light-Weight and Real-Time Line Segment Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0cceb0393b87d3ff65a1f0beea696ce40e889597",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19954": {
    "title": "Exploiting Fine-Grained Face Forgery Clues via Progressive Enhancement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "64266a2bea58937799631bfecda250f85b44bed1",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19955": {
    "title": "Delving into the Local: Dynamic Inconsistency Learning for DeepFake Video Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0f138a9b3d90f3874ca5b2b0dd25db13bc3ce32b",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19956": {
    "title": "Assessing a Single Image in Reference-Guided Image Synthesis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "819724765844c625b7ab701783770ffd102a1ad2",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19957": {
    "title": "Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-Supervised Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8a660d62fb5383a2ddd7088f869eeafd2bbb7c4c",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19958": {
    "title": "Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b7ae93cbec5c7cb2c96adf8c4524a09e4dde7f2f",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19959": {
    "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "70e7b914b76f32e312878534737e7e0372b93da4",
    "citation_count": 33
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19960": {
    "title": "Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1d230bd379e11c142f42e3cbb114389ecee2b83a",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19961": {
    "title": "Laneformer: Object-Aware Row-Column Transformers for Lane Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "74db14977f55a01cb11b5e5a9367669acc46570d",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19962": {
    "title": "Modify Self-Attention via Skeleton Decomposition for Effective Point Cloud Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c9288cfc84953ee77d5390fb718106ac5caea2af",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19963": {
    "title": "Generalizable Person Re-identification via Self-Supervised Batch Norm Test-Time Adaption",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d53c7c436feb7ff0ee29ad9b55e7be5b0cd2e309",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19964": {
    "title": "RRL: Regional Rotate Layer in Convolutional Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cc2ad0a41725792402aed2f432e724fb9add5671",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19965": {
    "title": "QueryProp: Object Query Propagation for High-Performance Video Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "74e2b895227dd800ab3b8c47d6ee70293e314849",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19966": {
    "title": "Flow-Based Unconstrained Lip to Speech Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "279a6f09f7255fd6d18226f215d12cbc777ced8b",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19967": {
    "title": "TransFG: A Transformer Architecture for Fine-Grained Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "860e24025c67487b9dd87b442c7b44e5bbf5a054",
    "citation_count": 81
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19968": {
    "title": "Self-Supervised Robust Scene Flow Estimation via the Alignment of Probability Density Functions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3b073f2d04fc758cf72ae5a64541fef21d12aab7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19969": {
    "title": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "002b12ad36d3b19a78d0754e6949f791987830dd",
    "citation_count": 26
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19970": {
    "title": "SECRET: Self-Consistent Pseudo Label Refinement for Unsupervised Domain Adaptive Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "37e509b067cee3ab98e497ba84ed679999eabfbc",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19971": {
    "title": "Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "99730b6be5721157ec3b93332a22da3410539e9a",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19972": {
    "title": "Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b74b9ad4d58670acbace98cf4fa19d24c365ca44",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19973": {
    "title": "Uncertainty-Driven Dehazing Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e075410a978ab6cdfeb7564ce8762f0c0efac767",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19974": {
    "title": "Shadow Generation for Composite Image in Real-World Scenes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7e02430d27bcb2ee91ba200d29211e64e83def49",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19975": {
    "title": "Shape-Adaptive Selection and Measurement for Oriented Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3bcc436f59b301bde1808f62202787d6938cd88f",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19976": {
    "title": "H^2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b1ef7a10061c97c4771fc7da0ff1915dbeec06e2",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19977": {
    "title": "Elastic-Link for Binarized Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3fb67941f0484b1e4b86b2c5c297e6a943fb38e1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19978": {
    "title": "FInfer: Frame Inference-Based Deepfake Detection for High-Visual-Quality Videos",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "114bdf03192f1bcf1c3711867feaf9e3be6d8389",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19979": {
    "title": "Bi-volution: A Static and Dynamic Coupled Filter",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e556ce96eabeda4acd200960b17100e0c8ce0a32",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19980": {
    "title": "AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "69cc6392972664e0e8a8dfab8b39d9fe3a934122",
    "citation_count": 23
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19981": {
    "title": "Divide-and-Regroup Clustering for Domain Adaptive Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1248e2b5a8fff3210edfcbe22fb7881bc416e4e5",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19982": {
    "title": "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c1bb6400bf88b929467bb8cc411d72c5e9eda56d",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19983": {
    "title": "Deconfounded Visual Grounding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a07c55085bc02daa7e05857532a4a58939f9b14d",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19984": {
    "title": "Learning to Model Pixel-Embedded Affinity for Homogeneous Instance Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "eb8348f95373bb6c391ef55417fb32b348b892c1",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19985": {
    "title": "Channelized Axial Attention – considering Channel Relation within Spatial Attention for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0bdcd07461d550475b27bc65353a01c0e554ade3",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19986": {
    "title": "UFPMP-Det:Toward Accurate and Efficient Object Detection on Drone Imagery",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cbd89606487ddbeae73bf423d5f70b22bdccbac2",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19987": {
    "title": "Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c7dbb5ada2c66911ae727763d9831c469bd48261",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19988": {
    "title": "MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "abde8a122570cd7111a1d1335ddb89a3ed38f99b",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19989": {
    "title": "An Unsupervised Way to Understand Artifact Generating Internal Units in Generative Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a5f31f5f46f4eec87fde33a9b856d52ac1ea2018",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19990": {
    "title": "FrePGAN: Robust Deepfake Detection Using Frequency-Level Perturbations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "51cd05dc52aa348f988d91a549c292a653dcdaca",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19991": {
    "title": "Learning Disentangled Attribute Representations for Robust Pedestrian Attribute Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c0b7cdd0e2249f9bc90b4d0218839f36c3965453",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19992": {
    "title": "Degrade Is Upgrade: Learning Degradation for Low-Light Image Enhancement",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "50b0c71a317d450330a36dac8ed962da10022399",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19993": {
    "title": "HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "62c35d9f6ffaf3dad56b1407292de47527ae72f5",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19994": {
    "title": "Coarse-to-Fine Generative Modeling for Graphic Layouts",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bfa186e680d02ab0c161d9400c0f151b5f7c73ac",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19995": {
    "title": "DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dee897c638f77086c46644e3e214c5a7de49b37c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19996": {
    "title": "LAGConv: Local-Context Adaptive Convolution Kernels with Global Harmonic Bias for Pansharpening",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c3dec38d79c888cbd5b369390af2f65ccc685ec1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19997": {
    "title": "Learning the Dynamics of Visual Relational Reasoning via Reinforced Path Routing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6bb962d694f6be9a5c74cd9a18d0cf2a6f6319d7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19998": {
    "title": "Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1bf21ec7f4371b28f8222cfa7e05e00893fc8961",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19999": {
    "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9d953680e9231c1bf88b890f6276169859349d28",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20000": {
    "title": "Learning Mixture of Domain-Specific Experts via Disentangled Factors for Autonomous Driving",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "03aaf8d61bb6ee4e5887e5839487de22e46f71bd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20001": {
    "title": "Towards Versatile Pedestrian Detector with Multisensory-Matching and Multispectral Recalling Memory",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "622be6e7b8402659fb0ecf3ed3ddd7a30fd4ed8e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20002": {
    "title": "Semantic Feature Extraction for Generalized Zero-Shot Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ebeae49c085faf85548079c9f1af9cafdce81037",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20003": {
    "title": "Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "56d80a28d2a1750511ed9d4f5d9b89e0f1bc0c1c",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20004": {
    "title": "Deep Translation Prior: Test-Time Training for Photorealistic Style Transfer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f1ee79f49040dd9279d6a8173430c865b8027f39",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20005": {
    "title": "PrivateSNN: Privacy-Preserving Spiking Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6b0867a5d61f66f594c31968d38a6e1901f612c7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20006": {
    "title": "NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6b1ac12dde02e4894c08fa7f4c340bd4911dc979",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20007": {
    "title": "Joint 3D Object Detection and Tracking Using Spatio-Temporal Representation of Camera Image and LiDAR Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "455df8920cf3037d022a1b9e3b2eb07918a0c15a",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20008": {
    "title": "Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e59343ecddcbb10edcbb03c6c3fe3b3bb7fa5856",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20009": {
    "title": "Siamese Network with Interactive Transformer for Video Object Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a5d61a0f974f82872b8c17257badf4850faba40d",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20010": {
    "title": "Adversarial Attack for Asynchronous Event-Based Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fc1fc9461506b470380e92ccb1853af5c8504916",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20011": {
    "title": "Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1b94586911f6db447c6c19ea0fd8735feba64180",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20012": {
    "title": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "df9dfbe775df0c66a57308ec52900a590a92c9f7",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20013": {
    "title": "Shrinking Temporal Attention in Transformers for Video Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "339fc2130705f4d3d5fb59fb87dc0e1c9cacfb15",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20014": {
    "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7cb4efb52c6acd192ce3993264a5b4e210445bca",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20015": {
    "title": "Interpretable Generative Adversarial Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e16f5b41141355051c60b1ce0c4ba06ac96241dd",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20016": {
    "title": "Cross-Modal Object Tracking: Modality-Aware Representations and a Unified Benchmark",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e3f52c0976f1e4492a875527e4292f4f1d399672",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20017": {
    "title": "You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c5dfab50eaecf6e2fde7bb8858e90733515b8ce1",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20018": {
    "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-Guided Feature Imitation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "998603aeedb725924fe980f6f6d8044a72fe08dc",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20019": {
    "title": "Rethinking Pseudo Labels for Semi-supervised Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1143a3c855a6536448352754b6908db2fd87985f",
    "citation_count": 18
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20020": {
    "title": "Action-Aware Embedding Enhancement for Image-Text Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "09504b7989824c5ecca84f0a06ac39f55bfc656d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20021": {
    "title": "Retinomorphic Object Detection in Asynchronous Visual Streams",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "df446afcb25605d5f176c2dba4e2e6032dfa8652",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20022": {
    "title": "Learning from Weakly-Labeled Web Videos via Exploring Sub-concepts",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "acc94eae52e80bfadf3296ada569fad6427d4e5f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20023": {
    "title": "Learning Universal Adversarial Perturbation by Adversarial Example",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1935668faf67ed68c6320d45144785ce4256fdf0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20024": {
    "title": "Logit Perturbation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "69f2e76490f94b5a6433f7513f92e52115a9a2ec",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20025": {
    "title": "Neighborhood-Adaptive Structure Augmented Metric Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9ad3694613eaa36582d812c002ee0ad7548c970a",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20026": {
    "title": "Stereo Neural Vernier Caliper",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d941a13a9d8bba6a3ec25d115019145f74d767e8",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20027": {
    "title": "EditVAE: Unsupervised Parts-Aware Controllable 3D Point Cloud Shape Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "88fef11717f22d9a2c2c353297ff87f3df253d16",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20028": {
    "title": "Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "84f3290630be8dff726ca43195ed6c82bb65d0d9",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20029": {
    "title": "TA2N: Two-Stage Action Alignment Network for Few-Shot Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "28386a2a3fa29f5490a60f677b5ad34a2d37ce90",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20030": {
    "title": "Best-Buddy GANs for Highly Detailed Image Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ca8e418b42b46c024cf417912fc0a6f2f98dcfba",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20031": {
    "title": "SCAN: Cross Domain Object Detection with Semantic Conditioned Adaptation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "95ce8550d84281be6d19783b727f1e1fefe2e19c",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20032": {
    "title": "Hybrid Instance-Aware Temporal Fusion for Online Video Instance Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "038a4a2e8ced15154c356a74dbff76521daf3822",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20033": {
    "title": "Close the Loop: A Unified Bottom-Up and Top-Down Paradigm for Joint Image Deraining and Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f34890b8aa91ca9ab6830409bae18f6b98308458",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20034": {
    "title": "Uncertainty Estimation via Response Scaling for Pseudo-Mask Noise Mitigation in Weakly-Supervised Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "47969582b6f61674da0fa538d36a211803fdc1a2",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20035": {
    "title": "Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "eb948a57c9e334797199513e7bfe26bad309c115",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20036": {
    "title": "Defending against Model Stealing via Verifying Embedded External Features",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20037": {
    "title": "Towards an Effective Orthogonal Dictionary Convolution Strategy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cb037f709612a51107c9b74b1a6389b37d0c4c23",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20038": {
    "title": "ELMA: Energy-Based Learning for Multi-Agent Activity Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "69e89edeadd896b2f7966ccd7c888b44fd032fb4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20039": {
    "title": "Equal Bits: Enforcing Equally Distributed Binary Network Weights",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "686193a33d56d6cc0f9df299f92f3f0bbf555810",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20040": {
    "title": "SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "88e119f047fa9327215124e5872a7392ba421181",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20041": {
    "title": "Improving Human-Object Interaction Detection via Phrase Learning and Label Composition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d0c5dfe88928a77232b1d91eb45009b45d2a5a18",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20042": {
    "title": "Rethinking the Optimization of Average Precision: Only Penalizing Negative Instances before Positive Ones Is Enough",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fe38ad10078fae9ca2e13d977ced9c0307e10c47",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20043": {
    "title": "Reliability Exploration with Self-Ensemble Learning for Domain Adaptive Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5244b86942d9ba20480cccc28c2548faecc3784c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20044": {
    "title": "Deconfounding Physical Dynamics with Global Causal Relation and Confounder Transmission for Counterfactual Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9c1cea9e9738a746fadb6cb7d565390d08dadfd3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20045": {
    "title": "One More Check: Making \"Fake Background\" Be Tracked Again",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b8692df72ff4207810aa39d1de4f1b97e4685722",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20046": {
    "title": "Semantically Contrastive Learning for Low-Light Image Enhancement",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0c39317191711aa9d20134b9851491d77a4b885f",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20047": {
    "title": "Self-Supervised Spatiotemporal Representation Learning by Exploiting Video Continuity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f92ff2143ac8f9796cffecdb71f3a0191a9e83ec",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20048": {
    "title": "Inharmonious Region Localization by Magnifying Domain Discrepancy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e4adffa920024e112f7920724f1cbed96e04dbf1",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20049": {
    "title": "Distribution Aware VoteNet for 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "205a7ae22ecd3afbdd7aedf705426313397e83ae",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20050": {
    "title": "Contrastive Instruction-Trajectory Learning for Vision-Language Navigation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9149931ce9dde23f573396b159bf8f82b8433a3e",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20051": {
    "title": "Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b66145e04fe54ea9ebcf644ca97e82d2a55b9404",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20052": {
    "title": "A Causal Debiasing Framework for Unsupervised Salient Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f9adf73144d5afecb8c09a5866fd0956e8a849d6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20053": {
    "title": "A Causal Inference Look at Unsupervised Video Anomaly Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2caeea7cb81973ba56bba6daf46137a6b7b279d8",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20054": {
    "title": "Unpaired Multi-Domain Stain Transfer for Kidney Histopathological Images",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1e2b44f44db69f18e4aecac55cc3887d30bf5940",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20055": {
    "title": "Dynamic Spatial Propagation Network for Depth Completion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0ea1dece34d1c6e8da5cc2c4dd6ff8cc08048c5a",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20056": {
    "title": "Local Similarity Pattern and Cost Self-Reassembling for Deep Stereo Matching Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "654849305b95c6ee30b91130f69b9b09a6327602",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20057": {
    "title": "FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9270a3b676b4f87aa5b9576ddf48d7a91a79d39b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20058": {
    "title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7fc933a06db0cc54a540478a62bf0b370e631aa9",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20059": {
    "title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f3aff0341bdf52cc45286fc5818075a5ec70c762",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20060": {
    "title": "Unsupervised Temporal Video Grounding with Deep Semantic Clustering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b760ffcb002d4bea7cb2c131c63a0e964ae0e314",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20061": {
    "title": "SpikeConverter: An Efficient Conversion Framework Zipping the Gap between Artificial Neural Networks and Spiking Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f8df151ec0ed755c153b7d58bea0bc3c6c6eb986",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20062": {
    "title": "Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "de4b03e5ed2a1b352f243a3ff68ee9ebf98afe09",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20063": {
    "title": "AnchorFace: Boosting TAR@FAR for Practical Face Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1ac1e7acde18299acc2e4ef70af07abae6d51b68",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20064": {
    "title": "Memory-Based Jitter: Improving Visual Recognition on Long-Tailed Data with Diversity in Memory",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "380493918881ab21f19f9ac68846f77090e28598",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20065": {
    "title": "Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "26750aea7f0573b47ea04d34e09cf9ae7bc80d6c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20066": {
    "title": "Parallel and High-Fidelity Text-to-Lip Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4623dd252bc7cfc8bda622a12163ed22ddfac211",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20067": {
    "title": "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-trained Siamese Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "53fd31cf9d1ed810a182afc1af8bbf5bae570d76",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20068": {
    "title": "Single-Domain Generalization in Medical Image Segmentation via Test-Time Adaptation from Shape Dictionary",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d754649f661eb29e0648ada875a35fd0985fbe4d",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20069": {
    "title": "Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c6c9750d9c45a4ebc2c5071a6ca866597ab7ed08",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20070": {
    "title": "OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "27d36a848d0c34a17373e64bcade441f47753b45",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20071": {
    "title": "Feature Generation and Hypothesis Verification for Reliable Face Anti-spoofing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dc4c28b752c0b3acf1a74027381d0c3b3b610db9",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20072": {
    "title": "Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8c332bdf0be56ecc96281c6d31e7b64544e78a78",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20073": {
    "title": "Visual Sound Localization in the Wild by Cross-Modal Interference Erasing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1ef85a73bdc6bd1f2865a9067e383610d608eec9",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20074": {
    "title": "Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2ac06626c66aa7c1b0e7a5b31ee51a088c630d88",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20075": {
    "title": "Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bc0acf5baa46df39dd39f588f4e22e6f06f1c84a",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20076": {
    "title": "DMN4: Few-Shot Learning via Discriminative Mutual Nearest Neighbor Neural Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4ee1d92eacfc04f9713d187cc3772a574610bd36",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20077": {
    "title": "Multi-Knowledge Aggregation and Transfer for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8525f452188a9eb5880fc680b3cb960f152a1d88",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20078": {
    "title": "Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "482411d98c2ee57cbf8e9acf2f75d50987f68484",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20079": {
    "title": "Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9176415ed922f8a7e7bb9ab7c5974a60c1ec6309",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20080": {
    "title": "Pose Guided Image Generation from Misaligned Sources via Residual Flow Based Correction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d46a99f72e9e7710a750b99723b3930c05aab00",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20081": {
    "title": "PMAL: Open Set Recognition via Robust Prototype Mining",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "95a676640a57cd0c3d98d38f3e645241a3af96ed",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20082": {
    "title": "Barely-Supervised Learning: Semi-supervised Learning with Very Few Labeled Images",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2bd5c2840ee9d499137895197b93cef129c22b60",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20083": {
    "title": "Learning Optical Flow with Adaptive Graph Reasoning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3ae69ec5a9fd4f28dfc495ddf1259e936b140000",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20084": {
    "title": "A Fusion-Denoising Attack on InstaHide with Data Augmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "61483fedb6d1cfc4b2786cc043c8f1c15ad5dfb7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20085": {
    "title": "Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "49898d5e12569415d13d39b30313e4c6a4f1b1d8",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20086": {
    "title": "Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9a4fb2cca17f777650e8de7bb135e9d6ce992485",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20087": {
    "title": "Adaptive Poincaré Point to Set Distance for Few-Shot Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2932825faa4751e7246dc871355851391226b0d5",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20088": {
    "title": "Generative Adaptive Convolutions for Real-World Noisy Image Denoising",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e6919b4e35a0565c438828b039c3dbed6e364e50",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20089": {
    "title": "REMOTE: Reinforced Motion Transformation Network for Semi-supervised 2D Pose Estimation in Videos",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "09f5470f779ddf25e99d4fd3c9078c1fe1782368",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20090": {
    "title": "Learning from the Target: Dual Prototype Network for Few Shot Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "755e0a666b0ae37206eb2387462e9f9e59c2331e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20091": {
    "title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e3e133722951392bb3c26b397e723f892ea98a6e",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20092": {
    "title": "Towards Bridging Sample Complexity and Model Capacity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c3ee33a96fabb4888f5e4368044e7bbede2160db",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20093": {
    "title": "Towards Accurate Facial Motion Retargeting with Identity-Consistent and Expression-Exclusive Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d4dff3dd33f9ab1642928b9a02faf9c7b2f6c075",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20094": {
    "title": "Can Vision Transformers Learn without Natural Images?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b9ce9fea4634d6bfed5af2f4de410822295b3630",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20095": {
    "title": "Federated Learning for Face Recognition with Gradient Correction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ebd34a9c319d66922f4fe61ffb6efea8811be6db",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20096": {
    "title": "Restorable Image Operators with Quasi-Invertible Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f5a30a8a0f3a3ff431687f421f114232cf8816d3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20097": {
    "title": "TEACh: Task-Driven Embodied Agents That Chat",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424",
    "citation_count": 37
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20098": {
    "title": "Label-Efficient Hybrid-Supervised Learning for Medical Image Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e16a91912ab81b47ba07d42f0a50d632e008a65c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20099": {
    "title": "Less Is More: Pay Less Attention in Vision Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a0964686d80e173529efca6377f47e6a1b2fe69a",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20100": {
    "title": "Unsupervised Representation for Semantic Segmentation by Implicit Cycle-Attention Contrastive Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4105b681f89911f3c70c7b929789f63163b6c0ac",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20101": {
    "title": "Graph-Based Point Tracker for 3D Object Tracking in Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "806c2f1ff94c937b9d0bcda11f4c76df1a996663",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20102": {
    "title": "SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "da0b6558b07566b6ad9c30067562004a3ede36b1",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20103": {
    "title": "Vision Transformers Are Robust Learners",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5e4f03f68c6867d850f457dc5cc36738e5dff6c1",
    "citation_count": 115
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20104": {
    "title": "Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e8f45eadf79f53c755c98bd94ecad63d5f940370",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20105": {
    "title": "Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "33bd9c2f77db502ac08ba1327e559b61cc19b5f8",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20106": {
    "title": "ReX: An Efficient Approach to Reducing Memory Cost in Image Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9830d489e32a3e348ccc510e55cf99e8fb2c909f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20107": {
    "title": "CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e9d12767240c615495861fac800fba68ad75e99d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20108": {
    "title": "Activation Modulation and Recalibration Scheme for Weakly Supervised Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fd711aa609221ea99f1be7c15d8beba3caca18bf",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20109": {
    "title": "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework Using Self-Supervised Multi-Task Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1fc1077c0245b753269bea93b5d08cdb6e722f54",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20110": {
    "title": "Deep Implicit Statistical Shape Models for 3D Medical Image Delineation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7925d5d8676a57ea563b027141137f71e9c915ad",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20111": {
    "title": "Decompose the Sounds and Pixels, Recompose the Events",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0f7746dfb0be9ae82a96474855fb6bed1770aee7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20112": {
    "title": "Learning from Label Proportions with Prototypical Contrastive Clustering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "222c3b3f0e43c412d82fa838d2ddedd352bd2850",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20113": {
    "title": "Beyond Learning Features: Training a Fully-Functional Classifier with ZERO Instance-Level Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "811a3df8d87cdc78a42853e3fcb8b43f9cebd97c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20114": {
    "title": "Reference-Guided Pseudo-Label Generation for Medical Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d19cf25333250a274b5883ee08820562733d238",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20115": {
    "title": "Information-Theoretic Bias Reduction via Causal View of Spurious Correlation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bea50c27597957c17f4a7085bca5d8e5881ede05",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20116": {
    "title": "Improving Scene Graph Classification by Exploiting Knowledge from Texts",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cd83523830c9180a7fb1fd32c9e9ca7062dcb540",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20117": {
    "title": "Reliable Inlier Evaluation for Unsupervised Point Cloud Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "93eb45a3ddcfe0058e018760fda8eb0dfb4f9fa7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20118": {
    "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20119": {
    "title": "Un-mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c96a5ee5c5018b1232b961ee2b5eaf73495a68fb",
    "citation_count": 44
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20120": {
    "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5beea6b725a73af57aa94c4520e46e97b3cd7b15",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20121": {
    "title": "Social Interpretable Tree for Pedestrian Trajectory Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "586b26a5ef4c70e48f0d02fe49094630eda7f179",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20122": {
    "title": "P^3-Net: Part Mobility Parsing from Point Cloud Sequences via Learning Explicit Point Correspondence",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "36776248e0319f5247edfc7448d41f350020911d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20123": {
    "title": "Improving Zero-Shot Phrase Grounding via Reasoning on External Knowledge and Spatial Relations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d9693584f21834535592dfad01abcbf206ddd1bb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20124": {
    "title": "Iterative Contrast-Classify for Semi-supervised Temporal Action Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1e23b607bc0ecc598ec5ad948a12334c5e9cbf0f",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20125": {
    "title": "JPV-Net: Joint Point-Voxel Representations for Accurate 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "835fb885a64793e5a0cb94ecbae78d60baf937d2",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20126": {
    "title": "Fully Attentional Network for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "13b9bfad15dd79e5775e7b97edbd1e63a92a6fff",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20127": {
    "title": "Self-Supervised Object Localization with Joint Graph Partition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "db888180afd09600e01e722d4e0159404350fd7b",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20128": {
    "title": "Correlation Field for Boosting 3D Object Detection in Structured Scenes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e58afcc06ab33ead9eb5baffd19221099ca14eda",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20129": {
    "title": "Boost Supervised Pretraining for Visual Transfer Learning: Implications of Self-Supervised Contrastive Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "48698cba618caa06d0ba353747b86a1529827440",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20130": {
    "title": "Dual Contrastive Learning for General Face Forgery Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d4720af01a0d3b3ff3203808e33b722322c4536",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20131": {
    "title": "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a841ef1b443ade089fef37db4d97076eaeebb662",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20132": {
    "title": "Adversarial Bone Length Attack on Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "98156c730d2a77bd4bec55c7196714578f764631",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20133": {
    "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "485c08025157973bb52a935c6aa3bee74f990c01",
    "citation_count": 21
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20134": {
    "title": "Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "482bd56530cb88e238d0ddbb2a6855442d634cba",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20135": {
    "title": "Transfer Learning for Color Constancy via Statistic Perspective",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "309efb8d1c4a471980b34f466f1e41b6fec6b44c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20136": {
    "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ff00791b780b10336cc02ee366446d16e1c5e17b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20137": {
    "title": "GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e2ad9bf3eff9adbf99c44d60c11bd3f98f7df4a6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20138": {
    "title": "MTLDesc: Looking Wider to Describe Better",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f7eb683d25d4c69a9a2ccd734347e0cdbd44a6c1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20139": {
    "title": "Active Boundary Loss for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c35ae7f62bd662eaf09834d962366202225d10dd",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20140": {
    "title": "Online-Updated High-Order Collaborative Networks for Single Image Deraining",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d6cb9791bf35ffe822fdaca940304f42e9acfb21",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20141": {
    "title": "FCA: Learning a 3D Full-Coverage Vehicle Camouflage for Multi-View Physical Adversarial Attack",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "695685be105a0b9bf8b5cd50d80a6dff78204081",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20142": {
    "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b52844a746dafd8a5051cef49abbbda64a312605",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20143": {
    "title": "Self-Supervised Representation Learning Framework for Remote Physiological Measurement Using Spatiotemporal Augmentation Loss",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2c8b82f018d64ec9b6035007e09ef9fe61a8c016",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20144": {
    "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7623a054b85bed188ebd608915ee7a15fc947f7f",
    "citation_count": 53
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20145": {
    "title": "Renovate Yourself: Calibrating Feature Representation of Misclassified Pixels for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9c8bd5a0adfd57941d454c4fd02d92a17aa8693c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20146": {
    "title": "Separated Contrastive Learning for Organ-at-Risk and Gross-Tumor-Volume Segmentation with Limited Annotation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a086fc7ee0f3f6f8befd257e94a4ebf156262d64",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20147": {
    "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d96ad7741c25001b9f3ec91b2d095d97b9629584",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20148": {
    "title": "Learning Temporally and Semantically Consistent Unpaired Video-to-Video Translation through Pseudo-Supervision from Synthetic Optical Flow",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b653d056feceeaaa06271d2b81cb724a264c5f09",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20149": {
    "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "032d1cf4bbca9aabd36e9d379697c706dbe8ab5f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20150": {
    "title": "Scaled ReLU Matters for Training Vision Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a9c352cce882f31e7f28dbe96794e10b089c6623",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20151": {
    "title": "CQA-Face: Contrastive Quality-Aware Attentions for Face Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c26a938b35652948e35e8e08517b555c87dde173",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20152": {
    "title": "Category-Specific Nuance Exploration Network for Fine-Grained Object Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "701507ba60b6f73943da688724618233bfd309c1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20153": {
    "title": "Detail-Preserving Transformer for Light Field Image Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cdd9036cfa2487c665033539978a1ba90f4f21a8",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20154": {
    "title": "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a5ec595ac6a70a50d9cb29702728ed14454d0515",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20155": {
    "title": "Pose-Guided Feature Disentangling for Occluded Person Re-identification Based on Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d5960b9a7794bf6a69f1835b974a0a129ecffa65",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20156": {
    "title": "FFNet: Frequency Fusion Network for Semantic Scene Completion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0921e3a65f5bd687618fa801d8517f09f33646a4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20157": {
    "title": "Privacy-Preserving Face Recognition in the Frequency Domain",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a41979f071e49ed20547840ce9971bdf7ef7a4c5",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20158": {
    "title": "Anchor DETR: Query Design for Transformer-Based Detector",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b4ce19f3b3819accb160acffabffa849f18f4758",
    "citation_count": 62
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20159": {
    "title": "Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c3136e616db4fce0549aef443a04e931bf787c2a",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20160": {
    "title": "End-to-End Transformer Based Model for Image Captioning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "23903b4c42fdbea0b7b35e3157b48d8dfd18e1a5",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20161": {
    "title": "Learning to Detect 3D Facial Landmarks via Heatmap Regression with Graph Convolutional Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "deb5d104de7cfb8de6d1ded9c7149a705f805300",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20162": {
    "title": "Low-Light Image Enhancement with Normalizing Flow",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4abede791129ff4eb42b91c9b1091c27f432114d",
    "citation_count": 27
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20163": {
    "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3ca89413394c6a236c27b026c026c30c8c4eb7b6",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20164": {
    "title": "Texture Reformer: Towards Fast and Universal Interactive Texture Transfer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "24f0014e9a77cfa500fe23a1a5b0b88d4e3a3739",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20165": {
    "title": "Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6df11ddfb66ca545bc3c0613ed10ceb19d8ed8b6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20166": {
    "title": "Can Semantic Labels Assist Self-Supervised Visual Representation Learning?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "87f17b5d24e1d8398d004fc6919264db3ef3efe4",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20167": {
    "title": "Rethinking the Two-Stage Framework for Grounded Situation Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3bd68745f52ce3270d7abc3212906cfcafce6641",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20168": {
    "title": "Boosting the Transferability of Video Adversarial Examples via Temporal Translation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cea5fec383a7bda5e7be8f89c54e9aec26cf39c0",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20169": {
    "title": "Towards Transferable Adversarial Attacks on Vision Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c2622daa8a658d5c85ea9869cb460a70b0f878d",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20170": {
    "title": "L-CoDe:Language-Based Colorization Using Color-Object Decoupled Conditions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b6d341d24393403708bcfb00bdd6eb8e124635f9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20171": {
    "title": "Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f7a0dc64f25581ca61a36261e4cbadb7e0d2bc3f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20172": {
    "title": "TDv2: A Novel Tree-Structured Decoder for Offline Mathematical Expression Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "743696c8d4cc736a83adf2aac02049bed3ea8260",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20173": {
    "title": "Learning Token-Based Representation for Image Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a6ded609a83b88ee2da5b00ad314d39be9843064",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20174": {
    "title": "Multi-Modal Answer Validation for Knowledge-Based VQA",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8dce342a435034fa0521b24b61393397df95c095",
    "citation_count": 25
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20175": {
    "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "93a054b86462fb918e1225c232c8902dedcb55c8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20176": {
    "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2a4024163826151303aa0bbb18320b8a67167794",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20177": {
    "title": "Style Mixing and Patchwise Prototypical Matching for One-Shot Unsupervised Domain Adaptive Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bd50790300adfc95af09fff4fac316fe8e685748",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20178": {
    "title": "Multi-Centroid Representation Network for Domain Adaptive Person Re-ID",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f62bf2ed53e413b874d0f714e367df1e6c68b8ad",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20179": {
    "title": "Efficient Non-local Contrastive Attention for Image Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "856bf400db06a5732d891ad480c65a1eec539595",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20180": {
    "title": "Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-Based Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d32ca9dc4fa729a0022ef08546d25b9f16cbd836",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20181": {
    "title": "Cross-Domain Collaborative Normalization via Structural Knowledge",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6aa6f15b6bbd0dd3433a88a35fd7b918e6b8431a",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20182": {
    "title": "ReMoNet: Recurrent Multi-Output Network for Efficient Video Denoising",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7c7597d95c6c1ea657a43e6edd41aa52b1e24f23",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20183": {
    "title": "Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2372e7b7dcab84495d9798d698ca6cfcf65959c0",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20184": {
    "title": "Video as Conditional Graph Hierarchy for Multi-Granular Question Answering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "10ab178bb5251f82c1ba3520135cf8471a2fa30c",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20185": {
    "title": "AdaptivePose: Human Parts as Adaptive Points",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a47f5faa7f23ecf536a4f421ffb0b5087d0ab78c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20186": {
    "title": "Learning Quality-Aware Representation for Multi-Person Pose Regression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0eafca483b74b5a8b0255d73f1f1b1e660ea44a6",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20187": {
    "title": "Attribute-Based Progressive Fusion Network for RGBT Tracking",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "96b0d333ea77ca85f74015f21f8a89380b092e5a",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20188": {
    "title": "Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "68ea434244ce953bb10c600767994326b48a8d42",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20189": {
    "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6cfd3a46078a7cf3a29235f0d67da1cfe69414ee",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20190": {
    "title": "Rendering-Aware HDR Environment Map Prediction from a Single Image",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f35d9f98880165d0595fdca39732e47bda2516c6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20191": {
    "title": "Topology-Aware Convolutional Neural Network for Efficient Skeleton-Based Action Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5ef1bff0a108cf3436a001e41399d9c669efc119",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20192": {
    "title": "Transcoded Video Restoration by Temporal Spatial Auxiliary Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dadb4e13c6a59cf19657fa703377dc9a0d28359b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20193": {
    "title": "DIRL: Domain-Invariant Representation Learning for Generalizable Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fb76d171599e542d6b52102dd3bcaf14992233fc",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20194": {
    "title": "Behind the Curtain: Learning Occluded Shapes for 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2bff1086d28fe704d259d2fb400a1bc64624e85f",
    "citation_count": 23
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20195": {
    "title": "Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "030557706326cbd43a97968da64521416ad58081",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20196": {
    "title": "Dual Attention Networks for Few-Shot Fine-Grained Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "58e8f34b4161112564f68799e8ea62ccc847f94d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20197": {
    "title": "Sparse Cross-Scale Attention Network for Efficient LiDAR Panoptic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "13a3eaf87de07f0e7b1f0ccf2536bf9ee6072505",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20198": {
    "title": "Towards Fully Sparse Training: Information Restoration with Spatial Similarity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "722e0640c064c4068c46a02fdf0ddf3037740ac3",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20199": {
    "title": "Hierarchical Image Generation via Transformer-Based Sequential Patch Selection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "72b8902d84526d7d712144a4a011f8d4948ce718",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20200": {
    "title": "Reliable Propagation-Correction Modulation for Video Object Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a6c29950e1f164a6fcd222e0938d465e2f67c2ba",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20201": {
    "title": "Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "693febb90a32545ee15e4170d2edcdc86bd4463c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20202": {
    "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d045133e6e022684329ff944d67f91888be1bc3b",
    "citation_count": 35
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20203": {
    "title": "MobileFaceSwap: A Lightweight Framework for Video Face Swapping",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5dc56bbcc75984ca8d147b1a24ed1a4e3265188a",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20204": {
    "title": "Clinical-BERT: Vision-Language Pre-training for Radiograph Diagnosis and Reports Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e015e07c76ff85f624b78112fd58937221f5ea0c",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20205": {
    "title": "Inferring Prototypes for Multi-Label Few-Shot Image Classification with Word Vector Guided Attention",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6db761cdb20a6c84687a6cbe97d9cf5a0505b37b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20206": {
    "title": "Unsupervised Domain Adaptive Salient Object Detection through Uncertainty-Aware Pseudo-Label Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "19193cafa7a9864ee89810ec573fd9d3bf66cf2f",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20207": {
    "title": "Transmission-Guided Bayesian Generative Model for Smoke Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b395119ed7fcb3f952faef16817bfcd4e041a383",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20208": {
    "title": "Cross-Species 3D Face Morphing via Alignment-Aware Controller",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2c2333c6f5982b34ccbfba616f1249e02345fc08",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20209": {
    "title": "Exploring Visual Context for Weakly Supervised Person Search",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0779d8b3cb32eeb0a2d4021fd325f6267647dd1e",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20210": {
    "title": "Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "261e9f21292a44dbf065385aa06996e9811d0ba2",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20211": {
    "title": "Mutual Contrastive Learning for Visual Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "edce12538963c689deb7504bb3b0664184ccfc20",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20212": {
    "title": "Temporal Action Proposal Generation with Background Constraint",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "274f1f618ae4e96ba476a455da6b82f7bd4bf6a9",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20213": {
    "title": "Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8b8622df31976bcb26d42b3b14e18612192d72dc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20214": {
    "title": "Polygon-to-Polygon Distance Loss for Rotated Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8e7e8845dbd0f38070cd9b61396912367a859997",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20215": {
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2672777d25562c9df6fc13b653181db62d39bece",
    "citation_count": 52
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20216": {
    "title": "ACGNet: Action Complement Graph Network for Weakly-Supervised Temporal Action Localization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fc911c620b95a29fc9f687d8f1330f8a566ed873",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20217": {
    "title": "Enhancing Pseudo Label Quality for Semi-supervised Domain-Generalized Medical Image Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8bdd1fe75f77dcc84159b3a2accf5c91781886eb",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20218": {
    "title": "Image Difference Captioning with Pre-training and Contrastive Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4993b87d3e26983c37bf1d05b07c8179d81ad196",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20219": {
    "title": "Safe Distillation Box",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2b76643e28f288d03f0dd5cbb01acf6c9c1c6969",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20220": {
    "title": "Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f02d30dc073c46636d837f26431d143e49d372fa",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20221": {
    "title": "Content-Variant Reference Image Quality Assessment via Knowledge Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "437fb14ee99e647501955e7be38384290d0cf3aa",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20222": {
    "title": "Width & Depth Pruning for Vision Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d451901a6a12c61179289cac7a4588a86c234112",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20223": {
    "title": "Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b3301a3b46bb4e9eda155bef82a54ab05865e7c5",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20224": {
    "title": "Self-Labeling Framework for Novel Category Discovery over Domains",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5641cf2602d0bd119d67bff71e5ea8e193d5e377",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20225": {
    "title": "Efficient Compact Bilinear Pooling via Kronecker Product",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8b402d7fa5a3827beb12d92a9ce52202b9d21251",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20226": {
    "title": "Hybrid Graph Neural Networks for Few-Shot Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5b14efe841ad41bbe5cb825179e45acb30e7469d",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20227": {
    "title": "SOIT: Segmenting Objects with Instance-Aware Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "12d0826a9cf2646d75cae9ad569a602dd7684c39",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20228": {
    "title": "MSML: Enhancing Occlusion-Robustness by Multi-Scale Segmentation-Based Mask Learning for Face Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "68e0d68882ec258ab4c090ec07fa8acd67dfc006",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20229": {
    "title": "Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a4355807a9e44313b8e2a2947a67631feeef6376",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20230": {
    "title": "Task-Level Self-Supervision for Cross-Domain Few-Shot Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e22db1c9eb5ce1ab4226e26309878126c6fe3d1f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20231": {
    "title": "Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-Supervised Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "afdb261639854c58066404ae3df346e4d3ee9908",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20232": {
    "title": "Homography Decomposition Networks for Planar Object Tracking",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ebf7a1f10bfb7112487976f414efb28af6e6a08c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20233": {
    "title": "Patch Diffusion: A General Module for Face Manipulation Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "487e79c5016c4ad8c4db777ee60da8ee9a832b58",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20234": {
    "title": "Semi-supervised Object Detection with Adaptive Class-Rebalancing Self-Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "86d6fb1f39a45712591d72ed2eac39c1b3574087",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20235": {
    "title": "Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5b430df8b79b609503ebb0cb2fea346194fbc8fa",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20236": {
    "title": "SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-resolution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a9a4246fda4484f0b79dad0c2506e3843074e9de",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20237": {
    "title": "Energy-Based Generative Cooperative Saliency Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ec2bbf8628197d01dec5fb0ca33cc621074b3f43",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20238": {
    "title": "Attention-Based Transformation from Latent Features to Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "880e2cc7715b277766c26a8cf07fc7c112ae4270",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20239": {
    "title": "Suppressing Static Visual Cues via Normalizing Flows for Self-Supervised Video Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9cc6e0ddd457f5a40c0f5e2534c0dd463ec3d8be",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20240": {
    "title": "LGD: Label-Guided Self-Distillation for Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2d572286f816c03bbfb733e4d5402cd2caf78fef",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20241": {
    "title": "Uncertainty Modeling with Second-Order Transformer for Group Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a6b028f1e09bb93b6e6e891d7e9a9776476c4604",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20242": {
    "title": "Deep Spatial Adaptive Network for Real Image Demosaicing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cdd86f086bfb7063eec8154fa69d823a2fc15454",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20243": {
    "title": "MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1e91743216e8b9a0a7a2908634e7412084d3fc0f",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20244": {
    "title": "Class Guided Channel Weighting Network for Fine-Grained Semantic Segmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "76c0f4ffbd4cac432a35fcfb0e879f331b814329",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20245": {
    "title": "Context-Based Contrastive Learning for Scene Text Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e3bb2f3d9296badad67daf7a00cf4fa3ada0f814",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20246": {
    "title": "Learning Network Architecture for Open-Set Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f3a8cf15e493d0636b633d41ea0c408c19bbefdb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20247": {
    "title": "An Adversarial Framework for Generating Unseen Images by Activation Maximization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "93dd8a502df263a8eef2444676be0dafd08dd542",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20248": {
    "title": "Contrastive Spatio-Temporal Pretext Learning for Self-Supervised Video Representation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9bdc9f9bb8b91a962d6661ccbf422fa07dcc66f8",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20249": {
    "title": "Pose-Invariant Face Recognition via Adaptive Angular Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f30f33b0b13b624360a619151d1f4e2b55de14ad",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20250": {
    "title": "End-to-End Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8e6c07e24e91bf1dbce40f9c0dd6a1642bed46db",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20251": {
    "title": "PetsGAN: Rethinking Priors for Single Image Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a1caeccf0902cf7cc5532f01ac3b331757ad9ca9",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20252": {
    "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f80775a79d42a1ddfc0df808ea760c57af4949d0",
    "citation_count": 43
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20253": {
    "title": "OA-FSUI2IT: A Novel Few-Shot Cross Domain Object Detection Framework with Object-Aware Few-Shot Unsupervised Image-to-Image Translation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a6686dbb3e5d53ba9e116dd83cbf8ae82566baa7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20254": {
    "title": "Static-Dynamic Co-teaching for Class-Incremental 3D Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "99f44f204344125f337575c30152f8eeafa4a490",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20255": {
    "title": "Local Surface Descriptor for Geometry and Feature Preserved Mesh Denoising",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "793cf5c359893bd65c26ef8004cad781bff92649",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20256": {
    "title": "Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features with Attribute Augmentation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6485c7873c503ab05de2297145b63443a750491d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20257": {
    "title": "Self-Supervised Pretraining for RGB-D Salient Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "94cc6f1218a45234ad1530aa48bcc289e8723eed",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20258": {
    "title": "Adaptive Logit Adjustment Loss for Long-Tailed Visual Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7fe905f0fbfa17d6aa554ef425a7a06ea9468836",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20259": {
    "title": "CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-Based Autonomous Urban Driving",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c0556e0a64ac5bffc246cd6d47179274b866db53",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20260": {
    "title": "Learning from the Tangram to Solve Mini Visual Tasks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9aeddad39085e3a5874fbe3436ce276b9451c0e6",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20261": {
    "title": "Handling Slice Permutations Variability in Tensor Recovery",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f78abfd45c6d570ef65ec4e0356780db0551fca0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20262": {
    "title": "Boosting Contrastive Learning with Relation Knowledge Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e7b9857d8314809ccaef058f430178e9932c3d12",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20263": {
    "title": "Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "93324dfb8b02f43edb4122d08ccc6b90d3a6b577",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20264": {
    "title": "Dual Decoupling Training for Semi-supervised Object Detection with Noise-Bypass Head",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "818ff32b6e04788226b07384a06eb348b845b55c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20265": {
    "title": "SCALoss: Side and Corner Aligned Loss for Bounding Box Regression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "04614b2a9c50902ffa2523b1c67aed8c54a65a8d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20266": {
    "title": "SepFusion: Finding Optimal Fusion Structures for Visual Sound Separation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "71c85b49eda8cd51adac408fd2271e3a3e67215c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20267": {
    "title": "Pan-Sharpening with Customized Transformer and Invertible Neural Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5c4cdc1375a0a18e04dfe28862a3a1fe46ae5162",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20268": {
    "title": "Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "52cfe9a77ebbd621e1e8f0f09d9637aab98aa865",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20269": {
    "title": "Edge-Aware Guidance Fusion Network for RGB–Thermal Scene Parsing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6ea00d90331b4f58574656d931241d681c347465",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20270": {
    "title": "TiGAN: Text-Based Interactive Image Generation and Manipulation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "839dc73c1adae268144d9cfb9d70985b2001304f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20271": {
    "title": "Cross-Domain Empirical Risk Minimization for Unbiased Long-Tailed Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cadc9f7c86cee6eccb50a1b7696ac18c3be189d9",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20272": {
    "title": "Deep Recurrent Neural Network with Multi-Scale Bi-directional Propagation for Video Deblurring",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "26a3f9072e995a9dcc39b0bcb2c6ce080d99b9ef",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20273": {
    "title": "I Can Find You! Boundary-Guided Separated Attention Network for Camouflaged Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "49160004f253af679a7ec67ea92481e069992a69",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20274": {
    "title": "MoCaNet: Motion Retargeting In-the-Wild via Canonicalization Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3e2b595476ed9da5a96c068fe250edfa804c2cb6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20275": {
    "title": "Robust Depth Completion with Uncertainty-Driven Loss Functions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d06ec1b2d24699cf1475663af6af974cc87d31a",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20276": {
    "title": "Efficient Model-Driven Network for Shadow Removal",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "06e4aca19539b79cd87b60279bea7fab1743f41f",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20277": {
    "title": "Learning Disentangled Classification and Localization Representations for Temporal Action Localization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dd1e216fe9cf4aeda25c85772c4de2967c69f94f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20278": {
    "title": "ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "077b76322c2953d4fb3c53e139c3685800cab54d",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20279": {
    "title": "Making Adversarial Examples More Transferable and Indistinguishable",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9aed5318e0b4c26b4b6e242002f3aae567ded3fb",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20280": {
    "title": "Undercover Boolean Matrix Factorization with MaxSAT",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ed7380a21f70a36c013f0fcd42f32c0a4f270b49",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20281": {
    "title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6a0d4cc369093089b7c2384bb9045f28fe78a42f",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20282": {
    "title": "GEQCA: Generic Qualitative Constraint Acquisition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6e934e00a612fd9d6deae355aa7d7cfc18cabf3c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20283": {
    "title": "Certified Symmetry and Dominance Breaking for Combinatorial Optimisation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2315a711e1eb8c5b0f1c785f847fee6cd6ae31aa",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20284": {
    "title": "The Perils of Learning Before Optimizing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "381df0bd337f75cf5352be6c90103ef6ea4db9a7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20285": {
    "title": "A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6dc958f2311526919827b6705aeed62fe930b75b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20286": {
    "title": "Resolving Inconsistencies in Simple Temporal Problems: A Parameterized Approach",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c642c0f4e1026f27f54fc997463c6bffc68a3f17",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20287": {
    "title": "Efficient Riemannian Meta-Optimization by Implicit Differentiation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "710a8acc80f9bcdd3fae6fb2e9a83898bc71718f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20288": {
    "title": "Faster Algorithms for Weak Backdoors",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5397eaffddcdc177ff0e0e2eeb04702b4a1bd29d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20289": {
    "title": "A Divide and Conquer Algorithm for Predict+Optimize with Non-convex Problems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "eb3ee0e0934965ec2113032d9ef00ff6e39766ac",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20290": {
    "title": "Computing Diverse Shortest Paths Efficiently: A Theoretical and Experimental Study",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "86b0d4e2f14cf4e03e3fa9cec6ea1c26b0bb5460",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20291": {
    "title": "Optimizing Binary Decision Diagrams with MaxSAT for Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d7d469eba0032dd287ca3ccc0319f61a18b4dc8",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20292": {
    "title": "Using MaxSAT for Efficient Explanations of Tree Ensembles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b3a4cc32f10648cf84c2a2725fb9a8f504edfa86",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20293": {
    "title": "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "61b821bf8fcd96901cbd87486dd2474dc19da4c3",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20294": {
    "title": "Learning to Search in Local Branching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ddcb55152e5e73b978e4fd8ba1bf656551154bbd",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20295": {
    "title": "Analysis of Pure Literal Elimination Rule for Non-uniform Random (MAX) k-SAT Problem with an Arbitrary Degree Distribution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "240bcb949881b8991e4c98d70c555a6cbfa986a0",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20296": {
    "title": "The SoftCumulative Constraint with Quadratic Penalty",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e3056ee58a6bf71adc43faa7f8978f639a99b2bc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20297": {
    "title": "Efficient Vertex-Oriented Polytopic Projection for Web-Scale Applications",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c895ba108ce257136ae244407951fa2b49b66c1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20298": {
    "title": "A Variant of Concurrent Constraint Programming on GPU",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ecada436b177dcf54aaada58372286921dd6d25d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20299": {
    "title": "Real-Time Driver-Request Assignment in Ridesourcing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "321c703aea583da14814654fea1ef35859f89f04",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20300": {
    "title": "Encoding Multi-Valued Decision Diagram Constraints as Binary Constraint Trees",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6be25eac0e7faaa04fa44b793a99fe0f79171ba7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20301": {
    "title": "Sample Average Approximation for Stochastic Optimization with Dependent Data: Performance Guarantees and Tractability",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c51705bee088be57d61d689c0a34f12f8459a8f2",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20302": {
    "title": "A Provably-Efficient Model-Free Algorithm for Infinite-Horizon Average-Reward Constrained Markov Decision Processes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c16e98201765272ccb8dec49b392ee7a3734f1f7",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20303": {
    "title": "TextHoaxer: Budgeted Hard-Label Adversarial Attacks on Text",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "88fb3feacc9826cf8305776489c69197ab78d478",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20304": {
    "title": "Two Compacted Models for Efficient Model-Based Diagnosis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "726b6261e1523982b86727ed27c482c665d583cc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20305": {
    "title": "Parameterized Approximation Algorithms for K-center Clustering and Variants",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6e518eaee58c9599b7367bc10c3364a48e529748",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20306": {
    "title": "How to Find a Good Explanation for Clustering?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e7906710360fe56a0bdc6a14313329d0c31c1b10",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20307": {
    "title": "Regularizing Graph Neural Networks via Consistency-Diversity Graph Augmentations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c4e7967f2ae109957f415818e1148b594ae2bb31",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20308": {
    "title": "Two-Stage Octave Residual Network for End-to-End Image Compression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ecfab429b242e9ab7e30a492a55eaff7a39a06aa",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20309": {
    "title": "DANets: Deep Abstract Networks for Tabular Data Classification and Regression",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "329e65711f346b76cee445d76025a80c7bc62e5d",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20310": {
    "title": "Fuzzy Logic Based Logical Query Answering on Knowledge Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ee6da7e7c6785f9aa7c610884ae3294f39958d1a",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20311": {
    "title": "TAG: Learning Timed Automata from Logs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9d284ee6ae05064ecbb5cb0fa5950d5546dc82d2",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20312": {
    "title": "Differentially Describing Groups of Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a9f46c63200541963d7bd2e62648d8d2566a493",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20313": {
    "title": "Molecular Contrastive Learning with Chemical Element Knowledge Graph",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "67d131c852c394bb748759a1b661671aff378aba",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20314": {
    "title": "Heterogeneity-Aware Twitter Bot Detection with Relational Graph Transformers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "69db89e6debb78e22f78a4443415e84ca735aea3",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20315": {
    "title": "Subspace Differential Privacy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "342c8ef336bcdf2c12fe6438857d7a8933a72027",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20316": {
    "title": "Orthogonal Graph Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "04319217b303156f5aa1c58b13de33d49fe8a3a8",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20317": {
    "title": "Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c80d7bfd847c9e30cf7e6e780d100d595a47f895",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20318": {
    "title": "GNN-Retro: Retrosynthetic Planning with Graph Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fb9b736ecad8607ee0240e83ad225c4ac90d638c",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20319": {
    "title": "Block Modeling-Guided Graph Convolutional Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "98b74858831ae4218d91de91f980893eb13ce870",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20320": {
    "title": "CATN: Cross Attentive Tree-Aware Network for Multivariate Time Series Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c5b23e07f786a7a8616f19d3ed2bd2aeb0152d0d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20321": {
    "title": "FPAdaMetric: False-Positive-Aware Adaptive Metric Learning for Session-Based Recommendation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "448f82280ccb085bc7cb2338faa2fdcfc3787676",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20322": {
    "title": "STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f999e08a588b7822c25ad7d34b3a120515ddc3cc",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20323": {
    "title": "Naming the Most Anomalous Cluster in Hilbert Space for Structures with Attribute Information",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ac9a3efc13c10c5c20db9c61e93159d2582dfe9c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20324": {
    "title": "Meta-Learning for Online Update of Recommender Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d3a88b229ee1eba9a350b75ef58cf41c4d1b1479",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20325": {
    "title": "The Triangle-Densest-K-Subgraph Problem: Hardness, Lovász Extension, and Application to Document Summarization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b2e45527e56d781b57b5db1f1c0fc364f378d090",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20326": {
    "title": "Obtaining Calibrated Probabilities with Personalized Ranking Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2eb715a97a91ec9c183bf0e94d98a796a55c3d25",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20327": {
    "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5363ca2113e74a294dbeaf2f309c4fd828a0babd",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20328": {
    "title": "Unsupervised Anomaly Detection by Robust Density Estimation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5aa877991da1b9d826d1600fc428c1cc704b8ca6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20329": {
    "title": "From One to All: Learning to Match Heterogeneous and Partially Overlapped Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2379c49e346e3c5486c1aa4b52770110f5d4f144",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20330": {
    "title": "TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e9da2ce19846c5dd2497e323ebbacd991fbe1c20",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20331": {
    "title": "Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b68b9b84f3a42abceffb2bc792aaaef0728a8983",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20332": {
    "title": "Unifying Knowledge Base Completion with PU Learning to Mitigate the Observation Bias",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "82d60aae09d5c06ef3156fd97d5ef311d8ef93bb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20333": {
    "title": "A Self-Supervised Mixed-Curvature Graph Neural Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "745e5ea4223575d460e3c7422b04d7c06ce01b3c",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20334": {
    "title": "MS-HGAT: Memory-Enhanced Sequential Hypergraph Attention Network for Information Diffusion Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cd29ac0d85605fc9054911303bfef03ed3127d1a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20335": {
    "title": "Graph Structure Learning with Variational Information Bottleneck",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7fc37fbd2f808984cfc5c78410a993f89eb0ef49",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20336": {
    "title": "Heterogeneous Peer Effects in the Linear Threshold Model",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4f45950443902323a28bd45e1072df6f51fb1db7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20337": {
    "title": "Exploring Relational Semantics for Inductive Knowledge Graph Completion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "397491a080855339052ed61f8c60778225851c6d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20338": {
    "title": "HAGEN: Homophily-Aware Graph Convolutional Recurrent Network for Crime Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a08bbf1956eb4137371334b138c6304d61d69771",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20339": {
    "title": "Calibrated Nonparametric Scan Statistics for Anomalous Pattern Detection in Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c32c10e3bf0ec2133ead4475c86b92e329d9c852",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20340": {
    "title": "Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "601d8306dac894f999542018519f8587285783a8",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20341": {
    "title": "ShuttleNet: Position-Aware Fusion of Rally Progress and Player Styles for Stroke Forecasting in Badminton",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9145c3426b54be4ab7013d20fc584be76d2e2d58",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20342": {
    "title": "Event-Aware Multimodal Mobility Nowcasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5f447bc454eb572374751576ee0932e5ecba7f8b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20343": {
    "title": "Discovering Interpretable Data-to-Sequence Generators",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5470dc35d234a36c174a6e7ea99c45acc7390f4b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20344": {
    "title": "DeepGPD: A Deep Learning Approach for Modeling Geospatio-Temporal Extreme Events",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "92b818a39e0eae157c15e73f1422d19f08f6831a",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20345": {
    "title": "SmartIdx: Reducing Communication Cost in Federated Learning by Exploiting the CNNs Structures",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6b089a9b211a41941d7a40af91e4b4ff1ffa31fe",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20346": {
    "title": "Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "85a5de740b2c4496a0df6fee2c7206c9a5fef70c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20347": {
    "title": "CoCoS: Enhancing Semi-supervised Learning on Graphs with Unlabeled Data via Contrastive Context Sharing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ff08f6c77d1e8014d2085244e5b5df4f124b89ab",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20348": {
    "title": "Ensemble Semi-supervised Entity Alignment via Cycle-Teaching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "390313403c892f396380946ff04ffae297d7ef4e",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20349": {
    "title": "Unsupervised Adversarially Robust Representation Learning on Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "aeda9bbae0e2060bd98d597befaab708f01bb557",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20350": {
    "title": "Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f9ebb29fb97651ebe90febff450379b2561fd0fd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20351": {
    "title": "PolygonE: Modeling N-ary Relational Data as Gyro-Polygons in Hyperbolic Space",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "31fd80ce52587af64e3277e5a5cb74d95e420de1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20352": {
    "title": "Cross-Task Knowledge Distillation in Multi-Task Recommendation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6bc729a25797ae1c4e08b832daa708f699c6669d",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20353": {
    "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1163c29ae6a26b1cd8d85280740410e99001a959",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20354": {
    "title": "Multi-Scale Distillation from Multiple Graph Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "74374b54ac2ce6464def966e74ef35ae1c9a9f49",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20355": {
    "title": "Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a896d05fcbdc55424a4b27f9b9138c02702db089",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20356": {
    "title": "Anisotropic Additive Quantization for Fast Inner Product Search",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "23cff8ecfc5feb4208dbb90ff3fa7a6adb214e9c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20357": {
    "title": "Robust Heterogeneous Graph Neural Networks against Adversarial Attacks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "07808da6b32d464f66aac31297f070c15499b92b",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20358": {
    "title": "Multi-Dimensional Prediction of Guild Health in Online Games: A Stability-Aware Multi-Task Learning Approach",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "af79b3e7ff74c9fbf5272cbefd1922f37bfa2e3b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20359": {
    "title": "Multi-View Intent Disentangle Graph Networks for Bundle Recommendation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "23944e3712669bfda7863567090bfcfebab944d1",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20360": {
    "title": "Multi-Type Urban Crime Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f50dbd3e6cebe0830ba2c95935137a83b3b8f3df",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20361": {
    "title": "Forecasting Asset Dependencies to Reduce Portfolio Risk",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "850ffdf395880e389a3a610df9802b70e4fe81b2",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20362": {
    "title": "Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-Supervision",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "52bb8cb9cd9c130e18f6dcd6935b8b23d9b470d1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20363": {
    "title": "Can Machines Read Coding Manuals Yet? – A Benchmark for Building Better Language Models for Code Understanding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5fde5c44197473ad2ac0645f643e577188b316c4",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20364": {
    "title": "No Task Left Behind: Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "494b1a644fbd3c167f5e8946e573c999f334a20d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20365": {
    "title": "Diaformer: Automatic Diagnosis via Symptoms Sequence Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "47d9133629a7dee1664483944547658e8cea83f2",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20366": {
    "title": "Zero-Shot Audio Source Separation through Query-Based Learning from Weakly-Labeled Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9cba6d03d13e18c0100ad9e7858eaa0ec2d18ee1",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20367": {
    "title": "DeepHardMark: Towards Watermarking Neural Network Hardware",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "67e1d3ff6268c487beb9d3382ccb58e02823945e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20368": {
    "title": "A Unified Framework for Real Time Motion Completion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9ebd72eab381d92e8f686f3a3f921b786f8c1112",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20369": {
    "title": "FactorVAE: A Probabilistic Dynamic Factor Model Based on Variational Autoencoder for Predicting Cross-Sectional Stock Returns",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b7fc2f1d9caf20e1e213f7032ac29e965b3ec063",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20370": {
    "title": "AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "740a4524801dd40e69326664b45bb54edc3f4cde",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20371": {
    "title": "SCIR-Net: Structured Color Image Representation Based 3D Object Detection Network from Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2e20fb355cf473d9fa666c6d66790abcd86b35ea",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20372": {
    "title": "Learning and Dynamical Models for Sub-seasonal Climate Forecasting: Comparison and Collaboration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2386e292d492931b9223143f2e9352ae8c237740",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20373": {
    "title": "Solving PDE-Constrained Control Problems Using Operator Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b22ba5a85a2eac31631eecc13991da49826e4496",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20374": {
    "title": "Proxy Learning of Visual Concepts of Fine Art Paintings from Styles through Language Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4af251d39fb92684f15e9f993dee612a66c673c0",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20375": {
    "title": "SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9f86031e05d8c290ab468ef23b5e90ccea47e6f5",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20376": {
    "title": "Intra-Inter Subject Self-Supervised Learning for Multivariate Cardiac Signals",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bf10bec6c7127df551f5431fa83e4a2f853d2515",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20377": {
    "title": "GeomGCL: Geometric Graph Contrastive Learning for Molecular Property Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0a3764d605a317c5618a8cc4e83589d87bfff3b9",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20378": {
    "title": "OAM: An Option-Action Reinforcement Learning Framework for Universal Multi-Intersection Control",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "de4ef43663c8f43b9cca14333e66f3e4267a0fac",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20379": {
    "title": "End-to-End Line Drawing Vectorization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0e2cefeb0c3b2a012e1f6d4fab1a1e07e0f2a9a9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20380": {
    "title": "Context-Aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1bf0d09ea0b71ecbd65b07f1744656209ad53a78",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20381": {
    "title": "Hyperverlet: A Symplectic Hypersolver for Hamiltonian Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d51d703711621e3ff03d4566589fdedd1d133347",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20382": {
    "title": "Learning Human Driving Behaviors with Sequential Causal Imitation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e2927337d24855728b7a5371f42104a3f546a7a7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20383": {
    "title": "EMVLight: A Decentralized Reinforcement Learning Framework for Efficient Passage of Emergency Vehicles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "92f745a53f3856cc5d9db0787de73abf0a53b521",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20384": {
    "title": "Constrained Prescriptive Trees via Column Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4787694a28aa60bce3062cc60dc37f5dc3b17d98",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20385": {
    "title": "DDGCN: Dual Dynamic Graph Convolutional Networks for Rumor Detection on Social Media",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f62479dae183fa83801bdc417bcb5f1ad765c649",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20386": {
    "title": "Contact-Distil: Boosting Low Homologous Protein Contact Map Prediction by Self-Supervised Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8b64f44dc6804ce6a5cc2924fc204c2098f1022e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20387": {
    "title": "EtinyNet: Extremely Tiny Network for TinyML",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ffc1fb1701aa12411534876d15ae969d1945c9af",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20388": {
    "title": "RepBin: Constraint-Based Graph Representation Learning for Metagenomic Binning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c1ab18add1bbb01dd11287f84af1e79c08be61bd",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20389": {
    "title": "NSGZero: Efficiently Learning Non-exploitable Policy in Large-Scale Network Security Games with Neural Monte Carlo Tree Search",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "450ae859b451e6917344369c7f22fc0d901417ca",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20390": {
    "title": "RID-Noise: Towards Robust Inverse Design under Noisy Environments",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5e9a46f0d5062f725d2368ee20985647bf27cf81",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20391": {
    "title": "Deepfake Network Architecture Attribution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "16317ac128c0583c926382a2dd143738c7a99209",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20392": {
    "title": "ZINB-Based Graph Embedding Autoencoder for Single-Cell RNA-Seq Interpretations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "aff7c738fd9a7b57eb9c1b011977cdbc7deb1018",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20393": {
    "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4e90ab5310055d1cc9a951a476ed241f97fe76b0",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20394": {
    "title": "AlphaHoldem: High-Performance Artificial Intelligence for Heads-Up No-Limit Poker via End-to-End Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4fcf18bda55414c5f31cc4be560bae92e8e4b7e9",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20395": {
    "title": "Hierarchical Multi-Supervision Multi-Interaction Graph Attention Network for Multi-Camera Pedestrian Trajectory Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3e8e31d1ecabfde69f471fd140c51a308b60df22",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20396": {
    "title": "6DCNN with Roto-Translational Convolution Filters for Volumetric Data Processing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f6825d3ffbd1b63446b41d6fb3bf802bc45444f8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20397": {
    "title": "Deeply Tensor Compressed Transformers for End-to-End Object Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "42b0f689a4209428e0f7963eadd377196bda3741",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20398": {
    "title": "Dynamic Manifold Learning for Land Deformation Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9b753cf07d751947dd600dc01abb4e75bdad8826",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20399": {
    "title": "Fully Adaptive Framework: Neural Computerized Adaptive Testing for Online Education",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b5498c96af1373438ed562aae2ffc951b5dc2cdb",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20400": {
    "title": "An Algorithmic Introduction to Savings Circles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a01181f29d874ea8a9aa2521725a0d4782d562d1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20401": {
    "title": "Locally Fair Partitioning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b4c1571ca03d3142cd2e97fbdc412634b9d415f3",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20402": {
    "title": "Maximizing Nash Social Welfare in 2-Value Instances",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8f188f43117119766e5d756f191b6353fda27fa6",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20403": {
    "title": "Truth-Tracking via Approval Voting: Size Matters",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "62ea8a00be1ddbdb242c1bab90828a8d14e5aeb4",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20404": {
    "title": "Dimensionality and Coordination in Voting: The Distortion of STV",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4b64e105076aec8ce706f4a107871a23201e9e95",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20405": {
    "title": "Fair and Truthful Giveaway Lotteries",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f4d4d3ccea5b6230bbb7f4b7eca5f187c4e8bf61",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20406": {
    "title": "Universal and Tight Online Algorithms for Generalized-Mean Welfare",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "96533400058669ee2a3d1f238e903843c392af99",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20407": {
    "title": "Truthful and Fair Mechanisms for Matroid-Rank Valuations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e080fbf23fa228a993d3613a5351afb79e97a0e9",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20408": {
    "title": "Truthful Cake Sharing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d0c9bfb0e60eeb824b2f52265c155632032611ff",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20409": {
    "title": "The Secretary Problem with Competing Employers on Random Edge Arrivals",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "34856f1bb8186a8192e55924eba237bf30cd8a3f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20410": {
    "title": "Almost Full EFX Exists for Four Agents",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1402f3d87e7dba38984bc69afd603d3077b142eb",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20411": {
    "title": "Sequential Blocked Matching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5fb6c2fcc0a634c2e23c8ec5735318cac63b1d11",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20412": {
    "title": "Combating Collusion Rings Is Hard but Possible",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2a6f7cd518b6a7d00264fdc19aedf5df08dda7b6",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20413": {
    "title": "Theory of and Experiments on Minimally Invasive Stability Preservation in Changing Two-Sided Matching Markets",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4baa4c3453b4167ff880910f863af9296c529468",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20414": {
    "title": "A Calculus for Computing Structured Justifications for Election Outcomes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "260881cb452d8091493eb50ec9d8ce6f594a0f13",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20415": {
    "title": "Single-Agent Dynamics in Additively Separable Hedonic Games",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1a631c38e3d0281154edd4fe254ed2003125b618",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20416": {
    "title": "On Improving Resource Allocations by Sharing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "940c39f73f66c03223b73d1b79c7a3af552efb21",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20417": {
    "title": "Liquid Democracy with Ranked Delegations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7455d85f4c68afa9db3c13ed78044f1d61899c2e",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20418": {
    "title": "Individual Representation in Approval-Based Committee Voting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "37c26b3593a971a2b2f66b518e59e4e082dd85bd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20419": {
    "title": "The Metric Distortion of Multiwinner Voting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7f8e0efd23e1f9647891b1155bfc650f86016776",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20420": {
    "title": "A Little Charity Guarantees Fair Connected Graph Partitioning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "beb8019bdae9832c48a6ba573a35355af7190dbc",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20421": {
    "title": "Truthful Aggregation of Budget Proposals with Proportionality Guarantees",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e8d7394d94213c6206937a8d73fa61516295105f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20422": {
    "title": "The Complexity of Learning Approval-Based Multiwinner Voting Rules",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6b956e8dc77d61475d2e253a8d3c3cd009a52eab",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20423": {
    "title": "Efficiency of Ad Auctions with Price Displaying",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e72015c2b022642baac6621e9159f2d31da55f42",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20424": {
    "title": "Signaling in Posted Price Auctions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2fa809a640bd2a6fbc578f61fdae68611752dfcf",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20425": {
    "title": "Weighted Fairness Notions for Indivisible Items Revisited",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "41817cbb07cebe5491b45a841b62ba48ebcabb49",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20426": {
    "title": "Pizza Sharing Is PPA-Hard",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "31fb5fecc62a1742a3ecde265d525586a0f7efa6",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20427": {
    "title": "Heterogeneous Facility Location with Limited Resources",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "050c545bc6d607337a3289cbcff8064ce1eb94b5",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20428": {
    "title": "Complexity of Deliberative Coalition Formation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2fcbe843222aaf33a4a5a58288126706d88df1a3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20429": {
    "title": "The Price of Justified Representation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ca137e945b6272126abdf9cde4572611afe66158",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20430": {
    "title": "The Complexity of Subelection Isomorphism Problems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "58500f5e4545f2b7cb0cc1dc12d68fbeb7769de3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20431": {
    "title": "Fast Payoff Matrix Sparsification Techniques for Structured Extensive-Form Games",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "caf1a70dccb75f948dd9dd7b9e10e590eb77c626",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20432": {
    "title": "Two-Price Equilibrium",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "68852389cfc1b82a9368cace7b8df00974cdc60a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20433": {
    "title": "Algorithmic Bayesian Persuasion with Combinatorial Actions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ff10f22a4474441a90783cf823c1363e902fcf24",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20434": {
    "title": "Bayesian Persuasion in Sequential Decision-Making",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "635f3cf9e820a0bf2841d72dbbfc0336c18d2a38",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20435": {
    "title": "Hedonic Diversity Games: A Complexity Picture with More than Two Colors",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dffcf6529ee9ae61aa985fb6302c954e03ac4049",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20436": {
    "title": "Fair and Efficient Allocations of Chores under Bivalued Preferences",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "509c56f46aae8b5f03cf5dc0740a0da5250c4623",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20437": {
    "title": "Secretary Matching with Vertex Arrivals and No Rejections",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "559fe6a8ac1b95d3f2a625a0214f6d2b33f20bde",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20438": {
    "title": "Machine-Learned Prediction Equilibrium for Dynamic Traffic Assignment",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "afe0d12eb3f458c1eabdbe5b35dd9b1739f787c6",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20439": {
    "title": "Multi-Leader Congestion Games with an Adversary",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e47374a278296b0a810a528b1c8fe17f6706acaa",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20440": {
    "title": "Approval-Based Committee Voting under Incomplete Information",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1a8cf6f3c1820007ee680e0f8b986cd8e5252225",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20441": {
    "title": "Reforming an Envy-Free Matching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "28abe52458b9ab0e34f61211df39cb15336df071",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20442": {
    "title": "The Complexity of Proportionality Degree in Committee Elections",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "80d3c20d23954d6435092a7c8d1c6a1f605e3ff3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20443": {
    "title": "Worst-Case Voting When the Stakes Are High",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d4d25594b362dfbf0067481555b97033056125fb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20444": {
    "title": "PageRank for Edges: Axiomatic Characterization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "aa4deb9f5224216fb676f39cde865a9ec4296088",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20445": {
    "title": "Safe Subgame Resolving for Extensive Form Correlated Equilibrium",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e74d0a1ba493420145f641f438d303952c78a978",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20446": {
    "title": "The Semi-random Likelihood of Doctrinal Paradoxes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ac092a72c6114e6644391dca599bfdc6df67300e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20447": {
    "title": "Is There a Strongest Die in a Set of Dice with the Same Mean Pips?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "390e7ba09536aa17bc45fb2da3a303bd9fc85d13",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20448": {
    "title": "Choices Are Not Independent: Stackelberg Security Games with Nested Quantal Response Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "61d3fbbe059702572fd35f7d7f9d55d60f61ce45",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20449": {
    "title": "Strictly Proper Contract Functions Can Be Arbitrage-Free",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6f46b9848550f5dcbdc4586a36890eb2e7002c7d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20450": {
    "title": "Characterization of Incentive Compatibility of an Ex-ante Constrained Player",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "33fa09e968713a02576aecda17419834324c08ca",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20451": {
    "title": "Online Elicitation of Necessarily Optimal Matchings",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f74351db7ce813bfb4e5ec92071818861b5a81d9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20452": {
    "title": "Generalized Dynamic Cognitive Hierarchy Models for Strategic Driving Behavior",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f4f47619f20a3bd4c612638e5a1febd660b86c00",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20453": {
    "title": "Improved Maximin Guarantees for Subadditive and Fractionally Subadditive Fair Allocation Problem",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c3da09454a5f1d4e22610ef60544460bd6c91815",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20454": {
    "title": "Proportional Public Decisions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "534161f1c043b4d7fbc0fb1385ed08a7f93b21b5",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20455": {
    "title": "Online Task Assignment Problems with Reusable Resources",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1bd19bdee67e6729b52408ac7aef41caf3ab08c4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20456": {
    "title": "Iterative Calculus of Voting under Plurality",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4b2692b96cc90a0afa966a264d3c15b52c68cb18",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20457": {
    "title": "Coordinating Followers to Reach Better Equilibria: End-to-End Gradient Descent for Stackelberg Games",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "759de41d8a03e8bfaf8bd8a2a9580ac6df96adeb",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20458": {
    "title": "Multi-Unit Auction in Social Networks with Budgets",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8a1d7a359d66b89fad2cab07b9ae040d186033ab",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20459": {
    "title": "The Strange Role of Information Asymmetry in Auctions—Does More Accurate Value Estimation Benefit a Bidder?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d538d5c5968d0f33223581e7daf2155659b20192",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20460": {
    "title": "AutoCFR: Learning to Design Counterfactual Regret Minimization Algorithms",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0ef355aff469bfeb96481aeeae914995d4a3b68f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20461": {
    "title": "Team Correlated Equilibria in Zero-Sum Extensive-Form Games via Tree Decompositions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "12cd4c73a649edadae04c4c747d13aa5e2a00233",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20462": {
    "title": "Planning with Participation Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a2b87426c3c41c4a714374187b5b9f9dfb12d2d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20463": {
    "title": "I Don't Think So\": Summarizing Policy Disagreements for Agent Comparison",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "41c721aec8c3be4334f433b6359ab8075fbbd189",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20464": {
    "title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "59cbd70b9bcdb4a15915c11b47bfcd93319d82c6",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20465": {
    "title": "Role of Human-AI Interaction in Selective Prediction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "76ffc00d9602ca557df0eaa9d9da0609d839e402",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20466": {
    "title": "How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20467": {
    "title": "Adversarial Learning from Crowds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e51575ce4261f82cfdefe1d1d03103d5ea78c0bf",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20468": {
    "title": "FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "337b96b55034e6ead100d26e547c942489ff2e93",
    "citation_count": 24
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20469": {
    "title": "Teaching Humans When to Defer to a Classifier via Exemplars",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "950414032f17dec4e024deef512402121f66cabc",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20470": {
    "title": "Deceptive Decision-Making under Uncertainty",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "08903e4d1b32df3b224893b589ad37296c597e39",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20471": {
    "title": "On Optimizing Interventions in Shared Autonomy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "67ccaeddfbea6b9d46870e59f96a2ae8e595a3ef",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20472": {
    "title": "Open Vocabulary Electroencephalography-to-Text Decoding and Zero-Shot Sentiment Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f8f5a3408683595c29e47f95c7fcd398d95064d4",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20473": {
    "title": "DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "373548edd6bcb8071460b93e74fa58220a14f4f1",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20474": {
    "title": "When Facial Expression Recognition Meets Few-Shot Learning: A Joint and Alternate Learning Framework",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9a9a2dfc310dae1901f3c2b4dd64148a960e96e4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20475": {
    "title": "Discovering State and Action Abstractions for Generalized Task and Motion Planning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5185759e89a8e52ee1184cbebba5917371673790",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20476": {
    "title": "Recurrent Neural Network Controllers Synthesis with Stability Guarantees for Partially Observed Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c14bfaaef13394ea48fbb19880395cb4acaf850c",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20477": {
    "title": "Random Mapping Method for Large-Scale Terrain Modeling",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9bfb50723df26dfe59766c6ea32cf414827b7820",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20478": {
    "title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cf9c24df550fafc9cb439b02e61208eb71d5e933",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20479": {
    "title": "CTIN: Robust Contextual Transformer Network for Inertial Navigation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bc528bfc34952401d698ea0b6d82205974a0084c",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20480": {
    "title": "Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "357256947116c1ca4a9c2c3cd19670ccd396e867",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20481": {
    "title": "Robust Adversarial Reinforcement Learning with Dissipation Inequation Constraint",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d6ebaa2daebcb509250f633badc3fb154ef53f72",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20482": {
    "title": "Sim2Real Object-Centric Keypoint Detection and Description",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "06b13fa2b4f18603b24174a2a1542e9d43e69948",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20483": {
    "title": "Incomplete Argumentation Frameworks: Properties and Complexity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "eccbfb775274d6f60307cce1ca2606dada2d2a93",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20484": {
    "title": "Trading Complexity for Sparsity in Random Forest Explanations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "022bedf7c1f314f787d1009d4b63506c2614f022",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20485": {
    "title": "From Actions to Programs as Abstract Actual Causes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8e11742352f88f084d2ade6e8e603e969fd16fa3",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20486": {
    "title": "Equivalence in Argumentation Frameworks with a Claim-Centric View – Classical Results with Novel Ingredients",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "794d108c43d49e4b2b3d23675daa91cf8ad2c743",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20487": {
    "title": "Finite Entailment of Local Queries in the Z Family of Description Logics",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "86222101f84ac87020fed242eb8cb20e82cd2d5a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20488": {
    "title": "The Price of Selfishness: Conjunctive Query Entailment for ALCSelf Is 2EXPTIME-Hard",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "233ef23c221030d259becd49fa19743aafff282c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20489": {
    "title": "Expressivity of Planning with Horn Description Logic Ontologies",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0f714a1e5f43b7a8a54c3569a3fa67e42c3b678c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20490": {
    "title": "ER: Equivariance Regularizer for Knowledge Graph Completion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "172f842cf4d68a949b2b47ba52903dd0766ba87f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20491": {
    "title": "Geometry Interaction Knowledge Graph Embeddings",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3f6861639551b33da7276b6e27a9933c467e9d3b",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20492": {
    "title": "Multi-Relational Graph Representation Learning with Bayesian Gaussian Process Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "68d0fc739612be741460af3bcf9e609bfda113e8",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20493": {
    "title": "ASP-Based Declarative Process Mining",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0e4ddca559f375c73448f92df1c20c7f454c1a7d",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20494": {
    "title": "On Testing for Discrimination Using Causal Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "49ea99ae1df26a4dfa8d1c04f41033917abc9159",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20495": {
    "title": "Monotone Abstractions in Ontology-Based Data Management",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cd455b96f21860f3727ee3ebd3acbfd09c67708e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20496": {
    "title": "Lower Bounds on Intermediate Results in Bottom-Up Knowledge Compilation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ce4d61a7e1ad9b82e292aa6c01a1fe1539b60b62",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20497": {
    "title": "Enforcement Heuristics for Argumentation with Deep Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "53105ee348452c7f96429bbeabba823d31221a4e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20498": {
    "title": "On the Computation of Necessary and Sufficient Explanations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "318147ab02c5afa7c0c49a119330dde1f38b03ce",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20499": {
    "title": "Machine Learning for Utility Prediction in Argument-Based Computational Persuasion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "415c64bcf7cc8fc7e7ea6aed2467f9e52dbbd6e2",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20500": {
    "title": "On the Complexity of Inductively Learning Guarded Clauses",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9a0b936b1b8514fa4bd966923602bd24ef1801fe",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20501": {
    "title": "Tractable Abstract Argumentation via Backdoor-Treewidth",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "028ff96d72307b51e849d13285a140122d6383ba",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20502": {
    "title": "Large-Neighbourhood Search for Optimisation in Answer-Set Solving",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "48b0e6717147c947930be605e559c8281ce2b0c6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20503": {
    "title": "Answering Queries with Negation over Existential Rules",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "03b92511cf9add7a56b40a88c81ee93574ab9248",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20504": {
    "title": "Axiomatization of Aggregates in Answer Set Programming",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2f00b2a724ca7546cd248e8fca4991f9ae7ae2b7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20505": {
    "title": "Linear-Time Verification of Data-Aware Dynamic Systems with Arithmetic",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "95fb13af7accf2f19f37344ba3290b7a205f4aeb",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20506": {
    "title": "Rushing and Strolling among Answer Sets – Navigation Made Easy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d65baf10a69841e91ed0952ae071bd0a69772f62",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20507": {
    "title": "Sufficient Reasons for Classifier Decisions in the Presence of Domain Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ae0708938c2b0d3afe231ee03d65d6d4ab4e28e0",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20508": {
    "title": "Reasoning about Causal Models with Infinitely Many Variables",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f577f0ade31f5f702000d68c431b64cf09df252a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20509": {
    "title": "An Axiomatic Approach to Revising Preferences",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8319cc48fac7a5214c02eddd4892ed1de469053d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20510": {
    "title": "BERTMap: A BERT-Based Ontology Alignment System",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3554aea851065fb2c8f59dd7b600b330d54c2da7",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20511": {
    "title": "Conditional Abstract Dialectical Frameworks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a83f4b1cc5adc69a2dac61ab0aa40776ade9d0a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20512": {
    "title": "MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "62b1d761efc49a49852d1f13d79f7f94f7a47c71",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20513": {
    "title": "Towards Explainable Action Recognition by Salient Qualitative Spatial Object Relation Chains",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "34f7e68aa2c8cb05f7e86a9642fd4155b3f0a105",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20514": {
    "title": "Tractable Explanations for d-DNNF Classifiers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d21caa6b41fd307480068ce36963373246c4a80d",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20515": {
    "title": "Understanding Enthymemes in Deductive Argumentation Using Semantic Distance Measures",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dc5832692a6871f781a7108e8eea982f73b2b15d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20516": {
    "title": "Inferring Lexicographically-Ordered Rewards from Preferences",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "303fac45d35975a1867ef47ad9d7cfecdc436d92",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20517": {
    "title": "Towards Fine-Grained Reasoning for Fake News Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "21d54922f3f9441d9fb1d05925d33ebbc60e0b12",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20518": {
    "title": "ApproxASP – a Scalable Approximate Answer Set Counter",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3bc6e1be0f006096a2d1b3fad8b11ed788594da3",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20519": {
    "title": "Unit Selection with Causal Diagram",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dcfde41983590f9a051e913a6db832c420e39807",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20520": {
    "title": "Bounds on Causal Effects and Application to High Dimensional Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3dfac124113ac1827dccfe06237784984642c969",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20521": {
    "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20522": {
    "title": "Multi-View Graph Representation for Programming Language Processing: An Investigation into Algorithm Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "184e9beb79f57e79c77c4a54736cb37afb80318c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20523": {
    "title": "Automated Synthesis of Generalized Invariant Strategies via Counterexample-Guided Strategy Refinement",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cb81f70e2ac876885ba5b13f50a3ec5a4f844f4d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20524": {
    "title": "Using Conditional Independence for Belief Revision",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a35a47218c983b8483b3d31a1300a03d27e8bc23",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20525": {
    "title": "Weighted Model Counting in FO2 with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0a0ae38f1a7e781d69b7d895d27e3732d2ffd685",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20526": {
    "title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1f836ccdd422f35c1e2e470b1bcc7201b863cc63",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20527": {
    "title": "Compilation of Aggregates in ASP Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f8d7be1cdba89869c529bcb9b4722e9675dd2789",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20528": {
    "title": "Prevailing in the Dark: Information Walls in Strategic Games",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f2616446d0aee89fc7c63f23a77814fe80168e03",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20529": {
    "title": "Knowledge Compilation Meets Logical Separability",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "520cfc932a56e58f887312d9d35471e3c0b6326e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20530": {
    "title": "Propositional Encodings of Acyclicity and Reachability by Using Vertex Elimination",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "55afe3569a93ef3a94d44f22e52557d702f64189",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20531": {
    "title": "Random vs. Best-First: Impact of Sampling Strategies on Decision Making in Model-Based Diagnosis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ac265ede6acd4d859ef815b1329c1a3692213ce1",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20532": {
    "title": "On Paraconsistent Belief Revision in LP",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bd6abf366429229ea120c888773f13d0a6c0d7b6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20533": {
    "title": "Weakly Supervised Neural Symbolic Learning for Cognitive Tasks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e0111007c19762f76ee4b69bb422cee51ffa8ca9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20534": {
    "title": "First Order Rewritability in Ontology-Mediated Querying in Horn Description Logics",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b6d0c7ad4b80eee8e8983f509464a77b922930b4",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20535": {
    "title": "MeTeoR: Practical Reasoning in Datalog with Metric Temporal Operators",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "80f48e7c279cb0986de5ca513e4b7a6bede02f9f",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20536": {
    "title": "SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c11ee33dea3f83cd77dcbc14684ee305b7a7e184",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20537": {
    "title": "Inductive Relation Prediction by BERT",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a54a72601f862517ce38984522a39e77cb9f4b3d",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20538": {
    "title": "Learning to Walk with Dual Agents for Knowledge Graph Reasoning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "28329a7f58f95f1b8127d93c3865b3c5edbbbca5",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20539": {
    "title": "Residual Similarity Based Conditional Independence Test and Its Application in Causal Discovery",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "13be7dfa6f53b2456ce83f736d5cc3d453814059",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20540": {
    "title": "Characterizing the Program Expressive Power of Existential Rule Languages",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c19a2e948b7862417cfa26b1b411fc597478640",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20541": {
    "title": "Context-Specific Representation Abstraction for Deep Option Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "48e841a2cf2a13152f507afd9cb4e83090d72039",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20542": {
    "title": "FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "60f34e40a2ad9231a76b7e4a79c9d957729add58",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20543": {
    "title": "Distributed Learning with Strategic Users: A Repeated Game Approach",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a755968310e6628e14987afc3c9a1ac5fb4c8fb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20544": {
    "title": "Private Rank Aggregation in Central and Local Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6e8b76095cb750fe44ab47e283964f0d8d70d356",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20545": {
    "title": "Combating Adversaries with Anti-adversaries",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5cd20c68a8c8554b89f762e0a5eac2bda03059c4",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20546": {
    "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "69c936f8f3b5f7ce4ee9d80dacb5043d32154950",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20547": {
    "title": "Latent Time Neural Ordinary Differential Equations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d7aeca7bdb9eeeddcf19e578bc445c8c7354d1af",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20548": {
    "title": "Beyond GNNs: An Efficient Architecture for Graph Problems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d64dfa7ae7f844755b362080f8b8982bb5fa66e3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20549": {
    "title": "Programmatic Modeling and Generation of Real-Time Strategic Soccer Environments for Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "54bb0ae305e0f99ff377e52d0497dbef6ab70050",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20550": {
    "title": "Admissible Policy Teaching through Reward Design",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4bf4065016e6a9150c63fd5bc71330a634a6a1f9",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20551": {
    "title": "Entropy-Based Logic Explanations of Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "829ac55e5f878d01f2bd62376b097e5da8ab8ee3",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20552": {
    "title": "Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2e1861457a87d185b14bdc7d194ed2ad4d36088a",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20553": {
    "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "03404882d623a00fb080ae0ecd611453279355d8",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20554": {
    "title": "Modeling Attrition in Recommender Systems with Departing Bandits",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c37b9ec2ff1828877575acc600b73c3bcde138f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20555": {
    "title": "Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8657e637a20ff1775307adcde2ed10494661a908",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20556": {
    "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f190deb9b5568fb50a8847712a29c9f2af4739e2",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20557": {
    "title": "ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1b2bdddfe81aa4fc6111d7cdcb6d7cf4a0493b35",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20558": {
    "title": "Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f58d73b7321f0d8ab0072a9f437607bc217975a7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20559": {
    "title": "Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f16dc2741d60f5f7fdf39cbeba970207f7f4ab2a",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20560": {
    "title": "Shrub Ensembles for Online Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f46792d464661494552201107c3455c7e365ee98",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20561": {
    "title": "NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f07bec152fc98259fcb3ebfd28403308272ffee3",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20562": {
    "title": "Leaping through Time with Gradient-Based Adaptation for Recommendation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8527f41495fd2cd40fb80ba8efc8ba6b42a9ba88",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20563": {
    "title": "Active Sampling for Text Classification with Subinstance Level Queries",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4c12c9b87e96109a96ccd33a6c6c3271656e3ce1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20564": {
    "title": "A Unifying Theory of Thompson Sampling for Continuous Risk-Averse Bandits",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "77a0fa0e2b81dcd82c0e64bf21b3e0486dcfdee6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20565": {
    "title": "Locally Private k-Means Clustering with Constant Multiplicative Approximation and Near-Optimal Additive Error",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c51dc0fa5a7f9013932b5ba06d1a278ef3063d6f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20566": {
    "title": "Safe Online Convex Optimization with Unknown Linear Safety Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0dd23ceed86694c73d7c658515bbaee1b33cc188",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20567": {
    "title": "Deconvolutional Density Network: Modeling Free-Form Conditional Distributions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4b2e27bee22dd3081e943b28ed548fd9d58b3f7e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20568": {
    "title": "Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "39b8b8a54362c6ae4cc24a18d2fc4665bbcc7b85",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20569": {
    "title": "Simultaneously Learning Stochastic and Adversarial Bandits under the Position-Based Model",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "de16e94def5424ac99218b526db2d533c2947137",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20570": {
    "title": "Clustering Interval-Censored Time-Series for Disease Phenotyping",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8cafd6998befef448c1c0f52c353758c484fc66c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20571": {
    "title": "Efficient Robust Training via Backward Smoothing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "407b4eeae564f353fc7c7e922675c08bd9d8e779",
    "citation_count": 22
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20572": {
    "title": "An Online Learning Approach to Sequential User-Centric Selection Problems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3d33f61738b6399107769d3c25ab21a1dd17301a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20573": {
    "title": "Better Parameter-Free Stochastic Optimization with ODE Updates for Coin-Betting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0d38b4dea08eb971b17bb704d8057919b54bff2b",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20574": {
    "title": "Mutual Nearest Neighbor Contrast and Hybrid Prototype Self-Training for Universal Domain Adaptation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d0b91657fe0ad99301c75cfea1a37e0db3389990",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20575": {
    "title": "Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b604c70173650ad0e72634a373eebdc663cb9c46",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20576": {
    "title": "Zero Stability Well Predicts Performance of Convolutional Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8f5c879cc6cef35a6faa7f36920220935d84554f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20577": {
    "title": "Semi-supervised Learning with Multi-Head Co-Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ea974dd3ce54791ff261565f510aa055ee94952d",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20578": {
    "title": "Instance Selection: A Bayesian Decision Theory Perspective",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b42cc0c8485001d508c2d8f83f51ac986105ae30",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20579": {
    "title": "Input-Specific Robustness Certification for Randomized Smoothing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "129d942368e1e75e6715a5c77d159d246ca110aa",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20580": {
    "title": "Multimodal Adversarially Learned Inference with Factorized Discriminators",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bc6e78be1b991c2666c70bd36d6e7986b7a7ba86",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20581": {
    "title": "Imbalance-Aware Uplift Modeling for Observational Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7fa311c276a674539e60eb35cd635dcb7edbd13d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20582": {
    "title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "966a110bea593cf5bd566e4a1f250f0e53cd77ed",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20583": {
    "title": "BScNets: Block Simplicial Complex Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "01169e9b2cda158ceb3025e6f1ea54aa1b3273d9",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20584": {
    "title": "ASM2TV: An Adaptive Semi-supervised Multi-Task Multi-View Learning Framework for Human Activity Recognition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "08c1e516852c15d6160ea0d3038139b3ade63707",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20585": {
    "title": "Identification of Linear Latent Variable Model with Arbitrary Distribution",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8519c7101d4ced7d18ca7b3d0bb75022776cf61f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20586": {
    "title": "DPNAS: Neural Architecture Search for Deep Learning with Differential Privacy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6f00003b27b702b8e8dd6894ad4e7d60daf580fb",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20587": {
    "title": "Graph Neural Controlled Differential Equations for Traffic Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8d6a348c4b00da0c30f9f5c77619ea40e441652b",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20588": {
    "title": "Differentially Private Regret Minimization in Episodic Markov Decision Processes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "57d006f1f2f82efa2099094fcc032eaff812225d",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20589": {
    "title": "Learning by Competition of Self-Interested Reinforcement Learning Agents",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "61e2f4fcb09263456188b702cad65eeb99884713",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20590": {
    "title": "How to Distribute Data across Tasks for Meta-Learning?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "06983a454b3ea2926db94bb98f5493c8a0c024d6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20591": {
    "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9daeff2818838d9cdd227414812790ed3fa3d7d4",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20592": {
    "title": "Learning Influence Adoption in Heterogeneous Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "045e07e4addfc48d8c99c23a9f47f110185ce9b2",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20593": {
    "title": "Graph-Wise Common Latent Factor Extraction for Unsupervised Graph Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "83a2cfc3644db2d94a378c20aab59aaa21de47f9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20594": {
    "title": "Reinforcement Learning with Stochastic Reward Machines",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "44f925cad517464c61c0bd21f397cce556a5dbc2",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20595": {
    "title": "Sparse-RS: A Versatile Framework for Query-Efficient Sparse Black-Box Adversarial Attacks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5b5a677bf371a4f357bb739bc954d376b41b0c9a",
    "citation_count": 30
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20596": {
    "title": "Learning Logic Programs Though Divide, Constrain, and Conquer",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "856b876b4d7e918b6d18250fb849c8325e021435",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20597": {
    "title": "Implicit Gradient Alignment in Distributed and Federated Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ef828c8b4fdd481d92fb3dfb6dbf2f548c7e0af7",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20598": {
    "title": "How Good Are Low-Rank Approximations in Gaussian Process Regression?",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5cca08cfa9e728febc7ff5e6360ab87a3bae48bb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20599": {
    "title": "KOALA: A Kalman Optimization Algorithm with Loss Adaptivity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0594fba89480e82864a4db4d11049e45427eb265",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20600": {
    "title": "First-Order Convex Fitting and Its Application to Economics and Optimization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d66a040ec7cc1df07e63358cf51fb1dd9fd99765",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20601": {
    "title": "Gradient Temporal Difference with Momentum: Stability and Convergence",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0175f4bd2399909e5aa93f03827808826aee583f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20602": {
    "title": "Distillation of RL Policies with Formal Guarantees via Variational Abstraction of Markov Decision Processes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1b81c291ea9dbc7d2149419ae674e6111400db79",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20603": {
    "title": "Reducing Flipping Errors in Deep Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "222f314912369fe7b660e343771710ae8d59882e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20604": {
    "title": "Bayesian Optimization over Permutation Spaces",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "781be6c466ed327891b42dd9eba941205feb492b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20605": {
    "title": "Meta Propagation Networks for Graph Few-shot Semi-supervised Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f10a78ca84de4c255095ff44c5281c2e70f6385c",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20606": {
    "title": "Online Certification of Preference-Based Fairness for Personalized Recommender Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7ade79eb6e55c632e5e37fcac7e0cf19802fff7d",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20607": {
    "title": "Disentangled Spatiotemporal Graph Generative Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "42cff9e8c3670d81ea78b944fa78e4b5160e4a3b",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20608": {
    "title": "Learning from the Dark: Boosting Graph Convolutional Neural Networks with Diverse Negative Samples",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b5a95b1044181b18af28dd684b98c97e4fe09140",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20609": {
    "title": "Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "90dea057c5c81b79ecf2da7cfe23872113cb7ff8",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20610": {
    "title": "Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d1f8960ee3b0a6e180776502e7d50543d06c4d9",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20611": {
    "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "917b18b8dad23284c0a42f665f2ba1984fa360de",
    "citation_count": 42
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20612": {
    "title": "Dynamic Nonlinear Matrix Completion for Time-Varying Data Imputation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5e2fa23bb072787d1f7f277e6ea0a57cc123e294",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20613": {
    "title": "Up to 100x Faster Data-Free Knowledge Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e9e6921c2c4646198dda64dbbe8aca14e42c9d0f",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20614": {
    "title": "Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "031a375b9dc796ec1e18c8f3c632f205c5ecabfb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20615": {
    "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5fb4947831352af6d6231a830a943f0f2069ee8b",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20616": {
    "title": "Scaling Neural Program Synthesis with Distribution-Based Search",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bbc0c3b126971df6ed644287a1c4cb7b0eb3f763",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20617": {
    "title": "Modification-Fair Cluster Editing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b7b23b82172a55921943e6c69cbd438a356f6e40",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20618": {
    "title": "Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3e72db97e20366bf52bf809b285b3f8746d2d90f",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20619": {
    "title": "JFB: Jacobian-Free Backpropagation for Implicit Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9e4b3bce118e16a94f1cea5baf1d29ff27d46485",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20620": {
    "title": "Smoothing Advantage Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d464b59ca7bcd9783254b2e6d55c2d91b5a981ab",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20621": {
    "title": "Enhancing Counterfactual Classification Performance via Self-Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "24757460dea113c60f3f1152f298d524f8308847",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20622": {
    "title": "Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6dfc71a75f74890bbf8680e9427cc61a45384544",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20623": {
    "title": "Algorithmic Concept-Based Explainable Reasoning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d6f06d2a07fcb9429c0b3b7b97e71e77e2f8bd6b",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20624": {
    "title": "Recovering the Propensity Score from Biased Positive Unlabeled Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b795da59c019d383de77ae49ac31fd6a1ef44282",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20625": {
    "title": "DiPS: Differentiable Policy for Sketching in Recommender Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b825116319d2a203b3e6673af8ef400bbe1c57b1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20626": {
    "title": "Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7680ca2be41594e4e06883e6a997a08106bf52b1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20627": {
    "title": "Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6a4523ed4dfe9c1849865d43a3913240ecc49df4",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20628": {
    "title": "Partial Multi-Label Learning via Large Margin Nearest Neighbour Embeddings",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ee87d9777a6932aba7d2aeb1083d9c6b0702a054",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20629": {
    "title": "LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a0aadb27b66c932dc1234cfd5ad6392bd87aa67d",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20630": {
    "title": "Semi-supervised Conditional Density Estimation with Wasserstein Laplacian Regularisation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "866c7447cc732025d6a2f70b61af9b5726635b58",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20631": {
    "title": "GoTube: Scalable Statistical Verification of Continuous-Depth Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "569107f1fffb2c1341c76618e3362a71ec347a96",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20632": {
    "title": "Balanced Self-Paced Learning for AUC Maximization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "797bbacf61d96c6af7918dcd99b7504606e821ca",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20633": {
    "title": "Theoretical Guarantees of Fictitious Discount Algorithms for Episodic Reinforcement Learning and Global Convergence of Policy Gradient Methods",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "24fda3cbf8b776aea69ef4f2d5ef11f92d3d4011",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20634": {
    "title": "Adaptive Orthogonal Projection for Batch and Online Continual Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6abfc9fc3716efc58843ac02aafb39e403426df2",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20635": {
    "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fd4970aa6c356d90cb1612896e4ed0870cc9f808",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20636": {
    "title": "Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b21a4856a58ee3b13c16a91a03ff4f1c81c78df2",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20637": {
    "title": "Improved Gradient-Based Adversarial Attacks for Quantized Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "631e3969b292f77035ca8d95a0b74347a336b627",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20638": {
    "title": "TIGGER: Scalable Generative Modelling for Temporal Interaction Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "03492c9764cadf512726d922de7d337ed9ef54fb",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20639": {
    "title": "A Generalized Bootstrap Target for Value-Learning, Efficiently Combining Value and Feature Predictions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "44b0ed8536919985c16179e39608996929dc9f75",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20640": {
    "title": "Oscillatory Fourier Neural Network: A Compact and Efficient Architecture for Sequential Processing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8408c4052f8f76126df5d66d68eb607c17508285",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20641": {
    "title": "End-to-End Probabilistic Label-Specific Feature Learning for Multi-Label Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dee1d892ed58cce60b07ce8c13760a7cce6808fc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20642": {
    "title": "Cross-Domain Few-Shot Graph Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a0ba53e5f7871840d754ed595202c8af548a6f00",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20643": {
    "title": "SpreadGNN: Decentralized Multi-Task Federated Learning for Graph Neural Networks on Molecular Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "adcdc3a73ca193b77b0a238c695dfb63e2e38d88",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20644": {
    "title": "Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b786f64e16475b6cb49cecea26e8d576e11666a5",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20645": {
    "title": "Wasserstein Unsupervised Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "65e36b8fc38819944528d232368d8669feb3b01a",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20646": {
    "title": "Multi-Mode Tensor Space Clustering Based on Low-Tensor-Rank Representation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c67e09cf55168a8c89ac9853d388e86b41fc700d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20647": {
    "title": "Toward Physically Realizable Quantum Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c22d5ca4ba0abbdb9784fb96c408a8cfa96ecc93",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20648": {
    "title": "Reinforcement Learning of Causal Variables Using Mediation Analysis",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "65bc0c923411934f35676db4df992c8c6c7675aa",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20649": {
    "title": "Anytime Guarantees under Heavy-Tailed Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1c099956d8db099e4bb514005caed9f3cd00f714",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20650": {
    "title": "Adversarial Examples Can Be Effective Data Augmentation for Unsupervised Machine Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "566cac7a6c6b0f38c21bd839ccdd53135b797b7f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20651": {
    "title": "Towards Automating Model Explanations with Certified Robustness Guarantees",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6b29cb5f89926cfaa001a9239dec82bd865cb350",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20652": {
    "title": "Multi-View Clustering on Topological Manifold",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6aaedd4edbb1548887d37023bea6b6302769f7d4",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20653": {
    "title": "Achieving Counterfactual Fairness for Causal Bandit",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f27dcb447f26b42224bc0b19162b03b8b5c9350c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20654": {
    "title": "Uncertainty-Aware Learning against Label Noise on Imbalanced Datasets",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6f963257c57065273e5a7b1723e1f1eb97d18b2c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20655": {
    "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4b3e0f88c23209cee6f910ad01fb0aafa4da514d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20656": {
    "title": "Causal Discovery in Hawkes Processes by Minimum Description Length",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "10f9f341e03e6e4759de9b3f69cbc0cf305d017a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20657": {
    "title": "Group-Aware Threshold Adaptation for Fair Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f74d4fa99ccedfea7a2662fe6944d99f34533912",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20658": {
    "title": "Towards Discriminant Analysis Classifiers Using Online Active Learning via Myoelectric Interfaces",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e45e004e50be78b00a48b51e0583b87f321ccd69",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20659": {
    "title": "Label Hallucination for Few-Shot Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8a1deed670e2361dbc74ee90dedcd5efb3b9264e",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20660": {
    "title": "Learning Expected Emphatic Traces for Deep RL",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c23a0a2bcd953ceb40ee6d1270ffabb5051525b3",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20661": {
    "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2e12b6fbd312042e1d38b19d7581cc249c59037f",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20662": {
    "title": "Fast Graph Neural Tangent Kernel via Kronecker Sketching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "830ab871618dfebdef7d033382ee5c6bb9f62dbd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20663": {
    "title": "Creativity of AI: Automatic Symbolic Option Discovery for Facilitating Deep Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "44c19ed8c8fd70d72155802257453f002af75932",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20664": {
    "title": "Adaptive Kernel Graph Neural Network",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "48075b2824eba15752a2a8afd87d4aee22b349a5",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20665": {
    "title": "Fully Spiking Variational Autoencoder",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "55601656841bf1456105d9b2ebbbf7c49a0a92e4",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20666": {
    "title": "Classifying Emails into Human vs Machine Category",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dc791f152c6ab2068c7e08b0cb4b845bccd7c5d9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20667": {
    "title": "Self-Supervised Enhancement of Latent Discovery in GANs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5d220f0fcfdb2c8f74996dfc0adeb27fdce453d5",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20668": {
    "title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "318a2253df36ca67ea50ab630439ce6d78cb41ad",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20669": {
    "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "79047a44789668db3c4f3bda759f245b0f74a532",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20670": {
    "title": "iDECODe: In-Distribution Equivariance for Conformal Out-of-Distribution Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "34d35e460b39edb19581ef345c4b32ce45aa9eae",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20671": {
    "title": "Partial Wasserstein Covering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8f4a59bfc9476265f0965416b545e6f57b1b4cf1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20672": {
    "title": "Optimal Tensor Transport",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a780e3829eb8048e89aa121ac01b125d70fff1a5",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20673": {
    "title": "Dist2Cycle: A Simplicial Neural Network for Homology Localization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "651986c4fd780db2cb866d2f9286254eab90adbb",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20674": {
    "title": "Same State, Different Task: Continual Reinforcement Learning without Interference",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "94cefa04e0f834272d85cc425e0adfb27fd17e08",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20675": {
    "title": "Spatial Frequency Bias in Convolutional Generative Adversarial Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c280679420af34ba8d99b42a951a7c10180fd39",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20676": {
    "title": "The Effect of Manifold Entanglement and Intrinsic Dimensionality on Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bec922f6e2b99a0a24e853ef4f2ca8770ad49e08",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20677": {
    "title": "A Computable Definition of the Spectral Bias",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c5179be7dd7e26daab9fb7514b53a9c1f15ca91f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20678": {
    "title": "A Nested Bi-level Optimization Framework for Robust Few Shot Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e7eefd2efd63a632269a9c3ab31fbad90ed0a9a0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20679": {
    "title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ac668af2e58c008636f653a186268afb1167b038",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20680": {
    "title": "Towards a Rigorous Evaluation of Time-Series Anomaly Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6e34bc57d8e78322c04733d02bec123febbaf453",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20681": {
    "title": "Introducing Symmetries to Black Box Meta Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "64582e814de758d4ceb7d7f74e0e75fd260ce16b",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20682": {
    "title": "Directed Graph Auto-Encoders",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2669fceff62034643805a7a1fd77d844e17046f7",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20683": {
    "title": "HNO: High-Order Numerical Architecture for ODE-Inspired Deep Unfolding Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "41485354058ed154fd8f75fe5351a4ea6ba0d259",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20684": {
    "title": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features across MDPs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "186cabbc394c560a458f94bf4317a5517edd8041",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20685": {
    "title": "Fast Approximations for Job Shop Scheduling: A Lagrangian Dual Deep Learning Method",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f78be2d5ae302a832574cedc9f60b0d0149d39c8",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20686": {
    "title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d85a9a6511428beaf0afe5168da542e55c46b47b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20687": {
    "title": "Gradient Based Activations for Accurate Bias-Free Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "de3501c3084c98e4deee7ca24ff08b03803f9c19",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20688": {
    "title": "TrustAL: Trustworthy Active Learning Using Knowledge Distillation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cfa1d074fc74d673b6511289fbac2fdebcc09e17",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20689": {
    "title": "Tight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "413fd9cdf87ead7eb4161190e4f1ade507ebbdde",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20690": {
    "title": "Learning Adversarial Markov Decision Processes with Delayed Feedback",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1c07876aea9dbab7d79dc4de2dba9b0cfe243eb2",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20691": {
    "title": "Learning Not to Learn: Nature versus Nurture In Silico",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9293fed98a28ba7b908d914f62c424b882c30590",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20692": {
    "title": "Optimization for Classical Machine Learning Problems on the GPU",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "183049fedc865cfe5023c257108740c140d1a01a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20693": {
    "title": "Interpretable Clustering via Multi-Polytope Machines",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d04185bce9802b9a2076dbc956d1d774f675ba93",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20694": {
    "title": "Episodic Policy Gradient Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4e6037db5ec29bb2bdbbaf3bbf7a8c595336bc83",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20695": {
    "title": "Stability Verification in Stochastic Control Systems via Neural Network Supermartingales",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5c5723b6ecc06b9a64b6e3f4f70f01d54fba5cac",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20696": {
    "title": "Learning Losses for Strategic Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "323fe67fda478a3844c90de23cb159d6a3239275",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20697": {
    "title": "Differentially Private Normalizing Flows for Synthetic Tabular Data Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "56a76ae99639c1238059f6d9a19e12880740d01b",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20698": {
    "title": "Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1811020a7c64358543900a5a50673cde7768403a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20699": {
    "title": "Fast and Efficient MMD-Based Fair PCA via Optimization over Stiefel Manifold",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a36a00db217fd98f1bd943aa2f2d6303adbc456",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20700": {
    "title": "Augmentation-Free Self-Supervised Learning on Graphs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2812cc5b514a5c87e29de2e051b3ebadc021897d",
    "citation_count": 22
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20701": {
    "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "bca31821e2b68b81f5e5fd1d108d5e20a68346e0",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20702": {
    "title": "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ccbd2b988ab12549a305f3009da414698a1e4616",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20703": {
    "title": "Invariant Information Bottleneck for Domain Generalization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "014c5aa0a61489f09c224d5f43b6a0eeb47a1687",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20704": {
    "title": "Chunk Dynamic Updating for Group Lasso with ODEs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1efa2d794fdfa7d9d95248a437adacd9b92267fc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20705": {
    "title": "Policy Learning for Robust Markov Decision Process with a Mismatched Generative Model",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "da33251754e6d7078b651450c4b9c89ea0fd313e",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20706": {
    "title": "A Fully Single Loop Algorithm for Bilevel Optimization without Hessian Inverse",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "dd6b4b7a1f9d220baaa3256f2fb0976b09ad7cfe",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20707": {
    "title": "A Hybrid Causal Structure Learning Algorithm for Mixed-Type Data",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b4207ae5e40904de4cbd41e64e94ccf63b02411b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20708": {
    "title": "Sharp Analysis of Random Fourier Features in Classification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8d4715523aaa8bb8573cdd82ab539cdc60fc9346",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20709": {
    "title": "Zeroth-Order Optimization for Composite Problems with Functional Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "428fc338ec8bb9a9c8278bccad05c68c88e67012",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20710": {
    "title": "Robust Graph-Based Multi-View Clustering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2a5c1f511e4a65381eeb2983e1a290a48aecc686",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20711": {
    "title": "Conditional Local Convolution for Spatio-Temporal Meteorological Forecasting",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f5074a176e126f9443ef3fa478599cf5aa2b74f8",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20712": {
    "title": "On the Use of Unrealistic Predictions in Hundreds of Papers Evaluating Graph Representations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "62b6e1e833ef2644fe3815d08bd5586e12466da8",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20713": {
    "title": "Deep Unsupervised Hashing with Latent Semantic Components",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ee6d8f39c3fb1cd1566130a7d1ebd0836dd5b072",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20714": {
    "title": "SCRIB: Set-Classifier with Class-Specific Risk Bounds for Blackbox Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "132aaf728e5fa8b196fdbfb1293ec8523ced11f8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20715": {
    "title": "RareGAN: Generating Samples for Rare Classes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "163ec520bfc7ef5f068de19c4e061940ad201317",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20716": {
    "title": "Conjugated Discrete Distributions for Distributional Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1f28c649eaecf3d72aade439793f1360dda4e243",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20717": {
    "title": "Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "26bd86d04fc5d741d36868ff4aceeb65db1be441",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20718": {
    "title": "Learning Parameterized Task Structure for Generalization to Unseen Entities",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1c708330bd81838ae38073220a96cc17f62f0bd7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20719": {
    "title": "Stationary Diffusion State Neural Estimation for Multiview Clustering",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3c36f6925f785513653efeb567e0132000b54a9e",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20720": {
    "title": "Deep Amortized Relational Model with Group-Wise Hierarchical Generative Process",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c1a499b99dda7b8b144c0b2cc5251e84ed17f39f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20721": {
    "title": "Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ae25c0c7d3e2de1c90346079e53b9e3e836c7de4",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20722": {
    "title": "Transformer with Memory Replay",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f512507e98f20528a21af522b7f63f2d16ba6841",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20723": {
    "title": "Efficient One-Pass Multi-View Subspace Clustering with Consensus Anchors",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "fd072a57e96ed20fe94d55d0f52b8491df48bf06",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20724": {
    "title": "Trusted Multi-View Deep Learning with Opinion Aggregation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7d60027df0fa6dd56cab08e79bf9a2cf4c81471a",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20725": {
    "title": "Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "313ca8cb33cdfbeb40f64d7a1aa56322713ca7e3",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20726": {
    "title": "Deep Graph Clustering via Dual Correlation Reduction",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8d50200b20e1a49dee65e5ed288a490d14609982",
    "citation_count": 25
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20727": {
    "title": "Optimistic Initialization for Exploration in Continuous Control",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "60306257fbac5e4fb8b94552b67094e09157f049",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20728": {
    "title": "Fast and Data Efficient Reinforcement Learning from Pixels via Non-parametric Value Approximation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ca3b3c89c247729505343262400d47912b52086d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20729": {
    "title": "Frozen Pretrained Transformers as Universal Computation Engines",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3aa2c10dd6c72267ea8a622c8f30b3c9240d5fab",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20730": {
    "title": "Adapt to Environment Sudden Changes by Learning a Context Sensitive Policy",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e2bc67e9a7bf30ba447b9cff051df11027815cfc",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20731": {
    "title": "Beyond Shared Subspace: A View-Specific Fusion for Multi-View Multi-Label Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3f27b3603695533eaf4cfe631c69c75654022f17",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20732": {
    "title": "Efficient Continuous Control with Double Actors and Regularized Critics",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f162abb9f391401df15d7cde9bf52f8da624dfb6",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20733": {
    "title": "Recursive Reasoning Graph for Multi-Agent Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "36054fe91a4a6a5b53c103b626cc86dd2d344d3e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20734": {
    "title": "Sharp Restricted Isometry Property Bounds for Low-Rank Matrix Recovery Problems with Corrupted Measurements",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a1a9c05f2990232746170f38121fb593c409a94c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20735": {
    "title": "Cross-Lingual Adversarial Domain Adaptation for Novice Programming",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6a0da1cacdd7eaaf6afef2bcc72cd98c1e6dfd0a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20736": {
    "title": "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "036b18662c47dc70a7b69b4184f9dabc81fdb3df",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20737": {
    "title": "Exploring Safer Behaviors for Deep Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b75ff79617904f769c5f61cea3693e021757c600",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20738": {
    "title": "fGOT: Graph Distances Based on Filters and Optimal Transport",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ce2d4a62b2273438d67bf25d6b9cbf3467e9a4d0",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20739": {
    "title": "When AI Difficulty Is Easy: The Explanatory Power of Predicting IRT Difficulty",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "9e08f1c08b93a8009fdb6b6f5d29931dc3e09c73",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20740": {
    "title": "Being Friends Instead of Adversaries: Deep Networks Learn from Data Simplified by Other Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "618a6e4668de2379f1dc2356ef4f292f9f1cadbf",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20741": {
    "title": "An Experimental Design Approach for Regret Minimization in Logistic Bandits",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8cce269769f18b96558b53df66a68ca41f493a9a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20742": {
    "title": "Coordinate Descent on the Orthogonal Group for Recurrent Neural Network Training",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a4d544e09b780b4ec90b6df3deb2f914eaeb0707",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20743": {
    "title": "Curiosity-Driven Exploration via Latent Bayesian Surprise",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8c2e0392ae194afa0173c26ec92fa6460707f241",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20744": {
    "title": "What Can We Learn Even from the Weakest? Learning Sketches for Programmatic Strategies",
    "abstract": "",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20745": {
    "title": "Top-Down Deep Clustering with Multi-Generator GANs",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5c9ed8382fccf6fa5f84069e89963fab2dfa299e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20746": {
    "title": "Temporal Knowledge Graph Completion Using Box Embeddings",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "918381ce2c8f6afa3a25ac88bf2008733c5a3255",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20747": {
    "title": "An Evaluative Measure of Clustering Methods Incorporating Hyperparameter Sensitivity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2b150b02c2c2daa6103dc42d2fadeb48ac006266",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20748": {
    "title": "Simple Unsupervised Graph Representation Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3cb7c04c80c1c75cd8df17c5b7ab6a5399563a65",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20749": {
    "title": "The Role of Adaptive Optimizers for Honest Private Hyperparameter Selection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f90f4b9f3a2159bf94181786a1b11bb4ae01f3f2",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20750": {
    "title": "Learning Bayesian Networks in the Presence of Structural Side Information",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "293c71bd87af1114868883660040c2d88d785a86",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20751": {
    "title": "Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "895d4a7449ac55ec259ee4663bf14be4f3c397ca",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20752": {
    "title": "Provable Guarantees for Understanding Out-of-Distribution Detection",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "075ddf01cb0a722143ae04f49ef7dabcd64f38da",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20753": {
    "title": "Constraint Sampling Reinforcement Learning: Incorporating Expertise for Faster Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "14aa7aa1249075940b8a460d01e8ae821f40ab32",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20754": {
    "title": "Unsupervised Reinforcement Learning in Multiple Environments",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8e63e0ccd18fdfc1f7a73fc9e0d4bc6c36316016",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20755": {
    "title": "Is Your Data Relevant?: Dynamic Selection of Relevant Data for Federated Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f974a71bc1ca0e211553a3a87b3dc193098ab6cc",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20756": {
    "title": "A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cfc0683a3db08140c56f848cda6adc74ee9e3fb8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20757": {
    "title": "Out of Distribution Data Detection Using Dropout Bayesian Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6a4a2cbae832beb692565c8a113052e96e33ac63",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20758": {
    "title": "Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "766c7366ca956634cb15d8238d15cadebe8217c1",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20759": {
    "title": "Improving Evidential Deep Learning via Multi-Task Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "34148f8d11b814739e524bd834d5fe1f4b911d4e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20760": {
    "title": "Clustering Approach to Solve Hierarchical Classification Problem Complexity",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6ba338ee726a81a05ade8d0e6f35a34332db79b9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20761": {
    "title": "Random Tensor Theory for Tensor Decomposition",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "af4dec5ad76a9e654d94873adca40217600d033d",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20762": {
    "title": "Bag Graph: Multiple Instance Learning Using Bayesian Graph Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "228faba0882d6ed2231023cf6e2053e460dea8ed",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20763": {
    "title": "Competing Mutual Information Constraints with Stochastic Competition-Based Activations for Learning Diversified Representations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "5a0e4c5bdf03bfaaaf30f736913561b133340789",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20764": {
    "title": "Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8dabfc8f58d4c1067a557fd658cf5c9a82260c11",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20765": {
    "title": "Deformable Graph Convolutional Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b8a1a2a81383aabfc719548af0b693e06e48b4a0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20766": {
    "title": "Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ca23d4f34bd48ef1608da43741cbdf78a2f66c06",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20767": {
    "title": "Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d3b5cd144e3da102015b79e9f9eb36ff7a30cd92",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20768": {
    "title": "CC-CERT: A Probabilistic Approach to Certify General Robustness of Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "513de0282764f44c13cc0818d3e12a524b208aad",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20769": {
    "title": "Covered Information Disentanglement: Model Transparency via Unbiased Permutation Importance",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cdcdc9fa9dfab2de6f3a13fee5c22039d0d0ff63",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20770": {
    "title": "On the Impossibility of Non-trivial Accuracy in Presence of Fairness Constraints",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cb6335d1114a8d31d628f3a9930a9e3e2579462a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20771": {
    "title": "Spiking Neural Networks with Improved Inherent Recurrence Dynamics for Sequential Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7793027bc575f7ca34643b362e8f64f84e7edfad",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20772": {
    "title": "How Private Is Your RL Policy? An Inverse RL Based Analysis Framework",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "d8b3aa1ca3896cc513b40e9c0a1354e4b7fdd85e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20773": {
    "title": "Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "4a07d64c47ef75bef8530874f47f9abde8523605",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20774": {
    "title": "DeepType 2: Superhuman Entity Linking, All You Need Is Type Interactions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f638bf8bbf63ae433223676510d50d9513fdd387",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20775": {
    "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "770144a0d32d5e66982d0cd7f6ec305de2af3b84",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20776": {
    "title": "I-SEA: Importance Sampling and Expected Alignment-Based Deep Distance Metric Learning for Time Series Analysis and Embedding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "47214fcc72a8a1c7624c73d8d0ccf5352baab99f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20777": {
    "title": "Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2539ebad34ca8ad2f5e72d2673f2e061302731e1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20778": {
    "title": "DISTREAL: Distributed Resource-Aware Learning in Heterogeneous Systems",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "1a09171c7a0463183b4846449d178157e68a6828",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20779": {
    "title": "Sublinear Time Approximation of Text Similarity Matrices",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "8e602cf8e647556bfd2afb565b178795fbf41c2d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20780": {
    "title": "Decision-Dependent Risk Minimization in Geometrically Decaying Dynamic Environments",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "740afdd1619d797145b056877865f941891e6a65",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20781": {
    "title": "On Causally Disentangled Representations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b4845ef70a5fd4d786038d0360ff32fe2cab8593",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20782": {
    "title": "Conditional Loss and Deep Euler Scheme for Time Series Generation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7da5ff9edf6d3432e0c5ec720cb36adb12b1df02",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20783": {
    "title": "Offline Reinforcement Learning as Anti-exploration",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "399806e861a2ef960a81b37b593c2176a728c399",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20784": {
    "title": "Interpretable Neural Subgraph Matching for Graph Retrieval",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0115465a98cf50988c8246ef3a900ccffcf9b801",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20785": {
    "title": "FedSoft: Soft Clustered Federated Learning with Proximal Local Updating",
    "abstract": "",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20786": {
    "title": "Knowledge Distillation via Constrained Variational Inference",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ba47986454ed5c0791246c260dc8a2aac4d8e723",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20787": {
    "title": "Hypergraph Modeling via Spectral Embedding Connection: Hypergraph Cut, Weighted Kernel k-Means, and Heat Kernel",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "46315c539361a20f750a329469f830df99e77ded",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20788": {
    "title": "Reverse Differentiation via Predictive Coding",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "0cd8f4c1183cc93078aad4f57004e0e8a3b5bb2f",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20789": {
    "title": "VACA: Designing Variational Graph Autoencoders for Causal Queries",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e9519d5a27dc9813b6d5d18b4fcab7dab98c7709",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20790": {
    "title": "Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3293be154f600be7c89389c0eba2ae22c3dd9dfa",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20791": {
    "title": "Scaling Up Influence Functions",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ef2a773c3c7848a6cc16b18164be5f8876a310af",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20792": {
    "title": "Chaining Value Functions for Off-Policy Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "cdee8ded4b2b8be42f26368dd1fab7ab1eec7bd4",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20793": {
    "title": "Graph Filtration Kernels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3e53ee173a14d3b6b0f2d19b8a4f436d687ed07c",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20794": {
    "title": "Neural Networks Classify through the Class-Wise Means of Their Representations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "876b613ff4d61dff07ce1277e3e29a9453452cfc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20795": {
    "title": "Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "90b0a90fbb88b8031b036b721ed516f8389301a4",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20796": {
    "title": "Max-Margin Contrastive Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "2c132fc9c5b99a2ace079856ef191a2a7fd5abec",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20797": {
    "title": "Learning to Transfer with von Neumann Conditional Divergence",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "3d761fa130a935be1326e915bc9af59c9fea5c9e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20798": {
    "title": "Online Apprenticeship Learning",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "006b38e300c2e449676f6f3aeb0722a45098f95d",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20799": {
    "title": "HoD-Net: High-Order Differentiable Deep Neural Networks and Applications",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "e620d02d658a7c9250cfe09cbcf28f27cc9a6362",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20800": {
    "title": "Conditional Generative Model Based Predicate-Aware Query Approximation",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "26c33ff8ea868b571858fbf37b6cafb10e2cee7d",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20801": {
    "title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and the Explanations",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "c3e7f95e7d1e37003bbef8dca1aee6dcf05a5c16",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20802": {
    "title": "Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "55411c7fb15f5f4f30bd39481a9d7ef28ae17bb8",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20803": {
    "title": "TRF: Learning Kernels with Tuned Random Features",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "b14f235352ed8213625a92fdb06e0bba66102e26",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20804": {
    "title": "Estimation of Local Average Treatment Effect by Data Combination",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7c0bb2463945431c789191c8bec449d015037f57",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20805": {
    "title": "Constraint-Driven Explanations for Black-Box ML Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "113d9fe223d6414709ec6ab80fb068c166f6db24",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20806": {
    "title": "Noise-Robust Learning from Multiple Unsupervised Sources of Inferred Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7f53c70a3b5aa22e2843db16c00bb4a3cbb0ba41",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20807": {
    "title": "QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "ea44927c57909af47d5680ff2af256d8f8597ce8",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20808": {
    "title": "EqGNN: Equalized Node Opportunity in Graphs",
    "abstract": "Graph neural networks (GNNs), has been widely used for supervised learning tasks in graphs reaching state-of-the-art results. However, little work was dedicated to creating unbiased GNNs, i.e., where the classification is uncorrelated with sensitive attributes, such as race or gender. Some ignore the sensitive attributes or optimize for the criteria of statistical parity for fairness. However, it has been shown that neither approaches ensure fairness, but rather cripple the utility of the prediction task.  In this work, we present a GNN framework that allows optimizing representations for the notion of Equalized Odds fairness criteria. The architecture is composed of three components: (1) a GNN classifier predicting the utility class, (2) a sampler learning the distribution of the sensitive attributes of the nodes given their labels. It generates samples fed into a (3) discriminator that discriminates between true and sampled sensitive attributes using a novel ``permutation loss'' function. Using these components, we train a model to neglect information regarding the sensitive attribute only with respect to its label. To the best of our knowledge, we are the first to optimize GNNs for the equalized odds criteria. We evaluate our classifier over several graph datasets and sensitive attributes and show our algorithm reaches state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "48661ae9be42f8be888f7e4577b57c9132c4eb70",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20809": {
    "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems",
    "abstract": "Due to the surge of cloud-assisted AI services, the problem of designing resilient prediction serving systems that can effectively cope with stragglers and minimize response delays has attracted much interest. The common approach for tackling this problem is replication which assigns the same prediction task to multiple workers. This approach, however, is inefficient and incurs significant resource overheads. Hence, a learning-based approach known as parity model (ParM) has been recently proposed which learns models that can generate ``parities’’ for a group of predictions to reconstruct the predictions of the slow/failed workers. While this learning-based approach is more resource-efficient than replication, it is tailored to the specific model hosted by the cloud and is particularly suitable for a small number of queries (typically less than four) and tolerating very few stragglers (mostly one). Moreover, ParM does not handle Byzantine adversarial workers. We propose a different approach, named Approximate Coded Inference (ApproxIFER), that does not require training any parity models, hence it is agnostic to the model hosted by the cloud and can be readily applied to different data domains and model architectures. Compared with earlier works, ApproxIFER can handle a general number of stragglers and scales significantly better with the number of queries. Furthermore, ApproxIFER is robust against Byzantine workers. Our extensive experiments on a large number of datasets and model architectures show significant degraded mode accuracy improvement by up to 58% over ParM",
    "volume": "main",
    "checked": true,
    "id": "09ce40faf1b73337d83e84f8b449ba7192c6fd82",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20810": {
    "title": "Feature Importance Explanations for Temporal Black-Box Models",
    "abstract": "Models in the supervised learning framework may capture rich and complex representations over the features that are hard for humans to interpret. Existing methods to explain such models are often specific to architectures and data where the features do not have a time-varying component. In this work, we propose TIME, a method to explain models that are inherently temporal in nature. Our approach (i) uses a model-agnostic permutation-based approach to analyze global feature importance, (ii) identifies the importance of salient features with respect to their temporal ordering as well as localized windows of influence, and (iii) uses hypothesis testing to provide statistical rigor",
    "volume": "main",
    "checked": true,
    "id": "5c7f4157152bad59ecbd3f20d1fffa5e1f75238b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20811": {
    "title": "Reward-Weighted Regression Converges to a Global Optimum",
    "abstract": "Reward-Weighted Regression (RWR) belongs to a family of widely known iterative Reinforcement Learning algorithms based on the Expectation-Maximization framework. In this family, learning at each iteration consists of sampling a batch of trajectories using the current policy and fitting a new policy to maximize a return-weighted log-likelihood of actions. Although RWR is known to yield monotonic improvement of the policy under certain circumstances, whether and under which conditions RWR converges to the optimal policy have remained open questions. In this paper, we provide for the first time a proof that RWR converges to a global optimum when no function approximation is used, in a general compact setting. Furthermore, for the simpler case with finite state and action spaces we prove R-linear convergence of the state-value function to the optimum",
    "volume": "main",
    "checked": true,
    "id": "03487212e74fbb61d14ac4217d85ed47764b1c85",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20812": {
    "title": "Gradient-Based Novelty Detection Boosted by Self-Supervised Binary Classification",
    "abstract": "Novelty detection aims to automatically identify out-of-distribution (OOD) data, without any prior knowledge of them. It is a critical step in data monitoring, behavior analysis and other applications, helping enable continual learning in the field. Conventional methods of OOD detection perform multi-variate analysis on an ensemble of data or features, and usually resort to the supervision with OOD data to improve the accuracy. In reality, such supervision is impractical as one cannot anticipate the anomalous data. In this paper, we propose a novel, self-supervised approach that does not rely on any pre-defined OOD data: (1) The new method evaluates the Mahalanobis distance of the gradients between the in-distribution and OOD data. (2) It is assisted by a self-supervised binary classifier to guide the label selection to generate the gradients, and maximize the Mahalanobis distance. In the evaluation with multiple datasets, such as CIFAR-10, CIFAR-100, SVHN and TinyImageNet, the proposed approach consistently outperforms state-of-the-art supervised and unsupervised methods in the area under the receiver operating characteristic (AUROC) and area under the precision-recall curve (AUPR) metrics. We further demonstrate that this detector is able to accurately learn one OOD class in continual learning",
    "volume": "main",
    "checked": true,
    "id": "6951a59e5e3d608bfb0ef9055bf6807111e028e1",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20813": {
    "title": "Deterministic and Discriminative Imitation (D2-Imitation): Revisiting Adversarial Imitation for Sample Efficiency",
    "abstract": "Sample efficiency is crucial for imitation learning methods to be applicable in real-world applications. Many studies improve sample efficiency by extending adversarial imitation to be off-policy regardless of the fact that these off-policy extensions could either change the original objective or involve complicated optimization. We revisit the foundation of adversarial imitation and propose an off-policy sample efficient approach that requires no adversarial training or min-max optimization. Our formulation capitalizes on two key insights: (1) the similarity between the Bellman equation and the stationary state-action distribution equation allows us to derive a novel temporal difference (TD) learning approach; and (2) the use of a deterministic policy simplifies the TD learning. Combined, these insights yield a practical algorithm, Deterministic and Discriminative Imitation (D2-Imitation), which oper- ates by first partitioning samples into two replay buffers and then learning a deterministic policy via off-policy reinforcement learning. Our empirical results show that D2-Imitation is effective in achieving good sample efficiency, outperforming several off-policy extension approaches of adversarial imitation on many control tasks",
    "volume": "main",
    "checked": true,
    "id": "01c9b70b02e73aee86a9c67e051c576f4c4135e9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20814": {
    "title": "Exploiting Mixed Unlabeled Data for Detecting Samples of Seen and Unseen Out-of-Distribution Classes",
    "abstract": "Out-of-Distribution (OOD) detection is essential in real-world applications, which has attracted increasing attention in recent years. However, most existing OOD detection methods require many labeled In-Distribution (ID) data, causing a heavy labeling cost. In this paper, we focus on the more realistic scenario, where limited labeled data and abundant unlabeled data are available, and these unlabeled data are mixed with ID and OOD samples. We propose the Adaptive In-Out-aware Learning (AIOL) method, in which we employ the appropriate temperature to adaptively select potential ID and OOD samples from the mixed unlabeled data and consider the entropy over them for OOD detection. Moreover, since the test data in realistic applications may contain OOD samples whose classes are not in the mixed unlabeled data (we call them unseen OOD classes), data augmentation techniques are brought into the method to further improve the performance. The experiments are conducted on various benchmark datasets, which demonstrate the superiority of our method",
    "volume": "main",
    "checked": true,
    "id": "fe2e2752f6baaf9810ca31f3c2f294a1a565dabb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20815": {
    "title": "Generalized Equivariance and Preferential Labeling for GNN Node Classification",
    "abstract": "Existing graph neural networks (GNNs) largely rely on node embeddings, which represent a node as a vector by its identity, type, or content. However, graphs with unattributed nodes widely exist in real-world applications (e.g., anonymized social networks). Previous GNNs either assign random labels to nodes (which introduces artefacts to the GNN) or assign one embedding to all nodes (which fails to explicitly distinguish one node from another). Further, when these GNNs are applied to unattributed node classification problems, they have an undesired equivariance property, which are fundamentally unable to address the data with multiple possible outputs. In this paper, we analyze the limitation of existing approaches to node classification problems. Inspired by our analysis, we propose a generalized equivariance property and a Preferential Labeling technique that satisfies the desired property asymptotically. Experimental results show that we achieve high performance in several unattributed node classification tasks",
    "volume": "main",
    "checked": true,
    "id": "66b6b1664d15490ea4b2141487817a3d39a25880",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20816": {
    "title": "Explainable and Local Correction of Classification Models Using Decision Trees",
    "abstract": "In practical machine learning, models are frequently updated, or corrected, to adapt to new datasets. In this study, we pose two challenges to model correction. First, the effects of corrections to the end-users need to be described explicitly, similar to standard software where the corrections are described as release notes. Second, the amount of corrections need to be small so that the corrected models perform similarly to the old models. In this study, we propose the first model correction method for classification models that resolves these two challenges. Our idea is to use an additional decision tree to correct the output of the old models. Thanks to the explainability of decision trees, the corrections are describable to the end-users, which resolves the first challenge. We resolve the second challenge by incorporating the amount of corrections when training the additional decision tree so that the effects of corrections to be small. Experiments on real data confirm the effectiveness of the proposed method compared to existing correction methods",
    "volume": "main",
    "checked": true,
    "id": "f60716d784941a97f6032deb86c5b581b9e95415",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20817": {
    "title": "Consistency Regularization for Adversarial Robustness",
    "abstract": "Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary `consistency' regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial",
    "volume": "main",
    "checked": true,
    "id": "33ca8d34d226e47e0830b6eb73c06e0b85ae7ab7",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20818": {
    "title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability",
    "abstract": "In the Bayesian reinforcement learning (RL) setting, a prior distribution over the unknown problem parameters -- the rewards and transitions -- is assumed, and a policy that optimizes the (posterior) expected return is sought. A common approximation, which has been recently popularized as meta-RL, is to train the agent on a sample of N problem instances from the prior, with the hope that for large enough N, good generalization behavior to an unseen test instance will be obtained.    In this work, we study generalization in Bayesian RL under the probably approximately correct (PAC) framework, using the method of algorithmic stability. Our main contribution is showing that by adding regularization, the optimal policy becomes uniformly stable in an appropriate sense. Most stability results in the literature build on strong convexity of the regularized loss -- an approach that is not suitable for RL as Markov decision processes (MDPs) are not convex. Instead, building on recent results of fast convergence rates for mirror descent in regularized MDPs, we show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability. This result, which may be of independent interest, allows us to study the effect of regularization on generalization in the Bayesian RL setting",
    "volume": "main",
    "checked": true,
    "id": "6dba50cad6956da4384538599f610ffdf9e3e98a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20819": {
    "title": "FedProto: Federated Prototype Learning across Heterogeneous Clients",
    "abstract": "Heterogeneity across clients in federated learning (FL) usually hinders the optimization convergence and generalization performance when the aggregation of clients' knowledge occurs in the gradient space. For example, clients may differ in terms of data distribution, network latency, input/output space, and/or model architecture, which can easily lead to the misalignment of their local gradients. To improve the tolerance to heterogeneity, we propose a novel federated prototype learning (FedProto) framework in which the clients and server communicate the abstract class prototypes instead of the gradients. FedProto aggregates the local prototypes collected from different clients, and then sends the global prototypes back to all clients to regularize the training of local models. The training on each client aims to minimize the classification error on the local data while keeping the resulting local prototypes sufficiently close to the corresponding global ones. Moreover, we provide a theoretical analysis to the convergence rate of FedProto under non-convex objectives. In experiments, we propose a benchmark setting tailored for heterogeneous FL, with FedProto outperforming several recent FL approaches on multiple datasets",
    "volume": "main",
    "checked": true,
    "id": "6d3b39d2b08ad848ee94b26c6bf9aa9e424466a7",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20820": {
    "title": "What about Inputting Policy in Value Function: Policy Representation and Policy-Extended Value Function Approximator",
    "abstract": "We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., value generalization among policies. We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or state-action pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks. For a representative instance of algorithm implementation, Proximal Policy Optimization (PPO) re-implemented under the paradigm of GPI with PeVFA achieves about 40% performance improvement on its vanilla counterpart in most environments",
    "volume": "main",
    "checked": true,
    "id": "a4e1cf41401674c40174fa8d6073d2b3e9be78d6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20821": {
    "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization",
    "abstract": "Running machine learning algorithms on large and rapidly growing volumes of data is often computationally expensive, one common trick to reduce the size of a data set, and thus reduce the computational cost of machine learning algorithms, is probability sampling. It creates a sampled data set by including each data point from the original data set with a known probability. Although the benefit of running machine learning algorithms on the reduced data set is obvious, one major concern is that the performance of the solution obtained from samples might be much worse than that of the optimal solution when using the full data set. In this paper, we examine the performance loss caused by probability sampling in the context of adaptive submodular maximization. We consider a simple probability sampling method which selects each data point with probability at least r. If we set r=1, our problem reduces to finding a solution based on the original full data set. We define sampling gap as the largest ratio between the optimal solution obtained from the full data set and the optimal solution obtained from the samples, over independence systems. Our main contribution is to show that if the sampling probability of each data point is at least r and the utility function is policywise submodular, then the sampling gap is both upper bounded and lower bounded by 1/r. We show that the property of policywise submodular can be found in a wide range of real-world applications, including pool-based active learning and adaptive viral marketing",
    "volume": "main",
    "checked": true,
    "id": "56b1bb665da6605f1bf99dd451d264cce08d1687",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20822": {
    "title": "With False Friends Like These, Who Can Notice Mistakes?",
    "abstract": "Adversarial examples crafted by an explicit adversary have attracted significant attention in machine learning. However, the security risk posed by a potential false friend has been largely overlooked. In this paper, we unveil the threat of hypocritical examples---inputs that are originally misclassified yet perturbed by a false friend to force correct predictions. While such perturbed examples seem harmless, we point out for the first time that they could be maliciously used to conceal the mistakes of a substandard (i.e., not as good as required) model during an evaluation. Once a deployer trusts the hypocritical performance and applies the \"well-performed\" model in real-world applications, unexpected failures may happen even in benign environments. More seriously, this security risk seems to be pervasive: we find that many types of substandard models are vulnerable to hypocritical examples across multiple datasets. Furthermore, we provide the first attempt to characterize the threat with a metric called hypocritical risk and try to circumvent it via several countermeasures. Results demonstrate the effectiveness of the countermeasures, while the risk remains non-negligible even after adaptive robust training",
    "volume": "main",
    "checked": true,
    "id": "4dea1701757513fdf7a52e64cb85750dbf0ef662",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20823": {
    "title": "Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling",
    "abstract": "In recent works, utilizing a deep network trained on meta-training set serves as a strong baseline in few-shot learning. In this paper, we move forward to refine novel-class features by finetuning a trained deep network. Finetuning is designed to focus on reducing biases in novel-class feature distributions, which we define as two aspects: class-agnostic and class-specific biases. Class-agnostic bias is defined as the distribution shifting introduced by domain difference, which we propose Distribution Calibration Module(DCM) to reduce. DCM owes good property of eliminating domain difference and fast feature adaptation during optimization. Class-specific bias is defined as the biased estimation using a few samples in novel classes, which we propose Selected Sampling(SS) to reduce. Without inferring the actual class distribution, SS is designed by running sampling using proposal distributions around support-set samples. By powering finetuning with DCM and SS, we achieve state-of-the-art results on Meta-Dataset with consistent performance boosts over ten datasets from different domains. We believe our simple yet effective method demonstrates its possibility to be applied on practical few-shot applications",
    "volume": "main",
    "checked": true,
    "id": "151ddaa95cccb7f965be0490c097d9c41bf073a5",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20824": {
    "title": "SMINet: State-Aware Multi-Aspect Interests Representation Network for Cold-Start Users Recommendation",
    "abstract": "Online travel platforms (OTPs), e.g., bookings.com and Ctrip.com, deliver travel experiences to online users by providing travel-related products. Although much progress has been made, the state-of-the-arts for cold-start problems are largely sub-optimal for user representation, since they do not take into account the unique characteristics exhibited from user travel behaviors. In this work, we propose a State-aware Multi-aspect Interests representation Network (SMINet) for cold-start users recommendation at OTPs, which consists of a multi-aspect interests extractor, a co-attention layer, and a state-aware gating layer. The key component of the model is the multi-aspect interests extractor, which is able to extract representations for the user's multi-aspect interests. Furthermore, to learn the interactions between the user behaviors in the current session and the above multi-aspect interests, we carefully design a co-attention layer which allows the cross attentions between the two modules. Additionally, we propose a travel state-aware gating layer to attentively select the multi-aspect interests. The final user representation is obtained by fusing the three components. Comprehensive experiments conducted both offline and online demonstrate the superior performance of the proposed model at user representation, especially for cold-start users, compared with state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "138ee1ae9cca79cb2f79a8b2b344927dbfcec95e",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20825": {
    "title": "SplitFed: When Federated Learning Meets Split Learning",
    "abstract": "Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings",
    "volume": "main",
    "checked": true,
    "id": "0294bd2e6638c9a3619d4baaa63202a3c511dccc",
    "citation_count": 85
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20826": {
    "title": "Listwise Learning to Rank Based on Approximate Rank Indicators",
    "abstract": "We study here a way to approximate information retrieval metrics through a softmax-based approximation of the rank indicator function. Indeed, this latter function is a key component in the design of information retrieval metrics, as well as in the design of the ranking and sorting functions. Obtaining a good approximation for it thus opens the door to differentiable approximations of many evaluation measures that can in turn be used in neural end-to-end approaches. We first prove theoretically that the approximations proposed are of good quality, prior to validate them experimentally on both learning to rank and text-based information retrieval tasks",
    "volume": "main",
    "checked": true,
    "id": "a890ecfa2eb8b5db0e47fd5e5562b35f92f5220e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20827": {
    "title": "PrivateMail: Supervised Manifold Learning of Deep Features with Privacy for Image Retrieval",
    "abstract": "Differential Privacy offers strong guarantees such as immutable privacy under any post-processing. In this work, we propose a differentially private mechanism called PrivateMail  for performing supervised manifold learning. We then apply it to the use case of private image retrieval to obtain nearest matches to a client’s target image from a server’s database.  PrivateMail releases the target image as part of a differentially private manifold embedding. We give bounds on the global sensitivity of the manifold learning map in order to obfuscate and release embeddings with differential privacy inducing noise. We show that PrivateMail obtains a substantially better performance in terms of the privacy-utility trade off in comparison to several baselines on various datasets. We share code for applying PrivateMail at http://tiny.cc/PrivateMail",
    "volume": "main",
    "checked": true,
    "id": "645c4e288450712aa929d120c8c3f06d8f04bdcc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20828": {
    "title": "Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models",
    "abstract": "Explainable machine learning (ML) has gained traction in recent years due to the increasing adoption of ML-based systems in many sectors. Algorithmic Recourses (ARs) provide \"what if\" feedback of the form \"if an input datapoint were x' instead of x, then an ML-based system's output would be y' instead of y.\" Recourses are attractive due to their actionable feedback, amenability to existing legal frameworks, and fidelity to the underlying ML model. Yet, current recourse approaches are single shot that is, they assume x can change to x' in a single time period. We propose a novel stochastic-control-based approach that generates sequential recourses, that is, recourses that allow x to move stochastically and sequentially across intermediate states to a final state x'. Our approach is model agnostic and black box. Furthermore, the calculation of recourses is amortized such that once trained, it applies to multiple datapoints without the need for re-optimization. In addition to these primary characteristics, our approach admits optional desiderata such as adherence to the data manifold, respect for causal relations, and sparsity identified by past research as desirable properties of recourses. We evaluate our approach using three real-world datasets and show successful generation of sequential recourses that respect other recourse desiderata",
    "volume": "main",
    "checked": true,
    "id": "71723f07a8e10db8ea72ad38c11c8922026338d9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20829": {
    "title": "Robust Optimal Classification Trees against Adversarial Examples",
    "abstract": "Decision trees are a popular choice of explainable model, but just like neural networks, they suffer from adversarial examples. Existing algorithms for fitting decision trees robust against adversarial examples are greedy heuristics and lack approximation guarantees. In this paper we propose ROCT, a collection of methods to train decision trees that are optimally robust against user-specified attack models. We show that the min-max optimization problem that arises in adversarial learning can be solved using a single minimization formulation for decision trees with 0-1 loss. We propose such formulations in Mixed-Integer Linear Programming and Maximum Satisfiability, which widely available solvers can optimize. We also present a method that determines the upper bound on adversarial accuracy for any model using bipartite matching. Our experimental results demonstrate that the existing heuristics achieve close to optimal scores while ROCT achieves state-of-the-art scores",
    "volume": "main",
    "checked": true,
    "id": "017d986cde19758d06fa760d2830129c06363261",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20830": {
    "title": "Spline-PINN: Approaching PDEs without Data Using Fast, Physics-Informed Hermite-Spline CNNs",
    "abstract": "Partial Differential Equations (PDEs) are notoriously difficult to solve. In general, closed form solutions are not available and numerical approximation schemes are computationally expensive. In this paper, we propose to approach the solution of PDEs based on a novel technique that combines the advantages of two recently emerging machine learning based approaches.   First, physics-informed neural networks (PINNs) learn continuous solutions of PDEs and can be trained with little to no ground truth data. However, PINNs do not generalize well to unseen domains. Second, convolutional neural networks provide fast inference and generalize but either require large amounts of training data or a physics-constrained loss based on finite differences that can lead to inaccuracies and discretization artifacts.   We leverage the advantages of both of these approaches by using Hermite spline kernels in order to continuously interpolate a grid-based state representation that can be handled by a CNN. This allows for training without any precomputed training data using a physics-informed loss function only and provides fast, continuous solutions that generalize to unseen domains.   We demonstrate the potential of our method at the examples of the incompressible Navier-Stokes equation and the damped wave equation. Our models are able to learn several intriguing phenomena such as Karman vortex streets, the Magnus effect, Doppler effect, interference patterns and wave reflections. Our quantitative assessment and an interactive real-time demo show that we are narrowing the gap in accuracy of unsupervised ML based methods to industrial solvers for computational fluid dynamics (CFD) while being orders of magnitude faster",
    "volume": "main",
    "checked": true,
    "id": "43c31e66c67259045ea94426159794e0c2b4f5a0",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20831": {
    "title": "Context Uncertainty in Contextual Bandits with Applications to Recommender Systems",
    "abstract": "Recurrent neural networks have proven effective in modeling sequential user feedbacks for recommender systems. However, they usually focus solely on item relevance and fail to effectively explore diverse items for users, therefore harming the system performance in the long run. To address this problem, we propose a new type of recurrent neural networks, dubbed recurrent exploration networks (REN), to jointly perform representation learning and effective exploration in the latent space. REN tries to balance relevance and exploration while taking into account the uncertainty in the representations. Our theoretical analysis shows that REN can preserve the rate-optimal sublinear regret even when there exists uncertainty in the learned representations. Our empirical study demonstrates that REN can achieve satisfactory long-term rewards on both synthetic and real-world recommendation datasets, outperforming state-of-the-art models",
    "volume": "main",
    "checked": true,
    "id": "87b59a30d9d50680b20d1f1ff6b42e5981e38d73",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20832": {
    "title": "Demystifying Why Local Aggregation Helps: Convergence Analysis of Hierarchical SGD",
    "abstract": "Hierarchical SGD (H-SGD) has emerged as a new distributed SGD algorithm for multi-level communication networks. In H-SGD, before each global aggregation, workers send their updated local models to local servers for aggregations. Despite recent research efforts, the effect of local aggregation on global convergence still lacks theoretical understanding. In this work, we first introduce a new notion of \"upward\" and \"downward\" divergences. We then use it to conduct a novel analysis to obtain a worst-case convergence upper bound for two-level H-SGD with non-IID data, non-convex objective function, and stochastic gradient. By extending this result to the case with random grouping, we observe that this convergence upper bound of H-SGD is between the upper bounds of two single-level local SGD settings, with the number of local iterations equal to the local and global update periods in H-SGD, respectively. We refer to this as the \"sandwich behavior\". Furthermore, we extend our analytical approach based on \"upward\" and \"downward\" divergences to study the convergence for the general case of H-SGD with more than two levels, where the \"sandwich behavior\" still holds. Our theoretical results provide key insights of why local aggregation can be beneficial in improving the convergence of H-SGD",
    "volume": "main",
    "checked": true,
    "id": "908c8a84a7a678a5513564fb5c3c99820acc6c2e",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20833": {
    "title": "Learngene: From Open-World to Your Learning Task",
    "abstract": "Although deep learning has made significant progress on fixed large-scale datasets, it typically encounters challenges regarding improperly detecting unknown/unseen classes in the open-world scenario, over-parametrized, and overfitting small samples. Since biological systems can overcome the above difficulties very well, individuals inherit an innate gene from collective creatures that have evolved over hundreds of millions of years and then learn new skills through few examples. Inspired by this, we propose a practical collective-individual paradigm where an evolution (expandable) network is trained on sequential tasks and then recognize unknown classes in real-world. Moreover, the learngene, i.e., the gene for learning initialization rules of the target model, is proposed to inherit the meta-knowledge from the collective model and reconstruct a lightweight individual model on the target task. Particularly, a novel criterion is proposed to discover learngene in the collective model, according to the gradient information. Finally, the individual model is trained only with few samples on the target learning tasks. We demonstrate the effectiveness of our approach in an extensive empirical study and theoretical analysis",
    "volume": "main",
    "checked": true,
    "id": "863f9919063a5385ab1aaa7b3171a9f8fbfd2e40",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20834": {
    "title": "Boosting Active Learning via Improving Test Performance",
    "abstract": "Central to active learning (AL) is what data should be selected for annotation. Existing works attempt to select highly uncertain or informative data for annotation. Nevertheless, it remains unclear how selected data impacts the test performance of the task model used in AL. In this work, we explore such an impact by theoretically proving that selecting unlabeled data of higher gradient norm leads to a lower upper-bound of test loss, resulting in a better test performance. However, due to the lack of label information, directly computing gradient norm for unlabeled data is infeasible. To address this challenge, we propose two schemes, namely expected-gradnorm and entropy-gradnorm. The former computes the gradient norm by constructing an expected empirical loss while the latter constructs an unsupervised loss with entropy. Furthermore, we integrate the two schemes in a universal AL framework. We evaluate our method on classical image classification and semantic segmentation tasks. To demonstrate its competency in domain applications and its robustness to noise, we also validate our method on a cellular imaging analysis task, namely cryo-Electron Tomography subtomogram classification. Results demonstrate that our method achieves superior performance against the state of the art. We refer readers to https://arxiv.org/pdf/2112.05683.pdf for the full version of this paper which includes the appendix and source code link",
    "volume": "main",
    "checked": true,
    "id": "4ec7a1cac8191d7f67c7159af398af1220fa98fe",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20835": {
    "title": "Efficient Algorithms for General Isotone Optimization",
    "abstract": "Monotonicity is often a fundamental assumption involved in the modeling of a number of real-world applications. From an optimization perspective, monotonicity is formulated as partial order constraints among the optimization variables, commonly known as isotone optimization. In this paper, we develop an efficient, provable convergent algorithm for solving isotone optimization problems. The proposed algorithm is general in the sense that it can handle any arbitrary isotonic constraints and a wide range of objective functions. We evaluate our algorithm and state-of-the-art methods with experiments involving both synthetic and real-world data. The experimental results demonstrate that our algorithm is more efficient by one to four orders of magnitude than the state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "4d45e7386336bea48a707c78d1cab6f291208af3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20836": {
    "title": "Efficient Causal Structure Learning from Multiple Interventional Datasets with Unknown Targets",
    "abstract": "We consider the problem of reducing the false discovery rate in multiple high-dimensional interventional datasets under unknown targets. Traditional algorithms merged directly multiple causal graphs learned, which ignores the contradictions of different datasets, leading to lots of inconsistent directions of edges. For reducing the contradictory information, we propose a new algorithm, which first learns an interventional Markov equivalence class (I-MEC) before merging multiple graphs. It utilizes the full power of the constraints available in interventional data and combines ideas from local learning, intervention, and search-and-score techniques in a principled and effective way in different intervention experiments. Specifically, local learning on multiple datasets is used to build a causal skeleton. Perfect intervention destroys some possible triangles, leading to the identification of more possible V-structures. And then a theoretically correct I-MEC is learned. Search and scoring techniques based on the learned I-MEC further identify the remaining unoriented edges. Both theoretical analysis and experiments on benchmark Bayesian networks with the number of variables from 20 to 724 validate that the effectiveness of our algorithm in reducing the false discovery rate in high-dimensional interventional data",
    "volume": "main",
    "checked": true,
    "id": "fad3ae7839cdc6720dec2af683c036a9461ced63",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20837": {
    "title": "Continual Learning through Retrieval and Imagination",
    "abstract": "Continual learning is an intellectual ability of artificial agents to learn new streaming labels from sequential data. The main impediment to continual learning is catastrophic forgetting, a severe performance degradation on previously learned tasks. Although simply replaying all previous data or continuously adding the model parameters could alleviate the issue, it is impractical in real-world applications due to the limited available resources. Inspired by the mechanism of the human brain to deepen its past impression, we propose a novel framework, Deep Retrieval and Imagination (DRI), which consists of two components: 1) an embedding network that constructs a unified embedding space without adding model parameters on the arrival of new tasks; and 2) a generative model to produce additional (imaginary) data based on the limited memory. By retrieving the past experiences and corresponding imaginary data, DRI distills knowledge and rebalances the embedding space to further mitigate forgetting. Theoretical analysis demonstrates that DRI can reduce the loss approximation error and improve the robustness through retrieval and imagination, bringing better generalizability to the network. Extensive experiments show that DRI performs significantly better than the existing state-of-the-art continual learning methods and effectively alleviates catastrophic forgetting",
    "volume": "main",
    "checked": true,
    "id": "99be6600f10c72c2227f16d6d0eeeb6432a1e61a",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20838": {
    "title": "Max-Min Grouped Bandits",
    "abstract": "In this paper, we introduce a multi-armed bandit problem termed max-min grouped bandits, in which the arms are arranged in possibly-overlapping groups, and the goal is to find a group whose worst arm has the highest mean reward. This problem is of interest in applications such as recommendation systems, and is also closely related to widely-studied robust optimization problems. We present two algorithms based successive elimination and robust optimization, and derive upper bounds on the number of samples to guarantee finding a max-min optimal or near-optimal group, as well as an algorithm-independent lower bound. We discuss the degree of tightness of our bounds in various cases of interest, and the difficulties in deriving uniformly tight bounds",
    "volume": "main",
    "checked": true,
    "id": "3cba5b6b50855282db6ffaa4ca2002b5f0b21a50",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20839": {
    "title": "Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic",
    "abstract": "Model-based reinforcement learning algorithms, which aim to learn a model of the environment to make decisions, are more sample efficient than their model-free counterparts. The sample efficiency of model-based approaches relies on whether the model can well approximate the environment. However, learning an accurate model is challenging, especially in complex and noisy environments. To tackle this problem, we propose the conservative model-based actor-critic (CMBAC), a novel approach that achieves high sample efficiency without the strong reliance on accurate learned models. Specifically, CMBAC learns multiple estimates of the Q-value function from a set of inaccurate models and uses the average of the bottom-k estimates---a conservative estimate---to optimize the policy. An appealing feature of CMBAC is that the conservative estimates effectively encourage the agent to avoid unreliable \"promising actions\"---whose values are high in only a small fraction of the models. Experiments demonstrate that CMBAC significantly outperforms state-of-the-art approaches in terms of sample efficiency on several challenging control tasks, and the proposed method is more robust than previous methods in noisy environments",
    "volume": "main",
    "checked": true,
    "id": "b2d7ffedc614de190df02ef8613743b5a76c578d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20840": {
    "title": "Controlling Underestimation Bias in Reinforcement Learning via Quasi-median Operation",
    "abstract": "How to get a good value estimation is one of the key problems in reinforcement learning (RL). Current off-policy methods, such as Maxmin Q-learning, TD3 and TADD, suffer from the underestimation problem when solving the overestimation problem. In this paper, we propose the Quasi-Median Operation, a novel way to mitigate the underestimation bias by selecting the quasi-median from multiple state-action values. Based on the quasi-median operation, we propose Quasi-Median Q-learning (QMQ) for the discrete action tasks and Quasi-Median Delayed Deep Deterministic Policy Gradient (QMD3) for the continuous action tasks. Theoretically, the underestimation bias of our method is improved while the estimation variance is significantly reduced compared to Maxmin Q-learning, TD3 and TADD. We conduct extensive experiments on the discrete and continuous action tasks, and results show that our method outperforms the state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "0437ec501424bf1637993128d0fbd682fb73e33b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20841": {
    "title": "Symbolic Brittleness in Sequence Models: On Systematic Generalization in Symbolic Mathematics",
    "abstract": "Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is defined by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the training set. We develop a methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier. Despite promising in-distribution performance of sequence-to-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and out-of-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically finds large collections of failures in a controllable manner. Our investigation highlights the difficulty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization",
    "volume": "main",
    "checked": true,
    "id": "aead4418733b998792deb9cbf198a834449e00d2",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20842": {
    "title": "Prune and Tune Ensembles: Low-Cost Ensemble Learning with Sparse Independent Subnetworks",
    "abstract": "Ensemble Learning is an effective method for improving generalization in machine learning. However, as state-of-the-art neural networks grow larger, the computational cost associated with training several independent networks becomes expensive. We introduce a fast, low-cost method for creating diverse ensembles of neural networks without needing to train multiple models from scratch. We do this by first training a single parent network. We then create child networks by cloning the parent and dramatically pruning the parameters of each child to create an ensemble of members with unique and diverse topologies. We then briefly train each child network for a small number of epochs, which now converge significantly faster when compared to training from scratch. We explore various ways to maximize diversity in the child networks, including the use of anti-random pruning and one-cycle tuning. This diversity enables \"Prune and Tune\" ensembles to achieve results that are competitive with traditional ensembles at a fraction of the training cost. We benchmark our approach against state of the art low-cost ensemble methods and display marked improvement in both accuracy and uncertainty estimation on CIFAR-10 and CIFAR-100",
    "volume": "main",
    "checked": true,
    "id": "502142121f85e78b1c38ece533ddfef016a9b26e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20843": {
    "title": "PluGeN: Multi-Label Conditional Generation from Pre-trained Models",
    "abstract": "Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to perform edits of only one attribute while leaving the others unchanged. To overcome these limitations we propose PluGeN (Plugin Generative Network), a simple yet effective generative technique that can be used as a plugin to pre-trained generative models. The idea behind our approach is to transform the entangled latent representation using a flow-based module into a multi-dimensional space where the values of each attribute are modeled as an independent one-dimensional distribution. In consequence, PluGeN can generate new samples with desired attributes as well as manipulate labeled attributes of existing examples. Due to the disentangling of the latent representation, we are even able to generate samples with rare or unseen combinations of attributes in the dataset, such as a young person with gray hair, men with make-up, or women with beards. We combined PluGeN with GAN and VAE models and applied it to conditional generation and manipulation of images and chemical molecule modeling. Experiments demonstrate that PluGeN preserves the quality of backbone models while adding the ability to control the values of labeled attributes. Implementation is available at https://github.com/gmum/plugen",
    "volume": "main",
    "checked": true,
    "id": "c6da67b676d4355b7ce6ac26e5d08bd49a397d5f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20844": {
    "title": "Structure Learning-Based Task Decomposition for Reinforcement Learning in Non-stationary Environments",
    "abstract": "Reinforcement learning (RL) agents empowered by deep neural networks have been considered a feasible solution to automate control functions in a cyber-physical system.    In this work, we consider an RL-based agent and address the issue of learning via continual interaction with a time-varying dynamic system modeled as a non-stationary Markov decision process (MDP).    We view such a non-stationary MDP as a time series of conventional MDPs that can be parameterized by hidden variables. To infer the hidden parameters, we present a task decomposition method that exploits CycleGAN-based structure learning.    This method enables the separation of time-variant tasks from a non-stationary MDP, establishing the task decomposition embedding specific to time-varying information.    To mitigate the adverse effect due to inherent noises of task embedding, we also leverage continual learning on sequential tasks by adapting the orthogonal gradient descent scheme with a sliding window.   Through various experiments, we demonstrate that our approach renders the RL agent adaptable to time-varying dynamic environment conditions, outperforming other methods including state-of-the-art non-stationary MDP algorithms",
    "volume": "main",
    "checked": true,
    "id": "476357ee1bdd49355aa2bef957c8d92c7c6f7994",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20845": {
    "title": "An Efficient Combinatorial Optimization Model Using Learning-to-Rank Distillation",
    "abstract": "Recently, deep reinforcement learning (RL) has proven its feasibility in solving combinatorial optimization problems (COPs). The learning-to-rank techniques have been studied in the field of information retrieval. While several COPs can be formulated as the prioritization of input items, as is common in the information retrieval, it has not been fully explored how the learning-to-rank techniques can be incorporated into deep RL for COPs.    In this paper, we present the learning-to-rank distillation-based COP framework, where a high-performance ranking policy obtained by RL for a COP can be distilled into a non-iterative, simple model, thereby achieving a low-latency COP solver. Specifically, we employ the approximated ranking distillation to render a score-based ranking model learnable via gradient descent. Furthermore, we use the efficient sequence sampling to improve the inference performance with a limited delay.    With the framework, we demonstrate that a distilled model not only achieves comparable performance to its respective, high-performance RL, but also provides several times faster inferences. We evaluate the framework with several COPs such as priority-based task scheduling and multidimensional knapsack, demonstrating the benefits of the framework in terms of inference latency and performance",
    "volume": "main",
    "checked": true,
    "id": "c7072f3a6217402fe767c541d8b2d54c7c992d5c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20846": {
    "title": "PUMA: Performance Unchanged Model Augmentation for Training Data Removal",
    "abstract": "Preserving the performance of a trained model while removing unique characteristics of marked training data points is challenging. Recent research usually suggests retraining a model from scratch with remaining training data or refining the model by reverting the model optimization on the marked data points. Unfortunately, aside from their computational inefficiency, those approaches inevitably hurt the resulting model's generalization ability since they remove not only unique characteristics but also discard shared (and possibly contributive) information. To address the performance degradation problem, this paper presents a novel approach called Performance Unchanged Model Augmentation (PUMA). The proposed PUMA framework explicitly models the influence of each training data point on the model's generalization ability with respect to various performance criteria. It then complements the negative impact of removing marked data by reweighting the remaining data optimally. To demonstrate the effectiveness of the PUMA framework, we compared it with multiple state-of-the-art data removal techniques in the experiments, where we show the PUMA can effectively and efficiently remove the unique characteristics of marked training data without retraining the model that can 1) fool a membership attack, and 2) resist performance degradation. In addition, as PUMA estimates the data importance during its operation, we show it could serve to debug mislabelled data points more efficiently than existing approaches",
    "volume": "main",
    "checked": true,
    "id": "2e2c019217dd187ab43f074f65529b4a3fd3e823",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20847": {
    "title": "Generalizing Reinforcement Learning through Fusing Self-Supervised Learning into Intrinsic Motivation",
    "abstract": "Despite the great potential of reinforcement learning (RL) in solving complex decision-making problems, generalization remains one of its key challenges, leading to difficulty in deploying learned RL policies to new environments. In this paper, we propose to improve the generalization of RL algorithms through fusing Self-supervised learning into Intrinsic Motivation (SIM). Specifically, SIM boosts representation learning through driving the cross-correlation matrix between the embeddings of augmented and non-augmented samples close to the identity matrix. This aims to increase the similarity between the embedding vectors of a sample and its augmented version while minimizing the redundancy between the components of these vectors. Meanwhile, the redundancy reduction based self-supervised loss is converted to an intrinsic reward to further improve generalization in RL via an auxiliary objective. As a general paradigm, SIM can be implemented on top of any RL algorithm. Extensive evaluations have been performed on a diversity of tasks. Experimental results demonstrate that SIM consistently outperforms the state-of-the-art methods and exhibits superior generalization capability and sample efficiency",
    "volume": "main",
    "checked": true,
    "id": "775c6ff63220e9aad98d5aee32788f1d45358075",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20848": {
    "title": "AdaLoss: A Computationally-Efficient and Provably Convergent Adaptive Gradient Method",
    "abstract": "We propose a computationally-friendly adaptive learning rate schedule, ``AdaLoss\", which directly uses the information of the loss function to adjust the stepsize in gradient descent methods. We prove that this schedule enjoys linear convergence in linear regression. Moreover, we extend the to the non-convex regime, in the context of two-layer over-parameterized neural networks. If the width is sufficiently large (polynomially), then AdaLoss converges robustly to the global minimum in polynomial time. We numerically verify the theoretical results and extend the scope of the numerical experiments by considering applications in LSTM models for text clarification and policy gradients for control problems",
    "volume": "main",
    "checked": true,
    "id": "d7bc06ae35a0cfd0c771eac0ce515a0c3809d7ac",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20849": {
    "title": "Towards Off-Policy Learning for Ranking Policies with Logged Feedback",
    "abstract": "Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods",
    "volume": "main",
    "checked": true,
    "id": "1c0e9ef2dc6bbb464352f3e41c6b5f3bc4854409",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20850": {
    "title": "Active Learning for Domain Adaptation: An Energy-Based Approach",
    "abstract": "Unsupervised domain adaptation has recently emerged as an effective paradigm for generalizing deep neural networks to new target domains. However, there is still enormous potential to be tapped to reach the fully supervised performance. In this paper, we present a novel active learning strategy to assist knowledge transfer in the target domain, dubbed active domain adaptation. We start from an observation that energy-based models exhibit free energy biases when training (source) and test (target) data come from different distributions. Inspired by this inherent mechanism, we empirically reveal that a simple yet efficient energy-based sampling strategy sheds light on selecting the most valuable target samples than existing approaches requiring particular architectures or computation of the distances. Our algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of target data that incorporate both domain characteristic and instance uncertainty into every selection round. Meanwhile, by aligning the free energy of target data compact around the source domain via a regularization term, domain gap can be implicitly diminished. Through extensive experiments, we show that EADA surpasses state-of-the-art methods on well-known challenging benchmarks with substantial improvements, making it a useful option in the open world. Code is available at https://github.com/BIT-DA/EADA",
    "volume": "main",
    "checked": true,
    "id": "0282c031d07bf12d807392601371af86a56cce27",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20851": {
    "title": "GearNet: Stepwise Dual Learning for Weakly Supervised Domain Adaptation",
    "abstract": "This paper studies a weakly supervised domain adaptation (WSDA) problem, where we only have access to the source domain with noisy labels, from which we need to transfer useful information to the unlabeled target domain. Although there have been a few studies on this problem, most of them only exploit unidirectional relationships from the source domain to the target domain. In this paper, we propose a universal paradigm called GearNet to exploit bilateral relationships between the two domains. Specifically, we take the two domains as different inputs to train two models alternately, and a symmetrical Kullback-Leibler loss is used for selectively matching the predictions of the two models in the same domain. This interactive learning schema enables implicit label noise canceling and exploit correlations between the source and target domains. Therefore, our GearNet has the great potential to boost the performance of a wide range of existing WSDA methods. Comprehensive experimental results show that the performance of existing methods can be significantly improved by equipping with our GearNet",
    "volume": "main",
    "checked": true,
    "id": "d0168d31eb0aa2be477206a85c81fe960ff976ac",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20852": {
    "title": "Reinforcement Learning Augmented Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits",
    "abstract": "We study a finite-horizon restless multi-armed bandit problem with multiple actions, dubbed as R(MA)^2B. The state of each arm evolves according to a controlled Markov decision process (MDP), and the reward of pulling an arm depends on both the current state and action of the corresponding MDP. Since finding the optimal policy is typically intractable, we propose a computationally appealing index policy entitled Occupancy-Measured-Reward Index Policy for the finite-horizon R(MA)^2B. Our index policy is well-defined without the requirement of indexability condition and is provably asymptotically optimal as the number of arms tends to infinity. We then adopt a learning perspective where the system parameters are unknown, and propose R(MA)^2B-UCB, a generative model based reinforcement learning augmented algorithm that can fully exploit the structure of Occupancy-Measured-Reward Index Policy. Compared to existing algorithms, R(MA)^2B-UCB performs close to offline optimum, and achieves a sub-linear regret and a low computational complexity all at once. Experimental results show that R(MA)^2B-UCB outperforms existing algorithms in both regret and running time",
    "volume": "main",
    "checked": true,
    "id": "7ac85dcb5ae3fe1b19488e189332c95191dd7a13",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20853": {
    "title": "Coordinating Momenta for Cross-Silo Federated Learning",
    "abstract": "Communication efficiency is crucial for federated learning (FL). Conducting local training steps in clients to reduce the communication frequency between clients and the server is a common method to address this issue. However, it leads to the client drift problem due to non-i.i.d. data distributions in different clients which severely deteriorates the performance. In this work, we propose a new method to improve the training performance in cross-silo FL via maintaining double momentum buffers. One momentum buffer tracks the server model updating direction, and the other tracks the local model updating direction. Moreover, we introduce a novel momentum fusion technique to coordinate the server and local momentum buffers. We also provide the first theoretical convergence analysis involving both the server and local standard momentum SGD. Extensive deep FL experimental results show a better training performance than FedAvg and existing standard momentum SGD variants",
    "volume": "main",
    "checked": true,
    "id": "6a3504e083e728255b51023e021b0afe056380d1",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20854": {
    "title": "Learning-Augmented Algorithms for Online Steiner Tree",
    "abstract": "This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the online Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm’s worst-case guarantee is far from desirable.This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the algorithms’ performance is parameterized by the number of incorrectly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new online algorithms have strong performance even with modestly correct predictions",
    "volume": "main",
    "checked": true,
    "id": "cfc11d07d200cfb9542288127c6f62c31c3e4fbc",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20855": {
    "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
    "abstract": "We study the problem of safe offline reinforcement learning (RL), the goal is to learn a policy that maximizes long-term reward while satisfying safety constraints given only offline data, without further interaction with the environment. This problem is more appealing for real world RL applications, in which data collection is costly or dangerous. Enforcing constraint satisfaction is non-trivial, especially in offline settings, as there is a potential large discrepancy between the policy distribution and the data distribution, causing errors in estimating the value of safety constraints. We show that naïve approaches that combine techniques from safe RL and offline RL can only learn sub-optimal solutions. We thus develop a simple yet effective algorithm, Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits the use of data generated by mixed behavior policies. We present a theoretical analysis and demonstrate empirically that our approach can learn robustly across a variety of benchmark control tasks, outperforming several baselines",
    "volume": "main",
    "checked": true,
    "id": "63572d61745c4c09709463d9c2df00bf9bf8e054",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20856": {
    "title": "Deep Incomplete Multi-View Clustering via Mining Cluster Complementarity",
    "abstract": "Incomplete multi-view clustering (IMVC) is an important unsupervised approach to group the multi-view data containing missing data in some views. Previous IMVC methods suffer from the following issues: (1) the inaccurate imputation or padding for missing data negatively affects the clustering performance, (2) the quality of features after fusion might be interfered by the low-quality views, especially the inaccurate imputed views. To avoid these issues, this work presents an imputation-free and fusion-free deep IMVC framework. First, the proposed method builds a deep embedding feature learning and clustering model for each view individually. Our method then nonlinearly maps the embedding features of complete data into a high-dimensional space to discover linear separability. Concretely, this paper provides an implementation of the high-dimensional mapping as well as shows the mechanism to mine the multi-view cluster complementarity. This complementary information is then transformed to the supervised information with high confidence, aiming to achieve the multi-view clustering consistency for the complete data and incomplete data. Furthermore, we design an EM-like optimization strategy to alternately promote feature learning and clustering. Extensive experiments on real-world multi-view datasets demonstrate that our method achieves superior clustering performance over state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "c37b6fdf9f4cbc27b91c23409501be7fe00f91ed",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20857": {
    "title": "Linearity-Aware Subspace Clustering",
    "abstract": "Obtaining a good similarity matrix is extremely important in subspace clustering. Current state-of-the-art methods learn the similarity matrix through self-expressive strategy. However, these methods directly adopt original samples as a set of basis to represent itself linearly. It is difficult to accurately describe the linear relation between samples in the real-world applications, and thus is hard to find an ideal similarity matrix. To better represent the linear relation of samples, we present a subspace clustering model, Linearity-Aware Subspace Clustering (LASC), which can consciously learn the similarity matrix by employing a linearity-aware metric. This is a new subspace clustering method that combines metric learning and subspace clustering into a joint learning framework. In our model, we first utilize the self-expressive strategy to obtain an initial subspace structure and discover a low-dimensional representation of the original data. Subsequently, we use the proposed metric to learn an intrinsic similarity matrix with linearity-aware on the obtained subspace. Based on such a learned similarity matrix, the inter-cluster distance becomes larger than the intra-cluster distances, and thus successfully obtaining a good subspace cluster result. In addition, to enrich the similarity matrix with more consistent knowledge, we adopt a collaborative learning strategy for self-expressive subspace learning and linearity-aware subspace learning. Moreover, we provide detailed mathematical analysis to show that the metric can properly characterize the linear correlation between samples",
    "volume": "main",
    "checked": true,
    "id": "bf5188a93dee4a21e9bb87c22a292d1c567f8b0e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20858": {
    "title": "Go Wider Instead of Deeper",
    "abstract": "More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference.    In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by 1.5% with 0.72 times trainable parameters. Using 0.46 times and 0.13 times parameters, our WideNet can still surpass ViT and ViT-MoE by 0.8% and 2.1%, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by 1.8% on average and surpass BERT using factorized embedding parameterization by 0.8% with fewer parameters",
    "volume": "main",
    "checked": true,
    "id": "ffcd58f453f207d48075627da011f62782334c8f",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20859": {
    "title": "Seizing Critical Learning Periods in Federated Learning",
    "abstract": "Federated learning (FL) is a popular technique to train machine learning (ML) models with decentralized data. Extensive works have studied the performance of the global model; however, it is still unclear how the training process affects the final test accuracy. Exacerbating this problem is the fact that FL executions differ significantly from traditional ML with heterogeneous data characteristics across clients, involving more hyperparameters. In this work, we show that the final test accuracy of FL is dramatically affected by the early phase of the training process, i.e., FL exhibits critical learning periods, in which small gradient errors can have irrecoverable impact on the final test accuracy. To further explain this phenomenon, we generalize the trace of the Fisher Information Matrix (FIM) to FL and define a new notation called FedFIM, a quantity reflecting the local curvature of each clients from the beginning of the training in FL. Our findings suggest that the initial learning phase plays a critical role in understanding the FL performance. This is in contrast to many existing works which generally do not connect the final accuracy of FL to the early phase training. Finally, seizing critical learning periods in FL is of independent interest and could be useful for other problems such as the choices of hyperparameters including but not limited to the number of client selected per round, batch size, so as to improve the performance of FL training and testing",
    "volume": "main",
    "checked": true,
    "id": "3a60c063650e76b29cee119c4979d2bfd4be5b9c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20860": {
    "title": "Learning to Identify Top Elo Ratings: A Dueling Bandits Approach",
    "abstract": "The Elo rating system is widely adopted to evaluate the skills of (chess) game and sports players. Recently it has been also integrated into machine learning algorithms in evaluating the performance of computerised AI agents. However, an accurate estimation of the Elo rating (for the top players) often requires many rounds of competitions, which can be expensive to carry out. In this paper, to minimize the number of comparisons and to improve the sample efficiency of the Elo evaluation (for top players), we propose an efficient online match scheduling algorithm. Specifically, we identify and match the top players through a dueling bandits framework and tailor the bandit algorithm to the gradient-based update of Elo. We show that it reduces the per-step memory and time complexity to constant, compared to the traditional likelihood maximization approaches requiring O(t) time. Our algorithm has a regret guarantee that is sublinear in the number of competition rounds and has been extended to the multidimensional Elo ratings for handling intransitive games. We empirically demonstrate that our method achieves superior convergence speed and time efficiency on a variety of gaming tasks",
    "volume": "main",
    "checked": true,
    "id": "e9dad1f76ba2428209b3305923ef9682be1214f3",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20861": {
    "title": "Q-Ball: Modeling Basketball Games Using Deep Reinforcement Learning",
    "abstract": "Basketball is one of the most popular types of sports in the world. Recent technological developments have made it possible to collect large amounts of data on the game, analyze it, and discover new insights. We propose a novel approach for modeling basketball games using deep reinforcement learning. By analyzing multiple aspects of both the players and the game, we are able to model the latent connections among players' movements, actions, and performance, into a single measure - the Q-Ball. Using Q-Ball, we are able to assign scores to the performance of both players and whole teams. Our approach has multiple practical applications, including evaluating and improving players' game decisions and producing tactical recommendations. We train and evaluate our approach on a large dataset of National Basketball Association games, and show that the Q-Ball is capable of accurately assessing the performance of players and teams. Furthermore, we show that Q-Ball is highly effective in recommending alternatives to players' actions",
    "volume": "main",
    "checked": true,
    "id": "f557ce7b93c4f414bc96d86c00d1e996cfddfbea",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20862": {
    "title": "Training a Resilient Q-network against Observational Interference",
    "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications.   In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an essential yet challenging task. In this paper, we consider a deep q-network (DQN) framework training with an auxiliary task of observational interferences such as artificial noises. Inspired by causal inference for observational interference, we propose a causal inference based DQN algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DQN environments with different types of interferences as auxiliary labels. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences",
    "volume": "main",
    "checked": true,
    "id": "a912cad9f9c7418240da2636c92c54df5d82006b",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20863": {
    "title": "Policy Optimization with Stochastic Mirror Descent",
    "abstract": "Improving sample efficiency has been a longstanding goal in reinforcement learning. This paper proposes VRMPO algorithm: a sample efficient policy gradient method with stochastic mirror descent. In VRMPO, a novel variance-reduced policy gradient estimator is presented to improve sample efficiency. We prove that the proposed VRMPO needs only O(ε−3) sample trajectories to achieve an ε-approximate first-order stationary point, which matches the best sample complexity for policy optimization. Extensive empirical results demonstrate that VRMP outperforms the state-of-the-art policy gradient methods in various settings",
    "volume": "main",
    "checked": true,
    "id": "c415206df64b8fb8e066a6fe680d94d6a0f735f7",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20864": {
    "title": "Graph Pointer Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have shown advantages in various graph-based applications. Most existing GNNs assume strong homophily of graph structure and apply permutation-invariant local aggregation of neighbors to learn a representation for each node. However, they fail to generalize to heterophilic graphs, where most neighboring nodes have different labels or features, and the relevant nodes are distant. Few recent studies attempt to address this problem by combining multiple hops of hidden representations of central nodes (i.e., multi-hop-based approaches) or sorting the neighboring nodes based on attention scores (i.e., ranking-based approaches). As a result, these approaches have some apparent limitations. On the one hand, multi-hop-based approaches do not explicitly distinguish relevant nodes from a large number of multi-hop neighborhoods, leading to a severe over-smoothing problem. On the other hand, ranking-based models do not joint-optimize node ranking with end tasks and result in sub-optimal solutions. In this work, we present Graph Pointer Neural Networks (GPNN) to tackle the challenges mentioned above. We leverage a pointer network to select the most relevant nodes from a large amount of multi-hop neighborhoods, which constructs an ordered sequence according to the relationship with the central node. 1D convolution is then applied to extract high-level features from the node sequence. The pointer-network-based ranker in GPNN is joint-optimized with other parts in an end-to-end manner. Extensive experiments are conducted on six public node classification datasets with heterophilic graphs. The results show that GPNN significantly improves the classification performance of state-of-the-art methods. In addition, analyses also reveal the privilege of the proposed GPNN in filtering out irrelevant neighbors and reducing over-smoothing",
    "volume": "main",
    "checked": true,
    "id": "abc881f2c2c12716c4ccc462c74d2d99ca14c601",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20865": {
    "title": "LOGICDEF: An Interpretable Defense Framework against Adversarial Examples via Inductive Scene Graph Reasoning",
    "abstract": "Deep vision models have provided new capability across a spectrum of applications in transportation, manufacturing, agriculture, commerce, and security. However, recent studies have demonstrated that these models are vulnerable to adversarial attack, exposing a risk-of-use in critical applications where untrusted parties have access to the data environment or even directly to the sensor inputs. Existing adversarial defense methods are either limited to specific types of attacks or are too complex to be applied to practical vision models. More importantly, these methods rely on techniques that are not interpretable to humans. In this work, we argue that an effective defense should produce an explanation as to why the system is attacked, and by using a representation that is easily readable by a human user, e.g. a logic formalism. To this end, we propose logic adversarial defense (LogicDef), a defense framework that utilizes the scene graph of the image to provide a contextual structure for detecting and explaining object classification. Our framework first mines inductive logic rules from the extracted scene graph, and then uses these rules to construct a defense model that alerts the user when the vision model violates the consistency rules. The defense model is interpretable and its robustness is further enhanced by incorporating existing relational commonsense knowledge from projects such as ConceptNet. In order to handle the hierarchical nature of such relational reasoning, we use a curriculum learning approach based on object taxonomy, yielding additional improvements to training and performance",
    "volume": "main",
    "checked": true,
    "id": "34a7c662375b0a254c65eff65ab63028441f1023",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20866": {
    "title": "Exploiting Invariance in Training Deep Neural Networks",
    "abstract": "Inspired by two basic mechanisms in animal visual systems, we introduce a feature transform technique that imposes invariance properties in the training of deep neural networks. The resulting algorithm requires less parameter tuning, trains well with an initial learning rate 1.0, and easily generalizes to different tasks. We enforce scale invariance with local statistics in the data to align similar samples at diverse scales. To accelerate convergence, we enforce a GL(n)-invariance property with global statistics extracted from a batch such that the gradient descent solution should remain invariant under basis change. Profiling analysis shows our proposed modifications takes 5% of the computations of the underlying convolution layer. Tested on convolutional networks and transformer networks, our proposed technique requires fewer iterations to train, surpasses all baselines by a large margin, seamlessly works on both small and large batch size training, and applies to different computer vision and language tasks",
    "volume": "main",
    "checked": true,
    "id": "35654f393a7d749c16482a05e5c1db5728e0362e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20867": {
    "title": "Lifelong Generative Modelling Using Dynamic Expansion Graph Model",
    "abstract": "Variational Autoencoders (VAEs) suffer from degenerated performance, when learning several successive tasks. This is caused by catastrophic forgetting. In order to address the knowledge loss, VAEs are using either Generative Replay (GR) mechanisms or Expanding Network Architectures (ENA). In this paper we study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by deriving an upper bound on the negative marginal log-likelihood. This theoretical analysis provides new insights into how VAEs forget the previously learnt knowledge during lifelong learning. The analysis indicates the best performance achieved when considering model mixtures, under the ENA framework, where there are no restrictions on the number of components. However, an ENA-based approach may require an excessive number of parameters. This motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM expands its architecture, according to the novelty associated with each new database, when compared to the information already learnt by the network from previous tasks. DEGM training optimizes knowledge structuring, characterizing the joint probabilistic representations corresponding to the past and more recently learned tasks. We demonstrate that DEGM guarantees optimal performance for each task while also minimizing the required number of parameters",
    "volume": "main",
    "checked": true,
    "id": "20233efc7bf13c004b458947fe9c20709c8b34aa",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20868": {
    "title": "Stage Conscious Attention Network (SCAN): A Demonstration-Conditioned Policy for Few-Shot Imitation",
    "abstract": "In few-shot imitation learning (FSIL), using behavioral cloning (BC) to solve unseen tasks with few expert demonstrations becomes a popular research direction. The following capabilities are essential in robotics applications: (1) Behaving in compound tasks that contain multiple stages. (2) Retrieving knowledge from few length-variant and misalignment demonstrations. (3) Learning from an expert different from the agent. No previous work can achieve these abilities at the same time. In this work, we conduct FSIL problem under the union of above settings and introduce a novel stage conscious attention network (SCAN) to retrieve knowledge from few demonstrations simultaneously. SCAN uses an attention module to identify each stage in length-variant demonstrations. Moreover, it is designed under demonstration-conditioned policy that learns the relationship between experts and agents. Experiment results show that SCAN can perform in complicated compound tasks without fine-tuning and provide the explainable visualization. Project page is at https://sites.google.com/view/scan-aaai2022",
    "volume": "main",
    "checked": true,
    "id": "2f9e6c49fd4fbab33250b057957cf06fc23b8670",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20869": {
    "title": "BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition",
    "abstract": "Model compression is very important for the efficient deployment of deep neural network (DNN) models on resource-constrained devices. Among various model compression approaches, high-order tensor decomposition is particularly attractive and useful because the decomposed model is very small and fully structured. For this category of approaches, tensor ranks are the most important hyper-parameters that directly determine the architecture and task performance of the compressed DNN models. However, as an NP-hard problem, selecting optimal tensor ranks under the desired budget is very challenging and the state-of-the-art studies suffer from unsatisfied compression performance and timing-consuming search procedures. To systematically address this fundamental problem, in this paper we propose BATUDE, a Budget-Aware TUcker DEcomposition-based compression approach that can efficiently calculate optimal tensor ranks via one-shot training. By integrating the rank selecting procedure to the DNN training process with a specified compression budget, the tensor ranks of the DNN models are learned from the data and thereby bringing very significant improvement on both compression ratio and classification accuracy for the compressed models. The experimental results on ImageNet dataset show that our method enjoys 0.33% top-5 higher accuracy with 2.52X less computational cost as compared to the uncompressed ResNet-18 model. For ResNet-50, the proposed approach enables 0.37% and 0.55% top-5 accuracy increase with 2.97X and 2.04X computational cost reduction, respectively, over the uncompressed model",
    "volume": "main",
    "checked": true,
    "id": "1779db2f79a70972c533193606dd75e894590fa1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20870": {
    "title": "Distributed Randomized Sketching Kernel Learning",
    "abstract": "We investigate the statistical and computational requirements for distributed kernel ridge regression with randomized sketching (DKRR-RS) and successfully achieve the optimal learning rates with only a fraction of computations. More precisely, the proposed DKRR-RS combines sparse randomized sketching, divide-and-conquer and KRR to scale up kernel methods and successfully derives the same learning rate as the exact KRR with greatly reducing computational costs in expectation, at the basic setting, which outperforms previous state of the art solutions. Then, for the sake of the gap between theory and experiments, we derive the optimal learning rate in probability for DKRR-RS to reflect its generalization performance. Finally, to further improve the learning performance, we construct an efficient communication strategy for DKRR-RS and demonstrate the power of communications via theoretical assessment. An extensive experiment validates the effectiveness of DKRR-RS and the communication strategy on real datasets",
    "volume": "main",
    "checked": true,
    "id": "f5c2e497fb47e746a8e449a84a20e8d3b4df880f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20871": {
    "title": "AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators",
    "abstract": "Contrastive learning has been widely applied to graph representation learning, where the view generators play a vital role in generating effective contrastive samples. Most of the existing contrastive learning methods employ pre-defined view generation methods, e.g., node drop or edge perturbation, which usually cannot adapt to input data or preserve the original semantic structures well. To address this issue, we propose a novel framework named Automated Graph Contrastive Learning (AutoGCL) in this paper. Specifically, AutoGCL employs a set of learnable graph view generators orchestrated by an auto augmentation strategy, where every graph view generator learns a probability distribution of graphs conditioned by the input. While the graph view generators in AutoGCL preserve the most representative structures of the original graph in generation of every contrastive sample, the auto augmentation learns policies to introduce adequate augmentation variances in the whole contrastive learning procedure. Furthermore, AutoGCL adopts a joint training strategy to train the learnable view generators, the graph encoder, and the classifier in an end-to-end manner, resulting in topological heterogeneity yet semantic similarity in the generation of contrastive samples. Extensive experiments on semi-supervised learning, unsupervised learning, and transfer learning demonstrate the superiority of our AutoGCL framework over the state-of-the-arts in graph contrastive learning. In addition, the visualization results further confirm that the learnable view generators can deliver more compact and semantically meaningful contrastive samples compared against the existing view generation methods. Our code is available at https://github.com/Somedaywilldo/AutoGCL",
    "volume": "main",
    "checked": true,
    "id": "a3e14674236c633a9188df3bdd45317e884b25a9",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20872": {
    "title": "BM-NAS: Bilevel Multimodal Neural Architecture Search",
    "abstract": "Deep neural networks (DNNs) have shown superior performances on various multimodal learning problems. However, it often requires huge efforts to adapt DNNs to individual multimodal tasks by manually engineering unimodal features and designing multimodal feature fusion strategies. This paper proposes Bilevel Multimodal Neural Architecture Search (BM-NAS) framework, which makes the architecture of multimodal fusion models fully searchable via a bilevel searching scheme. At the upper level, BM-NAS selects the inter/intra-modal feature pairs from the pretrained unimodal backbones. At the lower level, BM-NAS learns the fusion strategy for each feature pair, which is a combination of predefined primitive operations. The primitive operations are elaborately designed and they can be flexibly combined to accommodate various effective feature fusion modules such as multi-head attention (Transformer) and Attention on Attention (AoA). Experimental results on three multimodal tasks demonstrate the effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS achieves competitive performances with much less search time and fewer model parameters in comparison with the existing generalized multimodal NAS methods. Our code is available at https://github.com/Somedaywilldo/BM-NAS",
    "volume": "main",
    "checked": true,
    "id": "0d0d3d8d5be06b418ca6d5d1eb73d2c96214376f",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20873": {
    "title": "Early-Bird GCNs: Graph-Network Co-optimization towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets",
    "abstract": "Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. However, it remains notoriously challenging to train and inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because as the graph size grows, the sheer number of node features and the large adjacency matrix can easily explode the required memory and data movements. To tackle the aforementioned challenges, we explore the possibility of drawing lottery tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency matrix yet are capable of achieving accuracy comparable to or even better than their full graphs. Specifically, we for the first time discover the existence of graph early-bird (GEB) tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of such GEB tickets. Furthermore, we advocate graph-model co-optimization and develop a generic efficient GCN early-bird training framework dubbed GEBT that can significantly boost the efficiency of GCN training by (1) drawing joint early-bird tickets between the GCN graphs and models and (2) enabling simultaneously sparsification of both the GCN graphs and models. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2% ~ 85.6% and 84.6% ~ 87.5% savings of GCN training and inference costs while offering a comparable or even better accuracy as compared to state-of-the-art methods. Our source code and supplementary appendix are available at https://github.com/RICE-EIC/Early-Bird-GCN",
    "volume": "main",
    "checked": true,
    "id": "2eeef005ba04b0d384ad6afb5be2f849d61f2fff",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20874": {
    "title": "Hindsight Network Credit Assignment: Efficient Credit Assignment in Networks of Discrete Stochastic Units",
    "abstract": "Training neural networks with discrete stochastic variables presents a unique challenge. Backpropagation is not directly applicable, nor are the reparameterization tricks used in networks with continuous stochastic variables. To address this challenge, we present Hindsight Network Credit Assignment (HNCA), a novel gradient estimation algorithm for networks of discrete stochastic units. HNCA works by assigning credit to each unit based on the degree to which its output influences its immediate children in the network. We prove that HNCA produces unbiased gradient estimates with reduced variance compared to the REINFORCE estimator, while the computational cost is similar to that of backpropagation. We first apply HNCA in a contextual bandit setting to optimize a reward function that is unknown to the agent. In this setting, we empirically demonstrate that HNCA significantly outperforms REINFORCE, indicating that the variance reduction implied by our theoretical analysis is significant and impactful. We then show how HNCA can be extended to optimize a more general function of the outputs of a network of stochastic units, where the function is known to the agent. We apply this extended version of HNCA to train a discrete variational auto-encoder and empirically show it compares favourably to other strong methods. We believe that the ideas underlying HNCA can help stimulate new ways of thinking about efficient credit assignment in stochastic compute graphs",
    "volume": "main",
    "checked": true,
    "id": "be59a55febff717ee3027e4a11ca03dc890c3b7c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20875": {
    "title": "SAIL: Self-Augmented Graph Contrastive Learning",
    "abstract": "This paper studies learning node representations with graph neural networks (GNNs) for unsupervised scenario. Specifically, we derive a theoretical analysis and provide an empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the locality of graph structure. To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed SAIL - a novel self-augmented graph contrastive learning framework, with two complementary self-distilling regularization modules, i.e., intra- and inter-graph knowledge distillation. We demonstrate the competitive performance of SAIL on a variety of graph applications. Even with a single GNN layer, SAIL has consistently competitive or even better performance on various benchmark datasets, comparing with state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "a49f843d9a2ba8dcfb80b1da45e441aae5c942b6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20876": {
    "title": "Natural Black-Box Adversarial Examples against Deep Reinforcement Learning",
    "abstract": "Black-box attacks in deep reinforcement learning usually retrain substitute policies to mimic behaviors of target policies as well as craft adversarial examples, and attack the target policies with these transferable adversarial examples. However, the transferability of adversarial examples is not always guaranteed. Moreover, current methods of crafting adversarial examples only utilize simple pixel space metrics which neglect semantics in the whole images, and thus generate unnatural adversarial examples. To address these problems, we propose an advRL-GAN framework to directly generate semantically natural adversarial examples in the black-box setting, bypassing the transferability requirement of adversarial examples. It formalizes the black-box attack as a reinforcement learning (RL) agent, which explores natural and aggressive adversarial examples with generative adversarial networks and the feedback of target agents. To the best of our knowledge, it is the first RL-based adversarial attack on a deep RL agent. Experimental results on multiple environments demonstrate the effectiveness of advRL-GAN in terms of reward reductions and magnitudes of perturbations, and validate the sparse and targeted property of adversarial perturbations through visualization",
    "volume": "main",
    "checked": true,
    "id": "0c213604e7cc3942ad551bf51ca2d67ed56548bf",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20877": {
    "title": "Regularization Penalty Optimization for Addressing Data Quality Variance in OoD Algorithms",
    "abstract": "Due to the poor generalization performance of traditional empirical risk minimization (ERM) in the case of distributional shift, Out-of-Distribution (OoD) generalization algorithms receive increasing attention. However, OoD generalization algorithms overlook the great variance in the quality of training data, which significantly compromises the accuracy of these methods. In this paper, we theoretically reveal the relationship between training data quality and algorithm performance, and analyze the optimal regularization scheme for Lipschitz regularized invariant risk minimization. A novel algorithm is proposed based on the theoretical results to alleviate the influence of low quality data at both the sample level and the domain level. The experiments on both the regression and classification benchmarks validate the effectiveness of our method with statistical significance",
    "volume": "main",
    "checked": true,
    "id": "c0fc2daaeeddb2cab2039eb32bfc3d5c890c1c53",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20878": {
    "title": "Low-Pass Graph Convolutional Network for Recommendation",
    "abstract": "Spectral graph convolution is extremely time-consuming for large graphs, thus existing Graph Convolutional Networks (GCNs) reconstruct the kernel by a polynomial, which is (almost) fixed. To extract features from the graph data by learning kernels, Low-pass Collaborative Filter Network (LCFN) was proposed as a new paradigm with trainable kernels. However, there are two demerits of LCFN: (1) The hypergraphs in LCFN are constructed by mining 2-hop connections of the user-item bipartite graph, thus 1-hop connections are not used, resulting in serious information loss. (2) LCFN follows the general network structure of GCNs, which is suboptimal. To address these issues, we utilize the bipartite graph to define the graph space directly and explore the best network structure based on experiments. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of the proposed model. Codes are available on https://github.com/Wenhui-Yu/LCFN",
    "volume": "main",
    "checked": true,
    "id": "da6802b50714c63f216414a5588ce4345f9a5472",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20879": {
    "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-Grained Input-Adaptation",
    "abstract": "Vision transformers have recently demonstrated great success in various computer vision tasks, motivating a tremendously increased interest in their deployment into many real-world IoT applications.    However, powerful ViTs are often too computationally expensive to be fitted onto real-world resource-constrained platforms, due to (1) their quadratically increased complexity with the number of input tokens and (2) their overparameterized self-attention heads and model depth. In parallel, different images are of varied complexity and their different regions can contain various levels of visual information, e.g., a sky background is not as informative as a foreground object in object classification tasks, indicating that treating those regions equally in terms of model complexity is unnecessary while such opportunities for trimming down ViTs' complexity have not been fully exploited.   To this end, we propose a Multi-grained Input-Adaptive Vision Transformer framework dubbed MIA-Former that can input-adaptively adjust the structure of ViTs at three coarse-to-fine-grained granularities (i.e., model depth and the number of model heads/tokens).   In particular, our MIA-Former adopts a low-cost network trained with a hybrid supervised and reinforcement learning method to skip the unnecessary layers, heads, and tokens in an input adaptive manner, reducing the overall computational cost. Furthermore, an interesting side effect of our MIA-Former is that its resulting ViTs are naturally equipped with improved robustness against adversarial attacks over their static counterparts, because MIA-Former's multi-grained dynamic control improves the model diversity similar to the effect of ensemble and thus increases the    difficulty of adversarial attacks against all its sub-models.   Extensive experiments and ablation studies validate that the proposed MIA-Former framework can (1) effectively allocate adaptive computation budgets to the difficulty of input images, achieving state-of-the-art (SOTA) accuracy-efficiency trade-offs, e.g., up to 16.5\\% computation savings with the same or even a higher accuracy compared with the SOTA dynamic transformer models, and (2) boost ViTs' robustness accuracy under various adversarial attacks over their vanilla counterparts by 2.4\\% and 3.0\\%, respectively. Our code is available at https://github.com/RICE-EIC/MIA-Former",
    "volume": "main",
    "checked": true,
    "id": "b6e9f1189fd46dabd6f1059c546cd2f4589fe65d",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20880": {
    "title": "Unsupervised Learning of Compositional Scene Representations from Multiple Unspecified Viewpoints",
    "abstract": "Visual scenes are extremely rich in diversity, not only because there are infinite combinations of objects and background, but also because the observations of the same scene may vary greatly with the change of viewpoints. When observing a visual scene that contains multiple objects from multiple viewpoints, humans are able to perceive the scene in a compositional way from each viewpoint, while achieving the so-called ``object constancy'' across different viewpoints, even though the exact viewpoints are untold. This ability is essential for humans to identify the same object while moving and to learn from vision efficiently. It is intriguing to design models that have the similar ability. In this paper, we consider a novel problem of learning compositional scene representations from multiple unspecified viewpoints without using any supervision, and propose a deep generative model which separates latent representations into a viewpoint-independent part and a viewpoint-dependent part to solve this problem. To infer latent representations, the information contained in different viewpoints is iteratively integrated by neural networks. Experiments on several specifically designed synthetic datasets have shown that the proposed method is able to effectively learn from multiple unspecified viewpoints",
    "volume": "main",
    "checked": true,
    "id": "d209d4862d6257ae1cd150d52bcef1f289baf2fa",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20881": {
    "title": "TS2Vec: Towards Universal Representation of Time Series",
    "abstract": "This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec",
    "volume": "main",
    "checked": true,
    "id": "f79a319413b5c014e4a98bcd223e6f65e3f7901b",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20882": {
    "title": "Fractional Adaptive Linear Units",
    "abstract": "This work introduces Fractional Adaptive Linear Units (FALUs), a flexible generalization of adaptive activation functions. Leveraging principles from fractional calculus, FALUs define a diverse family of activation functions (AFs) that encompass many traditional and state-of-the-art activation functions. This family includes the Sigmoid, Gaussian, ReLU, GELU, and Swish functions, as well as a large variety of smooth interpolations between these functions. Our technique requires only a small number of additional trainable parameters, and needs no further specialized optimization or initialization procedures. For this reason, FALUs present a seamless and rich automated solution to the problem of activation function optimization. Through experiments on a variety of conventional tasks and network architectures, we demonstrate the effectiveness of FALUs when compared to traditional and state-of-the-art AFs. To facilitate practical use of this work, we plan to make our code publicly available",
    "volume": "main",
    "checked": true,
    "id": "9e6551105819b844918005aa62130504da16c4a8",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20883": {
    "title": "SimSR: Simple Distance-Based State Representations for Deep Reinforcement Learning",
    "abstract": "This work explores how to learn robust and generalizable state representation from image-based observations with deep reinforcement learning methods. Addressing the computational complexity, stringent assumptions and representation collapse challenges in existing work of bisimulation metric, we devise Simple State Representation (SimSR) operator. SimSR enables us to design a stochastic approximation method that can practically learn the mapping functions (encoders) from observations to latent representation space. In addition to the theoretical analysis and comparison with the existing work, we experimented and compared our work with recent state-of-the-art solutions in visual MuJoCo tasks. The results shows that our model generally achieves better performance and has better robustness and good generalization",
    "volume": "main",
    "checked": true,
    "id": "f20c2aee7b91ed3c72d059499a77f335faa45167",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20884": {
    "title": "Efficient Decentralized Stochastic Gradient Descent Method for Nonconvex Finite-Sum Optimization Problems",
    "abstract": "Decentralized stochastic gradient descent methods have attracted increasing interest in recent years. Numerous methods have been proposed for the nonconvex finite-sum optimization problem. However, existing methods have a large sample complexity, slowing down the empirical convergence speed. To address this issue, in this paper, we proposed a novel decentralized stochastic gradient descent method for the nonconvex finite-sum optimization problem, which enjoys a better sample and communication complexity than existing methods. To the best of our knowledge, our work is the first one achieving such favorable sample and communication complexities. Finally, we have conducted extensive experiments and the experimental results have confirmed the superior performance of our proposed method",
    "volume": "main",
    "checked": true,
    "id": "3b6393a0e2382d0241f4093dd9c6f5c78b27861e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20885": {
    "title": "MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning",
    "abstract": "Few-Shot Learning (FSL) is a challenging task, i.e., how to recognize novel classes with few examples? Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then predicting novel classes via a cosine nearest neighbor classifier with mean-based prototypes. Nevertheless, due to the data scarcity, the mean-based prototypes are usually biased. In this paper, we attempt to diminish the prototype bias by regarding it as a prototype optimization problem. To this end, we propose a novel meta-learning based prototype optimization framework to rectify prototypes, i.e., introducing a meta-optimizer to optimize prototypes. Although the existing meta-optimizers can also be adapted to our framework, they all overlook a crucial gradient bias issue, i.e., the mean-based gradient estimation is also biased on sparse data. To address the issue, we regard the gradient and its flow as meta-knowledge and then propose a novel Neural Ordinary Differential Equation (ODE)-based meta-optimizer to polish prototypes, called MetaNODE. In this meta-optimizer, we first view the mean-based prototypes as initial prototypes, and then model the process of prototype optimization as continuous-time dynamics specified by a Neural ODE. A gradient flow inference network is carefully designed to learn to estimate the continuous gradient flow for prototype dynamics. Finally, the optimal prototypes can be obtained by solving the Neural ODE. Extensive experiments on miniImagenet, tieredImagenet, and CUB-200-2011 show the effectiveness of our method",
    "volume": "main",
    "checked": true,
    "id": "5e4c58e1be010eb9e7ea6773e2b082d2095be5f1",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20886": {
    "title": "State Deviation Correction for Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning aims to maximize the expected cumulative rewards with a fixed collection of data. The basic principle of current offline reinforcement learning methods is to restrict the policy to the offline dataset action space. However, they ignore the case where the dataset's trajectories fail to cover the state space completely. Especially, when the dataset's size is limited, it is likely that the agent would encounter unseen states during test time. Prior policy-constrained methods are incapable of correcting the state deviation, and may lead the agent to its unexpected regions further. In this paper, we propose the state deviation correction (SDC) method to constrain the policy's induced state distribution by penalizing the out-of-distribution states which might appear during the test period. We first perturb the states sampled from the logged dataset, then simulate noisy next states on the basis of a dynamics model and the policy. We then train the policy to minimize the distances between the noisy next states and the offline dataset. In this manner, we allow the trained policy to guide the agent to its familiar regions. Experimental results demonstrate that our proposed method is competitive with the state-of-the-art methods in a GridWorld setup, offline Mujoco control suite, and a modified offline Mujoco dataset with a finite number of valuable samples",
    "volume": "main",
    "checked": true,
    "id": "3c21b026c7b418335742e14ace107353a945dc12",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20887": {
    "title": "Multi-Agent Reinforcement Learning with General Utilities via Decentralized Shadow Reward Actor-Critic",
    "abstract": "We posit a new mechanism for cooperation in multi-agent reinforcement learning (MARL) based upon any nonlinear function of the team's long-term state-action occupancy measure, i.e., a general utility. This subsumes the cumulative return but also allows one to incorporate risk-sensitivity, exploration, and priors.  We derive the Decentralized Shadow Reward Actor-Critic (DSAC) in which agents alternate between policy evaluation (critic), weighted averaging with neighbors (information mixing), and local gradient updates for their policy parameters (actor). DSAC augments the classic critic step by requiring agents to (i) estimate their local occupancy measure in order to (ii) estimate the derivative of the local utility with respect to their occupancy measure, i.e., the ``shadow reward\". DSAC converges to ϵ-stationarity in O(1/ϵ^2.5) or faster O(1/ϵ^2) steps with high probability, depending on the amount of communications. We further establish the non-existence of spurious stationary points for this problem, that is, DSAC finds the globally optimal policy. Experiments demonstrate the merits of goals beyond the cumulative return in cooperative MARL",
    "volume": "main",
    "checked": true,
    "id": "18247aad4c2f600c301234b2bacfa48643cc34f4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20888": {
    "title": "Co-promotion Predictions of Financing Market and Sales Market: A Cooperative-Competitive Attention Approach",
    "abstract": "Market popularity prediction has always been a hot research topic, such as sales prediction and crowdfunding prediction. Most of these studies put the perspective on isolated markets, relying on the knowledge of certain market to maximize the prediction performance. However, these market-specific approaches are restricted by the knowledge limitation of isolated markets and incapable of the complicated and potential relations among different markets, especially some with strong dependence such as the financing market and sales market. Fortunately, we discover potentially symbiotic relations between the financing market and the sales market, which provides us with an opportunity to co-promote the popularity predictions of both markets. Thus, for bridgly learning the knowledge interactions between financing market and sales market, we propose a cross-market approach, namely CATN: Cooperative-competitive Attention Transfer Network, which could effectively transfer knowledge of financing capability from the crowdfunding market and sales prospect from the E-commerce market. Specifically, for capturing the complicated relations especially the cooperation or complement of items and enhancing the knowledge transfer between the two heterogeneous markets, we design a novel Cooperative Attention; meanwhile, for finely computing the relations of items especially the competition in specific same market, we further design Competitive Attentions for the two markets respectively. Besides, we also distinguish aligned features and unique features to adapt the cross-market predictions. With the real-world datasets collected from Indiegogo and Amazon, we construct extensive experiments on three types of datasets from the two markets and the results demonstrate the effectiveness and generalization of our CATN model",
    "volume": "main",
    "checked": true,
    "id": "883063b6e6f91af4337551cf1abe7c97600e856c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20889": {
    "title": "Categorical Neighbour Correlation Coefficient (CnCor) for Detecting Relationships between Categorical Variables",
    "abstract": "Categorical data is common and, however, special in that its possible values exist only on a nominal scale so that many statistical operations such as mean, variance, and covariance become not applicable. Following the basic idea of the neighbour correlation coefficient (nCor), in this study, we propose a new measure named the categorical nCor (CnCor) to examine the association between categorical variables through using indicator functions to reform the distance metric and product-moment correlation coefficient. The proposed measure is easy to compute, and enables a direct test of statistical dependence without the need of converting the qualitative variables to quantitative ones. Compare to previous approaches, it is much more robust and effective in dealing with multi-categorical target variables especially when highly nonlinear relationships occurs in the multivariate case. We also applied the CnCor to implementing feature selection by the scheme of backward elimination. Finally, extensive experiments performed on both synthetic and real-world datasets are conducted to demonstrate the outstanding performance of the proposed methods, and draw comparisons with state-of-the-art association measures and feature selection algorithms",
    "volume": "main",
    "checked": true,
    "id": "67eb0cfcacc1bb285b8a8e53266f9e073489804f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20890": {
    "title": "Interpretable Domain Adaptation for Hidden Subdomain Alignment in the Context of Pre-trained Source Models",
    "abstract": "Domain adaptation aims to leverage source domain knowledge to predict target domain labels. Most domain adaptation methods tackle a single-source, single-target scenario, whereas source and target domain data can often be subdivided into data from different distributions in real-life applications (e.g., when the distribution of the collected data changes with time). However, such subdomains are rarely given and should be discovered automatically.    To this end, some recent domain adaptation works seek separations of hidden subdomains, w.r.t. a known or fixed number of subdomains.    In contrast, this paper introduces a new subdomain combination method that leverages a variable number of subdomains.    Precisely, we propose to use an inter-subdomain divergence maximization criterion to exploit hidden subdomains.    Besides, our proposition stands in a target-to-source domain adaptation scenario, where one exploits a pre-trained source model as a black box; thus, the proposed method is model-agnostic.   By providing interpretability at two complementary levels (transformation and subdomain levels), our method can also be easily interpreted by practitioners with or without machine learning backgrounds.   Experimental results over two fraud detection datasets demonstrate the efficiency of our method",
    "volume": "main",
    "checked": true,
    "id": "e7f0679bab93169b824ed12d6939533f5dc1c9b7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20891": {
    "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings",
    "abstract": "Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with L2 integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies",
    "volume": "main",
    "checked": true,
    "id": "1378b2b507fbbc6d475835eeb740f1c7426d7be5",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20892": {
    "title": "Gaussian Process Bandits with Aggregated Feedback",
    "abstract": "We consider the continuum-armed bandits problem, under a novel setting of recommending the best arms within a fixed budget under aggregated feedback. This is motivated by applications where the precise rewards are impossible or expensive to obtain, while an aggregated reward or feedback, such as the average over a subset, is available. We constrain the set of reward functions by assuming that they are from a Gaussian Process and propose the Gaussian Process Optimistic Optimisation (GPOO) algorithm. We adaptively construct a tree with nodes as subsets of the arm space, where the feedback is the aggregated reward of representatives of a node. We propose a new simple regret notion with respect to aggregated feedback on the recommended arms. We provide theoretical analysis for the proposed algorithm, and recover single point feedback as a special case. We illustrate GPOO and compare it with related algorithms on simulated data",
    "volume": "main",
    "checked": true,
    "id": "42c4a8bf4317d25bc11f438e50d02bd751edc32b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20893": {
    "title": "Rethinking Influence Functions of Neural Networks in the Over-Parameterized Regime",
    "abstract": "Understanding the black-box prediction for neural networks is challenging. To achieve this, early studies have designed influence function (IF) to measure the effect of removing a single training point on neural networks. However, the classic implicit Hessian-vector product (IHVP) method for calculating IF is fragile, and theoretical analysis of IF in the context of neural networks is still lacking. To this end, we utilize the neural tangent kernel (NTK) theory to calculate IF for the neural network trained with regularized mean-square loss, and prove that the approximation error can be arbitrarily small when the width is sufficiently large for two-layer ReLU networks. We analyze the error bound for the classic IHVP method in the over-parameterized regime to understand when and why it fails or not. In detail, our theoretical analysis reveals that (1) the accuracy of IHVP depends on the regularization term, and is pretty low under weak regularization; (2) the accuracy of IHVP has a significant correlation with the probability density of corresponding training points. We further borrow the theory from NTK to understand the IFs better, including quantifying the complexity for influential samples and depicting the variation of IFs during the training dynamics. Numerical experiments on real-world data confirm our theoretical results and demonstrate our findings",
    "volume": "main",
    "checked": true,
    "id": "ee2c7ae4f8c819eaba6427cb1beaccce6c154b40",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20894": {
    "title": "A Multi-Agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning",
    "abstract": "Federated learning (FL) is a training technique that enables client devices to jointly learn a shared model by aggregating locally computed models without exposing their raw data. While most of the existing work focuses on improving the FL model accuracy, in this paper, we focus on the improving the training efficiency, which is often a hurdle for adopting FL in real world applications. Specifically, we design an efficient FL framework which jointly optimizes model accuracy, processing latency and communication efficiency, all of which are primary design considerations for real implementation of FL. Inspired by the recent success of Multi Agent Reinforcement Learning (MARL) in solving complex control problems, we present FedMarl, a federated learning framework that relies on trained MARL agents to perform efficient run-time client selection. Experiments show that FedMarl can significantly improve model accuracy with much lower processing latency and communication cost",
    "volume": "main",
    "checked": true,
    "id": "3385b550a221316a1cd36b9b7a7c82992651a99c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20895": {
    "title": "Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition",
    "abstract": "Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), aiming to refine multi-modal representations and enhance discriminative capacity of each label. Specifically, we design an adversarial multi-modal refinement module to sufficiently explore the commonality among different modalities and strengthen the diversity of each modality. To further exploit label-modal dependence, we devise a BERT-like cross-modal encoder to gradually fuse private and common modality representations in a granularity descent way, as well as a label-guided decoder to adaptively generate a tailored representation for each label with the guidance of label semantics. In addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in both aligned and unaligned settings, which demonstrate the superiority of TAILOR over the state-of-the-arts",
    "volume": "main",
    "checked": true,
    "id": "435cd08e94668fc4a48cb53e31909a6a269ae9ef",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20896": {
    "title": "Fusion Multiple Kernel K-means",
    "abstract": "Multiple kernel clustering aims to seek an appropriate combination of base kernels to mine inherent non-linear information for optimal clustering. Late fusion algorithms generate base partitions independently and integrate them in the following clustering procedure, improving the overall efficiency. However, the separate base partition generation leads to inadequate negotiation with the clustering procedure and a great loss of beneficial information in corresponding kernel matrices, which negatively affects the clustering performance. To address this issue, we propose a novel algorithm, termed as Fusion Multiple Kernel k-means (FMKKM), which unifies base partition learning and late fusion clustering into one single objective function, and adopts early fusion technique to capture more sufficient information in kernel matrices. Specifically, the early fusion helps base partitions keep more beneficial kernel details, and the base partitions learning further guides the generation of consensus partition in the late fusion stage, while the late fusion provides positive feedback on two former procedures.   The close collaboration of three procedures results in a promising performance improvement. Subsequently, an alternate optimization method with promising convergence is developed to solve the resultant optimization problem. Comprehensive experimental results demonstrate that our proposed algorithm achieves state-of-the-art performance on multiple public datasets, validating its effectiveness. The code of this work is publicly available at https://github.com/ethan-yizhang/Fusion-Multiple-Kernel-K-means",
    "volume": "main",
    "checked": true,
    "id": "45ad9236f2a53c95a85823f5e69853440067e8e6",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20897": {
    "title": "Batch Active Learning with Graph Neural Networks via Multi-Agent Deep Reinforcement Learning",
    "abstract": "Graph neural networks (GNNs) have achieved tremendous success in many graph learning tasks such as node classification, graph classification and link prediction. For the classification task, GNNs' performance often highly depends on the number of labeled nodes and thus could be significantly hampered due to the expensive annotation cost. The sparse literature on active learning for GNNs has primarily focused on selecting only one sample each iteration, which becomes inefficient for large scale datasets. In this paper, we study the batch active learning setting for GNNs where the learning agent can acquire labels of multiple samples at each time. We formulate batch active learning as a cooperative multi-agent reinforcement learning problem and present a novel reinforced batch-mode active learning framework BiGeNe. To avoid the combinatorial explosion of the joint action space, we introduce a value decomposition method that factorizes the total Q-value into the average of individual Q-values. Moreover, we propose a novel multi-agent Q-network consisting of a graph convolutional network (GCN) component and a gated recurrent unit (GRU) component. The GCN component takes both the informativeness and inter-dependences between nodes into account and the GRU component enables the agent to consider interactions between selected nodes in the same batch. Experimental results on multiple public datasets demonstrate the effectiveness and efficiency of our proposed method",
    "volume": "main",
    "checked": true,
    "id": "acda0d4c1b3a63a73849f05cbc73dd283f87edf9",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20898": {
    "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
    "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions   made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.   Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts",
    "volume": "main",
    "checked": true,
    "id": "7de413da6e0a00e14270cfaed2a31666e7c28747",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20899": {
    "title": "Learning to Solve Travelling Salesman Problem with Hardness-Adaptive Curriculum",
    "abstract": "Various neural network models have been proposed to tackle combinatorial optimization problems such as the travelling salesman problem (TSP). Existing learning-based TSP methods adopt a simple setting that the training and testing data are independent and identically distributed. However, the existing literature fails to solve TSP instances when training and testing data have different distributions.  Concretely, we find that different training and testing distribution will result in more difficult TSP instances, i.e., the solution obtained by the model has a large gap from the optimal solution.  To tackle this problem, in this work, we study learning-based TSP methods when training and testing data have different distributions using adaptive-hardness, i.e., how difficult a TSP instance can be for a solver.   This problem is challenging because it is non-trivial to (1) define hardness measurement quantitatively; (2) efficiently and continuously generate sufficiently hard TSP instances upon model training; (3) fully utilize instances with different levels of hardness to learn a more powerful TSP solver.  To solve these challenges, we first propose a principled hardness measurement to quantify the hardness of TSP instances. Then, we propose a hardness-adaptive generator to generate instances with different hardness. We further propose a curriculum learner fully utilizing these instances to train the TSP solver. Experiments show that our hardness-adaptive generator can generate instances ten times harder than the existing methods, and our proposed method achieves significant improvement over state-of-the-art models in terms of the optimality gap. The codes are publicly available",
    "volume": "main",
    "checked": true,
    "id": "1ceefc6af868dadffeecdaace1a023975d8e316f",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20900": {
    "title": "Robust Action Gap Increasing with Clipped Advantage Learning",
    "abstract": "Advantage Learning (AL) seeks to increase the action gap between the optimal action and its competitors, so as to improve the robustness to estimation errors. However, the method becomes problematic when the optimal action induced by the approximated value function does not agree with the true optimal action. In this paper, we present a novel method, named clipped Advantage Learning (clipped AL), to address this issue. The method is inspired by our observation that increasing the action gap blindly for all given samples while not taking their necessities into account could accumulate more errors in the performance loss bound, leading to a slow value convergence, and to avoid that, we should adjust the advantage value adaptively. We show that our simple clipped AL operator not only enjoys fast convergence guarantee but also retains proper action gaps, hence achieving a good balance between the large action gap and the fast convergence. The feasibility and effectiveness of the proposed method are verified empirically on several RL benchmarks with promising performance",
    "volume": "main",
    "checked": true,
    "id": "0ec532b955b4effd2d6f077e027eddec2f041246",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20901": {
    "title": "Online Influence Maximization with Node-Level Feedback Using Standard Offline Oracles",
    "abstract": "We study the online influence maximization (OIM) problem in social networks, where in multiple rounds the learner repeatedly chooses seed nodes to generate cascades, observes the cascade feedback, and gradually learns the best seeds that generate the largest cascade. We focus on two major challenges in this paper. First, we work with node-level feedback instead of edge-level feedback. The edge-level feedback reveals all edges that pass through information in a cascade, whereas the node-level feedback only reveals the activated nodes with timestamps. The node-level feedback is arguably more realistic since in practice it is relatively easy to observe who is influenced but very difficult to observe from which relationship (edge) the influence comes. Second, we use standard offline oracles instead of offline pair-oracles. To compute a good seed set for the next round, an offline pair-oracle finds the best seed set and the best parameters within the confidence region simultaneously, and such an oracle is difficult to compute due to the combinatorial core of the OIM problem. So we focus on how to use the standard offline influence maximization oracle which finds the best seed set given the edge parameters as input. In this paper, we resolve these challenges for the famous independent cascade (IC) diffusion model. The past research only achieves edge-level feedback, while we present the first optimal regret algorithm for the node-level feedback. For the first challenge above, we apply a novel adaptation of the maximum likelihood estimation (MLE) approach to learn the graph parameters and its confidence region (a confidence ellipsoid). For the second challenge, we adjust the update procedure to dissect the confidence ellipsoid into confidence intervals on each parameter, so that the standard offline influence maximization oracle is enough",
    "volume": "main",
    "checked": true,
    "id": "6be667f07158d2c9e46ee7443c7ab7756e8b7f8d",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20902": {
    "title": "CLPA: Clean-Label Poisoning Availability Attacks Using Generative Adversarial Nets",
    "abstract": "Poisoning attacks are emerging threats to deep neural networks where the adversaries attempt to compromise the models by injecting malicious data points in the clean training data. Poisoning attacks target either the availability or integrity of a model. The availability attack aims to degrade the overall accuracy while the integrity attack causes misclassification only for specific instances without affecting the accuracy of clean data. Although clean-label integrity attacks are proven to be effective in recent studies, the feasibility of clean-label availability attacks remains unclear. This paper, for the first time, proposes a clean-label approach, CLPA, for the poisoning availability attack. We reveal that due to the intrinsic imperfection of classifiers, naturally misclassified inputs can be considered as a special type of poisoned data, which we refer to as \"natural poisoned data''. We then propose a two-phase generative adversarial net (GAN) based poisoned data generation framework along with a triplet loss function for synthesizing clean-label poisoned samples that locate in a similar distribution as natural poisoned data. The generated poisoned data are plausible to human perception and can also bypass the singular vector decomposition (SVD) based defense. We demonstrate the effectiveness of our approach on CIFAR-10 and ImageNet dataset over a variety type of models. Codes are available at: https://github.com/bxz9200/CLPA",
    "volume": "main",
    "checked": true,
    "id": "8216096d7268ca560c52eb3ca8cca680725b0c8f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20903": {
    "title": "FedInv: Byzantine-Robust Federated Learning by Inversing Local Model Updates",
    "abstract": "Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables multiple clients to collaboratively train statistical models without disclosing raw training data. However, the inaccessible local training data and uninspectable local training process make FL susceptible to various Byzantine attacks (e.g., data poisoning and model poisoning attacks), aiming to manipulate the FL model training process and degrade the model performance. Most of the existing Byzantine-robust FL schemes cannot effectively defend against stealthy poisoning attacks that craft poisoned models statistically similar to benign models. Things worsen when many clients are compromised or data among clients are highly non-independent and identically distributed (non-IID). In this work, to address these issues, we propose FedInv, a novel Byzantine-robust FL framework by inversing local model updates. Specifically, in each round of local model aggregation in FedInv, the parameter server first inverses the local model updates submitted by each client to generate a corresponding dummy dataset. Then, the server identifies those dummy datasets with exceptional Wasserstein distances from others and excludes the related local model updates from model aggregation. We conduct an exhaustive experimental evaluation of FedInv. The results demonstrate that FedInv significantly outperforms the existing robust FL schemes in defending against stealthy poisoning attacks under highly non-IID data partitions",
    "volume": "main",
    "checked": true,
    "id": "7e1dbebd35df4d7260ebfe56bca0f7a09c4847f5",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20904": {
    "title": "Well-Classified Examples Are Underestimated in Classification with Deep Neural Networks",
    "abstract": "The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex scenarios, such as imbalanced classification, OOD detection, and applications under adversarial attacks because our idea can solve these three issues. Code is available at https://github.com/lancopku/well-classified-examples-are-underestimated",
    "volume": "main",
    "checked": true,
    "id": "f8b12f9ce1cfbe13435243bdc7dabd4eb95c05fc",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20905": {
    "title": "Error-Based Knockoffs Inference for Controlled Feature Selection",
    "abstract": "Recently, the scheme of model-X knockoffs was proposed as a promising solution to address controlled feature selection under high-dimensional finite-sample settings. However, the procedure of model-X knockoffs depends heavily on the coefficient-based feature importance and only concerns the control of false discovery rate (FDR). To further improve its adaptivity and flexibility, in this paper, we propose an error-based knockoff inference method by integrating the knockoff features, the error-based feature importance statistics, and the stepdown procedure together. The proposed inference procedure does not require specifying a regression model and can handle feature selection with theoretical guarantees on controlling false discovery proportion (FDP), FDR, or k-familywise error rate (k-FWER). Empirical evaluations demonstrate the competitive performance of our approach on both simulated and real data",
    "volume": "main",
    "checked": true,
    "id": "22a9307e6108e7357f5198bcf4d33d97ab8e2982",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20906": {
    "title": "Online Missing Value Imputation and Change Point Detection with the Gaussian Copula",
    "abstract": "Missing value imputation is crucial for real-world data science workflows. Imputation is harder in the online setting, as it requires the imputation method itself to be able to evolve over time. For practical applications, imputation algorithms should produce imputations that match the true data distribution, handle data of mixed types, including ordinal, boolean, and continuous variables, and scale to large datasets. In this work we develop a new online imputation algorithm for mixed data using the Gaussian copula. The online Gaussian copula model produces meets all the desiderata: its imputations match the data distribution even for mixed data, improve over its offline counterpart on the accuracy when the streaming data has a changing distribution, and on the speed (up to an order of magnitude) especially on large scale datasets. By fitting the copula model to online data, we also provide a new method to detect change points in the multivariate dependence structure for mixed data with missing values. Experimental results on synthetic and real world data validate the performance of the proposed methods",
    "volume": "main",
    "checked": true,
    "id": "108be462fd92055fd83df07ad5ae3dfac9080702",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20907": {
    "title": "LaSSL: Label-Guided Self-Training for Semi-supervised Learning",
    "abstract": "The key to semi-supervised learning (SSL) is to explore adequate information to leverage the unlabeled data. Current dominant approaches aim to generate pseudo-labels on weakly augmented instances and train models on their corresponding strongly augmented variants with high-confidence results. However, such methods are limited in excluding samples with low-confidence pseudo-labels and under-utilization of the label information. In this paper, we emphasize the cruciality of the label information and propose a Label-guided Self-training approach to Semi-supervised Learning (LaSSL), which improves pseudo-label generations from two mutually boosted strategies. First, with the ground-truth labels and iteratively-polished pseudo-labels, we explore instance relations among all samples and then minimize a class-aware contrastive loss to learn discriminative feature representations that make same-class samples gathered and different-class samples scattered. Second, on top of improved feature representations, we propagate the label information to the unlabeled samples across the potential data manifold at the feature-embedding level, which can further improve the labelling of samples with reference to their neighbours. These two strategies are seamlessly integrated and mutually promoted across the whole training process. We evaluate LaSSL on several classification benchmarks under partially labeled settings and demonstrate its superiority over the state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "f25deb30cdc5a1e3ff1b7789e9646a95baca47ed",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20908": {
    "title": "Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms",
    "abstract": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation. We adopt this viewpoint and model the actor and critic interaction as a two-player general-sum game with a leader-follower structure known as a Stackelberg game. Given this abstraction, we propose a meta-framework for Stackelberg actor-critic algorithms where the leader player follows the total derivative of its objective instead of the usual individual gradient. From a theoretical standpoint, we develop a policy gradient theorem for the refined update and provide a local convergence guarantee for the Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an empirical standpoint, we demonstrate via simple examples that the learning dynamics we study mitigate cycling and accelerate convergence compared to the usual gradient dynamics given cost structures induced by actor-critic formulations. Finally, extensive experiments on OpenAI gym environments show that Stackelberg actor-critic algorithms always perform at least as well and often significantly outperform the standard actor-critic algorithm counterparts",
    "volume": "main",
    "checked": true,
    "id": "e28b9bc26f5f7eb3b0532d823713400202372da2",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20909": {
    "title": "Adaptive Pairwise Weights for Temporal Credit Assignment",
    "abstract": "How much credit (or blame) should an action taken in a state get for a future reward? This is the fundamental temporal credit assignment problem in Reinforcement Learning (RL). One of the earliest and still most widely used heuristics is to assign this credit based on a scalar coefficient, lambda (treated as a hyperparameter), raised to the power of the time interval between the state-action and the reward. In this empirical paper, we explore heuristics based on more general pairwise weightings that are functions of the state in which the action was taken, the state at the time of the reward, as well as the time interval between the two. Of course it isn't clear what these pairwise weight functions should be, and because they are too complex to be treated as hyperparameters we develop a metagradient procedure for learning these weight functions during the usual RL training of a policy. Our empirical work shows that it is often possible to learn these pairwise weight functions during learning of the policy to achieve better performance than competing approaches",
    "volume": "main",
    "checked": true,
    "id": "45b50ef6edf1cedb28f9ae5157ff88db79451558",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20910": {
    "title": "Programmatic Reward Design by Example",
    "abstract": "Reward design is a fundamental problem in reinforcement learning (RL). A misspecified or poorly designed reward can result in low sample efficiency and undesired behaviors. In this paper, we propose the idea of programmatic reward design, i.e. using programs to specify the reward functions in RL environments. Programs allow human engineers to express sub-goals and complex task scenarios in a structured and interpretable way. The challenge of programmatic reward design, however, is that while humans can provide the high-level structures, properly setting the low-level details, such as the right amount of reward for a specific sub-task, remains difficult. A major contribution of this paper is a probabilistic framework that can infer the best candidate programmatic reward function from expert demonstrations. Inspired by recent generative-adversarial approaches, our framework searches for themost likely programmatic reward function under whichthe optimally generated trajectories cannot be differen-tiated from the demonstrated trajectories. Experimental results show that programmatic reward functions learned using this framework can significantly outperform those learned using existing reward learning algorithms, and enable RL agents to achieve state-of-the-art performance on highly complex tasks",
    "volume": "main",
    "checked": true,
    "id": "e2eb7309a1bf6162549785c56e3e426918e4fd60",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20911": {
    "title": "Neural Piecewise-Constant Delay Differential Equations",
    "abstract": "Continuous-depth neural networks, such as the Neural Ordinary Differential Equations (ODEs), have aroused a great deal of interest from the communities of machine learning and data science in recent years, which bridge the connection between deep neural networks and dynamical systems. In this article, we introduce a new sort of continuous-depth neural network, called the Neural Piecewise-Constant Delay Differential Equations (PCDDEs). Here, unlike the recently proposed framework of the Neural Delay Differential Equations (DDEs), we transform the single delay into the piecewise-constant delay(s). The Neural PCDDEs with such a transformation, on one hand, inherit the strength of universal approximating capability in Neural DDEs. On the other hand, the Neural PCDDEs, leveraging the contributions of the information from the multiple previous time steps, further promote the modeling capability without augmenting the network dimension. With such a promotion, we show that the Neural PCDDEs do outperform the several existing continuous-depth neural frameworks on the one-dimensional piecewise-constant delay population dynamics and real-world datasets, including MNIST, CIFAR10, and SVHN",
    "volume": "main",
    "checked": true,
    "id": "c6ecbe85ee9b4d1b558028f8048277173630daec",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20912": {
    "title": "Structural Landmarking and Interaction Modelling: A \"SLIM\" Network for Graph Classification",
    "abstract": "Graph neural networks are a promising architecture for learning and inference with graph-structured data. Yet, how to generate informative, fixed dimensional features for graphs with varying size and topology can still be challenging. Typically, this is achieved through graph-pooling, which summarizes a graph by compressing all its nodes into a single vector. Is such a \"collapsing-style\" graph-pooling the only choice for graph classification? From complex system’s point of view, properties of a complex system arise largely from the interaction among its components. Therefore, we speculate that preserving the interacting relation between parts, instead of pooling them together, could benefit system level prediction. To verify this, we propose SLIM, a graph neural network model for Structural Landmarking and Interaction Modelling. The main idea is to compute a set of end-to-end optimizable sub-structure landmarks, so that any input graph can be projected onto these (spatially) local structural representatives for a faithful, global characterization. By doing so, explicit interaction between component parts of a graph can be leveraged directly in generating discriminative graph representation. Encouraging results are observed on benchmark datasets for graph classification, demonstrating the value of interaction modelling in the design of graph neural networks",
    "volume": "main",
    "checked": true,
    "id": "fe963686be76b5da13a3c159bbd74a9815ea95ef",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20913": {
    "title": "Invariant Action Effect Model for Reinforcement Learning",
    "abstract": "Good representations can help RL agents perform concise modeling of their surroundings, and thus support effective decision-making in complex environments.    Previous methods learn good representations by imposing extra constraints on dynamics.   However, in the causal perspective, the causation between the action and its effect is not fully considered in those methods, which leads to the ignorance of the underlying relations among the action effects on the transitions.    Based on the intuition that the same action always causes similar effects among different states, we induce such causation by taking the invariance of action effects among states as the relation.   By explicitly utilizing such invariance, in this paper, we show that a better representation can be learned and potentially improves the sample efficiency and the generalization ability of the learned policy.    We propose Invariant Action Effect Model (IAEM) to capture the invariance in action effects, where the effect of an action is represented as the residual of representations from neighboring states.   IAEM is composed of two parts:   (1) a new contrastive-based loss to capture the underlying invariance of action effects;   (2) an individual action effect and provides a self-adapted weighting strategy to tackle the corner cases where the invariance does not hold.   The extensive experiments on two benchmarks, i.e. Grid-World and Atari, show that the representations learned by IAEM preserve the invariance of action effects.    Moreover, with the invariant action effect, IAEM can accelerate the learning process by 1.6x, rapidly generalize to new environments by fine-tuning on a few components, and outperform other dynamics-based representation methods by 1.4x in limited steps",
    "volume": "main",
    "checked": true,
    "id": "1632f51c8e573264730061dc5c9e7db821535bc4",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20914": {
    "title": "Self-Adaptive Imitation Learning: Learning Tasks with Delayed Rewards from Sub-optimal Demonstrations",
    "abstract": "Reinforcement learning (RL) has demonstrated its superiority in solving sequential decision-making problems. However, heavy dependence on immediate reward feedback impedes the wide application of RL. On the other hand, imitation learning (IL) tackles RL without relying on environmental supervision by leveraging external demonstrations. In practice, however, collecting sufficient expert demonstrations can be prohibitively expensive, yet the quality of demonstrations typically limits the performance of the learning policy. To address a practical scenario, in this work, we propose Self-Adaptive Imitation Learning (SAIL), which, provided with a few demonstrations from a sub-optimal teacher, can perform well in RL tasks with extremely delayed rewards, where the only reward feedback is trajectory-wise ranking. SAIL bridges the advantages of IL and RL by interactively exploiting the demonstrations to catch up with the teacher and exploring the environment to yield demonstrations that surpass the teacher. Extensive empirical results show that not only does SAIL significantly improve the sample efficiency, but it also leads to higher asymptotic performance across different continuous control tasks, compared with the state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "ee2e080407e112abb764631405df5832934eeb57",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20915": {
    "title": "Locality Matters: A Scalable Value Decomposition Approach for Cooperative Multi-Agent Reinforcement Learning",
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) faces significant scalability issues due to state and action spaces that are exponentially large in the number of agents. As environments grow in size, effective credit assignment becomes increasingly harder and often results in infeasible learning times. Still, in many real-world settings, there exist simplified underlying dynamics that can be leveraged for more scalable solutions. In this work, we exploit such locality structures effectively whilst maintaining global cooperation. We propose a novel, value-based multi-agent algorithm called LOMAQ, which incorporates local rewards in the Centralized Training Decentralized Execution paradigm. Additionally, we provide a direct reward decomposition method for finding these local rewards when only a global signal is provided. We test our method empirically, showing it scales well compared to other methods, significantly improving performance and convergence speed",
    "volume": "main",
    "checked": true,
    "id": "e6f105477814f500a700ebaaf09df678c3b1250e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21156": {
    "title": "Hedonic Games with Fixed-Size Coalitions",
    "abstract": "In hedonic games, a set of n agents, having preferences over all possible coalition structures, needs to agree on a stable outcome. In this work, we initiate the study of hedonic games with fixed-size coalitions, where the set of possible coalition structures is restricted as follows: there are k coalitions, each coalition has a fixed size, and the sum of the sizes of all coalitions equals n.   We focus on the basic model of additively separable hedonic games with symmetric preferences, where an agent's preference is captured by a utility function which sums up a contribution due to any other agent in the same coalition. In this setting, an outcome is stable if no pair of agents can exchange coalitions and improve their utilities. Conditioned on the definition of improvement, three stability notions arise: swap stability under transferable utilities, which requires to improve the sum of the utilities of both agents, swap stability, which requires to improve the utility of one agent without decreasing the utility of the other one, and strict swap stability, requiring to improve the utilities of both agents simultaneously.  We analyse the fundamental questions of existence, complexity and efficiency of stable outcomes, and that of complexity of a social optimum",
    "volume": "main",
    "checked": true,
    "id": "d59be55999511f5ba1ad40e561b9f9739dbe5821",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21158": {
    "title": "Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams",
    "abstract": "When humans collaborate with each other, they often make decisions by observing others and considering the consequences that their actions may have on the entire team, instead of greedily doing what is best for just themselves. We would like our AI agents to effectively collaborate in a similar way by capturing a model of their partners. In this work, we propose and analyze a decentralized Multi-Armed Bandit (MAB) problem with coupled rewards as an abstraction of more general multi-agent collaboration. We demonstrate that naive extensions of single-agent optimal MAB algorithms fail when applied for decentralized bandit teams. Instead, we propose a Partner-Aware strategy for joint sequential decision-making that extends the well-known single-agent Upper Confidence Bound algorithm. We analytically show that our proposed strategy achieves logarithmic regret, and provide extensive experiments involving human-AI and human-robot collaboration to validate our theoretical findings. Our results show that the proposed partner-aware strategy outperforms other known methods, and our human subject studies suggest humans prefer to collaborate with AI agents implementing our partner-aware strategy",
    "volume": "main",
    "checked": true,
    "id": "a0ad7fd42bb4dc9489a4dcf6eddaee8baf4d5f06",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21160": {
    "title": "Fixation Maximization in the Positional Moran Process",
    "abstract": "The Moran process is a classic stochastic process that models invasion dynamics on graphs. A single mutant (e.g., a new opinion, strain, social trait etc.) invades a population of residents spread over the nodes of a graph. The mutant fitness advantage δ>=0 determines how aggressively mutants propagate to their neighbors. The quantity of interest is the fixation probability, i.e., the probability that the initial mutant eventually takes over the whole population. However, in realistic settings, the invading mutant has an advantage only in certain locations. E.g., the ability to metabolize a certain sugar is an advantageous trait to bacteria only when the sugar is actually present in their surroundings. In this paper we introduce the positional Moran process, a natural generalization in which the mutant fitness advantage is only realized on specific nodes called active nodes, and study the problem of fixation maximization: given a budget k, choose a set of k active nodes that maximize the fixation probability of the invading mutant. We show that the problem is NP-hard, while the optimization function is not submodular, thus indicating strong computational hardness. We focus on two natural limits. In the limit of δ to infinity (strong selection), although the problem remains NP-hard, the optimization function becomes submodular and thus admits a constant-factor approximation using a simple greedy algorithm. In the limit of δ to 0 (weak selection), we show that we can obtain a tight approximation in O(n^{2×ω}) time, where ω is the matrix-multiplication exponent. An experimental evaluation of the new algorithms along with some proposed heuristics corroborates our results",
    "volume": "main",
    "checked": true,
    "id": "60b306321e203a35132c4335bb2eb637fe91ac6b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21162": {
    "title": "Flex Distribution for Bounded-Suboptimal Multi-Agent Path Finding",
    "abstract": "Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths for multiple agents that minimize the sum of path costs. EECBS is a leading two-level algorithm that solves MAPF bounded-suboptimally, that is, within some factor w of the minimum sum of path costs C*. It uses focal search to find bounded-suboptimal paths on the low level and Explicit Estimation Search (EES) to resolve collisions on the high level. EES keeps track of a lower bound LB on C* to find paths whose sum of path costs is at most w LB in order to solve MAPF bounded-suboptimally. However, the costs of many paths are often much smaller than w times their minimum path costs, meaning that the sum of path costs is much smaller than w C*. In this paper, we therefore propose Flexible EECBS (FEECBS), which uses a flex(ible) distribution of the path costs (that relaxes the requirement to find bounded-suboptimal paths on the low level) in order to reduce the number of collisions that need to be resolved on the high level while still guaranteeing to solve MAPF bounded suboptimally. We address the drawbacks of flex distribution via techniques such as restrictions on the flex distribution, restarts of the high-level search with EECBS, and low-level focal-A* search. Our empirical evaluation shows that FEECBS substantially improves the efficiency of EECBS on MAPF instances with large maps and large numbers of agents",
    "volume": "main",
    "checked": true,
    "id": "1d3345101696bb414664cde830746e3ea6d4787e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21163": {
    "title": "Participatory Budgeting with Donations and Diversity Constraints",
    "abstract": "Participatory budgeting (PB) is a democratic process where citizens jointly decide on how to allocate public funds to indivisible projects. In this work, we focus on PB processes where citizens may provide additional money to projects they want to see funded. We introduce a formal framework for this kind of PB with donations. Our framework also allows for diversity constraints, meaning that each project belongs to one or more types, and there are lower and upper bounds on the number of projects of the same type that can be funded. We propose three general classes of methods for aggregating the citizens’ preferences in the presence of donations and analyze their axiomatic properties. Furthermore, we investigate the computational complexity of determining the outcome of a PB process with donations and of finding a citizen’s optimal donation strategy",
    "volume": "main",
    "checked": true,
    "id": "99f50bfdf75144dc6ea1aab6093621171d1b6222",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21164": {
    "title": "Pretrained Cost Model for Distributed Constraint Optimization Problems",
    "abstract": "Distributed Constraint Optimization Problems (DCOPs) are an important subclass of combinatorial optimization problems, where information and controls are distributed among multiple autonomous agents. Previously, Machine Learning (ML) has been largely applied to solve combinatorial optimization problems by learning effective heuristics. However, existing ML-based heuristic methods are often not generalizable to different search algorithms. Most importantly, these methods usually require full knowledge about the problems to be solved, which are not suitable for distributed settings where centralization is not realistic due to geographical limitations or privacy concerns. To address the generality issue, we propose a novel directed acyclic graph representation schema for DCOPs and leverage the Graph Attention Networks (GATs) to embed graph representations. Our model, GAT-PCM, is then pretrained with optimally labelled data in an offline manner, so as to construct effective heuristics to boost a broad range of DCOP algorithms where evaluating the quality of a partial assignment is critical, such as local search or backtracking search. Furthermore, to enable decentralized model inference, we propose a distributed embedding schema of GAT-PCM where each agent exchanges only embedded vectors, and show its soundness and complexity. Finally, we demonstrate the effectiveness of our model by combining it with a local search or a backtracking search algorithm. Extensive empirical evaluations indicate that the GAT-PCM-boosted algorithms significantly outperform the state-of-the-art methods in various benchmarks",
    "volume": "main",
    "checked": true,
    "id": "5012ecfad59793a330ec2c6311234acf590667b9",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21165": {
    "title": "Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems",
    "abstract": "When dealing with a series of imminent issues, humans can naturally concentrate on a subset of these concerning issues by prioritizing them according to their contributions to motivational indices, e.g., the probability of winning a game. This idea of concentration offers insights into reinforcement learning of sophisticated Large-scale Multi-Agent Systems (LMAS) participated by hundreds of agents. In such an LMAS, each agent receives a long series of entity observations at each step, which can overwhelm existing aggregation networks such as graph attention networks and cause inefficiency. In this paper, we propose a concentration network called ConcNet. First, ConcNet scores the observed entities considering several motivational indices, e.g., expected survival time and state value of the agents, and then ranks, prunes, and aggregates the encodings of observed entities to extract features. Second, distinct from the well-known attention mechanism, ConcNet has a unique motivational subnetwork to explicitly consider the motivational indices when scoring the observed entities. Furthermore, we present a concentration policy gradient architecture that can learn effective policies in LMAS from scratch. Extensive experiments demonstrate that the presented architecture has excellent scalability and flexibility, and significantly outperforms existing methods on LMAS benchmarks",
    "volume": "main",
    "checked": true,
    "id": "7bac30e24036a111eb0a20bf8e4a9cef3cb79559",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21166": {
    "title": "Cooperative Multi-Agent Fairness and Equivariant Policies",
    "abstract": "We study fairness through the lens of cooperative multi-agent learning. Our work is motivated by empirical evidence that naive maximization of team reward yields unfair outcomes for individual team members. To address fairness in multi-agent contexts, we introduce team fairness, a group-based fairness measure for multi-agent learning. We then prove that it is possible to enforce team fairness during policy optimization by transforming the team's joint policy into an equivariant map. We refer to our multi-agent learning strategy as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness empirically. We then introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of Fair-E and show that it reaches higher levels of utility than Fair-E and fairer outcomes than non-equivariant policies. Finally, we present novel findings regarding the fairness-utility trade-off in multi-agent settings; showing that the magnitude of the trade-off is dependent on agent skill",
    "volume": "main",
    "checked": true,
    "id": "f740690d5044901b959f93659999355f6a6825fc",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21167": {
    "title": "Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs",
    "abstract": "Active Directory is the default security management system for Windows domain networks. We study the shortest path edge interdiction problem for defending Active Directory style attack graphs. The problem is formulated as a Stackelberg game between one defender and one attacker. The attack graph contains one destination node and multiple entry nodes. The attacker's entry node is chosen by nature. The defender chooses to block a set of edges limited by his budget. The attacker then picks the shortest unblocked attack path. The defender aims to maximize the expected shortest path length for the attacker, where the expectation is taken over entry nodes.We observe that practical Active Directory attack graphs have small maximum attack path length and are structurally close to trees. We first show that even if the maximum attack path length is a constant, the problem is still w[1]-hard with respect to the defender's budget. Having a small maximum attack path length and a small budget is not enough to design fixed-parameter algorithms. If we further assume that the number of entry nodes is small, then we derive a fixed-parameter tractable algorithm.We then propose two other fixed-parameter algorithms by exploiting the tree-like features. One is based on tree decomposition and requires a small tree width. The other assumes a small number of splitting nodes (nodes with multiple out-going edges). Finally, the last algorithm is converted into a graph convolutional neural network based heuristic, which scales to larger graphs with more splitting nodes",
    "volume": "main",
    "checked": true,
    "id": "e968c08085aa917586620521249ed0683065c6ce",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21168": {
    "title": "Anytime Multi-Agent Path Finding via Machine Learning-Guided Large Neighborhood Search",
    "abstract": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths for a team of agents in a common environment. MAPF is NP-hard to solve optimally and, in some cases, also bounded-suboptimally. It is thus time-consuming for (bounded-sub)optimal solvers to solve large MAPF instances. Anytime algorithms find solutions quickly for large instances and then improve them to close-to-optimal ones over time. In this paper, we improve the current state-of-the-art anytime solver MAPF-LNS, that first finds an initial solution fast and then repeatedly replans the paths of subsets of agents via Large Neighborhood Search (LNS). It generates the subsets of agents for replanning by randomized destroy heuristics, but not all of them increase the solution quality substantially. We propose to use machine learning to learn how to select a subset of agents from a collection of subsets, such that replanning increases the solution quality more. We show experimentally that our solver, MAPF-ML-LNS, significantly outperforms MAPF-LNS on the standard MAPF benchmark set in terms of both the speed of improving the solution and the final solution quality",
    "volume": "main",
    "checked": true,
    "id": "81e43d43c6ae0fb9696b39c2d29cc1987a2b58c8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21169": {
    "title": "MDPGT: Momentum-Based Decentralized Policy Gradient Tracking",
    "abstract": "We propose a novel policy gradient method for multi-agent reinforcement learning, which leverages two different variance-reduction techniques and does not require large batches over iterations. Specifically, we propose a momentum-based decentralized policy gradient tracking (MDPGT) where a new momentum-based variance reduction technique is used to approximate the local policy gradient surrogate with importance sampling, and an intermediate parameter is adopted to track two consecutive policy gradient surrogates. MDPGT provably achieves the best available sample complexity of O(N -1 e -3) for converging to an e-stationary point of the global average of N local performance functions (possibly nonconcave). This outperforms the state-of-the-art sample complexity in decentralized model-free reinforcement learning and when initialized with a single trajectory, the sample complexity matches those obtained by the existing decentralized policy gradient methods. We further validate the theoretical claim for the Gaussian policy function. When the required error tolerance e is small enough, MDPGT leads to a linear speed up, which has been previously established in decentralized stochastic optimization, but not for reinforcement learning. Lastly, we provide empirical results on a multi-agent reinforcement learning benchmark environment to support our theoretical findings",
    "volume": "main",
    "checked": true,
    "id": "b8a34431cbe4d374a49f993e68337e7c92533839",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21170": {
    "title": "Shard Systems: Scalable, Robust and Persistent Multi-Agent Path Finding with Performance Guarantees",
    "abstract": "Modern multi-agent robotic systems increasingly require scalable, robust and persistent Multi-Agent Path Finding (MAPF) with performance guarantees. While many MAPF solvers that provide some of these properties exist, none provides them all. To fill this need, we propose a new MAPF framework, the shard system. A shard system partitions the workspace into geographic regions, called shards, linked by a novel system of buffers. Agents are routed optimally within a shard by a local controller to local goals set by a global controller. The buffer system novelly allows shards to plan with perfect parallelism, providing scalability. A novel global controller algorithm can rapidly generate an inter-shard routing plan for thousands of agents while minimizing the traffic routed through any shard. A novel workspace partitioning algorithm produces shards small enough to replan rapidly. These innovations allow a shard system to adjust its routing plan in real time if an agent is delayed or assigned a new goal, enabling robust, persistent MAPF. A shard system's local optimality and optimized inter-shard routing bring the sum-of-costs of its solutions to single-shot MAPF problems to < 20-60% of optimal on a diversity of workspaces. Its scalability allows it to plan paths for 1000s of agents in seconds. If any of their goals change or move actions fails, a shard system can replan in under a second",
    "volume": "main",
    "checked": true,
    "id": "d3bb2a4451c8ff90c7a5dced062b21cfec641421",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21171": {
    "title": "A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning",
    "abstract": "Centralized Training for Decentralized Execution, where training is done in a centralized offline fashion, has become a popular solution paradigm in Multi-Agent Reinforcement Learning.  Many such methods take the form of actor-critic with state-based critics, since centralized training allows access to the true system state, which can be useful during training despite not being available at execution time.  State-based critics have become a common empirical choice, albeit one which has had limited theoretical justification or analysis.  In this paper, we show that state-based critics can introduce bias in the policy gradient estimates, potentially undermining the asymptotic guarantees of the algorithm.  We also show that, even if the state-based critics do not introduce any bias, they can still result in a larger gradient variance, contrary to the common intuition.  Finally, we show the effects of the theories in practice by comparing different forms of centralized critics on a wide range of common benchmarks, and detail how various environmental properties are related to the effectiveness of different types of critics",
    "volume": "main",
    "checked": true,
    "id": "2fdd6e9f639fee9cf722a1a35b0cf8b306e59310",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21172": {
    "title": "When Can the Defender Effectively Deceive Attackers in Security Games?",
    "abstract": "This paper studies defender patrol deception in general Stackelberg security games (SSGs), where a defender attempts to alter the attacker's perception of the defender's patrolling intensity so as to influence the attacker's decision making. We are interested in understanding the complexity and effectiveness of optimal defender deception under different attacker behavior models. Specifically, we consider three different attacker strategies of response (to the defender's deception) with increasing sophistication, and design efficient polynomial-time algorithms to compute the equilibrium for each. Moreover, we prove formal separation for the effectiveness of patrol deception when facing an attacker of increasing sophistication, until it becomes even harmful to the defender when facing the most intelligent attacker we consider. Our results shed light on when and how deception should be used in SSGs. We conduct extensive experiments to illustrate our theoretical results in various game settings",
    "volume": "main",
    "checked": true,
    "id": "fb1d06a595cca5eced44b69abcb060ac4341aa68",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21173": {
    "title": "Generalization in Mean Field Games by Learning Master Policies",
    "abstract": "Mean Field Games (MFGs) can potentially scale multi-agent systems to extremely large populations of agents. Yet, most of the literature assumes a single initial distribution for the agents, which limits the practical applications of MFGs. Machine Learning has the potential to solve a wider diversity of MFG problems thanks to generalizations capacities. We study how to leverage these generalization properties to learn policies enabling a typical agent to behave optimally against any population distribution. In reference to the Master equation in MFGs, we coin the term \"Master policies\" to describe them and we prove that a single Master policy provides a Nash equilibrium, whatever the initial distribution. We propose a method to learn such Master policies. Our approach relies on three ingredients: adding the current population distribution as part of the observation, approximating Master policies with neural networks, and training via Reinforcement Learning and Fictitious Play. We illustrate on numerical examples not only the efficiency of the learned Master policy but also its generalization capabilities beyond the distributions used for training",
    "volume": "main",
    "checked": true,
    "id": "51e1f805fa2b04d7a7755eac55ac5086385e52c0",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21174": {
    "title": "Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems: Complexity, Special Case Algorithms and Heuristics",
    "abstract": "Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial-time algorithm for approximating a solution to this problem to within the factor n^(1 - epsilon) for any constant epsilon > 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to address the problem for networks of reasonable sizes. For solving the problem on larger networks, we propose a general heuristic framework along with greedy selection methods. Extensive experimental results on real-world networks demonstrate the effectiveness of the proposed heuristics. A full version of the manuscript, source code and data are  available at: https://github.com/bridgelessqiu/NMIN-FPE",
    "volume": "main",
    "checked": true,
    "id": "551cde23cafa0c926f8ae067a14dfeacfb3aa488",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21175": {
    "title": "How Many Representatives Do We Need? The Optimal Size of a Congress Voting on Binary Issues",
    "abstract": "Aggregating opinions of a collection of agents is a question of interest to a broad array of researchers, ranging from ensemble-learning theorists to political scientists designing democratic institutions. This work investigates the optimal number of agents needed to decide on a binary issue under majority rule. We take an epistemic view where the issue at hand has a ground truth ``correct'' outcome and each one of n voters votes correctly with a fixed probability, known as their competence level or competence. These competencies come from a fixed distribution D. Observing the competencies, we must choose a specific group that will represent the population. Finally, voters sample a decision (either correct or not), and the group is correct as long as more than half the chosen representatives voted correctly. Assuming that we can identify the best experts, i.e., those with the highest competence, to form an epistemic congress we find that the optimal congress size should be linear in the population size. This result is striking because it holds even when allowing the top representatives to become arbitrarily accurate, choosing the correct outcome with probabilities approaching 1. We then analyze real-world data, observing that the actual sizes of representative bodies are much smaller than the optimal ones our theoretical results suggest. We conclude by examining under what conditions congresses of sub-optimal sizes would still outperform direct democracy, in which all voters vote. We find that a small congress would beat direct democracy if the rate at which the societal bias towards the ground truth decreases with the population size fast enough, and we quantify the speed needed for constant and polynomial congress sizes",
    "volume": "main",
    "checked": true,
    "id": "26d8ef467f834004ca6cbb59f03f1b13168d604d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21176": {
    "title": "Decentralized Mean Field Games",
    "abstract": "Multiagent reinforcement learning algorithms have not been widely adopted in large scale environments with many agents as they often scale poorly with the number of agents. Using mean field theory to aggregate agents has been proposed as a solution to this problem. However, almost all previous methods in this area make a strong assumption of a centralized system where all the agents in the environment learn the same policy and are effectively indistinguishable from each other. In this paper, we relax this assumption about indistinguishable agents and propose a new mean field system known as Decentralized Mean Field Games, where each agent can be quite different from others. All agents learn independent policies in a decentralized fashion, based on their local observations. We define a theoretical solution concept for this system and provide a fixed point guarantee for a Q-learning based algorithm in this system. A practical consequence of our approach is that we can address a `chicken-and-egg' problem in empirical mean field reinforcement learning algorithms. Further, we provide Q-learning and actor-critic algorithms that use the decentralized mean field learning approach and give stronger performances compared to common baselines in this area. In our setting, agents do not need to be clones of each other and learn in a fully decentralized fashion. Hence, for the first time, we show the application of mean field learning methods in fully competitive environments, large-scale continuous action space environments, and other environments with heterogeneous agents. Importantly, we also apply the mean field method in a ride-sharing problem using a real-world dataset. We propose a decentralized solution to this problem, which is more practical than existing centralized training methods",
    "volume": "main",
    "checked": true,
    "id": "349df5e4a33588057bb4dedb3ff6dbef039d7262",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21177": {
    "title": "Incentivizing Collaboration in Machine Learning via Synthetic Data Rewards",
    "abstract": "This paper presents a novel collaborative generative modeling (CGM) framework that incentivizes collaboration among self-interested parties to contribute data to a pool for training a generative model (e.g., GAN), from which synthetic data are drawn and distributed to the parties as rewards commensurate to their contributions. Distributing synthetic data as rewards (instead of trained models or money) offers task- and model-agnostic benefits for downstream learning tasks and is less likely to violate data privacy regulation. To realize the framework, we firstly propose a data valuation function using maximum mean discrepancy (MMD) that values data based on its quantity and quality in terms of its closeness to the true data distribution and provide theoretical results guiding the kernel choice in our MMD-based data valuation function. Then, we formulate the reward scheme as a linear optimization problem that when solved, guarantees certain incentives such as fairness in the CGM framework. We devise a weighted sampling algorithm for generating synthetic data to be distributed to each party as reward such that the value of its data and the synthetic data combined matches its assigned reward value by the reward scheme. We empirically show using simulated and real-world datasets that the parties' synthetic data rewards are commensurate to their contributions",
    "volume": "main",
    "checked": true,
    "id": "8f181c54d3d7ca79218207471f00c368392b76d1",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21178": {
    "title": "Learning the Optimal Recommendation from Explorative Users",
    "abstract": "We propose a new problem setting to study the sequential interactions between a recommender system and a user. Instead of assuming the user is omniscient, static, and explicit, as the classical practice does, we sketch a more realistic user behavior model, under which the user: 1) rejects recommendations if they are clearly worse than others; 2) updates her utility estimation based on rewards from her accepted recommendations; 3) withholds realized rewards from the system. We formulate the interactions between the system and such an explorative user in a K-armed bandit framework and study the problem of learning the optimal recommendation on the system side. We show that efficient system learning is still possible but is more difficult. In particular, the system can identify the best arm with probability at least 1-delta within O(1/delta) interactions, and we prove this is tight. Our finding contrasts the result for the problem of best arm identification with fixed confidence, in which the best arm can be identified with probability 1-delta within O(log(1/delta)) interactions. This gap illustrates the inevitable cost the system has to pay when it learns from an explorative user's revealed preferences on its recommendations rather than from the realized rewards",
    "volume": "main",
    "checked": true,
    "id": "c2a8c6c03041222274799be59cb7ade30a68193a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21179": {
    "title": "Multi-Agent Incentive Communication via Decentralized Teammate Modeling",
    "abstract": "Effective communication can improve coordination in cooperative multi-agent reinforcement learning (MARL). One popular communication scheme is exchanging agents' local observations or latent embeddings and using them to augment individual local policy input. Such a communication paradigm can reduce uncertainty for local decision-making and induce implicit coordination. However, it enlarges agents' local policy spaces and increases learning complexity, leading to poor coordination in complex settings. To handle this limitation, this paper proposes a novel framework named Multi-Agent Incentive Communication (MAIC) that allows each agent to learn to generate incentive messages and bias other agents' value functions directly, resulting in effective explicit coordination. Our method firstly learns targeted teammate models, with which each agent can anticipate the teammate's action selection and generate tailored messages to specific agents. We further introduce a novel regularization to leverage interaction sparsity and improve communication efficiency. MAIC is agnostic to specific MARL algorithms and can be flexibly integrated with different value function factorization methods. Empirical results demonstrate that our method significantly outperforms baselines and achieves excellent performance on multiple cooperative MARL tasks",
    "volume": "main",
    "checked": true,
    "id": "fc8e507b1ff14de57157c6c5c730247372f961f0",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21180": {
    "title": "MLink: Linking Black-Box Models for Collaborative Multi-Model Inference",
    "abstract": "The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g. smart city), we need inference results of multiple ML models, but the cost budget (e.g. GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking. Model linking aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. Based on model links, we developed a scheduling algorithm, named MLink. Through collaborative multi-model inference enabled by model links, MLink can improve the accuracy of obtained inference results under the cost budget. We evaluated MLink on a multi-modal dataset with seven different ML models and two real-world video analytics systems with six ML models and 3,264 hours of video. Experimental results show that our proposed model links can be effectively built among various black-box models. Under the budget of GPU memory, MLink can save 66.7% inference computations while preserving 94% inference accuracy, which outperforms multi-task learning, deep reinforcement learning-based scheduler and frame filtering baselines",
    "volume": "main",
    "checked": true,
    "id": "89ce6565222c245826b454ca19050623fd1c5d0b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21181": {
    "title": "Equilibrium Finding in Normal-Form Games via Greedy Regret Minimization",
    "abstract": "We extend the classic regret minimization framework for approximating equilibria in normal-form games by greedily weighing iterates based on regrets observed at runtime. Theoretically, our method retains all previous convergence rate guarantees. Empirically, experiments on large randomly generated games and normal-form subgames of the AI benchmark Diplomacy show that greedy weights outperforms previous methods whenever sampling is used, sometimes by several orders of magnitude",
    "volume": "main",
    "checked": true,
    "id": "ece043304f2b608d776d3cd695a58d8ffc7a80a1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21182": {
    "title": "Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for Introduced Unfairness",
    "abstract": "In addition to reproducing discriminatory relationships in the training data, machine learning (ML) systems can also introduce or amplify discriminatory effects. We refer to this as introduced unfairness, and investigate the conditions under which it may arise. To this end, we propose introduced total variation as a measure of introduced unfairness, and establish graphical conditions under which it may be incentivised to occur. These criteria imply that adding the sensitive attribute as a feature removes the incentive for introduced variation under well-behaved loss functions. Additionally, taking a causal perspective, introduced path-specific effects shed light on the issue of when specific paths should be considered fair",
    "volume": "main",
    "checked": true,
    "id": "5bde6e5180841063adb15b0d041d7e5858df399e",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21183": {
    "title": "Incorporating Item Frequency for Differentially Private Set Union",
    "abstract": "We study the problem of releasing the set union of users' items subject to differential privacy. Previous approaches consider only the set of items for each user as the input. We propose incorporating the item frequency, which is typically available in set union problems, to boost the utility of private mechanisms. However, using the global item frequency over all users would largely increase privacy loss. We propose to use the local item frequency of each user to approximate the global item frequency without incurring additional privacy loss.    Local item frequency allows us to design greedy set union mechanisms that are differentially private, which is impossible for previous greedy proposals. Moreover, while all previous works have to use uniform sampling to limit the number of items each user would contribute to, our construction eliminates the sampling step completely and allows our mechanisms to consider all of the users' items.    Finally, we propose to transfer the knowledge of the global item frequency from a public dataset into our mechanism, which further boosts utility even when the public and private datasets are from different domains. We evaluate the proposed methods on multiple real-life datasets",
    "volume": "main",
    "checked": true,
    "id": "9fe931de36bb4bb83297de4edaa29d233044cae4",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21184": {
    "title": "Cosine Model Watermarking against Ensemble Distillation",
    "abstract": "Many model watermarking methods have been developed to prevent valuable deployed commercial models from being stealthily stolen by model distillations.   However, watermarks produced by most existing model watermarking methods can be easily evaded by ensemble distillation, because averaging the outputs of multiple ensembled models can significantly reduce or even erase the watermarks.   In this paper, we focus on tackling the challenging task of defending against ensemble distillation.   We propose a novel watermarking technique named CosWM to achieve outstanding model watermarking performance against ensemble distillation.   CosWM is not only elegant in design, but also comes with desirable theoretical guarantees.   Our extensive experiments on public data sets demonstrate the excellent performance of CosWM and its advantages over the state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "83a144e7425ee02c66f187e3e531c526620f310f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21185": {
    "title": "Towards Debiasing DNN Models from Spurious Feature Influence",
    "abstract": "Recent studies indicate that deep neural networks (DNNs) are prone to show discrimination towards certain demographic groups. We observe that algorithmic discrimination can be explained by the high reliance of the models on fairness sensitive features. Motivated by this observation, we propose to achieve fairness by suppressing the DNN models from capturing the spurious correlation between those fairness sensitive features with the underlying task. Specifically, we firstly train a bias-only teacher model which is explicitly encouraged to maximally employ fairness sensitive features for prediction. The teacher model then counter-teaches a debiased student model so that the interpretation of the student model is orthogonal to the interpretation of the teacher model. The key idea is that since the teacher model relies explicitly on fairness sensitive features for prediction, the orthogonal interpretation loss enforces the student network to reduce its reliance on sensitive features and instead capture more task relevant features for prediction. Experimental analysis indicates that our framework substantially reduces the model's attention on fairness sensitive features. Experimental results on four datasets further validate that our framework has consistently improved the fairness with respect to three group fairness metrics, with a comparable or even better accuracy",
    "volume": "main",
    "checked": true,
    "id": "4510c8e4e13fe1ab99c600ead23d2da0c00de04a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21186": {
    "title": "Path-Specific Objectives for Safer Agent Incentives",
    "abstract": "We present a general framework for training safe agents whose naive incentives are unsafe. As an example, manipulative or deceptive behaviour can improve rewards but should be avoided. Most approaches fail here: agents maximize expected return by any means necessary. We formally describe settings with `delicate' parts of the state which should not be used as a means to an end. We then train agents to maximize the causal effect of actions on the expected return which is not mediated by the delicate parts of state, using Causal Influence Diagram analysis. The resulting agents have no incentive to control the delicate state. We further show how our framework unifies and generalizes existing proposals",
    "volume": "main",
    "checked": true,
    "id": "6d624cb1e7126e3245d55c7208ef6f04141e5895",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21187": {
    "title": "Algorithmic Fairness Verification with Graphical Models",
    "abstract": "In recent years, machine learning (ML) algorithms have been deployed in safety-critical and high-stake decision-making, where the fairness of algorithms is of paramount importance. Fairness in ML centers on detecting bias towards certain demographic populations induced by an ML classifier and proposes algorithmic solutions to mitigate the bias with respect to different fairness definitions. To this end, several fairness verifiers have been proposed that compute the bias in the prediction of an ML classifier—essentially beyond a finite dataset—given the probability distribution of input features. In the context of verifying linear classifiers, existing fairness verifiers are limited by accuracy due to imprecise modeling of correlations among features and scalability due to restrictive formulations of the classifiers as SSAT/SMT formulas or by sampling.   In this paper, we propose an efficient fairness verifier, called FVGM, that encodes the correlations among features as a Bayesian network. In contrast to existing verifiers, FVGM proposes a stochastic subset-sum based approach for verifying linear classifiers. Experimentally, we show that FVGM leads to an accurate and scalable assessment for more diverse families of fairness-enhancing algorithms, fairness attacks, and group/causal fairness metrics than the state-of-the-art fairness verifiers. We also demonstrate that FVGM facilitates the computation of fairness influence functions as a stepping stone to detect the source of bias induced by subsets of features",
    "volume": "main",
    "checked": true,
    "id": "9d4c4638bd37007218a3d02121e633e1fb92bd4a",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21188": {
    "title": "Achieving Long-Term Fairness in Sequential Decision Making",
    "abstract": "In this paper, we propose a framework for achieving long-term fair sequential decision making. By conducting both the hard and soft interventions, we propose to take path-specific effects on the time-lagged causal graph as a quantitative tool for measuring long-term fairness. The problem of fair sequential decision making is then formulated as a constrained optimization problem with the utility as the objective and the long-term and short-term fairness as constraints. We show that such an optimization problem can be converted to a performative risk optimization. Finally, repeated risk minimization (RRM) is used for model training, and the convergence of RRM is theoretically analyzed. The empirical evaluation shows the effectiveness of the proposed algorithm on synthetic and semi-synthetic temporal datasets",
    "volume": "main",
    "checked": true,
    "id": "36c6a7fcf4917c1fc345b1a3540acef1309a36ba",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21189": {
    "title": "Fairness without Imputation: A Decision Tree Approach for Fair Prediction with Missing Values",
    "abstract": "We investigate the fairness concerns of training a machine learning model using data with missing values. Even though there are a number of fairness intervention methods in the literature, most of them require a complete training set as input. In practice, data can have missing values, and data missing patterns can depend on group attributes (e.g. gender or race). Simply applying off-the-shelf fair learning algorithms to an imputed dataset may lead to an unfair model. In this paper, we first theoretically analyze different sources of discrimination risks when training with an imputed dataset. Then, we propose an integrated approach based on decision trees that does not require a separate process of imputation and learning. Instead, we train a tree with missing incorporated as attribute (MIA), which does not require explicit imputation, and we optimize a fairness-regularized objective function. We demonstrate that our approach outperforms existing fairness intervention methods applied to an imputed dataset, through several experiments on real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "2220357eda604c56232949a3b18ad47075db2554",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21190": {
    "title": "Shaping Noise for Robust Attributions in Neural Stochastic Differential Equations",
    "abstract": "Neural SDEs with Brownian motion as noise lead to smoother attributions than traditional ResNets. Various attribution methods such as saliency maps, integrated gradients, DeepSHAP and DeepLIFT have been shown to be more robust for neural SDEs than for ResNets using the recently proposed sensitivity metric. In this paper, we show that neural SDEs with adaptive attribution-driven noise lead to even more robust attributions and smaller sensitivity metrics than traditional neural SDEs with Brownian motion as noise. In particular, attribution-driven shaping of noise leads to 6.7%, 6.9% and 19.4% smaller sensitivity metric for integrated gradients computed on three discrete approximations of neural SDEs with standard Brownian motion noise: stochastic ResNet-50, WideResNet-101 and ResNeXt-101 models respectively. The neural SDE model with adaptive attribution-driven noise leads to 25.7% and 4.8% improvement in the SIC metric over traditional ResNets and Neural SDEs with Brownian motion as noise. To the best of our knowledge, we are the first to propose the use of attributions for shaping the noise injected in neural SDEs, and demonstrate that this process leads to more robust attributions than traditional neural SDEs with standard Brownian motion as noise",
    "volume": "main",
    "checked": true,
    "id": "876a63ab3da9b93cc15e70e34049b5f2e14050a1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21191": {
    "title": "Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks",
    "abstract": "Data poisoning attacks and backdoor attacks aim to corrupt a machine learning classifier via modifying, adding, and/or removing some carefully selected training examples, such that the corrupted classifier makes incorrect predictions as the attacker desires. The key idea of state-of-the-art certified defenses against data poisoning attacks and backdoor attacks is to create a majority vote mechanism to predict the label of a testing example. Moreover, each voter is a base classifier trained on a subset of the training dataset. Classical simple learning algorithms such as k nearest neighbors (kNN) and radius nearest neighbors (rNN) have intrinsic majority vote mechanisms. In this work, we show that the intrinsic majority vote mechanisms in kNN and rNN already provide certified robustness guarantees against data poisoning attacks and backdoor attacks. Moreover, our evaluation results on MNIST and CIFAR10 show that the intrinsic certified robustness guarantees of kNN and rNN outperform those provided by state-of-the-art certified defenses. Our results serve as standard baselines for future certified defenses against data poisoning attacks and backdoor attacks",
    "volume": "main",
    "checked": true,
    "id": "cfdae0661a917d8d06a32ab52b2ffac73637b616",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21192": {
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "abstract": "Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which—unlike prior work on equalising the average group-wise distance from the decision boundary—explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier",
    "volume": "main",
    "checked": true,
    "id": "6b0a1a69ba65618a104b421cd434be08cca9a8eb",
    "citation_count": 21
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21193": {
    "title": "DeepAuth: A DNN Authentication Framework by Model-Unique and Fragile Signature Embedding",
    "abstract": "Along with the evolution of deep neural networks (DNNs) in many real-world applications, the complexity of model building has also dramatically increased. Therefore, it is vital to protect the intellectual property (IP) of the model builder and ensure the trustworthiness of the deployed models. Meanwhile, adversarial attacks on DNNs (e.g., backdoor and poisoning attacks) that seek to inject malicious behaviors have been investigated recently, demanding a means for verifying the integrity of the deployed model to protect the users. This paper presents a novel DNN authentication framework DeepAuth that embeds a unique and fragile signature to each protected DNN model. Our approach exploits sensitive key samples that are well crafted from the input space to latent space and then to logit space for producing signatures. After embedding, each model will respond distinctively to these key samples, which creates a model-unique signature as a strong tool for authentication and user identity. The signature embedding process is also designed to ensure the fragility of the signature, which can be used to detect malicious modifications such that an illegitimate user or an altered model should not have the intact signature. Extensive evaluations on various models over a wide range of datasets demonstrate the effectiveness and efficiency of the proposed DeepAuth",
    "volume": "main",
    "checked": true,
    "id": "b94924311f9b0945f50e3881ac355a89b1141464",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21194": {
    "title": "Fast Sparse Decision Tree Optimization via Reference Ensembles",
    "abstract": "Sparse decision tree optimization has been one of the most fundamental problems in AI since its inception and is a challenge at the core of interpretable machine learning. Sparse decision tree optimization is computationally hard, and despite steady effort since the 1960's, breakthroughs have been made on the problem only within the past few years, primarily on the problem of finding optimal sparse decision trees. However, current state-of-the-art algorithms often require impractical amounts of computation time and memory to find optimal or near-optimal trees for some real-world datasets, particularly those having several continuous-valued features. Given that the search spaces of these decision tree optimization problems are massive, can we practically hope to find a sparse decision tree that competes in accuracy with a black box machine learning model? We address this problem via smart guessing strategies that can be applied to any optimal branch-and-bound-based decision tree algorithm. The guesses come from knowledge gleaned from black box models. We show that by using these guesses, we can reduce the run time by multiple orders of magnitude while providing bounds on how far the resulting trees can deviate from the black box's accuracy and expressive power. Our approach enables guesses about how to bin continuous features, the size of the tree, and lower bounds on the error for the optimal decision tree. Our experiments show that in many cases we can rapidly construct sparse decision trees that match the accuracy of black box models. To summarize: when you are having trouble optimizing, just guess",
    "volume": "main",
    "checked": true,
    "id": "b563aa48719abbafa19ba4169a96ecc7be4aa1f6",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21195": {
    "title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-Box Model Explanation",
    "abstract": "We aim to explain a black-box classifier with the form: \"data X is classified as class Y because X has A, B and does not have C\" in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts. Finally, using multiple datasets, we demonstrate that the proposed method can discover useful concepts for explanation in this form",
    "volume": "main",
    "checked": true,
    "id": "5697da09cdfd3ce7768b126ced6b5a0b82e4e884",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21196": {
    "title": "Do Feature Attribution Methods Correctly Attribute Features?",
    "abstract": "Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of \"attribution\", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/",
    "volume": "main",
    "checked": true,
    "id": "426734685283b4a0c08b34cd9e996e2e30e7f7ee",
    "citation_count": 36
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21197": {
    "title": "Formal Semantics and Formally Verified Validation for Temporal Planning",
    "abstract": "We present a simple and concise semantics for temporal planning. Our semantics are developed and formalised in the logic of the interactive theorem prover Isabelle/HOL. We derive from those semantics a validation algorithm for temporal planning and show, using a formal proof in Isabelle/HOL, that this validation algorithm implements our semantics. We experimentally evaluate our verified validation algorithm and show that it is practical",
    "volume": "main",
    "checked": true,
    "id": "464851deaa2ff15a76e782fefc87ef2be4404dac",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21198": {
    "title": "Goal Recognition as Reinforcement Learning",
    "abstract": "Most approaches for goal recognition rely on specifications of the possible dynamics of the actor in the environment when pursuing a goal. These specifications suffer from two key issues. First, encoding these dynamics requires careful design by a domain expert, which is often not robust to noise at recognition time. Second, existing approaches often need costly real-time computations to reason about the likelihood of each potential goal. In this paper, we develop a framework that combines model-free reinforcement learning and goal recognition to alleviate the need for careful, manual domain design, and the need for costly online executions. This framework consists of two main stages: Offline learning of policies or utility functions for each potential goal, and online inference. We provide a first instance of this framework using tabular Q-learning for the learning stage, as well as three measures that can be used to perform the inference stage. The resulting instantiation achieves state-of-the-art performance against goal recognizers on standard evaluation domains and superior performance in noisy environments",
    "volume": "main",
    "checked": true,
    "id": "412b6cb8c965474a2d98ea7e4856a4463d17c429",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21199": {
    "title": "Online Search with Best-Price and Query-Based Predictions",
    "abstract": "In the online (time-series) search problem, a player is presented with a sequence of prices which are revealed in an online manner. In the standard definition of the problem, for each revealed price, the player must decide irrevocably whether to accept or reject it, without knowledge of future prices (other than an upper and a lower bound on their extreme values), and the objective is to minimize the competitive ratio, namely the worst case ratio between the maximum price in the sequence and the one selected by the player. The problem formulates several applications of decision-making in the face of uncertainty on the revealed samples.Previous work on this problem has largely assumed extreme scenarios in which either the player has almost no information about the input, or the player is provided with some powerful, and error-free advice. In this work, we study learning-augmented algorithms, in which there is a potentially erroneous prediction concerning the input. Specifically, we consider two different settings: the setting in which the prediction is related to the maximum price in the sequence, as well as well as the setting in which the prediction is obtained as a response to a number of binary queries. For both settings, we provide tight, or near-tight upper and lower bounds on the worst-case performance of search algorithms as a function of the prediction error. We also provide experimental results on data obtained from stock exchange markets that confirm the theoretical analysis, and explain how our techniques can be applicable to other learning-augmented applications",
    "volume": "main",
    "checked": true,
    "id": "fe4fbe8a56c7b5c5c2663ee37419c90380ddffd8",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21200": {
    "title": "Extended Goal Recognition Design with First-Order Computation Tree Logic",
    "abstract": "Goal recognition design (GRD) is the task of modifying environments for aiding observers to recognize the objectives of agents during online observations. The worst case distinctiveness (WCD), a widely used performance measure in GRD research, can fail to provide useful guidance to the redesign process when some goals are too hard to be distinguished. Moreover, the existing WCD-based approaches do not work when an agent aims for a sequence of goals instead of just one goal. The paper presents a new GRD framework called extended goal recognition design (EGRD) for goal recognition that involves multiple goals. The objective of EGRD is to modify an environment to minimize the worst case distinctiveness of a goal condition that describes how an agent can reach a set of goals. A goal condition can be formally expressed in first-order computation tree logic (FO-CTL) that can be evaluated by model checking. We introduce a novel graphical representation of FO-CTL sentences that is suitable for extended goal recognition. Moreover, we present a search algorithm for EGRD with a novel caching mechanism. Our experimental results show that the caching mechanism can greatly speed up our EGRD search algorithm by reusing the previous evaluation of FO-CTL sentences",
    "volume": "main",
    "checked": true,
    "id": "d8a8791133ed1b7d3ee4ce885d6eb661b235126e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21201": {
    "title": "Sampling-Based Robust Control of Autonomous Systems with Non-Gaussian Noise",
    "abstract": "Controllers for autonomous systems that operate in safety-critical settings must account for stochastic disturbances. Such disturbances are often modeled as process noise, and common assumptions are that the underlying distributions are known and/or Gaussian. In practice, however, these assumptions may be unrealistic and can lead to poor approximations of the true noise distribution. We present a novel planning method that does not rely on any explicit representation of the noise distributions. In particular, we address the problem of computing a controller that provides probabilistic guarantees on safely reaching a target. First, we abstract the continuous system into a discrete-state model that captures noise by probabilistic transitions between states. As a key contribution, we adapt tools from the scenario approach to compute probably approximately correct (PAC) bounds on these transition probabilities, based on a finite number of samples of the noise. We capture these bounds in the transition probability intervals of a so-called interval Markov decision process (iMDP). This iMDP is robust against uncertainty in the transition probabilities, and the tightness of the probability intervals can be controlled through the number of samples. We use state-of-the-art verification techniques to provide guarantees on the iMDP, and compute a controller for which these guarantees carry over to the autonomous system. Realistic benchmarks show the practical applicability of our method, even when the iMDP has millions of states or transitions",
    "volume": "main",
    "checked": true,
    "id": "ebc8a76ccae6b28b60fec99455560fc6ee2ae965",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21202": {
    "title": "Synthesis from Satisficing and Temporal Goals",
    "abstract": "Reactive synthesis from high-level specifications that combine hard constraints expressed in Linear Temporal Logic (LTL) with soft constraints expressed by discounted sum (DS) rewards has applications in planning and reinforcement learning. An existing approach combines techniques from LTL synthesis with optimization for the DS rewards but has failed to yield a sound algorithm. An alternative approach combining LTL synthesis with satisficing DS rewards (rewards that achieve a threshold) is sound and complete for integer discount factors, but, in practice, a fractional discount factor is desired. This work extends the existing satisficing approach, presenting the first sound algorithm for synthesis from LTL and DS rewards with fractional discount factors. The utility of our algorithm is demonstrated on robotic planning domains",
    "volume": "main",
    "checked": true,
    "id": "a409ef38d9043b53881557dab68f93bef1dfabc3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21203": {
    "title": "Making Translations to Classical Planning Competitive with Other HTN Planners",
    "abstract": "Translation-based approaches to planning allow for solving problems in complex and expressive formalisms via the means of highly efficient solvers for simpler formalisms.  To be effective, these translations have to be constructed appropriately. The current existing translation of the highly expressive formalism of HTN planning into the more simple formalism of classical planning is not on par with the performance of current dedicated HTN planners. With our contributions in this paper, we close this gap: we describe new versions of the translation that reach the performance of state-of-the-art dedicated HTN planners. We present new translation techniques both for the special case of totally-ordered HTNs as well as for the general partially-ordered case. In the latter, we show that our new translation generates only linearly many actions, while the previous encoding generates and exponential number of actions",
    "volume": "main",
    "checked": true,
    "id": "d4c723aea136ce2164e626fbff58bbafa262c91d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21204": {
    "title": "PlanVerb: Domain-Independent Verbalization and Summary of Task Plans",
    "abstract": "For users to trust planning algorithms, they must be able to understand the planner's outputs and the reasons for each action selection. This output does not tend to be user-friendly, often consisting of sequences of parametrised actions or task networks. And these may not be practical for non-expert users who may find it easier to read natural language descriptions. In this paper, we propose PlanVerb, a domain and planner-independent method for the verbalization of task plans. It is based on semantic tagging of actions and predicates. Our method can generate natural language descriptions of plans including causal explanations. The verbalized plans can be summarized by compressing the actions that act on the same parameters. We further extend the concept of verbalization space, previously applied to robot navigation, and apply it to planning to generate different kinds of plan descriptions for different user requirements. Our method can deal with PDDL and RDDL domains, provided that they are tagged accordingly. Our user survey evaluation shows that users can read our automatically generated plan descriptions and that the explanations help them answer questions about the plan",
    "volume": "main",
    "checked": true,
    "id": "8c3498d55e606b4ab207ec83ae57774d80047054",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21205": {
    "title": "Competing for Resources: Estimating Adversary Strategy for Effective Plan Generation",
    "abstract": "Effective decision making while competing for limited resources in adversarial environments is important for many real-world applications (e.g. two Taxi companies competing for customers). Decision-making techniques such as Automated planning have to take into account possible actions of adversary (or competing) agents. That said, the agent should know what the competitor will likely do and then generate its plan accordingly. In this paper we propose a novel approach for estimating strategies of the adversary (or the competitor), sampling its actions that might hinder agent's goals by interfering with the agent's actions. The estimated competitor strategies are used in plan generation such that agent's actions have to be applied prior to the ones of the competitor, whose estimated times dictate the deadlines. We empirically evaluate our approach leveraging sampling of competitor's actions by comparing it to the naive approach optimising the make-span (not taking the competing agent into account at all) and to Nash Equilibrium (mixed) strategies",
    "volume": "main",
    "checked": true,
    "id": "4dda064b1c25cd774882cf7ac9c28e4d8b8e7d9b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21206": {
    "title": "The FF Heuristic for Lifted Classical Planning",
    "abstract": "Heuristics for lifted planning are not yet as informed as the best heuristics for ground planning. Recent work introduced the idea of using Datalog programs to compute the additive heuristic over lifted tasks. Based on this work, we show how to compute the more informed FF heuristic in a lifted manner. We extend the Datalog program with executable annotations that can also be used to define other delete-relaxation heuristics. In our experiments, we show that a planner using the lifted FF implementation produces state-of-the-art results for lifted planners. It also reduces the gap to state-of-the-art ground planners in domains where grounding is feasible",
    "volume": "main",
    "checked": true,
    "id": "68c9387811d3975cf506fc80ef909c865126f8ab",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21207": {
    "title": "Inconsistent Planning: When in Doubt, Toss a Coin!",
    "abstract": "One of the most widespread human behavioral biases is the present bias -- the tendency to overestimate current costs by a bias factor. Kleinberg and Oren (2014) introduced an elegant graph-theoretical model of inconsistent planning capturing the behavior of a present-biased agent accomplishing a set of actions. The essential measure of the system introduced by Kleinberg and Oren is the cost of irrationality -- the ratio of the total cost of the actions performed by the present-biased agent to the optimal cost. This measure is vital for a task designer to estimate the aftermaths of human behavior related to time-inconsistent planning, including procrastination and abandonment.    As we prove in this paper, the cost of irrationality is highly susceptible to the agent's choices when faced with a few possible actions of equal estimated costs. To address this issue, we propose a modification of Kleinberg-Oren's model of inconsistent planning. In our model, when an agent selects from several options of minimum prescribed cost, he uses a randomized procedure. We explore the algorithmic complexity of computing and estimating the cost of irrationality in the new model",
    "volume": "main",
    "checked": true,
    "id": "8d53489cd07aeef21f2770e47fbb834ca8d9c9c7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21208": {
    "title": "Robustification of Online Graph Exploration Methods",
    "abstract": "Exploring unknown environments is a fundamental task in many domains, e.g., robot navigation, network security, and internet search. We initiate the study of a learning-augmented variant of the classical, notoriously hard online graph exploration problem by adding access to machine-learned predictions. We propose an algorithm that naturally integrates predictions into the well-known Nearest Neighbor (NN) algorithm and significantly outperforms any known online algorithm if the prediction is of high accuracy while maintaining good guarantees when the prediction is of poor quality. We provide theoretical worst-case bounds that gracefully degrade with the prediction error, and we complement them by computational experiments that confirm our results. Further, we extend our concept to a general framework to robustify algorithms. By interpolating carefully between a given algorithm and NN, we prove new performance bounds that leverage the individual good performance on particular inputs while establishing robustness to arbitrary inputs",
    "volume": "main",
    "checked": true,
    "id": "669324705305e48e62105194fa927489e229dcde",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21209": {
    "title": "Explainable Planner Selection for Classical Planning",
    "abstract": "Since no classical planner consistently outperforms all others, it is important to select a planner that works well for a given classical planning task. The two strongest approaches for planner selection use image and graph convolutional neural networks. They have the drawback that the learned models are complicated and uninterpretable. To obtain explainable models, we identify a small set of simple task features and show that elementary and interpretable machine learning techniques can use these features to solve roughly as many tasks as the complex approaches based on neural networks",
    "volume": "main",
    "checked": true,
    "id": "8805ba0b92fe0be5903adec89926b0ca706d30b6",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21210": {
    "title": "Operator-Potential Heuristics for Symbolic Search",
    "abstract": "Symbolic search, using Binary Decision Diagrams (BDDs) to represent sets of states, is a competitive approach to optimal planning. Yet heuristic search in this context remains challenging. The many advances on admissible planning heuristics are not directly applicable, as they evaluate one state at a time. Indeed, progress using heuristic functions in symbolic search has been limited and even very informed heuristics have been shown to be detrimental. Here we show how this connection can be made stronger for LP-based potential heuristics. Our key observation is that, for this family of heuristic functions, the change of heuristic value induced by each operator can be precomputed. This facilitates their smooth integration into symbolic search. Our experiments show that this can pay off significantly: we establish a new state of the art in optimal symbolic planning",
    "volume": "main",
    "checked": true,
    "id": "94a3e7d9064adb177c3fb404e61193dfc25a4594",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21211": {
    "title": "Reconfiguring Shortest Paths in Graphs",
    "abstract": "Reconfiguring two shortest paths in a graph means modifying one shortest path to the other by changing one vertex at a time, so that all the intermediate paths are also shortest paths. This problem has several natural applications, namely: (a) revamping road networks, (b) rerouting data packets in a synchronous multiprocessing setting, (c) the shipping container stowage problem, and (d) the train marshalling problem.When modelled as graph problems, (a) is the most general case while (b), (c) and (d) are restrictions to different graph classes. We show that (a) is intractable, even for relaxed variants of the problem. For (b), (c) and (d), we present efficient algorithms to solve the respective problems. We also generalise the problem to when at most k (for some k >= 2) contiguous vertices on a shortest path can be changed at a time",
    "volume": "main",
    "checked": true,
    "id": "9a40abe73a1f1c45d6d1192a8c7b5358a62c79be",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21212": {
    "title": "Homomorphisms of Lifted Planning Tasks: The Case for Delete-Free Relaxation Heuristics",
    "abstract": "Classical planning tasks are modelled in PDDL which is a schematic language based on first-order logic. Most of the current planners turn this lifted representation into a propositional one via a grounding process. However, grounding may cause an exponential blowup. Therefore it is important to investigate methods for searching for plans on the lifted level. To build a lifted state-based planner, it is necessary to invent lifted heuristics. We introduce maps between PDDL tasks preserving plans allowing to transform a PDDL task into a smaller one. We propose a novel method for computing lifted (admissible) delete-free relaxed heuristics via grounding of the smaller task and computing the (admissible) delete-free relaxed heuristics there. This allows us to transfer the knowledge about relaxed heuristics from the grounded level to the lifted level",
    "volume": "main",
    "checked": true,
    "id": "b0ad5bf16cb43807489bfa297ee985f31bcabc67",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21213": {
    "title": "Speeding Up the RUL¯ Dynamic-Controllability-Checking Algorithm for Simple Temporal Networks with Uncertainty",
    "abstract": "A Simple Temporal Network with Uncertainty (STNU) includes real-valued variables, called time-points; binary difference constraints on those time-points; and contingent links that represent actions with uncertain durations. STNUs have been used for robot control, web-service composition, and business processes. The most important property of an STNU is called dynamic controllability (DC); and algorithms for checking this property are called DC-checking algorithms. The DC-checking algorithm for STNUs with the best worst-case time-complexity is the RUL¯ algorithm due to Cairo, Hunsberger and Rizzi. Its complexity is O(mn + k²n + kn log n), where n is the number of time-points, m is the number of constraints, and k is the number of contingent links. It is expected that this worst-case complexity cannot be improved upon. However, this paper provides a new algorithm, called RUL2021, that improves its performance in practice by an order of magnitude, as demonstrated by a thorough empirical evaluation",
    "volume": "main",
    "checked": true,
    "id": "e5ac2bf9421edc24aa43b85fe637ce5048d354d1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21214": {
    "title": "Learning to Solve Routing Problems via Distributionally Robust Optimization",
    "abstract": "Recent deep models for solving routing problems always assume a single distribution of nodes for training, which severely impairs their cross-distribution generalization ability. In this paper, we exploit group distributionally robust optimization (group DRO) to tackle this issue, where we jointly optimize the weights for different groups of distributions and the parameters for the deep model in an interleaved manner during training. We also design a module based on convolutional neural network, which allows the deep model to learn more informative latent pattern among the nodes. We evaluate the proposed approach on two types of well-known deep models including GCN and POMO. The experimental results on the randomly synthesized instances and the ones from two benchmark dataset (i.e., TSPLib and CVRPLib) demonstrate that our approach could significantly improve the cross-distribution generalization performance over the original models",
    "volume": "main",
    "checked": true,
    "id": "9f6f85e59bf929a9c15055d6b73291f5395e94bd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21215": {
    "title": "Learning Probably Approximately Complete and Safe Action Models for Stochastic Worlds",
    "abstract": "We consider the problem of learning action models for planning in unknown stochastic environments that can be defined using the Probabilistic Planning Domain Description Language (PPDDL). As input, we are given a set of previously executed trajectories, and the main challenge is to learn an action model that has a similar goal achievement probability to the policies used to create these trajectories. To this end, we introduce a variant of PPDDL in which there is uncertainty about the transition probabilities, specified by an interval for each factor that contains the respective true transition probabilities. Then, we present SAM+, an algorithm that learns such an imprecise-PPDDL environment model. SAM+ has a polynomial time and sample complexity, and guarantees that with high probability, the true environment is indeed captured by the defined intervals. We prove that the action model SAM+ outputs has a goal achievement probability that is almost as good or better than that of the policies used to produced the training trajectories. Then, we show how to produce a PPDDL model based on this imprecise-PPDDL model that has similar properties",
    "volume": "main",
    "checked": true,
    "id": "cf62922723b788bcfa73dd11c4bd8ba46a946b04",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21216": {
    "title": "Bounding Quality in Diverse Planning",
    "abstract": "Diverse planning is an important problem in automated planning with many real world applications. Recently, diverse planning has seen renewed interest, with work that defines a taxonomy of computational problems with respect to both plan quality and solution diversity. However, despite the recent advances in diverse planning, the variety of approaches and the number of available planners are still quite limited, even nonexistent for several computational problems. In this work, we aim to extend the portfolio of planners for various computational problems in diverse planning. To that end, we introduce a novel approach to finding solutions for three computational problems within diverse planning and present planners for these three problems. For one of these problems, our approach is the first one that is able to provide solutions to the problem. For another, we show that top-k and top quality planners can provide, albeit naive, solutions to the problem and we extend these planners to improve the diversity of the solution. Finally, for the third problem, we show that some existing diverse planners already provide solutions to that problem. We suggest another approach and empirically show it to compare favorably with these existing planners",
    "volume": "main",
    "checked": true,
    "id": "dddef96dd045b3676c3c6e1794fe647b51469b85",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21217": {
    "title": "A* Search and Bound-Sensitive Heuristics for Oversubscription Planning",
    "abstract": "Oversubscription planning (OSP) is the problem of finding plans that maximize the utility value of their end state while staying within a specified cost bound. Recently, it has been shown that OSP problems can be reformulated as classical planning problems with multiple cost functions but no utilities. Here we take advantage of this reformulation to show that OSP problems can be solved optimally using the A* search algorithm, in contrast to previous approaches that have used variations on branch-and-bound search. This allows many powerful techniques developed for classical planning to be applied to OSP problems. We also introduce novel bound-sensitive heuristics, which are able to reason about the primary cost of a solution while taking into account secondary cost functions and bounds, to provide superior guidance compared to heuristics that do not take these bounds into account. We propose two such bound-sensitive variants of existing classical planning heuristics, and show experimentally that the resulting search is significantly more informed than with comparable heuristics that do not consider bounds",
    "volume": "main",
    "checked": true,
    "id": "70898c858aa72ec68e9c1fc5effd28baa5db10b1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21218": {
    "title": "NICE: Robust Scheduling through Reinforcement Learning-Guided Integer Programming",
    "abstract": "Integer programs provide a powerful abstraction for representing a wide range of real-world scheduling problems. Despite their ability to model general scheduling problems, solving large-scale integer programs (IP) remains a computational challenge in practice. The incorporation of more complex objectives such as robustness to disruptions further exacerbates the computational challenge. We present NICE (Neural network IP Coefficient Extraction), a novel technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. More specifically, NICE uses reinforcement learning to approximately represent complex objectives in an integer programming formulation. We use NICE to determine assignments of pilots to a flight crew schedule so as to reduce the impact of disruptions. We compare NICE with (1) a baseline integer programming formulation that produces a feasible crew schedule, and (2) a robust integer programming formulation that explicitly tries to minimize the impact of disruptions. Our experiments show that, across a variety of scenarios, NICE produces schedules resulting in 33% to 48% fewer disruptions than the baseline formulation. Moreover, in more severely constrained scheduling scenarios in which the robust integer program fails to produce a schedule within 90 minutes, NICE is able to build robust schedules in less than 2 seconds on average",
    "volume": "main",
    "checked": true,
    "id": "eb1ed3e65130fb14e86a4a3a1c6c70bae113e4dd",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21219": {
    "title": "Planning to Avoid Side Effects",
    "abstract": "In sequential decision making, objective specifications are often underspecified or incomplete, neglecting to take into account potential (negative) side effects. Executing plans without consideration of their side effects can lead to catastrophic outcomes -- a concern recently raised in relation to the safety of AI. In this paper we investigate how to avoid side effects in a symbolic planning setting. We study the notion of minimizing side effects in the context of a planning environment where multiple independent agents co-exist. We define (classes of) negative side effects in terms of their effect on the agency of those other agents. Finally, we show how plans which minimize side effects of different types can be computed via compilations to cost-optimizing symbolic planning, and investigate experimentally",
    "volume": "main",
    "checked": true,
    "id": "09de97b634c6b09964cef32fc5c1e4b1c5c99811",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21220": {
    "title": "Sample-Efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs",
    "abstract": "Recent advances in deep learning have enabled optimization of deep reactive policies (DRPs) for continuous MDP planning by encoding a parametric policy as a deep neural network and exploiting automatic differentiation in an end-to-end model-based gradient descent framework. This approach has proven effective for optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of sampled trajectories to learn effectively and can suffer from high variance in solution quality. In this work, we revisit the overall model-based DRP objective and instead take a minorization-maximization perspective to iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective. This novel formulation of DRP learning as iterative lower bound optimization (ILBO) is particularly appealing because (i) each step is structurally easier to optimize than the overall objective, (ii) it guarantees a monotonically improving objective under certain theoretical conditions, and (iii) it reuses samples between iterations thus lowering sample complexity. Empirical evaluation confirms that ILBO is significantly more sample-efficient than the state-of-the-art DRP planner and consistently produces better solution quality with lower variance. We additionally demonstrate that ILBO generalizes well to new problem instances (i.e., different initial states) without requiring retraining",
    "volume": "main",
    "checked": true,
    "id": "fc49c529169a12f70be3112284721a28491224af",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21221": {
    "title": "Bridging LTLf Inference to GNN Inference for Learning LTLf Formulae",
    "abstract": "Learning linear temporal logic on finite traces (LTLf) formulae aims to learn a target formula that characterizes the high-level behavior of a system from observation traces in planning. Existing approaches to learning LTLf formulae, however, can hardly learn accurate LTLf formulae from noisy data. It is challenging to design an efficient search mechanism in the large search space in form of arbitrary LTLf formulae while alleviating the wrong search bias resulting from noisy data. In this paper, we tackle this problem by bridging LTLf inference to GNN inference. Our key theoretical contribution is showing that GNN inference can simulate LTLf inference to distinguish traces. Based on our theoretical result, we design a GNN-based approach, GLTLf, which combines GNN inference and parameter interpretation to seek the target formula in the large search space. Thanks to the non-deterministic learning process of GNNs, GLTLf is able to cope with noise. We evaluate GLTLf on various datasets with noise. Our experimental results confirm the effectiveness of GNN inference in learning LTLf formulae and show that GLTLf is superior to the state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "ec90d9e5ec6e2b11d8076e274d4dec4744fef95c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21222": {
    "title": "Risk-Aware Stochastic Shortest Path",
    "abstract": "We treat the problem of risk-aware control for stochastic shortest path (SSP) on Markov decision processes (MDP). Typically, expectation is considered for SSP, which however is oblivious to the incurred risk. We present an alternative view, instead optimizing conditional value-at-risk (CVaR), an established risk measure. We treat both Markov chains as well as MDP and introduce, through novel insights, two algorithms, based on linear programming and value iteration, respectively. Both algorithms offer precise and provably correct solutions. Evaluation of our prototype implementation shows that risk-aware control is feasible on several moderately sized models",
    "volume": "main",
    "checked": true,
    "id": "a61a536c3a2de48d27152c8f4c14d7099c63029b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21223": {
    "title": "Differential Assessment of Black-Box AI Agents",
    "abstract": "Much of the research on learning symbolic models of AI agents focuses on agents with stationary models. This assumption fails to hold in settings where the agent's capabilities may change as a result of learning, adaptation, or other post-deployment modifications. Efficient assessment of agents in such settings is critical for learning the true capabilities of an AI system and for ensuring its safe usage. In this work, we propose a novel approach to differentially assess black-box AI agents that have drifted from their previously known models. As a starting point, we consider the fully observable and deterministic setting. We leverage sparse observations of the drifted agent's current behavior and knowledge of its initial model to generate an active querying policy that selectively queries the agent and computes an updated model of its functionality. Empirical evaluation shows that our approach is much more efficient than re-learning the agent model from scratch. We also show that the cost of differential assessment using our method is proportional to the amount of drift in the agent's functionality",
    "volume": "main",
    "checked": true,
    "id": "b96452dd9f472a24b4a918ccca4032e9fabb44be",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21224": {
    "title": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability Using Tree Search and Graph Neural Networks",
    "abstract": "Scheduling under uncertainty is an area of interest in artificial intelligence. We study the problem of Dynamic Controllability (DC) of Disjunctive Temporal Networks with Uncertainty (DTNU), which seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We present a tree search approach to determine whether or not a DTNU is R-TDC. Moreover, we leverage the learning capability of a Graph Neural Network (GNN) as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that GNN tree search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search",
    "volume": "main",
    "checked": true,
    "id": "4fda71d3c53f1dea835ae181921bcf259de59ebb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21225": {
    "title": "Deciding Unsolvability in Temporal Planning under Action Non-Self-Overlapping",
    "abstract": "The field of Temporal Planning (TP) is receiving increasing interest for its many real-world applications. Most of the literature focuses on the TP problem of finding a plan, with algorithms that are not guaranteed to terminate when the problem admits no solution. In this paper, we present sound and complete decision procedures that address the dual problem of proving that no plan exists, which has important applications in oversubscription, model validation and optimization. We focus on the expressive and practically relevant semantics of action non-self-overlapping, recently proved to be PSPACE-complete. For this subclass, we propose two approaches: a reduction of the planning problem to model-checking of Timed Transition Systems, and a heuristic-search algorithm where temporal constraints are represented by Difference Bound Matrices. We implemented the approaches, and carried out an experimental evaluation against other state-of-the-art TP tools. On benchmarks that admit no plans, both approaches dramatically outperform the other planners, while the heuristic-search algorithm remains competitive on solvable benchmarks",
    "volume": "main",
    "checked": true,
    "id": "44b8f26128ae7cfe86c16d198856587e27155ecf",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21226": {
    "title": "A Distributional Framework for Risk-Sensitive End-to-End Planning in Continuous MDPs",
    "abstract": "Recent advances in efficient planning in deterministic or stochastic high-dimensional domains with continuous action spaces leverage backpropagation through a model of the environment to directly optimize action sequences. However, existing methods typically do not take risk into account when optimizing in stochastic domains, which can be incorporated efficiently in MDPs by optimizing a nonlinear utility function of the return distribution. We bridge this gap by introducing Risk-Aware Planning using PyTorch (RAPTOR), a novel unified framework for risk-sensitive planning through end-to-end optimization of commonly-studied risk-sensitive utility functions such as entropic utility, mean-variance optimization and CVaR. A key technical difficulty of our approach is that direct optimization of general risk-sensitive utility functions by backpropagation is impossible due to the presence of environment stochasticity. The novelty of RAPTOR lies in leveraging reparameterization of the state distribution, leading to a unique distributional perspective of end-to-end planning where the return distribution is utilized for sampling as well as optimizing risk-aware objectives by backpropagation in a unified framework. We evaluate and compare RAPTOR on three highly stochastic MDPs, including nonlinear navigation, HVAC control, and linear reservoir control, demonstrating the ability of RAPTOR to manage risk in complex continuous domains according to different notions of risk-sensitive utility",
    "volume": "main",
    "checked": true,
    "id": "695473d49e5b6876fc6fef799ce82a6c74f77037",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21227": {
    "title": "Formula Synthesis in Propositional Dynamic Logic with Shuffle",
    "abstract": "We introduce the formula-synthesis problem for Propositional Dynamic Logic with Shuffle (PDL || ). This problem, which generalises the model-checking problem againsts PDL || is the following: given a finite transition system and a regular term-grammar that generates (possibly infinitely many) PDL || formulas, find a formula generated by the grammar that is true in the structure (or return that there is none). We prove that the problem is undecidable in general, but add certain restrictions on the input structure or on the input grammar to yield decidability. In particular, we prove that (1) if the grammar only generates formulas in PDL (without shuffle), then the problem is EXPTIME-complete, and a further restriction to linear grammars is PSPACE-complete, and a further restriction to non-recursive grammars is NP-complete, and (2) if one restricts the input structure to have only simple paths then the problem is in 2-EXPTIME. This work is motivated by and opens up connections to other forms of synthesis from hierarchical descriptions, including HTN problems in Planning and Attack-tree Synthesis problems in Security",
    "volume": "main",
    "checked": true,
    "id": "e49eb4c9c3f41491ad6ec2acd9d6b537e84bc0d7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21228": {
    "title": "Efficient Encoding of Cost Optimal Delete-Free Planning as SAT",
    "abstract": "We introduce a novel method for encoding cost optimal delete-free STRIPS Planning as SAT. Our method is based on representing relaxed plans as partial functions from the set of propositions to the set of actions. This function can map any proposition to a unique action that adds the proposition during execution of the relaxed plan. We show that a relaxed plan can be produced by maintaining acyclicity in the graph of all causal relations among propositions, represented by the mentioned partial function. We also show that by efficient encoding of action cost propagation and enforcing a series of upper bounds on the total costs of the output plan, an optimal plan can effectively be produced for a given delete-free STRIPS problem. Our empirical results indicate that this method is quite competitive with the state of the art, demonstrating a better coverage compared to that of competing methods on standard STRIPS planning benchmark problems",
    "volume": "main",
    "checked": true,
    "id": "cd5bcf5e8e2660d8e38fd0699fdccae4c8ce93c2",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21229": {
    "title": "Optimal Admission Control for Multiclass Queues with Time-Varying Arrival Rates via State Abstraction",
    "abstract": "We consider a novel queuing problem where the decision-maker must choose to accept or reject randomly arriving tasks into a no buffer queue which are processed by N identical servers. Each task has a price, which is a positive real number, and a class. Each class of task has a different price distribution, service rate, and arrives according to an inhomogenous Poisson process. The objective is to decide which tasks to accept so that the total price of tasks processed is maximised over a finite horizon. We formulate the problem using a discrete time Markov Decision Process (MDP) with a hybrid state space. We show that the optimal value function has a specific structure, which enables us to solve the hybrid MDP exactly. Moreover, we rigorously prove that as the gap between successive decision epochs grows smaller, the discrete time solution approaches the optimal solution to the original continuous time problem. To improve the scalability of our approach to a greater number of servers and task classes, we present an approximation based on state abstraction. We validate our approach on synthetic data, as well as a real financial fraud data set, which is the motivating application for this work",
    "volume": "main",
    "checked": true,
    "id": "b3c36ea2d00e2bcf5ea55b75df92e73deee5c901",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21230": {
    "title": "Enhancing Column Generation by a Machine-Learning-Based Pricing Heuristic for Graph Coloring",
    "abstract": "Column Generation (CG) is an effective method for solving large-scale optimization problems. CG starts by solving a subproblem with a subset of columns (i.e., variables) and gradually includes new columns that can improve the solution of the current subproblem. The new columns are generated as needed by repeatedly solving a pricing problem, which is often NP-hard and is a bottleneck of the CG approach. To tackle this, we propose a Machine-Learning-based Pricing Heuristic (MLPH) that can generate many high-quality columns efficiently. In each iteration of CG, our MLPH leverages an ML model to predict the optimal solution of the pricing problem, which is then used to guide a sampling method to efficiently generate multiple high-quality columns. Using the graph coloring problem, we empirically show that MLPH significantly enhances CG as compared to six state-of-the-art methods, and the improvement in CG can lead to substantially better performance of the branch-and-price exact method",
    "volume": "main",
    "checked": true,
    "id": "267a19fcc41cb964faaa6114aefd4391c5c1c98b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21231": {
    "title": "Qubit Routing Using Graph Neural Network Aided Monte Carlo Tree Search",
    "abstract": "Near-term quantum hardware can support two-qubit operations only on the qubits that can interact with each other. Therefore, to execute an arbitrary quantum circuit on the hardware, compilers have to first perform the task of qubit routing, i.e., to transform the quantum circuit either by inserting additional SWAP gates or by reversing existing CNOT gates to satisfy the connectivity constraints of the target topology. The depth of the transformed quantum circuits is minimized by utilizing the Monte Carlo tree search (MCTS) to perform qubit routing by making it both construct each action and search over the space of all actions. It is aided in performing these tasks by a Graph neural network that evaluates the value function and action probabilities for each state. Along with this, we propose a new method of adding mutex-lock like variables in our state representation which helps factor in the parallelization of the scheduled operations, thereby pruning the depth of the output circuit. Overall, our procedure (referred to as QRoute) performs qubit routing in a hardware agnostic manner, and it outperforms other available qubit routing implementations on various circuit benchmarks",
    "volume": "main",
    "checked": true,
    "id": "5648551b47561917a796db2694f01cb5232fcdd0",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21232": {
    "title": "Classical Planning with Avoid Conditions",
    "abstract": "It is often natural in planning to specify conditions that should be avoided, characterizing dangerous or highly undesirable behavior. PDDL3 supports this with temporal-logic state trajectory constraints. Here we focus on the simpler case where the constraint is a non-temporal formula ? - the avoid condition - that must be false throughout the plan. We design techniques tackling such avoid conditions effectively. We show how to learn from search experience which states necessarily lead into ?, and we show how to tailor abstractions to recognize that avoiding ? will not be possible starting from a given state. We run a large-scale experiment, comparing our techniques against compilation methods and against simple state pruning using ?. The results show that our techniques are often superior",
    "volume": "main",
    "checked": true,
    "id": "5c055ab009b82dbc00d2ea2e3d56682c053ec0c7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21233": {
    "title": "Stochastic Goal Recognition Design Problems with Suboptimal Agents",
    "abstract": "Goal Recognition Design (GRD) problems identify the minimum number of environmental modifications aiming to force an interacting agent to reveal its goal as early as possible. Researchers proposed several extensions to the original model, some of them handling stochastic agent action outcomes. While this generalization is useful, it assumes optimal acting agents, which limits its applicability to more realistic scenarios. This paper presents the Suboptimal Stochastic GRD model, where we consider boundedly rational agents that, due to limited resources, might follow a suboptimal policy. Inspired by theories on human behavior asserting that humans are (close to) optimal when making perceptual decisions, we assume the chosen policy has at most m suboptimal actions. Our contribution includes (I) Extending the stochastic goal recognition design framework by supporting suboptimal agents in cases where an observer has either full or partial observability; (ii) Presenting methods to evaluate the ambiguity of the model under these assumptions; and (iii) Evaluating our approach on a range of benchmark applications",
    "volume": "main",
    "checked": true,
    "id": "68520e8162d1f725620e8b92ae9a06c9a4411785",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21234": {
    "title": "Equity Promotion in Online Resource Allocation",
    "abstract": "We consider online resource allocation under a typical non-profit setting, where limited or even scarce resources are administered by a not-for-profit organization like a government. We focus on the internal-equity by assuming that arriving requesters are homogeneous in terms of their external factors like demands but heterogeneous for their internal attributes like demographics. Specifically, we associate each arriving requester with one or several groups based on their demographics (i.e., race, gender, and age), and we aim to design an equitable distributing strategy such that every group of requesters can receive a fair share of resources proportional to a preset target ratio.    We present two LP-based sampling algorithms and investigate them both theoretically (in terms of competitive-ratio analysis) and experimentally based on real COVID-19 vaccination data maintained by the Minnesota Department of Health. Both theoretical and numerical results show that our LP-based sampling strategies can effectively promote equity, especially when the arrival population is disproportionately represented, as observed in the early stage of the COVID-19 vaccine rollout",
    "volume": "main",
    "checked": true,
    "id": "1df817967454019f9a9392e210c9b2254ff52621",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21235": {
    "title": "Efficient Device Scheduling with Multi-Job Federated Learning",
    "abstract": "Recent years have witnessed a large amount of decentralized data in multiple (edge) devices of end-users, while the aggregation of the decentralized data remains difficult for machine learning jobs due to laws or regulations. Federated Learning (FL) emerges as an effective approach to handling decentralized data without sharing the sensitive raw data, while collaboratively training global machine learning models. The servers in FL need to select (and schedule) devices during the training process. However, the scheduling of devices for multiple jobs with FL remains a critical and open problem. In this paper, we propose a novel multi-job FL framework to enable the parallel training process of multiple jobs. The framework consists of a system model and two scheduling methods. In the system model, we propose a parallel training process of multiple jobs, and construct a cost model based on the training time and the data fairness of various devices during the training process of diverse jobs. We propose a reinforcement learning-based method and a Bayesian optimization-based method to schedule devices for multiple jobs while minimizing the cost. We conduct extensive experimentation with multiple jobs and datasets. The experimental results show that our proposed approaches significantly outperform baseline approaches in terms of training time (up to 8.67 times faster) and accuracy (up to 44.6% higher)",
    "volume": "main",
    "checked": true,
    "id": "5b0f61e0af7abf2c54b6601d8c40a69f09b49823",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21236": {
    "title": "MAPDP: Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems",
    "abstract": "Cooperative Pickup and Delivery Problem (PDP), as a variant of the typical Vehicle Routing Problems (VRP), is an important formulation in many real-world applications, such as on-demand delivery, industrial warehousing, etc. It is of great importance to efficiently provide high-quality solutions of cooperative PDP. However, it is not trivial to provide effective solutions directly due to two major challenges: 1) the structural dependency between pickup and delivery pairs require explicit modeling and representation. 2) the cooperation between different vehicles is highly related to the solution exploration and difficult to model. In this paper, we propose a novel multi-agent reinforcement learning based framework to solve the cooperative PDP (MAPDP). First, we design a paired context embedding to well measure the dependency of different nodes considering their structural limits. Second, we utilize cooperative multi-agent decoders to leverage the decision dependence among different vehicle agents based on a special communication embedding. Third, we design a novel cooperative A2C algorithm to train the integrated model. We conduct extensive experiments on a randomly generated dataset and a real-world dataset. Experiments result shown that the proposed MAPDP outperform all other baselines by at least 1.64\\% in all settings, and shows significant computation speed during solution inference",
    "volume": "main",
    "checked": true,
    "id": "2f1df6c61e66081e91defe3b0815e9854cae1589",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21237": {
    "title": "Entropy Estimation via Normalizing Flow",
    "abstract": "Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high dimensional problems. In this work we propose a transformbased method for high dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high dimensional entropy estimation problems",
    "volume": "main",
    "checked": true,
    "id": "14e5883b9d4e664deb20ca82e4efad476310098c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21238": {
    "title": "Fast and More Powerful Selective Inference for Sparse High-Order Interaction Model",
    "abstract": "Automated high-stake decision-making, such as medical diagnosis, requires models with high interpretability and reliability. We consider the sparse high-order interaction model as an interpretable and reliable model with a good prediction ability. However, finding statistically significant high-order interactions is challenging because of the intrinsically high dimensionality of the combinatorial effects. Another problem in data-driven modeling is the effect of ``cherry-picking\" (i.e., selection bias). Our main contribution is extending the recently developed parametric programming approach for selective inference to high-order interaction models. An exhaustive search over the cherry tree (all possible interactions) can be daunting and impractical, even for small-sized problems. We introduced an efficient pruning strategy and demonstrated the computational efficiency and statistical power of the proposed method using both synthetic and real data",
    "volume": "main",
    "checked": true,
    "id": "b4db4ce28b81d208519b66d582a4dff5697324c3",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21239": {
    "title": "Generalized Stochastic Matching",
    "abstract": "In this paper, we generalize the recently studied stochastic matching problem to more accurately model a significant medical process, kidney exchange, and several other applications. Up until now the stochastic matching problem that has been studied was as follows: given a graph G= (V,E), each edge is included in the realized sub-graph of G independently with probability pe, and the goal is to find a degree-bounded sub-graph Q of G that has an expected maximum matching that approximates the expected maximum matching of G. This model does not account for possibilities of vertex dropouts, which can be found in several applications, e.g. in kidney exchange when donors or patients opt out of the exchange process as well as in online freelancing and online dating when online profiles are found to be faked. Thus, we will study a more generalized model of stochastic matching in which vertices and edges are both realized independently with some probabilities pv, pe, respectively, which more accurately fits important applications than the previously studied model. We will discuss the first algorithms and analysis for this generalization of the stochastic matching model and prove that they achieve good approximation ratios. In particular, we show that the approximation factor of a natural algorithm for this problem is at least 0.6568 in unweighted graphs, and 1/2+ε in weighted graphs for some constant ε >0. We further improve our result for unweighted graphs to 2/3 using edge degree constrained sub-graphs (EDCS)",
    "volume": "main",
    "checked": true,
    "id": "56b69d4abba205fe537a4d5ea29eb37778dca76a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21240": {
    "title": "Robust Tests in Online Decision-Making",
    "abstract": "Bandit algorithms are widely used in sequential decision problems to maximize the cumulative reward. One potential application is mobile health, where the goal is to promote the user's health through personalized interventions based on user specific information acquired through wearable devices. Important considerations include the type of, and frequency with which data is collected (e.g. GPS, or continuous monitoring), as such factors can severely impact app performance and users’ adherence. In order to balance the need to collect data that is useful with the constraint of impacting app performance, one needs to be able to assess the usefulness of variables. Bandit feedback data are sequentially correlated, so traditional testing procedures developed for independent data cannot apply. Recently, a statistical testing procedure was developed for the actor-critic bandit algorithm. An actor-critic algorithm maintains two separate models, one for the actor, the action selection policy, and the other for the critic, the reward model. The performance of the algorithm as well as the validity of the test are guaranteed only when the critic model is correctly specified. However, misspecification is frequent in practice due to incorrect functional form or missing covariates. In this work, we propose a modified actor-critic algorithm which is robust to critic misspecification and derive a novel testing procedure for the actor parameters in this case",
    "volume": "main",
    "checked": true,
    "id": "f0fbb0627602cdfb07e779299eed051bb63875ce",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21241": {
    "title": "Local Differential Privacy for Belief Functions",
    "abstract": "In this paper, we propose two new definitions of local differential privacy for belief functions. One is based on Shafer’s semantics of randomly coded messages and the other from the perspective of imprecise probabilities. We show that such basic properties as composition and post-processing also hold for our new definitions. Moreover, we provide a hypothesis testing framework for these definitions and study the effect of \"don’t know\" in the trade-off between privacy and utility in discrete distribution estimation",
    "volume": "main",
    "checked": true,
    "id": "2f6cefb1cc06a63b99af19eb57da9192143b7219",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21242": {
    "title": "A Complete Criterion for Value of Information in Soluble Influence Diagrams",
    "abstract": "Influence diagrams have recently been used to analyse the safety and fairness properties of AI systems. A key building block for this analysis is a graphical criterion for value of information (VoI). This paper establishes the first complete graphical criterion for VoI in influence diagrams with multiple decisions. Along the way, we establish two techniques for proving properties of multi-decision influence diagrams: ID homomorphisms are structure-preserving transformations of influence diagrams, while a Tree of Systems is a collection of paths that captures how information and control can flow in an influence diagram",
    "volume": "main",
    "checked": true,
    "id": "a43196ca18c750472479c4d9a4ffe09659e5e3cd",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21243": {
    "title": "Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate",
    "abstract": "Uncertainty estimation is an essential step in the evaluation of the robustness for deep learning models in computer vision, especially when applied in risk-sensitive areas. However, most state-of-the-art deep learning models either fail to obtain uncertainty estimation or need significant modification (e.g., formulating a proper Bayesian treatment) to obtain it. Most previous methods are not able to take an arbitrary model off the shelf and generate uncertainty estimation without retraining or redesigning it. To address this gap, we perform a systematic exploration into training-free uncertainty estimation for dense regression, an unrecognized yet important problem, and provide a theoretical construction justifying such estimations. We propose three simple and scalable methods to analyze the variance of outputs from a trained network under tolerable perturbations: infer-transformation, infer-noise, and infer-dropout. They operate solely during the inference, without the need to re-train, re-design, or fine-tune the models, as typically required by state-of-the-art uncertainty estimation methods. Surprisingly, even without involving such perturbations in training, our methods produce comparable or even better uncertainty estimation when compared to training-required state-of-the-art methods. Code is available at https://github.com/lumi9587/train-free-uncertainty",
    "volume": "main",
    "checked": true,
    "id": "7b5a53106c08dcbb80c8efa6325911ee022abebe",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21244": {
    "title": "On the Impact of Spurious Correlation for Out-of-Distribution Detection",
    "abstract": "Modern neural networks can assign high confidence to inputs drawn from outside the training distribution, posing threats to models in real-world deployments. While much research attention has been placed on designing new out-of-distribution (OOD) detection methods, the precise definition of OOD is often left in vagueness and falls short of the desired notion of OOD in reality. In this paper, we present a new formalization and model the data shifts by taking into account both the invariant and environmental (spurious) features. Under such formalization, we systematically investigate how spurious correlation in the training set impacts OOD detection. Our results suggest that the detection performance is severely worsened when the correlation between spurious features and labels is increased in the training set. We further show insights on detection methods that are more effective in reducing the impact of spurious correlation, and provide theoretical analysis on why reliance on environmental features leads to high OOD detection error. Our work aims to facilitate better understanding of OOD samples and their formalization, as well as the exploration of methods that enhance OOD detection. Code is available at https://github.com/deeplearning-wisc/Spurious_OOD",
    "volume": "main",
    "checked": true,
    "id": "a98cc4d37b0af9be710c5b6f4a3579331e6fcfbe",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21245": {
    "title": "Inference and Learning with Model Uncertainty in Probabilistic Logic Programs",
    "abstract": "An issue that has so far received only limited attention in probabilistic logic programming (PLP) is the modelling of so-called epistemic uncertainty, the uncertainty about the model itself. Accurately quantifying this model uncertainty is paramount to robust inference, learning and ultimately decision making. We introduce BetaProbLog, a PLP language that can model epistemic uncertainty. BetaProbLog has sound semantics, an effective inference algorithm that combines Monte Carlo techniques with knowledge compilation, and a parameter learning algorithm. We empirically outperform state-of-the-art methods on probabilistic inference tasks in second-order Bayesian networks, digit classification and discriminative learning in the presence of epistemic uncertainty",
    "volume": "main",
    "checked": true,
    "id": "84e4f7b543d129678cf04b8adbbdd6decfb0722b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21246": {
    "title": "Domain-Lifted Sampling for Universal Two-Variable Logic and Extensions",
    "abstract": "Given a first-order sentence ? and a domain size n, how can one sample a model of ? on the domain {1, . . . , n} efficiently as n scales? We consider two variants of this problem: the uniform sampling regime, in which the goal is to sample a model uniformly at random, and the symmetric weighted sampling regime, in which models are weighted according to the number of groundings of each predicate appearing in them. Solutions to this problem have applications to the scalable generation of combinatorial structures, as well as sampling in several statistical-relational models such as Markov logic networks and probabilistic logic programs. In this paper, we identify certain classes of sentences that are domain-liftable under sampling, in the sense that they admit a sampling algorithm that runs in time polynomial in n. In particular, we prove that every sentence of the form ∀x∀y: ?(x, y) for some quantifier-free formula ?(x,y) is domain-liftable under sampling. We then further show that this result continues to hold in the presence of one or more cardinality constraints as well as a single tree axiom constraint",
    "volume": "main",
    "checked": true,
    "id": "632d8a8b73b851e5083f430c3c941befde5112a7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21247": {
    "title": "Identifiability of Linear AMP Chain Graph Models",
    "abstract": "We study identifiability of linear Andersson-Madigan-Perlman (AMP) chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. AMP models are described by DAGs on chain components which themselves are undirected graphs.  For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are equal (or more generally, monotone non-decreasing in topological order). This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the component decomposition is unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm based on submodular function minimization. We also conduct experiments comparing our algorithm's performance against existing baselines",
    "volume": "main",
    "checked": true,
    "id": "b08649b7b285cf9dfc189823715ee23ff8f28d07",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21248": {
    "title": "DeepStochLog: Neural Stochastic Logic Programming",
    "abstract": "Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks",
    "volume": "main",
    "checked": true,
    "id": "ef01e99f73593bcc53cb9251928d51c551068e8e",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21249": {
    "title": "Towards Robust Off-Policy Learning for Runtime Uncertainty",
    "abstract": "Off-policy learning plays a pivotal role in optimizing and evaluating policies prior to the online deployment. However, during the real-time serving, we observe varieties of interventions and constraints that cause inconsistency between the online and offline setting, which we summarize and term as runtime uncertainty. Such uncertainty cannot be learned from the logged data due to its abnormality and rareness nature. To assert a certain level of robustness, we perturb the off-policy estimators along an adversarial direction in view of the runtime uncertainty. It allows the resulting estimators to be robust not only to observed but also unexpected runtime uncertainties. Leveraging this idea, we bring runtime-uncertainty robustness to three major off-policy learning methods: the inverse propensity score method, reward-model method, and doubly robust method. We theoretically justify the robustness of our methods to runtime uncertainty, and demonstrate their effectiveness using both the simulation and the real-world online experiments",
    "volume": "main",
    "checked": true,
    "id": "f7ae0a0a8d36ae99368aafff53240c69aa1cf680",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21250": {
    "title": "Improving Bayesian Neural Networks by Adversarial Sampling",
    "abstract": "Bayesian neural networks (BNNs) have drawn extensive interest due to the unique probabilistic representation framework.   However, Bayesian neural networks have limited publicized deployments because of the relatively poor model performance in real-world applications.   In this paper, we argue that the randomness of sampling in Bayesian neural networks causes errors in the updating of model parameters during training and some sampled models with poor performance in testing.   To solve this, we propose to train Bayesian neural networks with Adversarial Distribution as a theoretical solution.    To avoid the difficulty of calculating Adversarial Distribution analytically, we further present the Adversarial Sampling method as an approximation in practice.    We conduct extensive experiments with multiple network structures on different datasets, e.g., CIFAR-10 and CIFAR-100.    Experimental results validate the correctness of the theoretical analysis and the effectiveness of the Adversarial Sampling on improving model performance.   Additionally, models trained with Adversarial Sampling still keep their ability to model uncertainties and perform better when predictions are retained according to the uncertainties, which further verifies the generality of the Adversarial Sampling approach",
    "volume": "main",
    "checked": true,
    "id": "ce5b1650af21f006b9ec4debba2a1aab78b98feb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21251": {
    "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient Descent",
    "abstract": "Optimal transport (OT) plays an essential role in various areas like machine learning and deep learning.   However, computing discrete optimal transport plan for large scale problems with adequate accuracy and efficiency is still highly challenging.    Recently, methods based on the Sinkhorn algorithm add an entropy regularizer to the prime problem and get a trade off between efficiency and accuracy.    In this paper, we propose a novel algorithm to further improve the efficiency and accuracy based on Nesterov's smoothing technique.    Basically, the non-smooth c-transform of the Kantorovich potential is approximated by the smooth Log-Sum-Exp function, which finally smooths the original non-smooth Kantorovich dual functional. The smooth Kantorovich functional can be optimized by the fast proximal gradient algorithm (FISTA) efficiently. Theoretically, the computational complexity of the proposed method is lower than   current estimation of the Sinkhorn algorithm in terms of the precision.   Empirically, compared with the Sinkhorn algorithm, our experimental results demonstrate that the proposed method achieves faster convergence and better accuracy with the same parameter",
    "volume": "main",
    "checked": true,
    "id": "e2179a9911d30cb5cbde20506addd62088818e3c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21252": {
    "title": "Local and Global Linear Convergence of General Low-Rank Matrix Recovery Problems",
    "abstract": "We study the convergence rate of gradient-based local search methods for solving low-rank matrix recovery problems with general objectives in both symmetric and asymmetric cases, under the assumption of the restricted isometry property. First, we develop a new technique to verify the Polyak-Lojasiewicz inequality in a neighborhood of the global minimizers, which leads to a local linear convergence region for the gradient descent method. Second, based on the local convergence result and a sharp strict saddle property proven in this paper, we present two new conditions that guarantee the global linear convergence of the perturbed gradient descent method. The developed local and global convergence results provide much stronger theoretical guarantees than the existing results. As a by-product, this work significantly improves the existing bounds on the RIP constant required to guarantee the non-existence of spurious solutions",
    "volume": "main",
    "checked": true,
    "id": "aeb960d8620c21f87afe144b3c87304844272c74",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21253": {
    "title": "A*+BFHS: A Hybrid Heuristic Search Algorithm",
    "abstract": "We present a new algorithm called A*+BFHS for solving problems with unit-cost operators where A* and IDA* fail due to memory limitations and/or the existence of many distinct paths between the same pair of nodes. A*+BFHS is based on A* and breadth-first heuristic search (BFHS). A*+BFHS combines advantages from both algorithms, namely A*'s node ordering, BFHS's memory savings, and both algorithms' duplicate detection. On easy problems, A*+BFHS behaves the same as A*. On hard problems, it is slower than A* but saves a large amount of memory. Compared to BFIDA*, A*+BFHS reduces the search time and/or memory requirement by several times on a variety of planning domains",
    "volume": "main",
    "checked": true,
    "id": "cae96a1ed6d8129c8ee433b8754e059eb1d4af6c",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21254": {
    "title": "NukCP: An Improved Local Search Algorithm for Maximum k-Club Problem",
    "abstract": "The maximum k-club problem (MkCP) is an important clique relaxation problem with wide applications. Previous MkCP algorithms only work on small-scale instances and are not applicable for large-scale instances. For solving instances with different scales, this paper develops an efficient local search algorithm named NukCP for the MkCP which mainly includes two novel ideas. First, we propose a dynamic reduction strategy, which makes a good balance between the time efficiency and the precision effectiveness of the upper bound calculation. Second, a stratified threshold configuration checking strategy is designed by giving different priorities for the neighborhood in the different levels. Experiments on a broad range of different scale instances show that NukCP significantly outperforms the state-of-the-art MkCP algorithms on most instances",
    "volume": "main",
    "checked": true,
    "id": "4145186c7be17a6c4ab28def4db7596fe8ea6f18",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21255": {
    "title": "Fourier Representations for Black-Box Optimization over Categorical Variables",
    "abstract": "Optimization of real-world black-box functions defined over purely categorical variables is an active area of research. In particular, optimization and design of biological sequences with specific functional or structural properties have a profound impact in medicine, materials science, and biotechnology. Standalone search algorithms, such as simulated annealing (SA) and Monte Carlo tree search (MCTS), are typically used for such optimization problems. In order to improve the performance and sample efficiency of such algorithms, we propose to use existing methods in conjunction with a surrogate model for the black-box evaluations over purely categorical variables. To this end, we present two different representations, a group-theoretic Fourier expansion and an abridged one-hot encoded Boolean Fourier expansion. To learn such representations, we consider two different settings to update our surrogate model. First, we utilize an adversarial online regression setting where Fourier characters of each representation are considered as experts and their respective coefficients are updated via an exponential weight update rule each time the black box is evaluated. Second, we consider a Bayesian setting where queries are selected via Thompson sampling and the posterior is updated via a sparse Bayesian regression model (over our proposed representation) with a regularized horseshoe prior. Numerical experiments over synthetic benchmarks as well as real-world RNA sequence optimization and design problems demonstrate the representational power of the proposed methods, which achieve competitive or superior performance compared to state-of-the-art counterparts, while improving the computation cost and/or sample efficiency, substantially",
    "volume": "main",
    "checked": true,
    "id": "a6fd6aa5e7b26a400a104bb3ef522d88bbe69cf7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21256": {
    "title": "New Results in Bounded-Suboptimal Search",
    "abstract": "In bounded-suboptimal heuristic search, one attempts to find a solution that costs no more than a prespecified factor of optimal as quickly as possible. This is an important setting, as it admits faster-than-optimal solving while retaining some control over solution cost. In this paper, we investigate several new algorithms for bounded-suboptimal search, including novel variants of EES and DPS, the two most prominent previous proposals, and methods inspired by recent work in bounded-cost search that leverages uncertainty estimates of the heuristic. We perform what is, to our knowledge, the most comprehensive empirical comparison of bounded-suboptimal search algorithms to date, including both search and planning benchmarks, and we find that one of the new algorithms, a simple alternating queue scheme, significantly outperforms previous work",
    "volume": "main",
    "checked": true,
    "id": "bfb5c87deb4e081d48ceee7bf3fce76070b404d6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21257": {
    "title": "An Exact Algorithm with New Upper Bounds for the Maximum k-Defective Clique Problem in Massive Sparse Graphs",
    "abstract": "The Maximum k-Defective Clique Problem (MDCP), as a clique relaxation model, has been used to solve various problems. Because it is a hard computational task, previous works can hardly solve the MDCP for massive sparse graphs derived from real-world applications. In this work, we propose a novel branch-and-bound algorithm to solve the MDCP based on several new techniques. First, we propose two new upper bounds of the MDCP as well as corresponding reduction rules to remove redundant vertices and edges. The proposed reduction rules are particularly useful for massive graphs. Second, we present another new upper bound by counting missing edges between fixed vertices and an unfixed vertex for cutting branches. We perform extensive computational experiments to evaluate our algorithm. Experimental results show that our reduction rules are very effective for removing redundant vertices and edges so that graphs are reduced greatly. Also, our algorithm can solve benchmark instances efficiently, and it has significantly better performance than state-of-the-art algorithms",
    "volume": "main",
    "checked": true,
    "id": "e2e36d9085fc3e6ed04c16bec800f02d7dbd923e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21258": {
    "title": "Learning from Mistakes – a Framework for Neural Architecture Search",
    "abstract": "Learning from one's mistakes is an effective human learning technique where the learners focus more on the topics where mistakes were made, so as to deepen their understanding. In this paper, we investigate if this human learning strategy can be applied in machine learning. We propose a novel machine learning method called Learning From Mistakes (LFM), wherein the learner improves its ability to learn by focusing more on the mistakes during revision. We formulate LFM as a three-stage optimization problem: 1) learner learns; 2) learner re-learns focusing on the mistakes, and; 3) learner validates its learning. We develop an efficient algorithm to solve the LFM problem. We apply the LFM framework to neural architecture search on CIFAR-10, CIFAR-100, and Imagenet. Experimental results strongly demonstrate the effectiveness of our model",
    "volume": "main",
    "checked": true,
    "id": "c2bbd54bc227d630c2018725d4edb52ea8fa81e3",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21259": {
    "title": "The Complexity of Temporal Vertex Cover in Small-Degree Graphs",
    "abstract": "Temporal graphs naturally model graphs whose underlying topology changes over time. Recently, the problems Temporal Vertex Cover (or TVC) and Sliding-Window Temporal Vertex Cover (or Delta-TVC for time-windows of a fixed-length Delta) have been established as natural extensions of the classic Vertex Cover problem on static graphs with connections to areas such as surveillance in sensor networks. In this paper we initiate a systematic study of the complexity of TVC and Delta-TVC on sparse graphs. Our main result shows that for every Delta geq 2, Delta-TVC is NP-hard even when the underlying topology is described by a path or a cycle. This resolves an open problem from literature and shows a surprising contrast between Delta-TVC and TVC for which we provide a polynomial-time algorithm in the same setting. To circumvent this hardness, we present a number of exact and approximation algorithms for temporal graphs whose underlying topologies are given by a path, that have bounded vertex degree in every time step, or that admit a small-sized temporal vertex cover",
    "volume": "main",
    "checked": true,
    "id": "54536fe6b21770eabc14a3315bff3ddab6d29bbb",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21260": {
    "title": "Provable Sensor Sets for Epidemic Detection over Networks with Minimum Delay",
    "abstract": "The efficient detection of outbreaks and other cascading phenomena is a fundamental problem in a number of domains, including disease spread, social networks, and infrastructure networks. In such settings, monitoring and testing a small group of pre-selected nodes from the susceptible population (i.e., a sensor set) is often the preferred testing regime. We study the problem of selecting a sensor set that minimizes the delay in detection---we refer to this as the MinDelSS problem. Prior methods for minimizing the detection time rely on greedy algorithms using submodularity. We show that this approach can sometimes lead to a worse approximation for minimizing the detection time than desired. We also show that MinDelSS is hard to approximate within an O(n^(1-1/g))-factor for any constant g greater than or equal to 2 for a graph with n nodes. This instead motivates seeking a bicriteria approximations. We present the algorithm RoundSensor, which gives a rigorous worst case O(log(n))-factor for the detection time, while violating the budget by a factor of O(log^2(n)). Our algorithm is based on the sample average approximation technique from stochastic optimization, combined with linear programming and rounding. We evaluate our algorithm on several networks, including hospital contact networks, which validates its effectiveness in real settings",
    "volume": "main",
    "checked": true,
    "id": "c6b4e98e8e2b0bb3c54df31d64a5a2d10eba1fca",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21261": {
    "title": "Towards Automated Discovery of God-Like Folk Algorithms for Rubik’s Cube",
    "abstract": "We present a multi-objective meta-search procedure that constructs candidate algorithms for state-space search puzzles like Rubik's cube. The candidate algorithms take the form of macro databases, i.e., rule tables that specify sequences of actions to perform in different states. Rules are repeatedly applied until the puzzle is solved. The objectives favor candidates that are god-like (solving the puzzle in fewer steps) and folk-like (having fewer rules in the macro database). We build each candidate with a non-deterministic rule table construction, and then optimize over the non-deterministic choice points to find candidates near the Pareto-optimal trades-offs between godliness and folksiness. We prove that the rule table construction is correct: it always terminates and solves every state at termination. This is verified empirically on the full 2x2x2 \"pocket\" cube, where correct (but unoptimized) constructions take under one hour and the total number of rules is less than 10% the number of possible states. We also empirically assess the multi-objective optimization on restricted variants of the cube with up to 29K possible states, showing relative improvements in the objectives between 14-20%. Avenues for scaling up the method in future work are discussed",
    "volume": "main",
    "checked": true,
    "id": "c81d5ee77ca18c6becdb867510b1144c9553b2f8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21262": {
    "title": "MIP-GNN: A Data-Driven Framework for Guiding Combinatorial Solvers",
    "abstract": "Mixed-integer programming (MIP) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program (MILP) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MILPs. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary MILPs. Our code and appendix are publicly available at https://github.com/lyeskhalil/mipGNN",
    "volume": "main",
    "checked": true,
    "id": "9cd1a80a5534d9288fccf1852503c89c9b57ac9b",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21263": {
    "title": "Bandit Limited Discrepancy Search and Application to Machine Learning Pipeline Optimization",
    "abstract": "Optimizing a machine learning (ML) pipeline has been an important topic of AI and ML. Despite recent progress, pipeline optimization remains a challenging problem, due to potentially many combinations to consider as well as slow training and validation. We present the BLDS algorithm for optimized algorithm selection (ML operations) in a fixed ML pipeline structure. BLDS performs multi-fidelity optimization for selecting ML algorithms trained with smaller computational overhead, while controlling its pipeline search based on multi-armed bandit and limited discrepancy search. Our experiments on well-known classification benchmarks show that BLDS is superior to competing algorithms. We also combine BLDS with hyperparameter optimization, empirically showing the advantage of BLDS",
    "volume": "main",
    "checked": true,
    "id": "81c004a5f4d0da6d5f61662b4b70cc6d368e2783",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21264": {
    "title": "PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Data Subset Selection",
    "abstract": "With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i)targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is under performing, and ii)guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided image-collection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits",
    "volume": "main",
    "checked": true,
    "id": "b49a2e372010577658e98abedc4fc0adc8e5f923",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21265": {
    "title": "Split Moves for Monte-Carlo Tree Search",
    "abstract": "In many games, moves consist of several decisions made by the player. These decisions can be viewed as separate moves, which is already a common practice in multi-action games for efficiency reasons. Such division of a player move into a sequence of simpler / lower level moves is called splitting. So far, split moves have been applied only in forementioned straightforward cases, and furthermore, there was almost no study revealing its impact on agents' playing strength. Taking the knowledge-free perspective, we aim to answer how to effectively use split moves within Monte-Carlo Tree Search (MCTS) and what is the practical impact of split design on agents' strength. This paper proposes a generalization of MCTS that works with arbitrarily split moves. We design several variations of the algorithm and try to measure the impact of split moves separately on efficiency, quality of MCTS, simulations, and action-based heuristics. The tests are carried out on a set of board games and performed using the Regular Boardgames General Game Playing formalism, where split strategies of different granularity can be automatically derived based on an abstract description of the game. The results give an overview of the behavior of agents using split design in different ways. We conclude that split design can be greatly beneficial for single- as well as multi-action games",
    "volume": "main",
    "checked": true,
    "id": "b7458f2cb41afb73d87029228c9509ae5435506f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21266": {
    "title": "MAPF-LNS2: Fast Repairing for Multi-Agent Path Finding via Large Neighborhood Search",
    "abstract": "Multi-Agent Path Finding (MAPF) is the problem of planning collision-free paths for multiple agents in a shared environment. In this paper, we propose a novel algorithm MAPF-LNS2 based on large neighborhood search for solving MAPF efficiently. Starting from a set of paths that contain collisions, MAPF-LNS2 repeatedly selects a subset of colliding agents and replans their paths to reduce the number of collisions until the paths become collision-free. We compare MAPF-LNS2 against a variety of state-of-the-art MAPF algorithms, including Prioritized Planning with random restarts, EECBS, and PPS, and show that MAPF-LNS2 runs significantly faster than them while still providing near-optimal solutions in most cases. MAPF-LNS2 solves 80% of the random-scenario instances with the largest number of agents from the MAPF benchmark suite with a runtime limit of just 5 minutes, which, to our knowledge, has not been achieved by any existing algorithms",
    "volume": "main",
    "checked": true,
    "id": "d47c6e9b5491b05f3b6da2eaf51e748728d203a6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21267": {
    "title": "Local and Global Convergence of General Burer-Monteiro Tensor Optimizations",
    "abstract": "Tensor optimization is crucial to massive machine learning and signal processing tasks. In this paper, we consider tensor optimization with a convex and well-conditioned objective function and reformulate it into a nonconvex optimization using the Burer-Monteiro type parameterization. We analyze the local convergence of applying vanilla gradient descent to the factored formulation and establish a local regularity condition under mild assumptions. We also provide a linear convergence analysis of the gradient descent algorithm started in a neighborhood of the true tensor factors. Complementary to the local analysis, this work also characterizes the global geometry of the best rank-one tensor approximation problem and demonstrates that for orthogonally decomposable tensors the problem has no spurious local minima and all saddle points are strict except for the one at zero which is a third-order saddle point",
    "volume": "main",
    "checked": true,
    "id": "caddb6bdff9d3202cad5b5a900fb6d6ec60fc9bd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21268": {
    "title": "Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval",
    "abstract": "Cross-modal hashing has attracted considerable attention for large-scale multimodal data. Recent supervised cross-modal hashing methods using multi-label networks utilize the semantics of multi-labels to enhance retrieval accuracy, where label hash codes are learned independently. However, all these methods assume that label annotations reliably reflect the relevance between their corresponding instances, which is not true in real applications. In this paper, we propose a novel framework called Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval (Bi-CMR), which exploits a bidirectional learning to relieve the negative impact of this assumption. Specifically, in the forward learning procedure, we highlight the representative labels and learn the reinforced multi-label hash codes by intra-modal semantic information, and further adjust similarity matrix. In the backward learning procedure, the reinforced multi-label hash codes and adjusted similarity matrix are used to guide the matching of instances. We construct two datasets with explicit relevance labels that reflect the semantic relevance of instance pairs based on two benchmark datasets. The Bi-CMR is evaluated by conducting extensive experiments over these two datasets. Experimental results prove the superiority of Bi-CMR over four state-of-the-art methods in terms of effectiveness",
    "volume": "main",
    "checked": true,
    "id": "b667a1d5943c34f1e7bc454f2566c085b871d64b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21269": {
    "title": "Improving Local Search Algorithms via Probabilistic Configuration Checking",
    "abstract": "Configuration checking (CC) has been confirmed to alleviate the cycling problem in local search for combinatorial optimization problems (COPs). When using CC heuristics in local search for graph problems, a critical concept is the configuration of the vertices. All existing CC variants employ either 1- or 2-level neighborhoods of a vertex as its configuration. Inspired by the idea that neighborhoods with different levels should have different contributions to solving COPs, we propose the probabilistic configuration (PC), which introduces probabilities for neighborhoods at different levels to consider the impact of neighborhoods of different levels on the CC strategy. Based on the concept of PC, we first propose probabilistic configuration checking (PCC), which can be developed in an automated and lightweight favor. We then apply PCC to two classic COPs which have been shown to achieve good results by using CC, and our preliminary results confirm that PCC improves the existing algorithms because PCC alleviates the cycling problem",
    "volume": "main",
    "checked": true,
    "id": "d5a447af4d3a9803ff3791325e94b6782971880d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21270": {
    "title": "PEA*+IDA*: An Improved Hybrid Memory-Restricted Algorithm",
    "abstract": "It is well-known that the search algorithms A* and Iterative Deepening A* (IDA*) can fail to solve state-space tasks optimally due to time and memory limits. The former typically fails in memory-restricted scenarios and the latter in time-restricted scenarios. Therefore, several algorithms were proposed to solve state-space tasks optimally using less memory than A* and less time than IDA*, such as A*+IDA*, a hybrid memory-restricted algorithm that combines A* and IDA*. In this paper, we present a hybrid memory-restricted algorithm that combines Partial Expansion A* (PEA*) and IDA*. This new algorithm has two phases, the same structure as the A*+IDA* algorithm. The first phase of PEA*+IDA* runs PEA* until it reaches a memory limit, and the second phase runs IDA* without duplicate detection on each node of PEA*'s Open. First, we present a model that shows how PEA*+IDA* can perform better than A*+IDA* although pure PEA* usually makes more expansions than pure A*. Later, we perform an experimental evaluation using three memory limits and show that, compared to A*+IDA* on classical planning domains, PEA*+IDA* has higher coverage and expands fewer nodes. Finally, we experimentally analyze both algorithms and show that having higher F-limits and better priority-queue composition given by PEA* have a considerable impact on the performance of the algorithms",
    "volume": "main",
    "checked": true,
    "id": "b4ffef1e4be4d4a52477077f37648c2a41cb7ace",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21271": {
    "title": "Search Strategies for Topological Network Optimization",
    "abstract": "We consider an application of combinatorial search to the optimization of topologies in series-parallel networks. We propose a recursive search over the space of decomposition trees, in which partial solutions are obtained by exploring k-way partitionings of expandable nodes. We present two complementary pruning techniques that bound the value of intermediate solutions from above and below, applying monotonic operations to the contents of unresolved leaves. We also develop a means to exploit the convexity of our objective function, so as to prevent the redundant recomputation of subcircuit configurations. Finally, we evaluate our approach on a parameterized benchmark suite of electrical circuits, demonstrating over an order of magnitude improvement in performance as compared to a baseline implementation",
    "volume": "main",
    "checked": true,
    "id": "c7a927ba3ea073f847b3f4de0512f2c95f10fc7a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21272": {
    "title": "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks",
    "abstract": "We report a new neural backdoor attack, named Hibernated Backdoor, which is stealthy, aggressive and devastating. The backdoor is planted in a hibernated mode to avoid being detected. Once deployed and fine-tuned on end-devices, the hibernated backdoor turns into the active state that can be exploited by the attacker. To the best of our knowledge, this is the first hibernated neural backdoor attack. It is achieved by maximizing the mutual information (MI) between the gradients of regular and malicious data on the model. We introduce a practical algorithm to achieve MI maximization to effectively plant the hibernated backdoor. To evade adaptive defenses, we further develop a targeted hibernated backdoor, which can only be activated by specific data samples and thus achieves a higher degree of stealthiness. We show the hibernated backdoor is robust and cannot be removed by existing backdoor removal schemes. It has been fully tested on four datasets with two neural network architectures, compared to five existing backdoor attacks, and evaluated using seven backdoor detection schemes. The experiments demonstrate the effectiveness of the hibernated backdoor attack under various settings",
    "volume": "main",
    "checked": true,
    "id": "89e9872cf16e058499877efbd578646ebb04db87",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21273": {
    "title": "Planning with Explanations for Finding Desired Meeting Points on Graphs",
    "abstract": "Combinatorial optimization problems are ubiquitous for decision making in planning social infrastructures.  In real-world scenarios, a decision-maker needs to solve his/her problem iteratively until he/she satisfies solutions, but such an iterative process remains challenging.  This paper studies a new explainable framework, particularly for finding meeting points, which is a key optimization problem for designing facility locations.  Our framework automatically fills the gap between its input instance and instances from which a user could obtain the desired outcome, where computed solutions are judged by the user.  The framework also provides users with explanations, representing the difference of instances for deeply understanding the process and its inside.  Explanations are clues for users to understand their situation and implement suggested results in practice (e.g., designing a coupon for free travel).  We experimentally demonstrate that our search-based framework is promising to solve instances with generating explanations in a sequential decision-making process",
    "volume": "main",
    "checked": true,
    "id": "781553b6576ea32de96f92681215b34f0599c42c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21274": {
    "title": "A Fast Local Search Algorithm for the Latin Square Completion Problem",
    "abstract": "The Latin square completion (LSC) problem is an important NP-complete problem with numerous applications. Given its theoretical and practical importance, several algorithms are designed for solving the LSC problem. In this work, to further improve the performance, a fast local search algorithm is developed based on three main ideas. Firstly, a reduction reasoning technique is used to reduce the scale of search space. Secondly, we propose a novel conflict value selection heuristic, which considers the history conflicting information of vertices as a selection criterion when more than one vertex have equal values on the primary scoring function. Thirdly, during the search phase, we record previous history search information and then make use of these information to restart the candidate solution. Experimental results show that our proposed algorithm significantly outperforms the state-of-the-art heuristic algorithms on almost all instances in terms of success rate and run time",
    "volume": "main",
    "checked": true,
    "id": "e33e9a26819460e4a7f7ab7a072a7270b69c0f47",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21275": {
    "title": "Sparsification of Decomposable Submodular Functions",
    "abstract": "Submodular functions are at the core of many machine learning and data mining tasks. The underlying submodular functions for many of these tasks are decomposable, i.e., they are sum of several simple submodular functions. In many data intensive applications, however, the number of underlying submodular functions in the original function is so large that we need prohibitively large amount of time to process it and/or it does not even fit in the main memory. To overcome this issue, we introduce the notion of sparsification for decomposable submodular functions whose objective is to obtain an accurate approximation of the original function that is a (weighted) sum of only a few submodular functions. Our main result is a polynomial-time randomized sparsification algorithm such that the expected number of functions used in the output is independent of the number of underlying submodular functions in the original function. We also study the effectiveness of our algorithm under various constraints such as matroid and cardinality constraints. We complement our theoretical analysis with an empirical study of the performance of our algorithm",
    "volume": "main",
    "checked": true,
    "id": "01d395d2192e2d137c94bb4491e559067817c421",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21276": {
    "title": "Subset Approximation of Pareto Regions with Bi-objective A",
    "abstract": "In bi-objective search, we are given a graph in which each directed arc is associated with a pair of non-negative weights, and the objective is to find the Pareto-optimal solution set. Unfortunately, in many practical settings, this set is too large, and therefore its computation is very time-consuming. In addition, even though bi-objective search algorithms generate the Pareto set incrementally, they do so exhaustively. This means that early during search the solution set covers is not diverse, being concentrated in a small region of the solution set. To address this issue, we present a new approach to subset approximation of the solution set, that can be used as the basis for an anytime bi-objective search algorithm. Our approach transforms the given task into a target bi-objective search task using two real parameters. For each particular parameter setting, the solutions to the target task is a subset of the solution set of the original task. Depending on the parameters used, the solution set of the target task may be computed very quickly. This   allows us to obtain, in challenging road map benchmarks, a rich variety of solutions in times that may be orders of magnitude smaller than the time needed to compute the solution set. We show that by running the algorithm with an appropriate sequence of parameters, we obtain a growing sequence of solutions that converges to the full solution set. We prove that our approach is correct and that Bi-Objective A* prunes at least as many nodes when run over the target task",
    "volume": "main",
    "checked": true,
    "id": "3dd3490cf590b90c3a8ff42d63e484675908728c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21277": {
    "title": "On Probabilistic Generalization of Backdoors in Boolean Satisfiability",
    "abstract": "The paper proposes a probabilistic generalization of the well-known Strong Backdoor Set (SBS) concept applied to the Boolean Satisfiability Problem (SAT). We call a set of Boolean variables B a ρ-backdoor, if for a fraction of at least ρ of possible assignments of variables from B, assigning their values to variables in a Boolean formula in Conjunctive Normal Form (CNF) results in polynomially solvable formulas. Clearly, a ρ-backdoor with ρ=1 is an SBS. For a given set B it is possible to efficiently construct an (ε, δ)-approximation of parameter ρ using the Monte Carlo method. Thus, we define an (ε, δ)-SBS as such a set B for which the conclusion \"parameter ρ deviates from 1 by no more than ε\" is true with probability no smaller than 1 - δ. We consider the problems of finding the minimum SBS and the minimum (ε, δ)-SBS. To solve the former problem, one can use the algorithm described by R. Williams, C. Gomes and B. Selman in 2003. In the paper we propose a new probabilistic algorithm to solve the latter problem, and show that the asymptotic estimation of the worst-case complexity of the proposed algorithm is significantly smaller than that of the algorithm by Williams et al. For practical applications, we suggest a metaheuristic optimization algorithm based on the penalty function method to seek the minimal (ε, δ)-SBS. Results of computational experiments show that the use of (ε, δ)-SBSes found by the proposed algorithm allows speeding up solving of test problems related to equivalence checking and hard crafted and combinatorial benchmarks compared to state-of-the-art SAT solvers",
    "volume": "main",
    "checked": true,
    "id": "fd2df3e9941353581aaaf0151fec27332370022e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21278": {
    "title": "A Novel Approach to Solving Goal-Achieving Problems for Board Games",
    "abstract": "Goal-achieving problems are puzzles that set up a specific situation with a clear objective. An example that is well-studied is the category of life-and-death (L&D) problems for Go, which helps players hone their skill of identifying region safety. Many previous methods like lambda search try null moves first, then derive so-called relevance zones (RZs), outside of which the opponent does not need to search. This paper first proposes a novel RZ-based approach, called the RZ-Based Search (RZS), to solving L&D problems for Go. RZS tries moves before determining whether they are null moves post-hoc. This means we do not need to rely on null move heuristics, resulting in a more elegant algorithm, so that it can also be seamlessly incorporated into AlphaZero's super-human level play in our solver. To repurpose AlphaZero for solving, we also propose a new training method called Faster to Life (FTL), which modifies AlphaZero to entice it to win more quickly. We use RZS and FTL to solve L&D problems on Go, namely solving 68 among 106 problems from a professional L&D book while a previous state-of-the-art program TSUMEGO-EXPLORER solves 11 only. Finally, we discuss that the approach is generic in the sense that RZS is applicable to solving many other goal-achieving problems for board games",
    "volume": "main",
    "checked": true,
    "id": "d83cd7ca6622910c38ecbe8adf5d7509351852af",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21279": {
    "title": "Machine Learning for Online Algorithm Selection under Censored Feedback",
    "abstract": "In online algorithm selection (OAS), instances of an algorithmic problem class are presented to an agent one after another, and the agent has to quickly select a presumably best algorithm from a fixed set of candidate algorithms. For decision problems such as satisfiability (SAT), quality typically refers to the algorithm's runtime. As the latter is known to exhibit a heavy-tail distribution, an algorithm is normally stopped when exceeding a predefined upper time limit. As a consequence, machine learning methods used to optimize an algorithm selection strategy in a data-driven manner need to deal with right-censored samples, a problem that has received little attention in the literature so far. In this work, we revisit multi-armed bandit algorithms for OAS and discuss their capability of dealing with the problem. Moreover, we adapt them towards runtime-oriented losses, allowing for partially censored data while keeping a space- and time-complexity independent of the time horizon. In an extensive experimental evaluation on an adapted version of the ASlib benchmark, we demonstrate that theoretically well-founded methods based on Thompson sampling perform specifically strong and improve in comparison to existing methods",
    "volume": "main",
    "checked": true,
    "id": "83cc6596965705a8657961f97d1ce4cfc90f29bc",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21280": {
    "title": "Procrastinated Tree Search: Black-Box Optimization with Delayed, Noisy, and Multi-Fidelity Feedback",
    "abstract": "In black-box optimization problems, we aim to maximize an unknown objective function, where the function is only accessible through feedbacks of an evaluation or simulation oracle. In real-life, the feedbacks of such oracles are often noisy and available after some unknown delay that may depend on the computation time of the oracle. Additionally, if the exact evaluations are expensive but coarse approximations are available at a lower cost, the feedbacks can have multi-fidelity. In order to address this problem, we propose a generic extension of hierarchical optimistic tree search (HOO), called ProCrastinated Tree Search (PCTS), that flexibly accommodates a delay and noise-tolerant bandit algorithm. We provide a generic proof technique to quantify regret of PCTS under delayed, noisy, and multi-fidelity feedbacks. Specifically, we derive regret bounds of PCTS enabled with delayed-UCB1 (DUCB1) and delayed-UCB-V (DUCBV) algorithms. Given a horizon T, PCTS retains the regret bound of non-delayed HOO for expected delay of O(log T), and worsens by T^((1-α)/(d+2)) for expected delays of O(T^(1-α)) for α ∈ (0,1]. We experimentally validate on multiple synthetic functions and hyperparameter tuning problems that PCTS outperforms the state-of-the-art black-box optimization methods for feedbacks with different noise levels, delays, and fidelity",
    "volume": "main",
    "checked": true,
    "id": "e7346fd7f63d61f20aa1b3af5f2effc49da77b85",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21281": {
    "title": "DPCD: Discrete Principal Coordinate Descent for Binary Variable Problems",
    "abstract": "Binary optimization, a representative subclass of discrete optimization, plays an important role in mathematical optimization and has various applications in computer vision and machine learning. Generally speaking, binary optimization problems are NP-hard and difficult to solve due to the binary constraints, especially when the number of variables is very large. Existing methods often suffer from high computational costs or large accumulated quantization errors, or are only designed for specific tasks. In this paper, we propose an efficient algorithm, named Discrete Principal Coordinate Descent (DPCD), to find effective approximate solutions for general binary optimization problems. The proposed algorithm iteratively solves optimization problems related to the linear approximation of loss functions, which leads to updating the binary variables that most impact the value of the loss functions at each step. Our method supports a wide range of empirical objective functions with/without restrictions on the numbers of 1s and -1s in the binary variables. Furthermore, the theoretical convergence of our algorithm is proven, and the explicit convergence rates are derived for objective functions with Lipschitz continuous gradients, which are commonly adopted in practice. Extensive experiments on binary hashing tasks and large-scale datasets demonstrate the superiority of the proposed algorithm over several state-of-the-art methods in terms of both effectiveness and efficiency",
    "volume": "main",
    "checked": true,
    "id": "71d8807013ce356245f70d41ab35437224c505c3",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21282": {
    "title": "Optimize What You Evaluate With: Search Result Diversification Based on Metric Optimization",
    "abstract": "Most of the existing methods for search result diversification (SRD) appeal to the greedy strategy for generating diversified results, which is formulated as a sequential process of selecting documents one-by-one, and the locally optimal choice is made at each round. Unfortunately, this strategy suffers from the following shortcomings: (1) Such a one-by-one selection process is rather time-consuming for both training and inference. (2) It works well on the premise that the preceding choices are optimal or close to the optimal solution. (3) The mismatch between the objective function used in training and the final evaluation measure used in testing has not been taken into account. We propose a novel framework through direct metric optimization for SRD (referred to as MO4SRD) based on the score-and-sort strategy. Specifically, we represent the diversity score of each document that determines its rank position based on a probability distribution. These distributions over scores naturally give rise to expectations over rank positions. Armed with this advantage, we can get the differentiable variants of the widely used diversity metrics. Thanks to this, we are able to directly optimize the evaluation measure used in testing. Moreover, we have devised a novel probabilistic neural scoring function. It jointly scores candidate documents by taking into account both cross-document interaction and permutation equivariance, which makes it possible to generate a diversified ranking via a simple sorting. The experimental results on benchmark collections show that the proposed method achieves significantly improved performance over the state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "9013d208a2b257dc406ff894eb6a8929a40b495b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21283": {
    "title": "A First Mathematical Runtime Analysis of the Non-dominated Sorting Genetic Algorithm II (NSGA-II)",
    "abstract": "The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size larger than the Pareto front size by a constant factor, the NSGA-II with two classic mutation operators and three different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LOTZ benchmark functions. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front (for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front). Our experiments confirm the above findings. This paper for the Hot-off-the-Press track at GECCO 2022 summarizes the work Weijie Zheng, Yufei Liu, Benjamin Doerr: A First Mathematical Runtime Analysis of the Non-dominated Sorting Genetic Algorithm II (NSGA-II). AAAI2022, accepted [17]",
    "volume": "main",
    "checked": true,
    "id": "176e33373baf0c0417fb18002c3da33a939fcbca",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21284": {
    "title": "Pinpointing Fine-Grained Relationships between Hateful Tweets and Replies",
    "abstract": "Recent studies in the hate and counter hate domain have provided the grounds for investigating how to detect this pervasive content in social media. These studies mostly work with synthetic replies to hateful content written by annotators on demand rather than replies written by real users. We argue that working with naturally occurring replies to hateful content is key to study the problem. Building on this motivation, we create a corpus of 5,652 hateful tweets and replies. We analyze their fine-grained relationships by indicating whether the reply (a) is hate or counter hate speech, (b) provides a justification, (c) attacks the author of the tweet, and (d) adds additional hate. We also present linguistic insights into the language people use depending on these fine-grained relationships. Experimental results show improvements (a) taking into account the hateful tweet in addition to the reply and (b) pretraining with related tasks",
    "volume": "main",
    "checked": true,
    "id": "8a2af9b73e5044f820e45080566be3d258a2554a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21285": {
    "title": "Cross-Modal Coherence for Text-to-Image Retrieval",
    "abstract": "Common image-text joint understanding techniques presume that images and the associated text can universally be characterized by a single implicit model. However, co-occurring images and text can be related in qualitatively different ways, and explicitly modeling it could improve the performance of current joint understanding models. In this paper, we train a Cross-Modal Coherence Model for text-to-image retrieval task. Our analysis shows that models trained with image–text coherence relations can retrieve images originally paired with target text more often than coherence-agnostic models. We also show via human evaluation that images retrieved by the proposed coherence-aware model are preferred over a coherence-agnostic baseline by a huge margin. Our findings provide insights into the ways that different modalities communicate and the role of coherence relations in capturing commonsense inferences in text and imagery",
    "volume": "main",
    "checked": true,
    "id": "315004717da2a6bdb122622b8306af01542d714e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21286": {
    "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
    "abstract": "Large transformer-based language models have achieved incredible success at various tasks which require narrative comprehension, including story completion, answering questions about stories, and generating stories ex nihilo. However, due to the limitations of finite context windows, these language models struggle to produce or understand stories longer than several thousand tokens. In order to mitigate the document length limitations that come with finite context windows, we introduce a novel architecture that augments story processing with an external dynamic knowledge graph. In contrast to static commonsense knowledge graphs which hold information about the real world, these dynamic knowledge graphs reflect facts extracted from the story being processed. Our architecture uses these knowledge graphs to create information-rich prompts which better facilitate story comprehension than prompts composed only of story text. We apply our architecture to the tasks of question answering and story completion. To complement this line of research, we introduce two long-form question answering tasks, LF-SQuAD and LF-QUOREF, in which the document length exceeds the size of the language model's context window, and introduce a story completion evaluation method that bypasses the stochastic nature of language model generation. We demonstrate broad improvement over typical prompt formulation methods for both question answering and story completion using GPT-2, GPT-3 and XLNet",
    "volume": "main",
    "checked": true,
    "id": "fc92ed618ded1d1463f7ec736093f420b05827db",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21287": {
    "title": "Diagnostics-Guided Explanation Generation",
    "abstract": "Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks",
    "volume": "main",
    "checked": true,
    "id": "d592d56418c4df24b443c038a88ec5606a615cde",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21288": {
    "title": "Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic",
    "abstract": "Acquiring high-quality temporal common sense (TCS) knowledge from free-form text is a crucial but challenging problem for event-centric natural language understanding, due to the language reporting bias problem: people rarely report the commonly observed events but highlight the special cases. For example, one may rarely report \"I get up from bed in 1 minute\", but we can observe \"It takes me an hour to get up from bed every morning'' in text. Models directly trained upon such corpus would capture distorted TCS knowledge, which could influence the model performance. Prior work addresses this issue mainly by exploiting the interactions among temporal dimensions (e.g., duration, temporal relation between events) in a multi-task view. However, this line of work suffers the limitation of implicit, inadequate and unexplainable interactions modeling. In this paper, we propose a novel neural-logic based Soft Logic Enhanced Event Temporal Reasoning (SLEER) model for acquiring unbiased TCS knowledge, in which the complementary relationship among dimensions are explicitly represented as logic rules and modeled by t-norm fuzzy logics. SLEER can utilize logic rules to regularize its inference process. Experimental results on four intrinsic evaluation datasets and two extrinsic datasets show the efficiency of our proposed method",
    "volume": "main",
    "checked": true,
    "id": "98f19ca97512361b12475b42b67a617de14d33a1",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21289": {
    "title": "Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",
    "abstract": "Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms. A common way to improve model robustness is adversarial training which follows two steps—collecting adversarial examples by attacking a target model, and fine-tuning the model on the augmented dataset with these adversarial examples. The objective of traditional adversarial training is to make a model produce the same correct predictions on an original/adversarial example pair. However, the consistency between model decision-makings on two similar texts is ignored. We argue that a robust model should behave consistently on original/adversarial example pairs, that is making the same predictions (what) based on the same reasons (how) which can be reflected by consistent interpretations. In this work, we propose a novel feature-level adversarial training method named FLAT. FLAT aims at improving model robustness in terms of both predictions and interpretations. FLAT incorporates variational word masks in neural networks to learn global word importance and play as a bottleneck teaching the model to make predictions based on important words. FLAT explicitly shoots at the vulnerability problem caused by the mismatch between model understandings on the replaced words and their synonyms in original/adversarial example pairs by regularizing the corresponding global word importance scores. Experiments show the effectiveness of FLAT in improving the robustness with respect to both predictions and interpretations of four neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks on four text classification tasks. The models trained via FLAT also show better robustness than baseline models on unforeseen adversarial examples across different attacks",
    "volume": "main",
    "checked": true,
    "id": "1afc53f501e0801a7385084965aed84d28d35cf4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21290": {
    "title": "Unsupervised Editing for Counterfactual Stories",
    "abstract": "Creating what-if stories requires reasoning about prior statements and possible outcomes of the changed conditions. One can easily generate coherent endings under new conditions, but it would be challenging for current systems to do it with minimal changes to the original story. Therefore, one major challenge is the trade-off between generating a logical story and rewriting with minimal-edits. In this paper, we propose EDUCAT, an editing-based unsupervised approach for counterfactual story rewriting. EDUCAT includes a target position detection strategy based on estimating causal effects of the what-if conditions, which keeps the causal invariant parts of the story. EDUCAT then generates the stories under fluency, coherence and minimal-edits constraints. We also propose a new metric to alleviate the shortcomings of current automatic metrics and better evaluate the trade-off. We evaluate EDUCAT on a public counterfactual story rewriting benchmark. Experiments show that EDUCAT achieves the best trade-off over unsupervised SOTA methods according to both automatic and human evaluation. The resources of EDUCAT are available at: https://github.com/jiangjiechen/EDUCAT",
    "volume": "main",
    "checked": true,
    "id": "76a4ad0a90c9374fd81ed4b50621727e87ff0f4f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21291": {
    "title": "LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification",
    "abstract": "Given a natural language statement, how to verify its veracity against a large-scale textual knowledge source like Wikipedia? Most existing neural models make predictions without giving clues about which part of a false claim goes wrong. In this paper, we propose LOREN, an approach for interpretable fact verification. We decompose the verification of the whole claim at phrase-level, where the veracity of the phrases serves as explanations and can be aggregated into the final verdict according to logical rules. The key insight of LOREN is to represent claim phrase veracity as three-valued latent variables, which are regularized by aggregation logical rules. The final claim verification is based on all latent variables. Thus, LOREN enjoys the additional benefit of interpretability --- it is easy to explain how it reaches certain results with claim phrase veracity. Experiments on a public fact verification benchmark show that LOREN is competitive against previous approaches while enjoying the merit of faithful and accurate interpretability. The resources of LOREN are available at: https://github.com/jiangjiechen/LOREN",
    "volume": "main",
    "checked": true,
    "id": "0177d4e486872f2b95d018dc836dea98f686dd2b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21292": {
    "title": "ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification",
    "abstract": "Few-shot text classification has recently been promoted by the meta-learning paradigm which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. Despite their success, existing works building their meta-learner based on Prototypical Networks are unsatisfactory in learning discriminative text representations between similar classes, which may lead to contradictions during label prediction. In addition, the task-level and instance-level overfitting problems in few-shot text classification caused by a few training examples are not sufficiently tackled. In this work, we propose a contrastive learning framework named ContrastNet to tackle both discriminative representation and overfitting problems in few-shot text classification. ContrastNet learns to pull closer text representations belonging to the same class and push away text representations belonging to different classes, while simultaneously introducing unsupervised contrastive regularization at both task-level and instance-level to prevent overfitting. Experiments on 8 few-shot text classification datasets show that ContrastNet outperforms the current state-of-the-art models",
    "volume": "main",
    "checked": true,
    "id": "5657a053bb3081be8b3974c33cb6d73c4c347c15",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21293": {
    "title": "From Good to Best: Two-Stage Training for Cross-Lingual Machine Reading Comprehension",
    "abstract": "Cross-lingual Machine Reading Comprehension (xMRC) is a challenging task due to the lack of training data in low-resource languages. Recent approaches use training data only in a resource-rich language (such as English) to fine-tune large-scale cross-lingual pre-trained language models, which transfer knowledge from resource-rich languages (source) to low-resource languages (target). Due to the big difference between languages, the model fine-tuned only by the source language may not perform well for target languages. In our study, we make an interesting observation that while the top 1 result predicted by the previous approaches may often fail to hit the ground-truth answer, there are still good chances for the correct answer to be contained in the set of top k predicted results. Intuitively, the previous approaches have empowered the model certain level of capability to roughly distinguish good answers from bad ones. However, without sufficient training data, it is not powerful enough to capture the nuances between the accurate answer and those approximate ones. Based on this observation, we develop a two-stage approach to enhance the model performance. The first stage targets at recall; we design a hard-learning (HL) algorithm to maximize the likelihood that the top k predictions contain the accurate answer. The second stage focuses on precision, where an answer-aware contrastive learning (AA-CL) mechanism is developed to learn the minute difference between the accurate answer and other candidates. Extensive experiments show that our model significantly outperforms strong baselines on two cross-lingual MRC benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "01b35b50a5c3df50477382b6ef5ff28cc8776482",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21294": {
    "title": "Probing Linguistic Information for Logical Inference in Pre-trained Language Models",
    "abstract": "Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods",
    "volume": "main",
    "checked": true,
    "id": "9b17e77056110abc1beb1ac271f76f48508bbc0b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21295": {
    "title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets",
    "abstract": "Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks.    In this work, we study what specific traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks.   We propose to use artificially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have.   By fine-tuning the pre-trained models on GLUE benchmark, we can learn how beneficial it is to transfer the knowledge from the model trained on the dataset possessing that specific trait.   We define and discuss three different characteristics in the artificial dataset: 1) matching the token's uni-gram or bi-gram distribution between pre-training and downstream fine-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence.    Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance.   Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies.   Based on our analysis, we find that models pre-trained with artificial datasets are prone to learn spurious correlation in downstream tasks.   Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences.    This result helps us understand the exceptional transferability of pre-trained LMs",
    "volume": "main",
    "checked": true,
    "id": "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21296": {
    "title": "C2L: Causally Contrastive Learning for Robust Text Classification",
    "abstract": "Despite the super-human accuracy of recent deep models in NLP tasks, their robustness is reportedly limited due to their reliance on spurious patterns. We thus aim to leverage contrastive learning and counterfactual augmentation for robustness. For augmentation, existing work either requires humans to add counterfactuals to the dataset or machines to automatically matches near-counterfactuals already in the dataset. Unlike existing augmentation is affected by spurious correlations, ours, by synthesizing \"a set\" of counterfactuals, and making a collective decision on the distribution of predictions on this set, can robustly supervise the causality of each term. Our empirical results show that our approach, by collective decisions, is less sensitive to task model bias of attribution-based synthesis, and thus achieves significant improvements, in diverse dimensions: 1) counterfactual robustness, 2) cross-domain generalization, and 3) generalization from scarce data",
    "volume": "main",
    "checked": true,
    "id": "a04b3744834b1b167a590bbb3b6230a75e20accc",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21297": {
    "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
    "abstract": "Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation",
    "volume": "main",
    "checked": true,
    "id": "23e8ed7568454e11d9a6fecb8242e1d16b1828d5",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21298": {
    "title": "Flexible Instance-Specific Rationalization of NLP Models",
    "abstract": "Recent research on model interpretability in natural language processing extensively uses feature scoring methods for identifying which parts of the input are the most important for a model to make a prediction (i.e. explanation or rationale). However, previous research has shown that there is no clear best scoring method across various text classification tasks while practitioners typically have to make several other ad-hoc choices regarding the length and the type of the rationale (e.g. short or long, contiguous or not). Inspired by this, we propose a simple yet effective and flexible method that allows selecting optimally for each data instance: (1) a feature scoring method; (2) the length; and (3) the type of the rationale. Our method is inspired by input erasure approaches to interpretability which assume that the most faithful rationale for a prediction should be the one with the highest difference between the model's output distribution using the full text and the text after removing the rationale as input respectively. Evaluation on four standard text classification datasets shows that our proposed method provides more faithful, comprehensive and highly sufficient explanations compared to using a fixed feature scoring method, rationale length and type. More importantly, we demonstrate that a practitioner is not required to make any ad-hoc choices in order to extract faithful rationales using our approach",
    "volume": "main",
    "checked": true,
    "id": "a88eb2104efba1a421700501eb56a995b3bb2b1c",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21299": {
    "title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation",
    "abstract": "Assessing the quality of natural language generation (NLG) systems through human annotation is very expensive. Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU or ROUGE) have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms. In this paper, we introduce InfoLM a family of untrained metrics that can be viewed as a string-based metric that addresses the aforementioned flaws thanks to a pre-trained masked language model. This family of metrics also makes use of information measures allowing the possibility to adapt InfoLM to different evaluation criteria. Using direct assessment, we demonstrate that InfoLM achieves statistically significant improvement and two figure correlation gains in many configurations compared to existing metrics on both summarization and data2text generation tasks",
    "volume": "main",
    "checked": true,
    "id": "ae36bf4d3f6167f89149a3a7e9da9a602a768cde",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21300": {
    "title": "Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation",
    "abstract": "Sarcasm is a pervading linguistic phenomenon and highly challenging to explain due to its subjectivity, lack of context and deeply-felt opinion. In the multimodal setup, sarcasm is conveyed through the incongruity between the text and visual entities. Although recent approaches deal with sarcasm as a classification problem, it is unclear why an online post is identified as sarcastic. Without proper explanation, end users may not be able to perceive the underlying sense of irony. In this paper, we propose a novel problem -- Multimodal Sarcasm Explanation (MuSE) -- given a multimodal sarcastic post containing an image and a caption, we aim to generate a natural language explanation to reveal the intended sarcasm. To this end, we develop MORE, a new dataset with explanation of 3510 sarcastic multimodal posts. Each explanation is a natural language (English) sentence describing the hidden irony. We benchmark MORE by employing a multimodal Transformer-based architecture. It incorporates a cross-modal attention in the Transformer's encoder which attends to the distinguishing features between the two modalities. Subsequently, a BART-based auto-regressive decoder is used as the generator. Empirical results demonstrate convincing results over various baselines (adopted for MuSE) across five evaluation metrics. We also conduct human evaluation on predictions and obtain Fleiss' Kappa score of 0.4 as a fair agreement among 25 evaluators",
    "volume": "main",
    "checked": true,
    "id": "be97ecf88d6bdf0c35d6bc00751bb6c8d6a0c484",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21301": {
    "title": "Zero-Shot Commonsense Question Answering with Cloze Translation and Consistency Optimization",
    "abstract": "Commonsense question answering (CQA) aims to test if models can answer questions regarding commonsense knowledge that everyone knows. Prior works that incorporate external knowledge bases have shown promising results, but knowledge bases are expensive to construct and are often limited to a fixed set of relations. In this paper, we instead focus on better utilizing the implicit knowledge stored in pre-trained language models. While researchers have found that the knowledge embedded in pre-trained language models can be extracted by having them fill in the blanks of carefully designed prompts for relation extraction and text classification, it remains unclear if we can adopt this paradigm in CQA where the inputs and outputs take much more flexible forms. To this end, we investigate four translation methods that can translate natural questions into cloze-style sentences to better solicit commonsense knowledge from language models, including a syntactic-based model, an unsupervised neural model, and two supervised neural models. In addition, to combine the different translation methods, we propose to encourage consistency among model predictions on different translated questions with unlabeled data. We demonstrate the effectiveness of our methods on three CQA datasets in zero-shot settings. We show that our methods are complementary to a knowledge base improved model, and combining them can lead to state-of-the-art zero-shot performance. Analyses also reveal distinct characteristics of the different cloze translation methods and provide insights on why combining them can lead to great improvements. Code/dataset is available at https://github.com/PlusLabNLP/zero_shot_cqa",
    "volume": "main",
    "checked": true,
    "id": "c3a454e50ec0610f1380d55b1988a5eb5d45207b",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21302": {
    "title": "Synthetic Disinformation Attacks on Automated Fact Verification Systems",
    "abstract": "Automated fact-checking is a needed technology to curtail the spread of online misinformation. One current framework for such solutions proposes to verify claims by retrieving supporting or refuting evidence from related textual sources. However, the realistic use cases for fact-checkers will require verifying claims against evidence sources that could be affected by the same misinformation. Furthermore, the development of modern NLP tools that can produce coherent, fabricated content would allow malicious actors to systematically generate adversarial disinformation for fact-checkers.In this work, we explore the sensitivity of automated fact-checkers to synthetic adversarial evidence in two simulated settings: ADVERSARIAL ADDITION, where we fabricate documents and add them to the evidence repository available to the fact-checking system, and ADVERSARIAL MODIFICATION, where existing evidence source documents in the repository are automatically altered. Our study across multiple models on three benchmarks demonstrates that these systems suffer significant performance drops against these attacks. Finally, we discuss the growing threat of modern NLG systems as generators of disinformation in the context of the challenges they pose to automated fact-checkers",
    "volume": "main",
    "checked": true,
    "id": "536dd3e54ad83e64d27217485db230ea09b19f51",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21303": {
    "title": "Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement",
    "abstract": "End-to-end speech-to-text translation (E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus〈speech, transcription, translation〉, the conventional high-quality E2E-ST system leverages the〈speech, transcription〉pair to pre-train the model and then utilizes the〈speech, translation〉pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by a pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs while achieving better performance in the automatic speech recognition task",
    "volume": "main",
    "checked": true,
    "id": "139a83b5eab873dd464e2cf897887a95c8d07e13",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21304": {
    "title": "Play the Shannon Game with Language Models: A Human-Free Approach to Summary Evaluation",
    "abstract": "The goal of a summary is to concisely state the most important information in a document. With this principle in mind, we introduce new reference-free summary evaluation metrics that use a pretrained language model to estimate the information content shared between a document and its summary. These metrics are a modern take on the Shannon Game, a method for summary quality scoring proposed decades ago, where we replace human annotators with language models. We also view these metrics as an extension of BLANC, a recently proposed approach to summary quality measurement based on the performance of a language model with and without the help of a summary. Using transformer based language models, we empirically verify that our metrics achieve state-of-the-art correlation with human judgement of the summary quality dimensions of both coherence and relevance, as well as competitive correlation with human judgement of consistency and fluency",
    "volume": "main",
    "checked": true,
    "id": "c64f56e46b74540038cf79c0f35ac20e1b91f521",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21305": {
    "title": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis",
    "abstract": "In recent years, pretrained language models have revolutionized the NLP world, while achieving state of the art performance in various downstream tasks. However, in many cases, these models do not perform well when labeled data is scarce and the model is expected to perform in the zero or few shot setting. Recently, several works have shown that continual pretraining or performing a second phase of pretraining (inter-training) which is better aligned with the downstream task, can lead to improved results, especially in the scarce data setting. Here, we propose to leverage sentiment-carrying discourse markers to generate large-scale weakly-labeled data, which in turn can be used to adapt language models for sentiment analysis. Extensive experimental results show the value of our approach on various benchmark datasets, including the finance domain. Code, models and data are available at https://github.com/ibm/tslm-discourse-markers",
    "volume": "main",
    "checked": true,
    "id": "3013bf2c987db18ec6fb4c6f28ecd1c9f1f669f4",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21306": {
    "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models",
    "abstract": "We investigate the use of multimodal information contained in images as an effective method for enhancing the commonsense of Transformer models for text generation. We perform experiments using BART and T5 on concept-to-text generation, specifically the task of generative commonsense reasoning, or CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text Generation. VisCTG involves captioning images representing appropriate everyday scenarios, and using these captions to enrich and steer the generation process. Comprehensive evaluation and analysis demonstrate that VisCTG noticeably improves model performance while successfully addressing several issues of the baseline generations, including poor commonsense, fluency, and specificity",
    "volume": "main",
    "checked": true,
    "id": "299983121dec88d4cc8e4ea2aa06514787d8d878",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21307": {
    "title": "Language Model Priming for Cross-Lingual Event Extraction",
    "abstract": "We present a novel, language-agnostic approach to \"priming\" language models for the task of event extraction, providing particularly effective performance in low-resource and zero-shot cross-lingual settings. With priming, we augment the input to the transformer stack's language model differently depending on the question(s) being asked of the model at runtime. For instance, if the model is being asked to identify arguments for the trigger \"protested\", we will provide that trigger as part of the input to the language model, allowing it to produce different representations for candidate arguments than when it is asked about arguments for the trigger \"arrest\" elsewhere in the same sentence. We show that by enabling the language model to better compensate for the deficits of sparse and noisy training data, our approach improves both trigger and argument detection and classification significantly over the state of the art in a zero-shot cross-lingual setting",
    "volume": "main",
    "checked": true,
    "id": "e500a0500a42e4e7fe919a46e88524a5148eb1a8",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21308": {
    "title": "Language Modelling via Learning to Rank",
    "abstract": "We consider language modelling (LM) as a multi-label structured prediction task by re-framing training from solely predicting a single ground-truth word to ranking a set of words which could continue a given context. To avoid annotating top-k ranks, we generate them using pre-trained LMs: GPT-2, BERT, and Born-Again models. This leads to a rank-based form of knowledge distillation (KD). We also develop a method using N-grams to create a non-probabilistic teacher which generates the ranks without the need of a pre-trained LM.We confirm the hypotheses: that we can treat LMing as a ranking task and that we can do so without the use of a pre-trained LM.   We show that rank-based KD generally gives a modest improvement to perplexity (PPL) -- though often with statistical significance -- when compared to Kullback–Leibler-based KD. Surprisingly, given the naivety of the method, the N-grams act as competitive teachers and achieve similar performance as using either BERT or a Born-Again model teachers. Unsurprisingly, GPT-2 always acts as the best teacher.    Using it and a Transformer-XL student on Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and against a KL-based KD of 56.70",
    "volume": "main",
    "checked": true,
    "id": "f21be3f230cb2721904671c7747165edad8bd033",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21309": {
    "title": "NAREOR: The Narrative Reordering Problem",
    "abstract": "Many implicit inferences exist in text depending on how it is structured that can critically impact the text's interpretation and meaning. One such structural aspect present in text with chronology is the order of its presentation. For narratives or stories, this is known as the narrative order. Reordering a narrative can impact the temporal, causal, event-based, and other inferences readers draw from it, which in turn can have strong effects both on its interpretation and interestingness. In this paper, we propose and investigate the task of Narrative Reordering (NAREOR) which involves rewriting a given story in a different narrative order while preserving its plot. We present a dataset, NAREORC, with human rewritings of stories within ROCStories in non-linear orders, and conduct a detailed analysis of it. Further, we propose novel task-specific training methods with suitable evaluation metrics. We perform experiments on NAREORC using state-of-the-art models such as BART and T5 and conduct extensive automatic and human evaluations. We demonstrate that although our models can perform decently, NAREOR is a challenging task with potential for further exploration. We also investigate two applications of NAREOR: generation of more interesting variations of stories and serving as adversarial sets for temporal/event-related tasks, besides discussing other prospective ones, such as for pedagogical setups related to language skills like essay writing and applications to medicine involving clinical narratives",
    "volume": "main",
    "checked": true,
    "id": "3a14e36108aca336e37b28d89a6f4c529f73954c",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21310": {
    "title": "UNISON: Unpaired Cross-Lingual Image Captioning",
    "abstract": "Image captioning has emerged as an interesting research field in recent years due to its broad application scenarios. The traditional paradigm of image captioning relies on paired image-caption datasets to train the model in a supervised manner. However, creating such paired datasets for every target language is prohibitively expensive, which hinders the extensibility of captioning technology and deprives a large part of the world population of its benefit. In this work, we present a novel unpaired cross-lingual method to generate image captions without relying on any caption corpus in the source or the target language. Specifically, our method consists of two phases: (1) a cross-lingual auto-encoding process, which utilizing a sentence parallel (bitext) corpus to learn the mapping from the source to the target language in the scene graph encoding space and decode sentences in the target language, and (2) a cross-modal unsupervised feature mapping, which seeks to map the encoded scene graph features from image modality to language modality. We verify the effectiveness of our proposed method on the Chinese image caption generation task. The comparisons against several existing methods demonstrate the effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "a39e6c647c483721dbc2843ccddb60d2f247c85b",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21311": {
    "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch",
    "abstract": "Transformer-based pre-trained language models like BERT and its variants have recently achieved promising performance in various natural language processing (NLP) tasks. However, the conventional paradigm constructs the backbone by purely stacking the manually designed global self-attention layers, introducing inductive bias and thus leads to sub-optimal. In this work, we make the first attempt to automatically discover novel pre-trained language model (PLM) backbone on a flexible search space containing the most fundamental operations from scratch. Specifically, we propose a well-designed search space which (i) contains primitive math operations in the intra-layer level to explore novel attention structures, and (ii) leverages convolution blocks to be the supplementary for attentions in the inter-layer level to better learn local dependency. To enhance the efficiency for finding promising architectures, we propose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm, which optimizes both the search algorithm and evaluation of candidate models. Specifically, we propose Operation-Priority (OP) evolution strategy to facilitate model search via balancing exploration and exploitation. Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for fast model evaluation. Extensive experiments show that the searched architecture (named AutoBERT-Zero) significantly outperforms BERT and its variants of different model capacities in various downstream tasks, proving the architecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set",
    "volume": "main",
    "checked": true,
    "id": "47354f49a4768719add414ea853977cb868faf25",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21312": {
    "title": "ISEEQ: Information Seeking Question Generation Using Dynamic Meta-Information Retrieval and Knowledge Graphs",
    "abstract": "Conversational Information Seeking (CIS) is a relatively new research area within conversational AI that attempts to seek information from end-users in order to understand and satisfy the users' needs. If realized, such a CIS system has far-reaching benefits in the real world; for example, CIS systems can assist clinicians in pre-screening or triaging patients in healthcare. A key open sub-problem in CIS that remains unaddressed in the literature is generating Information Seeking Questions (ISQs) based on a short initial query from the end-user. To address this open problem, we propose Information SEEking Question generator (ISEEQ), a novel approach for generating ISQs from just a short user query, given a large text corpus relevant to the user query. Firstly, ISEEQ uses a knowledge graph to enrich the user query. Secondly, ISEEQ uses the knowledge-enriched query to retrieve relevant context passages to ask coherent ISQs adhering to a conceptual flow. Thirdly, ISEEQ introduces a new deep generative-adversarial reinforcement learning-based approach for generating ISQs. We show that ISEEQ can generate high-quality ISQs to promote the development of CIS agents. ISEEQ significantly outperforms comparable baselines on five ISQ evaluation metrics across four datasets having user queries from diverse domains. Further, we argue that ISEEQ is transferable across domains for generating ISQs, as it shows the acceptable performance when trained and tested on different pairs of domains. A qualitative human evaluation confirms that ISEEQ generated ISQs are comparable in quality to human-generated questions, and it outperformed the best comparable baseline",
    "volume": "main",
    "checked": true,
    "id": "77d2456630d7b22efe84bffcc7d4ad495ce50a6d",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21313": {
    "title": "Explainable Metaphor Identification Inspired by Conceptual Metaphor Theory",
    "abstract": "Metaphor is not only a linguistic phenomenon but also reflects the concept projection between source and target domains in human cognition. Previous sequence tagging-based metaphor identification methods could not model the concept projection, resulting in a limitation that the outputs of these models are unexplainable in the predictions of the metaphoricity labels. In this work, we propose the first explainable metaphor identification model, inspired by Conceptual Metaphor Theory. The model is based on statistic learning, a lexical resource, and a novel reward mechanism. Our model can identify the metaphoricity on the word-pair level, and explain the predicted metaphoricity labels via learned concept mappings. The use of the reward mechanism allows the model to learn the optimal concept mappings without knowing their true labels. Our method is also applicable for the concepts that are out of training domains by using the lexical resource. The automatically generated concept mappings demonstrate the implicit human thoughts in metaphoric expressions. Our experiments show the effectiveness of the proposed model in metaphor identification, and concept mapping tasks, respectively",
    "volume": "main",
    "checked": true,
    "id": "57be279d11ad05f8268e680c3c255cee25c50bc1",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21314": {
    "title": "Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss",
    "abstract": "Data-driven methods have achieved notable performance on intent detection, which is a task to comprehend user queries. Nonetheless, they are controversial for over-confident predictions. In some scenarios, users do not only care about the accuracy but also the confidence of model. Unfortunately, mainstream neural networks are poorly calibrated, with a large gap between accuracy and confidence. To handle this problem defined as confidence calibration, we propose a model using the hyperspherical space and rebalanced accuracy-uncertainty loss. Specifically, we project the label vector onto hyperspherical space uniformly to generate a dense label representation matrix, which mitigates over-confident predictions due to overfitting sparse one-hot label matrix. Besides, we rebalance samples of different accuracy and uncertainty to better guide model training. Experiments on the open datasets verify that our model outperforms the existing calibration methods and achieves a significant improvement on the calibration metric",
    "volume": "main",
    "checked": true,
    "id": "30b123fbae05d5f3fdea81b69178041d6b2aff49",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21315": {
    "title": "SSAST: Self-Supervised Audio Spectrogram Transformer",
    "abstract": "Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST",
    "volume": "main",
    "checked": true,
    "id": "12601dd02aa624c4c676f42c8f7b034fa3457e37",
    "citation_count": 34
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21316": {
    "title": "Block-Skim: Efficient Question Answering for Transformer",
    "abstract": "Transformer models have achieved promising results on natural language processing (NLP) tasks including extractive question answering (QA). Common Transformer encoders used in NLP tasks process the hidden states of all input tokens in the context paragraph throughout all layers. However, different from other tasks such as sequence classification, answering the raised question does not necessarily need all the tokens in the context paragraph. Following this motivation, we propose Block-skim, which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their full-size counterparts. Block-Skim improves QA models' accuracy on different datasets and achieves 3 times speedup on BERT-base model",
    "volume": "main",
    "checked": true,
    "id": "f5a3dbc0518df5ca1b6333ae93244dde7f793736",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21317": {
    "title": "Deep Clustering of Text Representations for Supervision-Free Probing of Syntax",
    "abstract": "We explore deep clustering of multilingual text representations for unsupervised model interpretation and induction of syntax. As these representations are high-dimensional, out-of-the-box methods like K-means do not work well. Thus, our approach jointly transforms the representations into a lower-dimensional cluster-friendly space and clusters them. We consider two notions of syntax: Part of Speech Induction (POSI) and Constituency Labelling (CoLab) in this work. Interestingly, we find that Multilingual BERT (mBERT) contains surprising amount of syntactic knowledge of English; possibly even as much as English BERT (E-BERT). Our model can be used as a supervision-free probe which is arguably a less-biased way of probing. We find that unsupervised probes show benefits from higher layers as compared to supervised probes. We further note that our unsupervised probe utilizes E-BERT and mBERT representations differently, especially for POSI. We validate the efficacy of our probe by demonstrating its capabilities as a unsupervised syntax induction technique. Our probe works well for both syntactic formalisms by simply adapting the input representations. We report competitive performance of our probe on 45-tag English POSI, state-of-the-art performance on 12-tag POSI across 10 languages, and competitive results on CoLab. We also perform zero-shot syntax induction on resource impoverished languages and report strong results",
    "volume": "main",
    "checked": true,
    "id": "c38184c7ed9d798c83dbb48c8231e5a950a9b420",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21318": {
    "title": "Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-training",
    "abstract": "The goal of stance detection is to determine the viewpoint expressed in a piece of text towards a target. These viewpoints or contexts are often expressed in many different languages depending on the user and the platform, which can be a local news outlet, a social media platform, a news forum, etc. Most research on stance detection, however, has been limited to working with a single language and on a few limited targets, with little work on cross-lingual stance detection. Moreover, non-English sources of labelled data are often scarce and present additional challenges. Recently, large multilingual language models have substantially improved the performance on many non-English tasks, especially such with a limited number of examples. This highlights the importance of model pre-training and its ability to learn from few examples. In this paper, we present the most comprehensive study of cross-lingual stance detection to date: we experiment with 15 diverse datasets in 12 languages from 6 language families, and with 6 low-resource evaluation settings each. For our experiments, we build on pattern-exploiting training (PET), proposing the addition of a novel label encoder to simplify the verbalisation procedure. We further propose sentiment-based generation of stance data for pre-training, which shows sizeable improvement of more than 6% F1 absolute in few-shot learning settings compared to several strong baselines",
    "volume": "main",
    "checked": true,
    "id": "ad18b16df23000dc15fa70bcde93125bc4b4a486",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21319": {
    "title": "Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation",
    "abstract": "Controlling neural network-based models for natural language generation (NLG) to realize desirable attributes in the generated outputs has broad applications in numerous areas such as machine translation, document summarization, and dialog systems. Approaches that enable such control in a zero-shot manner would be of great importance as, among other reasons, they remove the need for additional annotated data and training. In this work, we propose novel approaches for controlling encoder-decoder transformer-based NLG models in zero shot. While zero-shot control has previously been observed in massive models (e.g., GPT3), our method enables such control for smaller models. This is done by applying two control knobs, attention biasing and context augmentation, to these models directly during decoding and without additional training or auxiliary models. These knobs control the generation process by directly manipulating trained NLG models (e.g., biasing cross-attention layers). We show that not only are these NLG models robust to such manipulations but also their behavior could be controlled without an impact on their generation performance",
    "volume": "main",
    "checked": true,
    "id": "bccfcca7b28c4792bd59659c56e1d7843542dfe9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21320": {
    "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection",
    "abstract": "Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings. For reproducibility, we release the code and data at https://github.com/siat-nlp/GALAXY",
    "volume": "main",
    "checked": true,
    "id": "127ffc8697630a76b1b4149c24d1350f69205f41",
    "citation_count": 29
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21321": {
    "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
    "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples",
    "volume": "main",
    "checked": true,
    "id": "2569a7309142e40815cf556b6417059df9abbda8",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21322": {
    "title": "BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents",
    "abstract": "Key information extraction (KIE) from document images requires understanding the contextual and spatial semantics of texts in two-dimensional (2D) space.  Many recent studies try to solve the task by developing pre-trained language models focusing on combining visual features from document images with texts and their layout.  On the other hand, this paper tackles the problem by going back to the basic: effective combination of text and layout.   Specifically, we propose a pre-trained language model, named BROS (BERT Relying On Spatiality), that encodes relative positions of texts in 2D space and learns from unlabeled documents with area-masking strategy.  With this optimized training scheme for understanding texts in 2D space, BROS shows comparable or better performance compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and SciTSR) without relying on visual features.  This paper also reveals two real-world challenges in KIE tasks--(1) minimizing the error from incorrect text ordering and (2) efficient learning from fewer downstream examples--and demonstrates the superiority of BROS over previous methods",
    "volume": "main",
    "checked": true,
    "id": "a0e4521d440e8013d09fcea3d9c6d4b24bbdefc7",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21323": {
    "title": "Non-autoregressive Translation with Layer-Wise Prediction and Deep Supervision",
    "abstract": "How do we perform efficient inference while retaining high translation quality? Existing neural machine translation models, such as Transformer, achieve high performance, but they decode words one by one, which is inefficient. Recent non-autoregressive translation models speed up the inference, but their quality is still inferior. In this work, we propose DSLP, a highly efficient and high-performance model for machine translation. The key insight is to train a non-autoregressive Transformer with Deep Supervision and feed additional Layer-wise Predictions. We conducted extensive experiments on four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO). Results show that our approach consistently improves the BLEU scores compared with respective base models. Specifically, our best variant outperforms the autoregressive model on three translation tasks, while being 14.8 times more efficient in inference",
    "volume": "main",
    "checked": true,
    "id": "3db8de1e088e1837146c4f2f3c9d13a6c40434da",
    "citation_count": 22
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21324": {
    "title": "Word Level Robustness Enhancement: Fight Perturbation with Perturbation",
    "abstract": "State-of-the-art deep NLP models have achieved impressive improvements on many tasks. However, they are found to be vulnerable to some perturbations. Before they are widely adopted, the fundamental issues of robustness need to be addressed. In this paper, we design a robustness enhancement method to defend against word substitution perturbation, whose basic idea is to fight perturbation with perturbation. We find that: although many well-trained deep models are not robust in the setting of the presence of adversarial samples, they satisfy weak robustness. That means they can handle most non-crafted perturbations well. Taking advantage of the weak robustness property of deep models, we utilize non-crafted perturbations to resist the adversarial perturbations crafted by attackers. Our method contains two main stages. The first stage is using randomized perturbation to conform the input to the data distribution. The second stage is using randomized perturbation to eliminate the instability of prediction results and enhance the robustness guarantee. Experimental results show that our method can significantly improve the ability of deep models to resist the state-of-the-art adversarial attacks while maintaining the prediction performance on the original clean data",
    "volume": "main",
    "checked": true,
    "id": "2ee1bbce0238092e8087dccd2e26b6ecb293367c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21325": {
    "title": "Predicting Above-Sentence Discourse Structure Using Distant Supervision from Topic Segmentation",
    "abstract": "RST-style discourse parsing plays a vital role in many NLP tasks, revealing the underlying semantic/pragmatic structure of potentially complex and diverse documents. Despite its importance, one of the most prevailing limitations in modern day discourse parsing is the lack of large-scale datasets. To overcome the data sparsity issue, distantly supervised approaches from tasks like sentiment analysis and summarization have been recently proposed. Here, we extend this line of research by exploiting distant supervision from topic segmentation, which can arguably provide a strong and oftentimes complementary signal for high-level discourse structures. Experiments on two human-annotated discourse treebanks confirm that our proposal generates accurate tree structures on sentence and paragraph level, consistently outperforming previous distantly supervised models on the sentence-to-document task and occasionally reaching even higher scores on the sentence-to-paragraph level",
    "volume": "main",
    "checked": true,
    "id": "c88f908e9e7c9a6f839f72920322bf86289bb99b",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21326": {
    "title": "Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge",
    "abstract": "Humans usually have conversations by making use of prior knowledge about a topic and background information of the people whom they are talking to. However, existing conversational agents and datasets do not consider such comprehensive information, and thus they have a limitation in generating the utterances where the knowledge and persona are fused properly. To address this issue, we introduce a call For Customized conversation (FoCus) dataset where the customized answers are built with the user's persona and Wikipedia knowledge. To evaluate the abilities to make informative and customized utterances of pre-trained language models, we utilize BART and GPT-2 as well as transformer-based models. We assess their generation abilities with automatic scores and conduct human evaluations for qualitative results. We examine whether the model reflects adequate persona and knowledge with our proposed two sub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we show that the utterances of our data are constructed with the proper knowledge and persona through grounding quality assessment",
    "volume": "main",
    "checked": true,
    "id": "c65c96cfc8b448fe9fd6bbfe1aaaea727515f4f7",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21327": {
    "title": "Towards Building ASR Systems for the Next Billion Users",
    "abstract": "Recent methods in speech and language technology pretrain very large models which are fine-tuned for specific tasks. However, the benefits of such large models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vectors of similar sounding phonemes are shared across languages, representations across layers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent",
    "volume": "main",
    "checked": true,
    "id": "a2d2ec16aa26226c3f59fcbf227ac869bbb0f999",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21328": {
    "title": "Span-Based Semantic Role Labeling with Argument Pruning and Second-Order Inference",
    "abstract": "We study graph-based approaches to span-based semantic role labeling. This task is difficult due to the need to enumerate all possible predicate-argument pairs and the high degree of imbalance between positive and negative samples. Based on these difficulties, high-order inference that considers interactions between multiple arguments and predicates is often deemed beneficial but has rarely been used in span-based semantic role labeling. Because even for second-order inference, there are already O(n^5) parts for a sentence of length n, and exact high-order inference is intractable. In this paper, we propose a framework consisting of two networks: a predicate-agnostic argument pruning network that reduces the number of candidate arguments to O(n), and a semantic role labeling network with an optional second-order decoder that is unfolded from an approximate inference algorithm. Our experiments show that our framework achieves significant and consistent improvement over previous approaches",
    "volume": "main",
    "checked": true,
    "id": "0fd98f53c596cef4e0da0f0716146e50e4268ef8",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21329": {
    "title": "Incorporating Constituent Syntax for Coreference Resolution",
    "abstract": "Syntax has been shown to benefit Coreference Resolution from incorporating long-range dependencies and structured information captured by syntax trees, either in traditional statistical machine learning based systems or recently proposed neural models. However, most leading systems use only dependency trees. We argue that constituent trees also encode important information, such as explicit span-boundary signals captured by nested multi-word phrases, extra linguistic labels and hierarchical structures useful for detecting anaphora. In this work, we propose a simple yet effective graph-based method to incorporate constituent syntactic structures. Moreover, we also explore to utilise higher-order neighbourhood information to encode rich structures in constituent trees. A novel message propagation mechanism is therefore proposed to enable information flow among elements in syntax trees. Experiments on the English and Chinese portions of OntoNotes 5.0 benchmark show that our proposed model either beats a strong baseline or achieves new state-of-the-art performance. Code is available at https://github.com/Fantabulous-J/Coref-Constituent-Graph",
    "volume": "main",
    "checked": true,
    "id": "b0471deb442c048e3b2bb6b109d3d45ea9576f4d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21330": {
    "title": "XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge",
    "abstract": "Cross-lingual pre-training has achieved great successes using monolingual and bilingual plain text corpora. However, most pre-trained models neglect multilingual knowledge, which is language agnostic but comprises abundant cross-lingual structure alignment. In this paper, we propose XLM-K, a cross-lingual language model incorporating multilingual knowledge in pre-training. XLM-K augments existing multilingual pre-training with two knowledge tasks, namely Masked Entity Prediction Task and Object Entailment Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly demonstrate significant improvements over existing multilingual language models. The results on MLQA and NER exhibit the superiority of XLM-K in knowledge related tasks. The success in XNLI shows a better cross-lingual transferability obtained in XLM-K. What is more, we provide a detailed probing analysis to confirm the desired knowledge captured in our pre-training regimen. The code is available at https://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk",
    "volume": "main",
    "checked": true,
    "id": "607fa844e35b70a3a72d86c639312fe8670ba342",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21331": {
    "title": "Hierarchical Context Tagging for Utterance Rewriting",
    "abstract": "Utterance rewriting aims to recover coreferences and omitted information from the latest turn of a multi-turn dialogue. Recently, methods that tag rather than linearly generate sequences have proven stronger in both in- and out-of-domain rewriting settings. This is due to a tagger's smaller search space as it can only copy tokens from the dialogue context. However, these methods may suffer from low coverage when phrases that must be added to a source utterance cannot be covered by a single context span. This can occur in languages like English that introduce tokens such as prepositions into the rewrite for grammaticality. We propose a hierarchical context tagger (HCT) that mitigates this issue by predicting slotted rules (e.g., \"besides _\") whose slots are later filled with context spans. HCT (i) tags the source string with token-level edit actions and slotted rules and (ii) fills in the resulting rule slots with spans from the dialogue context. This rule tagging allows HCT to add out-of-context tokens and multiple spans at once; we further cluster the rules to truncate the long tail of the rule distribution. Experiments on several benchmarks show that HCT can outperform state-of-the-art rewriting systems by ~2 BLEU points",
    "volume": "main",
    "checked": true,
    "id": "f327a27515c72d0c7c92e8d2e83475477e68f877",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21332": {
    "title": "Search and Learn: Improving Semantic Coverage for Data-to-Text Generation",
    "abstract": "Data-to-text generation systems aim to generate text descriptions based on input data (often represented in the tabular form). A typical system uses huge training samples for learning the correspondence between tables and texts. However, large training sets are expensive to obtain, limiting the applicability of these approaches in real-world scenarios. In this work, we focus on few-shot data-to-text generation. We observe that, while fine-tuned pretrained language models may generate plausible sentences, they suffer from the low semantic coverage problem in the few-shot setting. In other words, important input slots tend to be missing in the generated text. To this end, we propose a search-and-learning approach that leverages pretrained language models but inserts the missing slots to improve the semantic coverage. We further finetune our system based on the search results to smooth out the search noise, yielding better-quality text and improving inference efficiency to a large extent. Experiments show that our model achieves high performance on E2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E, largely alleviating the low coverage problem",
    "volume": "main",
    "checked": true,
    "id": "3aaa5a41a3d6cc7e465163a2529cecaf2a988f74",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21333": {
    "title": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations",
    "abstract": "Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching (unification) of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge (the \"knowledge acquisition\" problem). To address these issues, we devise a novel logical reasoner called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query. We use a simple QA example from a children’s story to motivate Braid’s design and explain how the various components work together to produce a coherent logical explanation. Finally, we evaluate Braid on the ROC Story Cloze test and achieve close to state-of-the-art results while providing frame-based explanations",
    "volume": "main",
    "checked": true,
    "id": "6e399a6840c73013c389c2212bef5a87bdf2e996",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21334": {
    "title": "Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data",
    "abstract": "Multimodal pre-training for audio-and-text has recently been proved to be effective and has significantly improved the performance of many downstream speech understanding tasks. However, these state-of-the-art pre-training audio-text models work well only when provided with large amount of parallel audio-and-text data, which brings challenges on many languages that are rich in unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we investigate whether it is possible to pre-train an audio-text multimodal model with extremely low-resource parallel data and extra non-parallel unimodal data. Our pre-training framework consists of the following components: (1) Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input text (audio) representations from a noisy version of itself. (2) Cross-modal Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input text (audio), given both a noisy version of the input text (audio) and the corresponding translated noisy audio features (text embeddings). (3) Iterative Denoising Process (IDP), which iteratively translates raw audio (text) and the corresponding text embeddings (audio features) translated from previous iteration into the new less-noisy text embeddings (audio features). We adapt a dual cross-modal Transformer as our backbone model which consists of two unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our method achieves comparable performance on multiple downstream speech understanding tasks compared with the model pre-trained on fully parallel data, demonstrating the great potential of the proposed method",
    "volume": "main",
    "checked": true,
    "id": "09f61bf7b7d02112166620571fa8d958ba6cd7b5",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21335": {
    "title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech",
    "abstract": "Learning to understand grounded language, which connects natural language to percepts, is a critical research area. Prior work in grounded language acquisition has focused primarily on textual inputs. In this work, we demonstrate the feasibility of performing grounded language acquisition on paired visual percepts and raw speech inputs. This will allow human-robot interactions in which language about novel tasks and environments is learned from end-users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems. We leverage recent work in self-supervised speech representation models and show that learned representations of speech can make language grounding systems more inclusive towards specific groups while maintaining or even increasing general performance",
    "volume": "main",
    "checked": true,
    "id": "844a83ef632af2b6ab8a83e56055a1ee2c65c7d6",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21336": {
    "title": "ALP: Data Augmentation Using Lexicalized PCFGs for Few-Shot Text Classification",
    "abstract": "Data augmentation has been an important ingredient for boosting performances of learned models. Prior data augmentation methods for few-shot text classification have led to great performance boosts. However, they have not been designed to capture the intricate compositional structure of natural language. As a result, they fail to generate samples with plausible and diverse sentence structures. Motivated by this, we present the data Augmentation using Lexicalized Probabilistic context-free grammars (ALP) that generates augmented samples with diverse syntactic structures with plausible grammar. The lexicalized PCFG parse trees consider both the constituents and dependencies to produce a syntactic frame that maximizes a variety of word choices in a syntactically preservable manner without specific domain experts. Experiments on few-shot text classification tasks demonstrate that ALP enhances many state-of-the-art classification methods. As a second contribution, we delve into the train-val splitting methodologies when a data augmentation method comes into play. We argue empirically that the traditional splitting of training and validation sets is sub-optimal compared to our novel augmentation-based splitting strategies that further expand the training split with the same number of labeled data. Taken together, our contributions on the data augmentation strategies yield a strong training recipe for few-shot text classification tasks",
    "volume": "main",
    "checked": true,
    "id": "1c740daa31bb2b68a4ac85c4e5c1b5b9367252c7",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21337": {
    "title": "CAISE: Conversational Agent for Image Search and Editing",
    "abstract": "Demand for image editing has been increasing as users' desire for expression is also increasing. However, for most users, image editing tools are not easy to use since the tools require certain expertise in photo effects and have complex interfaces. Hence, users might need someone to help edit their images, but having a personal dedicated human assistant for every user is impossible to scale. For that reason, an automated assistant system for image editing is desirable. Additionally, users want more image sources for diverse image editing works, and integrating an image search functionality into the editing tool is a potential remedy for this demand. Thus, we propose a dataset of an automated Conversational Agent for Image Search and Editing (CAISE). To our knowledge, this is the first dataset that provides conversational image search and editing annotations, where the agent holds a grounded conversation with users and helps them to search and edit images according to their requests. To build such a system, we first collect image search and editing conversations between pairs of annotators. The assistant-annotators are equipped with a customized image search and editing tool to address the requests from the user-annotators. The functions that the assistant-annotators conduct with the tool are recorded as executable commands, allowing the trained system to be useful for real-world application execution. We also introduce a generator-extractor baseline model for this task, which can adaptively select the source of the next token (i.e., from the vocabulary or from textual/visual contexts) for the executable command. This serves as a strong starting point while still leaving a large human-machine performance gap for useful future work. Data and code are available: https://github.com/hyounghk/CAISE",
    "volume": "main",
    "checked": true,
    "id": "5c01c60319cc81e4ba2486de9af4ce5580e5df48",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21338": {
    "title": "Dual Task Framework for Improving Persona-Grounded Dialogue Dataset",
    "abstract": "This paper introduces a simple yet effective data-centric approach for the task of improving persona-conditioned dialogue agents. Prior model-centric approaches unquestioningly depend on the raw crowdsourced benchmark datasets such as Persona-Chat. In contrast, we aim to fix annotation artifacts in benchmarking, which is orthogonally applicable to any dialogue model. Specifically, we augment relevant personas to improve dialogue dataset/agent, by leveraging the primal-dual structure of the two tasks, predicting dialogue responses and personas based on each other. Experiments on Persona-Chat show that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of accuracy",
    "volume": "main",
    "checked": true,
    "id": "1afeaef110134ebbba11604a206a54be547fdee0",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21339": {
    "title": "Minimally-Supervised Joint Learning of Event Volitionality and Subject Animacy Classification",
    "abstract": "Volitionality and subject animacy are fundamental and closely related properties of an event. Their classification is challenging because it requires contextual text understanding and a huge amount of labeled data. This paper proposes a novel method that jointly learns volitionality and subject animacy at a low cost, heuristically labeling events in a raw corpus. Volitionality labels are assigned using a small lexicon of volitional and non-volitional adverbs such as deliberately and accidentally; subject animacy labels are assigned using a list of animate and inanimate nouns obtained from ontological knowledge. We then consider the problem of learning a classifier from the labeled events so that it can perform well on unlabeled events without the words used for labeling. We view the problem as a bias reduction or unsupervised domain adaptation problem and apply the techniques. We conduct experiments with crowdsourced gold data in Japanese and English and show that our method effectively learns volitionality and subject animacy without manually labeled data",
    "volume": "main",
    "checked": true,
    "id": "d6155f8eaf8c4a70c809d9a8c5eaaa160e4e0724",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21340": {
    "title": "From Fully Trained to Fully Random Embeddings: Improving Neural Machine Translation with Compact Word Embedding Tables",
    "abstract": "Embedding matrices are key components in neural natural language processing (NLP) models that are responsible to provide numerical representations of input tokens (i.e. words or subwords). In this paper, we analyze the impact and utility of such matrices in the context of neural machine translation (NMT). We show that detracting syntactic and semantic information from word embeddings and running NMT systems with random embeddings is not as damaging as it initially sounds. We also show how incorporating only a limited amount of task-specific knowledge from fully-trained embeddings can boost the performance NMT systems. Our findings demonstrate that in exchange for negligible deterioration in performance, any NMT model can be run with partially random embeddings. Working with such structures means a minimal memory requirement as there is no longer need to store large embedding tables, which is a significant gain in industrial and on-device settings. We evaluated our embeddings in translating English into German and French and achieved a 5.3x compression rate. Despite having a considerably smaller architecture, our models in some cases are even able to outperform state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "db3f38d1499ac678fa237ca70991144eaeff1636",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21341": {
    "title": "SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems",
    "abstract": "Zero/few-shot transfer to unseen services is a critical challenge in task-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset introduced a paradigm for enabling models to support any service in zero-shot through schemas, which describe service APIs to models in natural language. We explore the robustness of dialogue systems to linguistic variations in schemas by designing SGD-X - a benchmark extending SGD with semantically similar yet stylistically diverse variants for every schema. We observe that two top state tracking models fail to generalize well across schema variants, measured by joint goal accuracy and a novel metric for measuring schema sensitivity. Additionally, we present a simple model-agnostic data augmentation method to improve schema robustness",
    "volume": "main",
    "checked": true,
    "id": "3e9044c80d65a0c9aae8bcec4ed5c8997bcf7e2e",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21342": {
    "title": "Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",
    "abstract": "Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model’s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations",
    "volume": "main",
    "checked": true,
    "id": "876c32f0648e1cbcea525a5cc76bd9e15095f81e",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21343": {
    "title": "Text Revision By On-the-Fly Representation Optimization",
    "abstract": "Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text formality and simplicity. Current state-of-the-art methods formulate these tasks as sequence-to-sequence learning problems, which rely on large-scale parallel training corpus. In this paper, we present an iterative inplace editing approach for text revision, which requires no parallel data. In this approach, we simply fine-tune a pre-trained Transformer with masked language modeling and attribute classification. During inference, the editing at each iteration is realized by two-step span replacement. At the first step, the distributed representation of the text optimizes on the fly towards an attribute function. At the second step, a text span is masked and another new one is proposed conditioned on the optimized representation. The empirical experiments on two typical and important text revision tasks, text formalization and text simplification, show the effectiveness of our approach. It achieves competitive and even better performance than state-of-the-art supervised methods on text simplification, and gains better performance than strong unsupervised methods on text formalization",
    "volume": "main",
    "checked": true,
    "id": "40cabeaeea1c7d7688f9834b7c8081564ece6664",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21344": {
    "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
    "abstract": "So far, named entity recognition (NER) has been involved with three major types, including flat, overlapped (aka. nested), and discontinuous NER, which have mostly been studied individually. Recently, a growing interest has been built for unified NER, tackling the above three jobs concurrently with one single model. Current best-performing methods mainly include span-based and sequence-to-sequence models, where unfortunately the former merely focus on boundary identification and the latter may suffer from exposure bias. In this work, we present a novel alternative by modeling the unified NER as word-word relation classification, namely W^2NER. The architecture resolves the kernel bottleneck of unified NER by effectively modeling the neighboring relations between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in which the unified NER is modeled as a 2D grid of word pairs. We then propose multi-granularity 2D convolutions for better refining the grid representations. Finally, a co-predictor is used to sufficiently reason the word-word relations. We perform extensive experiments on 14 widely-used benchmark datasets for flat, overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our model beats all the current top-performing baselines, pushing the state-of-the-art performances of unified NER",
    "volume": "main",
    "checked": true,
    "id": "7915f5b00d5fba4806184d2afa2ab96435ad1078",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21345": {
    "title": "Sequence-to-Action: Grammatical Error Correction with Action Guided Sequence Generation",
    "abstract": "The task of Grammatical Error Correction (GEC) has received remarkable attention with wide applications in Natural Language Processing (NLP) in recent years. While one of the key principles of GEC is to keep the correct parts unchanged and avoid over-correction, previous sequence-to-sequence (seq2seq) models generate results from scratch, which are not guaranteed to follow the original sentence structure and may suffer from the over-correction problem. In the meantime, the recently proposed sequence tagging models can overcome the over-correction problem by only generating edit operations, but are conditioned on human designed language-specific tagging labels. In this paper, we combine the pros and alleviate the cons of both models by proposing a novel Sequence-to-Action (S2A) module. The S2A module jointly takes the source and target sentences as input, and is able to automatically generate a token-level action sequence before predicting each token, where each action is generated from three choices named SKIP, COPY and GENerate. Then the actions are fused with the basic seq2seq framework to provide final predictions. We conduct experiments on the benchmark datasets of both English and Chinese GEC tasks. Our model consistently outperforms the seq2seq baselines, while being able to significantly alleviate the over-correction problem as well as holding better generality and diversity in the generation results compared to the sequence tagging models",
    "volume": "main",
    "checked": true,
    "id": "fd39b8c41ef56a25b38acec3f5372671cfa18b47",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21346": {
    "title": "Dynamic Key-Value Memory Enhanced Multi-Step Graph Reasoning for Knowledge-Based Visual Question Answering",
    "abstract": "Knowledge-based visual question answering (VQA) is a vision-language task that requires an agent to correctly answer image-related questions using knowledge that is not presented in the given image. It is not only a more challenging task than regular VQA but also a vital step towards building a general VQA system. Most existing knowledge-based VQA systems process knowledge and image information similarly and ignore the fact that the knowledge base (KB) contains complete information about a triplet, while the extracted image information might be incomplete as the relations between two objects are missing or wrongly detected. In this paper, we propose a novel model named dynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which performs explicit and implicit reasoning over a key-value knowledge memory module and a spatial-aware image graph, respectively. Specifically, the memory module learns a dynamic knowledge representation and generates a knowledge-aware question representation at each reasoning step. Then, this representation is used to guide a graph attention operator over the spatial-aware image graph. Our model achieves new state-of-the-art accuracy on the KRVQR and FVQA datasets. We also conduct ablation experiments to prove the effectiveness of each component of the proposed model",
    "volume": "main",
    "checked": true,
    "id": "18ec13d5c66f6ba6c38ede6ba9b451f89e986432",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21347": {
    "title": "Knowledge Bridging for Empathetic Dialogue Generation",
    "abstract": "Lack of external knowledge makes empathetic dialogue systems difficult to perceive implicit emotions and learn emotional interactions from limited dialogue history. To address the above problems, we propose to leverage external knowledge, including commonsense knowledge and emotional lexical knowledge, to explicitly understand and express emotions in empathetic dialogue generation. We first enrich the dialogue history by jointly interacting with external knowledge and construct an emotional context graph. Then we learn emotional context representations from the knowledge-enriched emotional context graph and distill emotional signals, which are the prerequisites to predicate emotions expressed in responses. Finally, to generate the empathetic response, we propose an emotional cross-attention mechanism to learn the emotional dependencies from the emotional context graph. Extensive experiments conducted on a benchmark dataset verify the effectiveness of the proposed method. In addition, we find the performance of our method can be further improved by integrating with a pre-trained model that works orthogonally",
    "volume": "main",
    "checked": true,
    "id": "01caaf3a67ad31c93048a29fff90e62ad3dac167",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21348": {
    "title": "Contrast and Generation Make BART a Good Dialogue Emotion Recognizer",
    "abstract": "In dialogue systems, utterances with similar semantics may have distinctive emotions under different contexts. Therefore, modeling long-range contextual emotional relationships with speaker dependency plays a crucial part in dialogue emotion recognition. Meanwhile, distinguishing the different emotion categories is non-trivial since they usually have semantically similar sentiments. To this end, we adopt supervised contrastive learning to make different emotions mutually exclusive to identify similar emotions better. Meanwhile, we utilize an auxiliary response generation task to enhance the model's ability of handling context information, thereby forcing the model to recognize emotions with similar semantics in diverse contexts. To achieve these objectives, we use the pre-trained encoder-decoder model BART as our backbone model since it is very suitable for both understanding and generation tasks. The experiments on four datasets demonstrate that our proposed model obtains significantly more favorable results than the state-of-the-art model in dialogue emotion recognition. The ablation study further demonstrates the effectiveness of supervised contrastive loss and generative loss",
    "volume": "main",
    "checked": true,
    "id": "fe67243b9929f820c637cd9836deff82713290c8",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21349": {
    "title": "A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues",
    "abstract": "Identifying breakdowns in ongoing dialogues helps to improve communication effectiveness. Most prior work on this topic relies on human annotated data and data augmentation to learn a classification model. While quality labeled dialogue data requires human annotation and is usually expensive to obtain, unlabeled data is easier to collect from various sources. In this paper, we propose a novel semi-supervised teacher-student learning framework to tackle this task. We introduce two teachers which are trained on labeled data and perturbed labeled data respectively. We leverage unlabeled data to improve classification in student training where we employ two teachers to refine the labeling of unlabeled data through teacher-student learning in a bootstrapping manner. Through our proposed training approach, the student can achieve improvements over single-teacher performance. Experimental results on the Dialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify Follow-Up Questions dataset LIF show that our approach outperforms all previous published approaches as well as other supervised and semi-supervised baseline methods",
    "volume": "main",
    "checked": true,
    "id": "3cd02d5e8421612aff2525c1fb294ed229cfaa68",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21350": {
    "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism",
    "abstract": "Singing voice synthesis (SVS) systems are built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing.   In this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain that iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generate realistic outputs.   To further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we propose boundary prediction methods to locate the intersection and determine the shallow step adaptively.  The evaluations conducted on a Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Extensional experiments also prove the generalization of our methods on text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io. Codes: https://github.com/MoonInTheRiver/DiffSinger",
    "volume": "main",
    "checked": true,
    "id": "fe92f3f7ceec008118842d42b578dc25bcba63f9",
    "citation_count": 24
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21351": {
    "title": "KGR4: Retrieval, Retrospect, Refine and Rethink for Commonsense Generation",
    "abstract": "Generative commonsense reasoning requires machines to generate sentences describing an everyday scenario given several concepts, which has attracted much attention recently. However, existing models cannot perform as well as humans, since sentences they produce are often implausible and grammatically incorrect. In this paper, inspired by the process of humans creating sentences, we propose a novel Knowledge-enhanced Commonsense Generation framework, termed KGR4, consisting of four stages: Retrieval, Retrospect, Refine, Rethink. Under this framework, we first perform retrieval to search for relevant sentences from external corpus as the prototypes. Then, we train the generator that either edits or copies these prototypes to generate candidate sentences, of which potential errors will be fixed by an autoencoder-based refiner. Finally, we select the output sentence from candidate sentences produced by generators with different hyper-parameters. Experimental results and in-depth analysis on the CommonGen benchmark strongly demonstrate the effectiveness of our framework. Particularly, KGR4 obtains 33.56 SPICE in the official leaderboard, outperforming the previously-reported best result by 2.49 SPICE and achieving state-of-the-art performance. We release the code at https://github.com/DeepLearnXMU/KGR-4",
    "volume": "main",
    "checked": true,
    "id": "dacc2d8ec6b8eaeda511d7b08e526565a9a2618c",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21352": {
    "title": "Improving Biomedical Information Retrieval with Neural Retrievers",
    "abstract": "Information retrieval (IR) is essential in search engines and dialogue systems as well as natural language processing tasks such as open-domain question answering. IR serve an important function in the biomedical domain, where content and sources of scientific knowledge may evolve rapidly. Although neural retrievers have surpassed traditional IR approaches such as TF-IDF and BM25 in standard open-domain question answering tasks, they are still found lacking in the biomedical domain. In this paper, we seek to improve information retrieval (IR) using neural retrievers (NR) in the biomedical domain, and achieve this goal using a three-pronged approach. First, to tackle the relative lack of data in the biomedical domain, we propose a template-based question generation method that can be leveraged to train neural retriever models. Second, we develop two novel pre-training tasks that are closely aligned to the downstream task of information retrieval. Third, we introduce the ``Poly-DPR'' model which encodes each context into multiple context vectors. Extensive experiments and analysis on the BioASQ challenge suggest that our proposed method leads to large gains over existing neural approaches and beats BM25 in the small-corpus setting. We show that BM25 and our method can complement each other, and a simple hybrid model leads to further gains in the large corpus setting",
    "volume": "main",
    "checked": true,
    "id": "15031e6a94f9d6d19f74740c224a5523ec64d975",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21353": {
    "title": "The King Is Naked: On the Notion of Robustness for Natural Language Processing",
    "abstract": "There is growing evidence that the classical notion of adversarial robustness originally introduced for images has been adopted as a de facto standard by a large part of the NLP research community.   We show that this notion is problematic in the context of NLP as it considers a narrow spectrum of linguistic phenomena. In this paper, we argue for semantic robustness, which is better aligned with the human concept of linguistic fidelity. We characterize semantic robustness in terms of biases that it is expected to induce in a model. We study semantic robustness of a range of vanilla and robustly trained architectures using a template-based generative test bed. We complement the analysis with empirical evidence that, despite being harder to implement, semantic robustness can improve performance %gives guarantees for on complex linguistic phenomena where models robust in the classical sense fail",
    "volume": "main",
    "checked": true,
    "id": "5e86c1adbe5b7cfccf2201e1c34400c819cdcdab",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21354": {
    "title": "Selecting Optimal Context Sentences for Event-Event Relation Extraction",
    "abstract": "Understanding events entails recognizing the structural and temporal orders between event mentions to build event structures/graphs for input documents. To achieve this goal, our work addresses the problems of subevent relation extraction (SRE) and temporal event relation extraction (TRE) that aim to predict subevent and temporal relations between two given event mentions/triggers in texts. Recent state-of-the-art methods for such problems have employed transformer-based language models (e.g., BERT) to induce effective contextual representations for input event mention pairs. However, a major limitation of existing transformer-based models for SRE and TRE is that they can only encode input texts of limited length (i.e., up to 512 sub-tokens in BERT), thus unable to effectively capture important context sentences that are farther away in the documents. In this work, we introduce a novel method to better model document-level context with important context sentences for event-event relation extraction. Our method seeks to identify the most important context sentences for a given entity mention pair in a document and pack them into shorter documents to be consume entirely by transformer-based language models for representation learning. The REINFORCE algorithm is employed to train models where novel reward functions are presented to capture model performance, and context-based and knowledge-based similarity between sentences for our problem. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "caf084d6f1f46f1aad60691efbb1b9bb0a77d257",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21355": {
    "title": "Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-Based Encoder",
    "abstract": "We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for semantic parsing in task-oriented dialog. Our model consists of an encoder network that incrementally builds the semantic parse tree by predicting the non-terminal label and its positions in the linearized tree. At the generation time, the model constructs the semantic parse tree by recursively inserting the predicted non-terminal labels at the predicted positions until termination. RINE achieves state-of-the-art exact match accuracy on low- and high-resource versions of the conversational semantic parsing benchmark TOP, outperforming strong sequence-to-sequence models and transition-based parsers. We also show that our model design is applicable to nested named entity recognition task, where it performs on par with state-of-the-art approach designed for that task. Finally, we demonstrate that our approach is 2-3.5 times faster than the sequence-to-sequence model at inference time",
    "volume": "main",
    "checked": true,
    "id": "6bb6a37a0754093035734d53ddc36df8803d4f47",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21356": {
    "title": "CINS: Comprehensive Instruction for Few-Shot Learning in Task-Oriented Dialog Systems",
    "abstract": "As the labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge is to learn different tasks with the least amount of labeled data. Recently, pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, ie. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompt",
    "volume": "main",
    "checked": true,
    "id": "20da8033ed8b696e2e27ec40b1aa8a0ab82b964c",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21357": {
    "title": "Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes",
    "abstract": "The quadratic memory complexity of transformers prevents long document summarization in low computational resource scenarios. State-of-the-art models need to apply input truncation, thus discarding and ignoring potential summary-relevant contents, leading to a performance drop. Furthermore, this loss is generally destructive for semantic text analytics in high-impact domains such as the legal one. In this paper, we propose a novel semantic self-segmentation (Se3) approach for long document summarization to address the critical problems of low-resource regimes, namely to process inputs longer than the GPU memory capacity and produce accurate summaries despite the availability of only a few dozens of training instances. Se3 segments a long input into semantically coherent chunks, allowing transformers to summarize very long documents without truncation by summarizing each chunk and concatenating the results. Experimental outcomes show the approach significantly improves the performance of abstractive summarization transformers, even with just a dozen of labeled data, achieving new state-of-the-art results on two legal datasets of different domains and contents. Finally, we report ablation studies to evaluate each contribution of the components of our method to the performance gain",
    "volume": "main",
    "checked": true,
    "id": "4eb45f33446018175e266738be22f4d830ed697e",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21358": {
    "title": "Eye of the Beholder: Improved Relation Generalization for Text-Based Reinforcement Learning Agents",
    "abstract": "Text-based games (TBGs) have become a popular proving ground for the demonstration of learning-based agents that make decisions in quasi real-world settings. The crux of the problem for a reinforcement learning agent in such TBGs is identifying the objects in the world, and those objects' relations with that world. While the recent use of text-based resources for increasing an agent's knowledge and improving its generalization have shown promise, we posit in this paper that there is much yet to be learned from visual representations of these same worlds. Specifically, we propose to retrieve images that represent specific instances of text observations from the world and train our agents on such images. This improves the agent's overall understanding of the game scene and objects' relationships to the world around them, and the variety of visual representations on offer allow the agent to generate a better generalization of a relationship. We show that incorporating such images improves the performance of agents in various TBG settings",
    "volume": "main",
    "checked": true,
    "id": "0529d32254a10195ab45e3f04e823113b284c074",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21359": {
    "title": "Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation",
    "abstract": "Current state-of-the-art cross-lingual summarization models employ multi-task learning paradigm, which works on a shared vocabulary module and relies on the self-attention mechanism to attend among tokens in two languages. However, correlation learned by self-attention is often loose and implicit, inefficient in capturing crucial cross-lingual representations between languages. The matter worsens when performing on languages with separate morphological or structural features, making the cross-lingual alignment more challenging, resulting in the performance drop. To overcome this problem, we propose a novel Knowledge-Distillation-based framework for Cross-Lingual Summarization, seeking to explicitly construct cross-lingual correlation by distilling the knowledge of the monolingual summarization teacher into the cross-lingual summarization student. Since the representations of the teacher and the student lie on two different vector spaces, we further propose a Knowledge Distillation loss using Sinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy between those teacher and student representations. Due to the intuitively geometric nature of Sinkhorn Divergence, the student model can productively learn to align its produced cross-lingual hidden states with monolingual hidden states, hence leading to a strong correlation between distant languages. Experiments on cross-lingual summarization datasets in pairs of distant languages demonstrate that our method outperforms state-of-the-art models under both high and low-resourced settings",
    "volume": "main",
    "checked": true,
    "id": "12b744b5a14048c07ede827181d80617edbfe858",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21360": {
    "title": "HiTKG: Towards Goal-Oriented Conversations via Multi-Hierarchy Learning",
    "abstract": "Human conversations are guided by short-term and long-term goals. We study how to plan short-term goal sequences as coherently as humans do and naturally direct them to an assigned long-term goal in open-domain conversations. Goal sequences are a series of knowledge graph (KG) entity-relation connections generated by KG walkers that traverse through the KG. The existing recurrent and graph attention based KG walkers either insufficiently utilize the conversation states or lack global guidance. In our work, a hierarchical model learns goal planning in a hierarchical learning framework. We present HiTKG, a hierarchical transformer-based graph walker that leverages multiscale inputs to make precise and flexible predictions on KG paths. Furthermore, we propose a two-hierarchy learning framework that employs two stages to learn both turn-level (short-term) and global-level (long-term) conversation goals. Specifically, at the first stage, HiTKG is trained in a supervised fashion to learn how to plan turn-level goal sequences; at the second stage, HiTKG tries to naturally approach the assigned global goal via reinforcement learning. In addition, we propose MetaPath as the backbone method for KG path representation to exploit the entity and relation information concurrently. We further propose Multi-source Decoding Inputs and Output-level Length Head to improve the decoding controllability. Our experiments show that HiTKG achieves a significant improvement in the performance of turn-level goal learning compared with state-of-the-art baselines. Additionally, both automatic and human evaluation prove the effectiveness of the two-hierarchy learning framework for both short-term and long-term goal planning",
    "volume": "main",
    "checked": true,
    "id": "68b34d61ea71ab60129fae0e27f37493a1076674",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21361": {
    "title": "Is Discourse Role Important for Emotion Recognition in Conversation?",
    "abstract": "A conversation is a sequence of utterances, where each utterance plays a specific discourse role while expressing a particular emotion. This paper proposes a novel method to exploit latent discourse role information of an utterance to determine the emotion it conveys in a conversation. Specifically, we use a variant of the Variational-Autoencoder (VAE) to model the context-aware latent discourse roles of each utterance in an unsupervised way. The latent discourse role representation further equips the utterance representation with a salient clue for more accurate emotion recognition. Our experiments show that our proposed method beats the best-reported performances on three public Emotion Recognition in Conversation datasets. This proves that the discourse role information of an utterance plays an important role in the emotion recognition task, which no previous work has studied",
    "volume": "main",
    "checked": true,
    "id": "37fd19b0daa4401ea95cd774c1d047ccbb6bc1b5",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21362": {
    "title": "Improved Text Classification via Contrastive Adversarial Training",
    "abstract": "We propose a simple and general method to regularize the fine-tuning of Transformer-based encoders for text classification tasks. Specifically, during fine-tuning we generate adversarial examples by perturbing the word embedding matrix of the model and perform contrastive learning on clean and adversarial examples in order to teach the model to learn noise-invariant representations. By training on both clean and adversarial examples along with the additional contrastive objective, we observe consistent improvement over standard fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned Bert_Large model outperforms Bert_Large baseline by 1.7% on average, and our fine-tuned Roberta_Large improves over Roberta_Large baseline by 1.3%. We additionally validate our method in different domains using three intent classification datasets, where our fine-tuned Roberta_Large outperforms Roberta_Large baseline by 1-2% on average. For the challenging low-resource scenario, we train our system using half of the training data (per intent) in each of the three intent classification datasets, and achieve similar performance compared to the baseline trained with full training data",
    "volume": "main",
    "checked": true,
    "id": "35db4af3d2311547dc57ed41e7e13a8a7bbffc52",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21363": {
    "title": "LeSICiN: A Heterogeneous Graph-Based Approach for Automatic Legal Statute Identification from Indian Legal Documents",
    "abstract": "The task of Legal Statute Identification (LSI) aims to identify the legal statutes that are relevant to a given description of facts or evidence of a legal case.    Existing methods only utilize the textual content of facts and legal articles to guide such a task. However, the citation network among case documents and legal statutes is a rich source of additional information, which is not considered by existing models.    In this work, we take the first step towards utilising both the text and the legal citation network for the LSI task.   We curate a large novel dataset for this task, including facts of cases from several major Indian Courts of Law, and statutes from the Indian Penal Code (IPC).    Modeling the statutes and training documents as a heterogeneous graph, our proposed model LeSICiN can learn rich textual and graphical features, and can also tune itself to correlate these features.    Thereafter, the model can be used to inductively predict links between test documents (new nodes whose graphical features are not available to the model) and statutes (existing nodes).    Extensive experiments on the dataset show that our model comfortably outperforms several state-of-the-art baselines, by exploiting the graphical structure along with textual features",
    "volume": "main",
    "checked": true,
    "id": "499b172ae03e36dcad9f584983d18d70bfd2cdf2",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21364": {
    "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention",
    "abstract": "Transformers are state-of-the-art in a wide range of NLP tasks and have also been applied to many real-world products. Understanding the reliability and certainty of transformer models is crucial for building trustable machine learning applications, e.g., medical diagnosis. Although many recent transformer extensions have been proposed, the study of the uncertainty estimation of transformer models is under-explored. In this work, we propose a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance. This is achieved by learning hierarchical stochastic self-attention that attends to values and a set of learnable centroids, respectively. Then new attention heads are formed with a mixture of sampled centroids using the Gumbel-Softmax trick. We theoretically show that the self-attention approximation by sampling from a Gumbel distribution is upper bounded. We empirically evaluate our model on two text classification tasks with both in-domain (ID) and out-of-domain (OOD) datasets.   The experimental results demonstrate that our approach: (1) achieves the best predictive-uncertainty trade-off among compared methods; (2) exhibits very competitive (in most cases, better) predictive performance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble methods in uncertainty estimation on OOD datasets",
    "volume": "main",
    "checked": true,
    "id": "3f7fae7ace36c562158c2e9b24f02d824f4a203c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21365": {
    "title": "STEPS: Semantic Typing of Event Processes with a Sequence-to-Sequence Approach",
    "abstract": "Enabling computers to comprehend the intent of human actions by processing language is one of the fundamental goals of Natural Language Understanding.    An emerging task in this context is that of free-form event process typing, which aims at understanding the overall goal of a protagonist in terms of an action and an object, given a sequence of events.   This task was initially treated as a learning-to-rank problem by exploiting the similarity between processes and action/object textual definitions.    However, this approach appears to be overly complex, binds the output types to a fixed inventory for possible word definitions and, moreover, leaves space for further enhancements as regards performance.   In this paper, we advance the field by reformulating the free-form event process typing task as a sequence generation problem and put forward STEPS, an end-to-end approach for producing user intent in terms of actions and objects only, dispensing with the need for their definitions.   In addition to this, we eliminate several dataset constraints set by previous works, while at the same time significantly outperforming them.    We release the data and software at https://github.com/SapienzaNLP/steps",
    "volume": "main",
    "checked": true,
    "id": "1b5285a955ecc0e3b2aeb718e3b9720275ea82b1",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21366": {
    "title": "Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification",
    "abstract": "Recently, graph neural networks (GNNs) have been widely used for document classification. However, most existing methods are based on static word co-occurrence graphs without sentence-level information, which poses three challenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual dependency. To address these challenges, we propose a novel GNN-based sparse structure learning model for inductive document classification. Specifically, a document-level graph is initially generated by a disjoint union of sentence-level word co-occurrence graphs. Our model collects a set of trainable edges connecting disjoint words between sentences, and employs structure learning to sparsely select edges with dynamic contextual dependencies. Graphs with sparse structure can jointly exploit local and global contextual information in documents through GNNs. For inductive learning, the refined document graph is further fed into a general readout function for graph-level classification and optimization in an end-to-end manner. Extensive experiments on several real-world datasets demonstrate that the proposed model outperforms most state-of-the-art results, and reveal the necessity to learn sparse structures for each document",
    "volume": "main",
    "checked": true,
    "id": "7805cb9b197433ae3739c67580cfcae6e00d6a19",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21367": {
    "title": "STEM: Unsupervised STructural EMbedding for Stance Detection",
    "abstract": "Stance detection is an important task, supporting many downstream tasks such as discourse parsing and modeling the propagation of fake news, rumors, and science denial. In this paper, we propose a novel framework for stance detection. Our framework is unsupervised and domain-independent. Given a claim and a multi-participant discussion -- we construct the interaction network from which we derive topological embedding for each speaker. These speaker embedding enjoy the following property: speakers with the same stance tend to be represented by similar vectors, while antipodal vectors represent speakers with opposing stances. These embedding are then used to divide the speakers into stance-partitions. We evaluate our method on three different datasets from different platforms. Our method outperforms or is comparable with supervised models while providing confidence levels for its output. Furthermore, we demonstrate how the structural embedding relate to the valence expressed by the speakers. Finally, we discuss some limitations inherent to the framework",
    "volume": "main",
    "checked": true,
    "id": "109217e18523c50ace18d72fd10574265eb5526d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21368": {
    "title": "ValueNet: A New Dataset for Human Value Driven Dialogue System",
    "abstract": "Building a socially intelligent agent involves many challenges, one of which is to teach the agent to speak guided by its value like a human. However, value-driven chatbots are still understudied in the area of dialogue systems. Most existing datasets focus on commonsense reasoning or social norm modeling. In this work, we present a new large-scale human value dataset called ValueNet, which contains human attitudes on 21,374 text scenarios. The dataset is organized in ten dimensions that conform to the basic human value theory in intercultural research. We further develop a Transformer-based value regression model on ValueNet to learn the utility distribution. Comprehensive empirical results show that the learned value model could benefit a wide range of dialogue tasks. For example, by teaching a generative agent with reinforcement learning and the rewards from the value model, our method attains state-of-the-art performance on the personalized dialog generation dataset: Persona-Chat. With values as additional features, existing emotion recognition models enable capturing rich human emotions in the context, which further improves the empathetic response generation performance in the EmpatheticDialogues dataset. To the best of our knowledge, ValueNet is the first large-scale text dataset for human value modeling, and we are the first one trying to incorporate a value model into emotionally intelligent dialogue systems. The dataset is available at https://liang-qiu.github.io/ValueNet/",
    "volume": "main",
    "checked": true,
    "id": "3af37400f1f9a4f4f211c4a472e18963edc2b34f",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21369": {
    "title": "Post-OCR Document Correction with Large Ensembles of Character Sequence-to-Sequence Models",
    "abstract": "In this paper, we propose a novel method to extend sequence-to-sequence models to accurately process sequences much longer than the ones used during training while being sample- and resource-efficient, supported by thorough experimentation. To investigate the effectiveness of our method, we apply it to the task of correcting documents already processed with Optical Character Recognition (OCR) systems using sequence-to-sequence models based on characters. We test our method on nine languages of the ICDAR 2019 competition on post-OCR text correction and achieve a new state-of-the-art performance in five of them. The strategy with the best performance involves splitting the input document in character n-grams and combining their individual corrections into the final output using a voting scheme that is equivalent to an ensemble of a large number of sequence models. We further investigate how to weigh the contributions from each one of the members of this ensemble. Our code for post-OCR correction is shared at https://github.com/jarobyte91/post_ocr_correction",
    "volume": "main",
    "checked": true,
    "id": "039f9c4ffdf0727fd6e8de756498ece0d87a362f",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21370": {
    "title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding",
    "abstract": "Recently, there has been an increasing interest in building question answering (QA) models that reason across multiple modalities, such as text and images. However, QA using images is often limited to just picking the answer from a pre-defined set of options. In addition, images in the real world, especially in news, have objects that are co-referential to the text, with complementary information from both modalities. In this paper, we present a new QA evaluation benchmark with 1,384 questions over news articles that require cross-media grounding of objects in images onto text. Specifically, the task involves multi-hop questions that require reasoning over image-caption pairs to identify the grounded visual object being referred to and then predicting a span from the news body text to answer the question. In addition, we introduce a novel multimedia data augmentation framework, based on cross-media knowledge extraction and synthetic question-answer generation, to automatically augment data that can provide weak supervision for this task. We evaluate both pipeline-based and end-to-end pretraining-based multimedia QA models on our benchmark, and show that they achieve promising performance, while considerably lagging behind human performance hence leaving large room for future work on this challenging new task",
    "volume": "main",
    "checked": true,
    "id": "bba57c53ab9b600f71d888601ed0aa03812c8199",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21371": {
    "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
    "abstract": "Investigating the reasoning abilities of transformer models, and discovering new challenging tasks for them, has been a topic of much interest. Recent studies have found these models to be surprisingly strong at performing deductive reasoning over formal logical theories expressed in natural language. A shortcoming of these studies, however, is that they do not take into account that logical theories, when sampled uniformly at random, do not necessarily lead to hard instances. We propose a new methodology for creating challenging algorithmic reasoning datasets that focus on natural language satisfiability (NLSat) problems. The key idea is to draw insights from empirical sampling of hard propositional SAT problems and from complexity-theoretic studies of language. This methodology allows us to distinguish easy from hard instances, and to systematically increase the complexity of existing reasoning benchmarks such as RuleTaker. We find that current transformers, given sufficient training data, are surprisingly robust at solving the resulting NLSat problems of substantially increased difficulty. They also exhibit some degree of scale-invariance—the ability to generalize to problems of larger size and scope. Our results, however, reveal important limitations too: careful sampling of training data is crucial for building models that generalize to larger problems, and transformer models’ limited scale-invariance suggests they are far from learning robust deductive reasoning algorithms",
    "volume": "main",
    "checked": true,
    "id": "63f17017257063ee034c4082d93005dc4b25d42d",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21372": {
    "title": "SFSRNet: Super-resolution for Single-Channel Audio Source Separation",
    "abstract": "The problem of single-channel audio source separation is to recover (separate) multiple audio sources that are mixed in a single-channel audio signal (e.g. people talking over each other). Some of the best performing single-channel source separation methods utilize downsampling to either make the separation process faster or make the neural networks bigger and increase accuracy. The problem concerning downsampling is that it usually results in information loss. In this paper, we tackle this problem by introducing SFSRNet which contains a super-resolution (SR) network. The SR network is trained to reconstruct the missing information in the upper frequencies of the audio signal by operating on the spectrograms of the output audio source estimations and the input audio mixture. Any separation method where the length of the sequence is a bottleneck in speed and memory can be made faster or more accurate by using the SR network.  Based on the WSJ0-2mix benchmark where estimations of the audio signal of two speakers need to be extracted from the mixture, in our experiments our proposed SFSRNet reaches a scale-invariant signal-to-noise-ratio improvement (SI-SNRi) of 24.0 dB outperforming the state-of-the-art solution SepFormer which reaches an SI-SNRi of 22.3 dB",
    "volume": "main",
    "checked": true,
    "id": "da1d7c3ac8799dee652e5dc9136ca8d0c0c463a0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21373": {
    "title": "CEM: Commonsense-Aware Empathetic Response Generation",
    "abstract": "A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user’s emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user’s emotion, cognitive understanding of the user’s situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user’s situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EMPATHETICDIALOGUES, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses. Our code is available at https://github.com/Sahandfer/CEM",
    "volume": "main",
    "checked": true,
    "id": "604d426c0100ca35b7a0fe3a2279679a11cac675",
    "citation_count": 19
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21374": {
    "title": "Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning over Text",
    "abstract": "Neural Module Networks (NMNs) have been quite successful in incorporating explicit reasoning as learnable modules in various question answering tasks, including the most generic form of numerical reasoning over text in Machine Reading Comprehension (MRC). However to achieve this, contemporary Neural Module Networks models obtain strong supervision in form of specialized program annotation from the QA pairs through various heuristic parsing and exhaustive computation of all possible discrete operations on discrete arguments. Consequently they fail to generalize to more open-ended settings without such supervision. Hence, we propose Weakly Supervised Neuro-Symbolic Module Network (WNSMN) trained with answers as the sole supervision for numerical reasoning based MRC. WNSMN learns to execute a noisy heuristic program obtained from the dependency parse of the query, as discrete actions over both neural and symbolic reasoning modules and trains it end-to-end in a reinforcement learning framework with discrete reward from answer matching. On the subset of DROP having numerical answers, WNSMN outperforms NMN by 32% and the reasoning-free generative language model GenBERT by 8% in exact match accuracy under comparable weakly supervised settings. This showcases the effectiveness of modular networks that can handle explicit discrete reasoning over noisy programs in an end-to-end manner",
    "volume": "main",
    "checked": true,
    "id": "a48b1b5390d7c8621c0dbb55d6e675da83a2027a",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21375": {
    "title": "Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective",
    "abstract": "In recent years, joint text-image embeddings have significantly improved thanks to the development of transformer-based Vision-Language models. Despite these advances, we still need to better understand the representations produced by those models. In this paper, we compare pre-trained and fine-tuned representations at a vision, language and multimodal level. To that end, we use a set of probing tasks to evaluate the performance of state-of-the-art Vision-Language models and introduce new datasets specifically for multimodal probing. These datasets are carefully designed to address a range of multimodal capabilities while minimizing the potential for models to rely on bias. Although the results confirm the ability of Vision-Language models to understand color at a multimodal level, the models seem to prefer relying on bias in text data for object position and size. On semantically adversarial examples, we find that those models are able to pinpoint fine-grained multimodal differences. Finally, we also notice that fine-tuning a Vision-Language model on multimodal tasks does not necessarily improve its multimodal ability. We make all datasets and code available to replicate experiments",
    "volume": "main",
    "checked": true,
    "id": "8a73b0775d36d3c707ec582d19573a38a1196673",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21376": {
    "title": "Entailment Relation Aware Paraphrase Generation",
    "abstract": "We introduce a new task of entailment relation aware paraphrase generation which aims at generating a paraphrase conforming to a given entailment relation (e.g. equivalent, forward entailing, or reverse entailing) with respect to a given  input. We propose a reinforcement learning-based weakly-supervised paraphrasing system, ERAP, that can be trained using existing paraphrase and natural language inference (NLI) corpora without an explicit task-specific corpus. A combination of automated and human evaluations show that ERAP generates paraphrases conforming to the specified entailment relation and are of good quality as compared to the baselines and uncontrolled paraphrasing systems. Using ERAP for augmenting training data for downstream textual entailment task improves performance over an uncontrolled paraphrasing system, and introduces fewer training artifacts, indicating the benefit of explicit control during paraphrasing",
    "volume": "main",
    "checked": true,
    "id": "c44a192fc5bd431f0b30691b92f6c72207c8b8eb",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21377": {
    "title": "Visual Definition Modeling: Challenging Vision & Language Models to Define Words and Objects",
    "abstract": "Architectures that model language and vision together havereceived much attention in recent years. Nonetheless, most tasks in this field focus on end-to-end applications without providing insights on whether it is the underlying semantics of visual objects or words that is captured. In this paper we draw on the established Definition Modeling paradigm and enhance it by grounding, for the first time, textual definitions to visual representations. We name this new task Visual Definition Modeling and put forward DEMETER and DIONYSUS, two benchmarks where, given an image as context, models have to generate a textual definition for a target being either i) a word that describes the image, or ii) an object patch therein. To measure the difficulty of our tasks we finetuned six different baselines and analyzed their performances, which show that a text-only encoder-decoder model is more effective than models pretrained for handling inputs of both modalities concurrently. This demonstrates the complexity of our benchmarks and encourages more research on text generation conditioned on multimodal inputs. The datasets for both benchmarks are available at https://github.com/SapienzaNLP/visual-definition-modeling as well as the code to reproduce our models",
    "volume": "main",
    "checked": true,
    "id": "b2a68e0d59c0f0ddd3eb63b02290a8b166eb085a",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21378": {
    "title": "Active Learning on Pre-trained Language Model with Task-Independent Triplet Loss",
    "abstract": "Active learning attempts to maximize a task model’s performance gain by obtaining a set of informative samples from an unlabeled data pool. Previous active learning methods usually rely on specific network architectures or task-dependent sample acquisition algorithms. Moreover, when selecting a batch sample, previous works suffer from insufficient diversity of batch samples because they only consider the informativeness of each sample. This paper proposes a task-independent batch acquisition method using triplet loss to distinguish hard samples in an unlabeled data pool with similar features but difficult to identify labels. To assess the effectiveness of the proposed method, we compare the proposed method with state-of-the-art active learning methods on two tasks, relation extraction and sentence classification. Experimental results show that our method outperforms baselines on the benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4496f4817a91e5002207da28be3b14d55e51ca6a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21379": {
    "title": "OneRel: Joint Entity and Relation Extraction with One Module in One Step",
    "abstract": "Joint entity and relation extraction is an essential task in natural language processing and knowledge graph construction. Existing approaches usually decompose the joint extraction task into several basic modules or processing steps to make it easy to conduct. However, such a paradigm ignores the fact that the three elements of a triple are interdependent and indivisible. Therefore, previous joint methods suffer from the problems of cascading errors and redundant information. To address these issues, in this paper, we propose a novel joint entity and relation extraction model, named OneRel, which casts joint extraction as a fine-grained triple classification problem. Specifically, our model consists of a scoring-based classifier and a relation-specific horns tagging strategy. The former evaluates whether a token pair and a relation belong to a factual triple. The latter ensures a simple but effective decoding process. Extensive experimental results on two widely used datasets demonstrate that the proposed method performs better than the state-of-the-art baselines, and delivers consistent performance gain on complex scenarios of various overlapping patterns and multiple triples",
    "volume": "main",
    "checked": true,
    "id": "80caf669f7369bcd13a9dac8507328492e5f138f",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21380": {
    "title": "KATG: Keyword-Bias-Aware Adversarial Text Generation for Text Classification",
    "abstract": "Recent work has shown that current text classification models are vulnerable to small adversarial perturbation to inputs, and adversarial training that re-trains the models with the support of adversarial examples is the most popular way to alleviate the impact of the perturbation. However, current adversarial training methods have two principal problems: worse model generalization and ineffective defending against other text attacks. In this paper, we propose a Keyword-bias-aware Adversarial Text Generation model (KATG) that implicitly generates adversarial sentences using a generator-discriminator structure. Instead of using a benign sentence to generate an adversarial sentence, the KATG model utilizes extra multiple benign sentences (namely prior sentences) to guide adversarial sentence generation. Furthermore, to cover more perturbation used in existing attacks, a keyword-bias-aware sampling is proposed to select sentences containing biased words as prior sentences. Besides, to effectively utilize prior sentences, a generative flow mechanism is proposed to construct latent semantic space and learn a latent representation for the prior sentences. Experiments demonstrate that adversarial sentences generated by our KATG model can strengthen the victim model's robustness and generalization",
    "volume": "main",
    "checked": true,
    "id": "95db3a56b925c2e599a673d7bf1a146e7ae23394",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21381": {
    "title": "Unsupervised Deep Keyphrase Generation",
    "abstract": "Keyphrase generation aims to summarize long documents with a collection of salient phrases. Deep neural models have demonstrated remarkable success in this task, with the capability of predicting keyphrases that are even absent from a document. However, such abstractiveness is acquired at the expense of a substantial amount of annotated data. In this paper, we present a novel method for keyphrase generation, AutoKeyGen, without the supervision of any annotated doc-keyphrase pairs. Motivated by the observation that an absent keyphrase in a document may appear in other places, in whole or in part, we construct a phrase bank by pooling all phrases extracted from a corpus. With this phrase bank, we assign phrase candidates to new documents by a simple partial matching algorithm, and then we rank these candidates by their relevance to the document from both lexical and semantic perspectives. Moreover, we bootstrap a deep generative model using these top-ranked pseudo keyphrases to produce more absent candidates. Extensive experiments demonstrate that AutoKeyGen outperforms all unsupervised baselines and can even beat a strong supervised method in certain cases",
    "volume": "main",
    "checked": true,
    "id": "de0f5298fe818745db3e9a1787536d95bc076e30",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21382": {
    "title": "Generation-Focused Table-Based Intermediate Pre-training for Free-Form Question Answering",
    "abstract": "Question answering over semi-structured tables has attracted significant attention in the NLP community.   However, most of the existing work focus on questions that can be answered with short-form answer, i.e. the answer is often a table cell or aggregation of multiple cells.    This can mismatch with the intents of users who want to ask more complex questions that require free-form answers such as explanations.    To bridge the gap, most recently, pre-trained sequence-to-sequence language models such as T5 are used for generating free-form answers based on the question and table inputs.    However, these pre-trained language models have weaker encoding abilities over table cells and schema.    To mitigate this issue, in this work, we present an intermediate pre-training framework, Generation-focused Table-based Intermediate Pre-training (GENTAP), that jointly learns representations of natural language questions and tables.   GENTAP learns to generate via two training objectives to enhance the question understanding and table representation abilities for complex questions.    Based on experimental results, models that leverage GENTAP framework outperform the existing baselines on FETAQA benchmark.    The pre-trained models are not only useful for free-form question answering, but also for few-shot data-to-text generation task, thus showing good transfer ability by obtaining new state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "ea98db542a68f6e4dafe7a51eab32486586524b1",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21383": {
    "title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts",
    "abstract": "Inferring spatial relations in natural language is a crucial ability an intelligent system should possess. The bAbI dataset tries to capture tasks relevant to this domain (task 17 and 19). However, these tasks have several limitations. Most importantly, they are limited to fixed expressions, they are limited in the number of reasoning steps required to solve them, and they fail to test the robustness of models to input that contains irrelevant or redundant information. In this paper, we present a new Question-Answering dataset called StepGame for robust multi-step spatial reasoning in texts. Our experiments demonstrate that state-of-the-art models on the bAbI dataset struggle on the StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental results on both datasets show that our model outperforms all the baselines with superior generalization and robustness performance",
    "volume": "main",
    "checked": true,
    "id": "2a1acf6dce142d9a87d353c7edbda0127dbe69d7",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21384": {
    "title": "MINIMAL: Mining Models for Universal Adversarial Triggers",
    "abstract": "It is well known that natural language models are vulnerable to adversarial attacks, which are mostly input-specific in nature. Recently, it has been shown that there also exist input-agnostic attacks in NLP models, called universal adversarial triggers. However, existing methods to craft universal triggers are data intensive. They require large amounts of data samples to generate adversarial triggers, which are typically inaccessible by attackers. For instance, previous works take 3000 data samples per class for the SNLI dataset to generate adversarial triggers. In this paper, we present a novel data-free approach, MINIMAL, to mine input-agnostic adversarial triggers from models. Using the triggers produced with our data-free algorithm, we reduce the accuracy of Stanford Sentiment Treebank’s positive class from 93.6% to 9.6%. Similarly, for the Stanford Natural LanguageInference (SNLI), our single-word trigger reduces the accuracy of the entailment class from 90.95% to less than 0.6%. Despite being completely data-free, we get equivalent accuracy drops as data-dependent methods",
    "volume": "main",
    "checked": true,
    "id": "08275408b5975e35788e7e88b055f657a590235d",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21385": {
    "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
    "abstract": "The task of summarization often requires a non-trivial understanding of the given text at the semantic level. In this work, we essentially incorporate the constituent structure into the single document summarization via the Graph Neural Networks to learn the semantic meaning of tokens. More specifically, we propose a novel hierarchical heterogeneous graph attention network over constituency-based parse trees for syntax-aware summarization. This approach reflects psychological findings that humans will pinpoint specific selection patterns to construct summaries hierarchically. Extensive experiments demonstrate that our model is effective for both the abstractive and extractive summarization tasks on five benchmark datasets from various domains. Moreover, further performance improvement can be obtained by virtue of state-of-the-art pre-trained models",
    "volume": "main",
    "checked": true,
    "id": "998234184c6b6188483b2b27c06e7a2e5095fb8f",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21386": {
    "title": "Supervising Model Attention with Human Explanations for Robust Natural Language Inference",
    "abstract": "Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, impacting how well they generalise to other unseen datasets. Existing de-biasing approaches focus on preventing the models from learning these biases, which can result in restrictive models and lower performance. We instead investigate teaching the model how a human would approach the NLI task, in order to learn features that will generalise better to previously unseen examples. Using natural language explanations, we supervise the model’s attention weights to encourage more attention to be paid to the words present in the explanations, significantly improving model performance. Our experiments show that the in-distribution improvements of this method are also accompanied by out-of-distribution improvements, with the supervised models learning from features that generalise better to other NLI datasets. Analysis of the model indicates that human explanations encourage increased attention on the important words, with more attention paid to words in the premise and less attention paid to punctuation and stopwords",
    "volume": "main",
    "checked": true,
    "id": "11d57e1af3c1f939efcee67640ccbf9814290016",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21387": {
    "title": "Hyperbolic Disentangled Representation for Fine-Grained Aspect Extraction",
    "abstract": "Automatic identification of salient aspects from user reviews is especially useful for opinion analysis. There has been significant progress in utilizing weakly supervised approaches, which require only a small set of seed words for training aspect classifiers. However, there is always room for improvement. First, no weakly supervised approaches fully utilize latent hierarchies between words. Second, each seed word’s representation should have different latent semantics and be distinct when it represents a different aspect. In this paper we propose HDAE, a hyperbolic disentangled aspect extractor in which a hyperbolic aspect classifier captures words’ latent hierarchies, and an aspect-disentangled representation models the distinct latent semantics of each seed word. Compared to previous baselines, HDAE achieves average F1 performance gains of 18.2% and 24.1% on Amazon product review and restaurant review datasets, respectively. In addition, the embedding visualization experience demonstrates that HDAE is a more effective approach to leveraging seed words. An ablation study and a case study further attest the effectiveness of the proposed components",
    "volume": "main",
    "checked": true,
    "id": "63d3d493b0da02da23bbaf4aa3775dcb0646907e",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21388": {
    "title": "Procedural Text Understanding via Scene-Wise Evolution",
    "abstract": "Procedural text understanding requires machines to reason about entity states within the dynamical narratives. Current procedural text understanding approaches are commonly entity-wise, which separately track each entity and independently predict different states of each entity. Such an entity-wise paradigm does not consider the interaction between entities and their states. In this paper, we propose a new scene-wise paradigm for procedural text understanding, which jointly tracks states of all entities in a scene-by-scene manner. Based on this paradigm, we propose Scene Graph Reasoner (SGR), which introduces a series of dynamically evolving scene graphs to jointly formulate the evolution of entities, states and their associations throughout the narrative. In this way, the deep interactions between all entities and states can be jointly captured and simultaneously derived from scene graphs. Experiments show that SGR not only achieves the new state-of-the-art performance but also significantly accelerates the speed of reasoning",
    "volume": "main",
    "checked": true,
    "id": "110743395cec1dcb0b7d2e87e692530e615a1f4c",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21389": {
    "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning",
    "abstract": "Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance",
    "volume": "main",
    "checked": true,
    "id": "d1eb051c6b13eba8a9b333d5ee0a55250717195d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21390": {
    "title": "Chess as a Testbed for Language Model State Tracking",
    "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \"full attention\". Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models",
    "volume": "main",
    "checked": true,
    "id": "453fc588d97958c6fefad96e79edd896873b3e09",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21391": {
    "title": "Contrast-Enhanced Semi-supervised Text Classification with Few Labels",
    "abstract": "Traditional text classification requires thousands of annotated data or an additional Neural Machine Translation (NMT) system, which are expensive to obtain in real applications. This paper presents a Contrast-Enhanced Semi-supervised Text Classification (CEST) framework under label-limited settings without incorporating any NMT systems. We propose a certainty-driven sample selection method and a contrast-enhanced similarity graph to utilize data more efficiently in self-training, alleviating the annotation-starving problem. The graph imposes a smoothness constraint on the unlabeled data to improve the coherence and the accuracy of pseudo-labels. Moreover, CEST formulates the training as a \"learning from noisy labels\" problem and performs the optimization accordingly. A salient feature of this formulation is the explicit suppression of the severe error propagation problem in conventional semi-supervised learning. With solely 30 labeled data per class for both training and validation dataset, CEST outperforms the previous state-of-the-art algorithms by 2.11% accuracy and only falls within the 3.04% accuracy range of fully-supervised pre-training language model fine-tuning on thousands of labeled data",
    "volume": "main",
    "checked": true,
    "id": "1d091415de4f1a4b369d72cdbf4a3545d9f74cb7",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21392": {
    "title": "Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration",
    "abstract": "Regenerating natural language explanations in the scientific domain has been proposed as a benchmark to evaluate complex multi-hop and explainable inference. In this context, large language models can achieve state-of-the-art performance when employed as cross-encoder architectures and fine-tuned on human-annotated explanations. However, while much attention has been devoted to the quality of the explanations, the problem of performing inference efficiently is largely under studied. Cross-encoders, in fact, are intrinsically not scalable, possessing limited applicability to real-world scenarios that require inference on massive facts banks. To enable complex multi-hop reasoning at scale, this paper focuses on bi-encoder architectures, investigating the problem of scientific explanation regeneration at the intersection of dense and sparse models. Specifically, we present SCAR (for Scalable Autoregressive Inference), a hybrid framework that iteratively combines a Transformer-based bi-encoder with a sparse model of explanatory power, designed to leverage explicit inference patterns in the explanations. Our experiments demonstrate that the hybrid framework significantly outperforms previous sparse models, achieving performance comparable with that of state-of-the-art cross-encoders while being approx 50 times faster and scalable to corpora of millions of facts. Further analyses on semantic drift and multi-hop question answering reveal that the proposed hybridisation boosts the quality of the most challenging explanations, contributing to improved performance on downstream inference tasks",
    "volume": "main",
    "checked": true,
    "id": "9fac50713a0597f40416e753d0f3b6fe3b19ec1c",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21393": {
    "title": "DetIE: Multilingual Open Information Extraction Inspired by Object Detection",
    "abstract": "State of the art neural methods for open information extraction (OpenIE) usually extract triplets (or tuples) iteratively in an autoregressive or predicate-based manner in order not to produce duplicates. In this work, we propose a different approach to the problem that can be equally or more successful. Namely, we present a novel single-pass method for OpenIE inspired by object detection algorithms from computer vision. We use an order-agnostic loss based on bipartite matching that forces unique predictions and a Transformer-based encoder-only architecture for sequence labeling. The proposed approach is faster and shows superior or similar performance in comparison with state of the art models on standard benchmarks in terms of both quality metrics and inference time. Our model sets the new state of the art performance of 67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference than previous state of the art. We also evaluate the multilingual version of our model in the zero-shot setting for two languages and introduce a strategy for generating synthetic multilingual data to fine-tune the model for each specific language. In this setting, we show performance improvement of 15% on multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish languages. Code and models are available at https://github.com/sberbank-ai/DetIE",
    "volume": "main",
    "checked": true,
    "id": "6ee1df036087234d003ad42737f3f6d54629644a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21394": {
    "title": "Hybrid Neural Networks for On-Device Directional Hearing",
    "abstract": "On-device directional hearing requires audio source separation from a given direction while achieving stringent human-imperceptible latency requirements. While neural nets can achieve significantly better performance than traditional beamformers, all existing models fall short of supporting low-latency causal inference on computationally-constrained wearables. We present DeepBeam, a hybrid model that combines traditional beamformers with a custom lightweight neural net. The former reduces the computational burden of the latter and also improves its generalizability, while the latter is designed to further reduce the memory and computational overhead to enable real-time and low-latency operations. Our evaluation shows comparable performance to state-of-the-art causal inference models on synthetic data while achieving a 5x reduction of model size, 4x reduction of computation per second, 5x reduction in processing time and generalizing better to real hardware data. Further, our real-time hybrid model runs in 8 ms on mobile CPUs designed for low-power wearable devices and achieves an end-to-end latency of 17.5 ms",
    "volume": "main",
    "checked": true,
    "id": "5b353f3f9f67057fb164f8ea232916aa2905cfd3",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21395": {
    "title": "Non-parametric Online Learning from Human Feedback for Neural Machine Translation",
    "abstract": "We study the problem of online learning with human feedback in the human-in-the-loop machine translation, in which the human translators revise the machine-generated translations and then the corrected translations are used to improve the neural machine translation (NMT) system. However, previous methods require online model updating or additional translation memory networks to achieve high-quality performance, making them inflexible and inefficient in practice.   In this paper, we propose a novel non-parametric online learning method without changing the model structure.   This approach introduces two k-nearest-neighbor (KNN) modules: one module memorizes the human feedback, which is the correct sentences provided by human translators,    while the other balances the usage of the history human feedback and original NMT models adaptively.    Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate that our proposed method obtains substantial improvements on translation accuracy and achieves better adaptation performance with less repeating human correction operations",
    "volume": "main",
    "checked": true,
    "id": "41da1bf3b4a328cc32193000ffc75a3797c49f4a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21396": {
    "title": "Parameter Differentiation Based Multilingual Neural Machine Translation",
    "abstract": "Multilingual neural machine translation (MNMT) aims to translate multiple languages with a single model and has been proved successful thanks to effective knowledge transfer among different languages with shared parameters. However, it is still an open question which parameters should be shared and which ones need to be task-specific. Currently, the common practice is to heuristically design or search language-specific modules, which is difficult to find the optimal configuration. In this paper, we propose a novel parameter differentiation based method that allows the model to determine which parameters should be language-speciﬁc during training. Inspired by cellular differentiation, each shared parameter in our method can dynamically differentiate into more specialized types. We further deﬁne the differentiation criterion as inter-task gradient similarity. Therefore, parameters with conﬂicting inter-task gradients are more likely to be language-specific. Extensive experiments on multilingual datasets have demonstrated that our method signiﬁcantly outperforms various strong baselines with different parameter sharing conﬁgurations. Further analysis reveals that the parameter sharing configuration obtained by our method correlates well with the linguistic proximities",
    "volume": "main",
    "checked": true,
    "id": "b6d553cfdaa31456cb17259c3190dd0cf68e0b30",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21397": {
    "title": "DisenCite: Graph-Based Disentangled Representation Learning for Context-Specific Citation Generation",
    "abstract": "Citing and describing related literature are crucial to scientific writing. Many existing approaches show encouraging performance in citation recommendation, but are unable to accomplish the more challenging and onerous task of citation text generation. In this paper, we propose a novel disentangled representation based model DisenCite to automatically generate the citation text through integrating paper text and citation graph. A key novelty of our method compared with existing approaches is to generate context-specific citation text, empowering the generation of different types of citations for the same paper. In particular, we first build and make available a graph enhanced contextual citation dataset (GCite) with 25K edges in different types characterized by citation contained sections over 4.8K research papers. Based on this dataset, we encode each paper according to both textual contexts and structure information in the heterogeneous citation graph. The resulted paper representations are then disentangled by the mutual information regularization between this paper and its neighbors in graph. Extensive experiments demonstrate the superior performance of our method comparing to state-of-the-art approaches. We further conduct ablation and case studies to reassure that the improvement of our method comes from generating the context-specific citation through incorporating the citation graph",
    "volume": "main",
    "checked": true,
    "id": "f4749ec7b46d0c04ea7ca9b64e2317d877b901c0",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21398": {
    "title": "HEAL: A Knowledge Graph for Distress Management Conversations",
    "abstract": "The demands of the modern world are increasingly responsible for causing psychological burdens and bringing adverse impacts on our mental health. As a result, neural conversational agents with empathetic responding and distress management capabilities have recently gained popularity. However, existing end-to-end empathetic conversational agents often generate generic and repetitive empathetic statements such as \"I am sorry to hear that\", which fail to convey specificity to a given situation. Due to the lack of controllability in such models, they also impose the risk of generating toxic responses. Chatbots leveraging reasoning over knowledge graphs is seen as an efficient and fail-safe solution over end-to-end models. However, such resources are limited in the context of emotional distress. To address this, we introduce HEAL, a knowledge graph developed based on 1M distress narratives and their corresponding consoling responses curated from Reddit. It consists of 22K nodes identifying different types of stressors, speaker expectations, responses, and feedback types associated with distress dialogues and forms 104K connections between different types of nodes. Each node is associated with one of 41 affective states. Statistical and visual analysis conducted on HEAL reveals emotional dynamics between speakers and listeners in distress-oriented conversations and identifies useful response patterns leading to emotional relief. Automatic and human evaluation experiments show that HEAL's responses are more diverse, empathetic, and reliable compared to the baselines",
    "volume": "main",
    "checked": true,
    "id": "5962e4c4ec6cc2bec0f7f35e1aef189019b1278c",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21399": {
    "title": "Deep Fusing Pre-trained Models into Neural Machine Translation",
    "abstract": "Pre-training and fine-tuning have become the de facto paradigm in many natural language processing (NLP) tasks. However, compared to other NLP tasks, neural machine translation (NMT) aims to generate target language sentences through the contextual representation from the source language counterparts. This characteristic means the optimization objective of NMT is far from that of the universal pre-trained models (PTMs), leading to the standard procedure of pre-training and fine-tuning does not work well in NMT. In this paper, we propose a novel framework to deep fuse the pre-trained representation into NMT, fully exploring the potential of PTMs in NMT. Specifically, we directly replace the randomly initialized Transformer encoder with a pre-trained encoder and propose a layer-wise coordination structure to coordinate PTM and NMT decoder learning. Then, we introduce a partitioned multi-task learning method to fine-tune the pre-trained parameter, reducing the gap between PTM and NMT by progressively learning the task-specific representation. Experimental results show that our approach achieves considerable improvements on WMT14 En2De, WMT14 En2Fr, and WMT16 Ro2En translation benchmarks and outperforms previous work in both autoregressive and non-autoregressive NMT models",
    "volume": "main",
    "checked": true,
    "id": "698113eac0df3bd6d8cf724ea6b5fa94a5bc57d9",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21400": {
    "title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models",
    "abstract": "We introduce VAST, the Valence-Assessing Semantics Test, a novel intrinsic evaluation task for contextualized word embeddings (CWEs). Despite the widespread use of contextualizing language models (LMs), researchers have no intrinsic evaluation task for understanding the semantic quality of CWEs and their unique properties as related to contextualization, the change in the vector representation of a word based on surrounding words; tokenization, the breaking of uncommon words into subcomponents; and LM-specific geometry learned during training. VAST uses valence, the association of a word with pleasantness, to measure the correspondence of word-level LM semantics with widely used human judgments, and examines the effects of contextualization, tokenization, and LM-specific geometry. Because prior research has found that CWEs from OpenAI's 2019 English-language causal LM GPT-2 perform poorly on other intrinsic evaluations, we select GPT-2 as our primary subject, and include results showing that VAST is useful for 7 other LMs, and can be used in 7 languages. GPT-2 results show that the semantics of a word are more similar to the semantics of context in layers closer to model output, such that VAST scores diverge between our contextual settings, ranging from Pearson’s rho of .55 to .77 in layer 11. We also show that multiply tokenized words are not semantically encoded until layer 8, where they achieve Pearson’s rho of .46, indicating the presence of an encoding process for multiply tokenized words which differs from that of singly tokenized words, for which rho is highest in layer 0. We find that a few neurons with values having greater magnitude than the rest mask word-level semantics in GPT-2’s top layer, but that word-level semantics can be recovered by nullifying non-semantic principal components: Pearson’s rho in the top layer improves from .32 to .76. Downstream POS tagging and sentence classification experiments indicate that the GPT-2 uses these principal components for non-semantic purposes, such as to represent sentence-level syntax relevant to next-word prediction. After isolating semantics, we show the utility of VAST for understanding LM semantics via improvements over related work on four word similarity tasks, with a score of .50 on SimLex-999, better than the previous best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests, which compare differences in word embedding associations between groups of words, exhibit more stereotype-congruent biases after isolating semantics, indicating that non-semantic structures in LMs also mask social biases",
    "volume": "main",
    "checked": true,
    "id": "bce35b3d1cba7a8dd7bb350213780202a2922ec7",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21401": {
    "title": "A Label Dependence-Aware Sequence Generation Model for Multi-Level Implicit Discourse Relation Recognition",
    "abstract": "Implicit discourse relation recognition (IDRR) is a challenging but crucial task in discourse analysis. Most existing methods train multiple models to predict multi-level labels independently, while ignoring the dependence between hierarchically structured labels. In this paper, we consider multi-level IDRR as a conditional label sequence generation task and propose a Label Dependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we first design a label attentive encoder to learn the global representation of an input instance and its level-specific contexts, where the label dependence is integrated to obtain better label embeddings. Then, we employ a label sequence decoder to output the predicted labels in a top-down manner, where the predicted higher-level labels are directly used to guide the label prediction at the current level. We further develop a mutual learning enhanced training method to exploit the label dependence in a bottom-up direction, which is captured by an auxiliary decoder introduced during training. Experimental results on the PDTB dataset show that our model achieves the state-of-the-art performance on multi-level IDRR. We release our code at https://github.com/nlpersECJTU/LDSGM",
    "volume": "main",
    "checked": true,
    "id": "b0db941b5f9cd3a9c1ace0cb9fd9b65acb7dd219",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21402": {
    "title": "Fast and Constrained Absent Keyphrase Generation by Prompt-Based Learning",
    "abstract": "Generating absent keyphrases, which do not appear in the input document, is challenging in the keyphrase prediction task. Most previous works treat the problem as an autoregressive sequence-to-sequence generation task, which demonstrates promising results for generating grammatically correct and fluent absent keyphrases. However, such an end-to-end process with a complete data-driven manner is unconstrained, which is prone to generate keyphrases inconsistent with the input document. In addition, the existing autoregressive decoding method makes the generation of keyphrases must be done from left to right, leading to slow speed during inference. In this paper, we propose a constrained absent keyphrase generation method in a prompt-based learning fashion. Specifically, the prompt will be created firstly based on the keywords, which are defined as the overlapping words between absent keyphrase and document. Then, a mask-predict decoder is used to complete the absent keyphrase on the constraint of prompt. Experiments on keyphrase generation benchmarks have demonstrated the effectiveness of our approach. In addition, we evaluate the performance of constrained absent keyphrases generation from an information retrieval perspective. The result shows that our approach can generate more consistent keyphrases, which can improve document retrieval performance. What’s more, with a non-autoregressive decoding manner, our model can speed up the absent keyphrase generation by 8.67× compared with the autoregressive method",
    "volume": "main",
    "checked": true,
    "id": "1ca0b2ccd7e9ade375c780b8bc9c7266dd34039d",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21403": {
    "title": "GraphMemDialog: Optimizing End-to-End Task-Oriented Dialog Systems Using Graph Memory Networks",
    "abstract": "Effectively integrating knowledge into end-to-end task-oriented dialog systems remains a challenge. It typically requires incorporation of an external knowledge base (KB) and capture of the intrinsic semantics of the dialog history. Recent research shows promising results by using Sequence-to-Sequence models, Memory Networks, and even Graph Convolutional Networks. However, current state-of-the-art models are less effective at integrating dialog history and KB into task-oriented dialog systems in the following ways: 1. The KB representation is not fully context-aware. The dynamic interaction between the dialog history and KB is seldom explored. 2. Both the sequential and structural information in the dialog history can contribute to capturing the dialog semantics, but they are not studied concurrently. In this paper, we propose a novel Graph Memory Network (GMN) based Seq2Seq model, GraphMemDialog, to effectively learn the inherent structural information hidden in dialog history, and to model the dynamic interaction between dialog history and KBs. We adopt a modified graph attention network to learn the rich structural representation of the dialog history, whereas the context-aware representation of KB entities are learnt by our novel GMN. To fully exploit this dynamic interaction, we design a learnable memory controller coupled with external KB entity memories to recurrently incorporate dialog history context into KB entities through a multi-hop reasoning mechanism. Experiments on three public datasets show that our GraphMemDialog model achieves state-of-the-art performance and outperforms strong baselines by a large margin, especially on datatests with more complicated KB information",
    "volume": "main",
    "checked": true,
    "id": "cd4a820130d715effe7dace59376e6ac0d77f05b",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21404": {
    "title": "Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling",
    "abstract": "Unified opinion role labeling (ORL) aims to detect all possible opinion structures of 'opinion-holder-target' in one shot, given a text. The existing transition-based unified method, unfortunately, is subject to longer opinion terms and fails to solve the term overlap issue. Current top performance has been achieved by employing the span-based graph model, which however still suffers from both high model complexity and insufficient interaction among opinions and roles. In this work, we investigate a novel solution by revisiting the transition architecture, and augmenting it with a pointer network (PointNet). The framework parses out all opinion structures in linear-time complexity, meanwhile breaks through the limitation of any length of terms with PointNet. To achieve the explicit opinion-role interactions, we further propose a unified dependency-opinion graph (UDOG), co-modeling the syntactic dependency structure and the partial opinion-role structure. We then devise a relation-centered graph aggregator (RCGA) to encode the multi-relational UDOG, where the resulting high-order representations are used to promote the predictions in the vanilla transition system. Our model achieves new state-of-the-art results on the MPQA benchmark. Analyses further demonstrate the superiority of our methods on both efficacy and efficiency",
    "volume": "main",
    "checked": true,
    "id": "8724ec52e653d92dc6b867a0b0d56c09b7a60253",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21405": {
    "title": "A Graph Convolutional Network with Adaptive Graph Generation and Channel Selection for Event Detection",
    "abstract": "Graph convolutional networks have been successfully applied to the task of event detection. However, existing works rely heavily on a fixed syntactic parse tree structure from an external parser. In addition, the information content extracted for aggregation is determined simply by the (syntactic) edge direction or type but irrespective of what semantics the vertices have, which is somewhat rigid. With this work, we propose a novel graph convolutional method that combines an adaptive graph generation technique and a multi-channel selection strategy. The adaptive graph generation technique enables the gradients to pass through the graph sampling layer by using the ST-Gumbel-Softmax trick. The multi-channel selection strategy allows two adjacent vertices to automatically determine which information channels to get through for information extraction and aggregation. The proposed method achieves the state-of-the-art performance on ACE2005 dataset",
    "volume": "main",
    "checked": true,
    "id": "4262331838e5071f93783f686cef07f7825f3742",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21406": {
    "title": "Leashing the Inner Demons: Self-Detoxification for Language Models",
    "abstract": "Language models (LMs) can reproduce (or amplify) toxic language seen during training, which poses a risk to their practical application. In this paper, we conduct extensive experiments to study this phenomenon. We analyze the impact of prompts, decoding strategies and training corpora on the output toxicity. Based on our findings, we propose a simple yet effective unsupervised method for language models to ``detoxify'' themselves without an additional large corpus or external discriminator. Compared to a supervised baseline, our proposed method shows better toxicity reduction with good generation quality in the generated content under multiple settings. Warning: some examples shown in the paper may contain uncensored offensive content",
    "volume": "main",
    "checked": true,
    "id": "7ccbc36d047eb61ab1deb743dba10f2ec7853151",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21407": {
    "title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-sentence Dependency Graph",
    "abstract": "We target the task of cross-lingual Machine Reading Comprehension (MRC) in the direct zero-shot setting, by incorporating syntactic features from Universal Dependencies (UD), and the key features we use are the syntactic relations within each sentence. While previous work has demonstrated effective syntax-guided MRC models, we propose to adopt the inter-sentence syntactic relations, in addition to the rudimentary intra-sentence relations, to further utilize the syntactic dependencies in the multi-sentence input of the MRC task. In our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting dependency trees to form global syntactic relations across sentences. We then propose the ISDG encoder that encodes the global dependency graph, addressing the inter-sentence relations via both one-hop and multi-hop dependency paths explicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA, TyDiQA-GoldP) show that our encoder that is only trained on English is able to improve the zero-shot performance on all 14 test sets covering 8 languages, with up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on certain languages. Further analysis shows the improvement can be attributed to the attention on the cross-linguistically consistent syntactic path. Our code is available at https://github.com/lxucs/multilingual-mrc-isdg",
    "volume": "main",
    "checked": true,
    "id": "494c1f745dbb7625e86e9a222c480e40949b8dad",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21408": {
    "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
    "abstract": "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm.    With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs.    However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability.    To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning.    It is designed as a general framework, compatible with both structured and unstructured pruning.    Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge.    Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning.    Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios.    With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks.    In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability",
    "volume": "main",
    "checked": true,
    "id": "08c2e7812ff224db1c877b4d14730d6288d529aa",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21409": {
    "title": "Sequence Level Contrastive Learning for Text Summarization",
    "abstract": "Contrastive learning models have achieved great success in unsupervised visual representation learning, which maximize the similarities between feature representations of different views of the same image, while minimize the similarities between feature representations of views of different images. In text summarization, the output summary is a shorter form of the input document and they have similar meanings. In this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a document, its gold summary and its model generated summaries as different views of the same mean representation and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model achieves better faithfulness ratings compared to its counterpart without contrastive objectives. We release our code at https://github.com/xssstory/SeqCo",
    "volume": "main",
    "checked": true,
    "id": "4b2af8e5da894a72f6236ab9347753760cfea7fd",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21410": {
    "title": "Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer",
    "abstract": "Expert-layman text style transfer technologies have the potential to improve communication between members of scientific communities and the general public. High-quality information produced by experts is often filled with difficult jargon laypeople struggle to understand. This is a particularly notable issue in the medical domain, where layman are often confused by medical text online. At present, two bottlenecks interfere with the goal of building high-quality medical expert-layman style transfer systems: a dearth of pretrained medical-domain language models spanning both expert and layman terminologies and a lack of parallel corpora for training the transfer task itself. To mitigate the first issue, we propose a novel language model (LM) pretraining task, Knowledge Base Assimilation, to synthesize pretraining data from the edges of a graph of expert- and layman-style medical terminology terms into an LM during self-supervised learning. To mitigate the second issue, we build a large-scale parallel corpus in the medical expert-layman domain using a margin-based criterion. Our experiments show that transformer-based models pretrained on knowledge base assimilation and other well-established pretraining tasks fine-tuning on our new parallel corpus leads to considerable improvement against expert-layman transfer benchmarks, gaining an average relative improvement of our human evaluation, the Overall Success Rate (OSR), by 106%",
    "volume": "main",
    "checked": true,
    "id": "bf2e9a6e40f2e7d797b59285b68b8abd54fb58c6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21411": {
    "title": "Text Is No More Enough! A Benchmark for Profile-Based Spoken Language Understanding",
    "abstract": "Current researches on spoken language understanding (SLU) heavily are limited to a simple setting: the plain text-based SLU that takes the user utterance as input and generates its corresponding semantic frames (e.g., intent and slots). Unfortunately, such a simple setting may fail to work in complex real-world scenarios when an utterance is semantically ambiguous, which cannot be achieved by the text-based SLU models. In this paper, we first introduce a new and important task, Profile-based Spoken Language Understanding (ProSLU), which requires the model that not only relies on the plain text but also the supporting profile information to predict the correct intents and slots. To this end, we further introduce a large-scale human-annotated Chinese dataset with over 5K utterances and their corresponding supporting profile information (Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition, we evaluate several state-of-the-art baseline models and explore a multi-level knowledge adapter to effectively incorporate profile information. Experimental results reveal that all existing text-based SLU models fail to work when the utterances are semantically ambiguous and our proposed framework can effectively fuse the supporting information for sentence-level intent detection and token-level slot filling. Finally, we summarize key challenges and provide new points for future directions, which hopes to facilitate the research",
    "volume": "main",
    "checked": true,
    "id": "073a404154f66e57e029af6618d0e171d0c74fa8",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21412": {
    "title": "SAS: Self-Augmentation Strategy for Language Model Pre-training",
    "abstract": "The core of self-supervised learning for pre-training language models includes pre-training task design as well as appropriate data augmentation. Most data augmentations in language model pre-training are context-independent. A seminal contextualized augmentation was recently proposed in ELECTRA and achieved state-of-the-art performance by introducing an auxiliary generation network (generator) to produce contextualized data augmentation for the training of a main discrimination network (discriminator). This design, however, introduces extra computation cost of the generator and a need to adjust the relative capability between the generator and the discriminator. In this paper, we propose a self-augmentation strategy (SAS) where a single network is utilized for both regular pre-training and contextualized data augmentation for the training in later epochs. Essentially, this strategy eliminates a separate generator and uses the single network to jointly conduct two pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced Token Detection) heads. It avoids the challenge to search for an appropriate size of the generator, which is critical to the performance as evidenced in ELECTRA and its subsequent variant models. In addition, SAS is a general strategy that can be seamlessly combined with many new techniques emerging recently or in the future, such as the disentangled attention mechanism from DeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other state-of-the-art models in the GLUE tasks with similar or less computation cost",
    "volume": "main",
    "checked": true,
    "id": "4757b2dcb1d63b871ecce014ce8560a649652b4b",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21413": {
    "title": "Hybrid Curriculum Learning for Emotion Recognition in Conversation",
    "abstract": "Emotion recognition in conversation (ERC) aims to detect the emotion label for each utterance.   Motivated by recent studies which have proven that feeding training examples in a meaningful order rather than considering them randomly can boost the performance of models, we propose an ERC-oriented hybrid curriculum learning framework. Our framework consists of two curricula: (1) conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In CC, we construct a difficulty measurer based on ``emotion shift'' frequency within a conversation, then the conversations are scheduled in an ``easy to hard\" schema according to the difficulty score returned by the difficulty measurer. For UC, it is implemented from an emotion-similarity perspective, which progressively strengthens the model’s ability in identifying the confusing emotions. With the proposed model-agnostic hybrid curriculum learning strategy, we observe significant performance boosts over a wide range of existing ERC models and we are able to achieve new state-of-the-art results on four public ERC datasets",
    "volume": "main",
    "checked": true,
    "id": "c829e17951ad1979e4ef724f674cc886b4870e00",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21414": {
    "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting",
    "abstract": "Financial forecasting has been an important and active area of machine learning research because of the challenges it presents and the potential rewards that even minor improvements in prediction accuracy or forecasting may entail. Traditionally, financial forecasting has heavily relied on quantitative indicators and metrics derived from structured financial statements. Earnings conference call data, including text and audio, is an important source of unstructured data that has been used for various prediction tasks using deep earning and related approaches. However, current deep learning-based methods are limited in the way that they deal with numeric data; numbers are typically treated as plain-text tokens without taking advantage of their underlying numeric structure. This paper describes a numeric-oriented hierarchical transformer model (NumHTML) to predict stock returns, and financial risk using multi-modal aligned earnings calls data by taking advantage of the different categories of numbers (monetary, temporal, percentages etc.) and their magnitude. We present the results of a comprehensive evaluation of NumHTML against several state-of-the-art baselines using a real-world publicly available dataset. The results indicate that NumHTML significantly outperforms the current state-of-the-art across a variety of evaluation metrics and that it has the potential to offer significant financial gains in a practical trading context",
    "volume": "main",
    "checked": true,
    "id": "b1f10c9fbb8dceff45e9c5aa9d7ca925b742d9ac",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21415": {
    "title": "Tracing Text Provenance via Context-Aware Lexical Substitution",
    "abstract": "Text content created by humans or language models is often stolen or misused by adversaries. Tracing text provenance can help claim the ownership of text content or identify the malicious users who distribute misleading content like machine-generated fake news. There have been some attempts to achieve this, mainly based on watermarking techniques. Specifically, traditional text watermarking methods embed watermarks by slightly altering text format like line spacing and font, which, however, are fragile to cross-media transmissions like OCR. Considering this, natural language watermarking methods represent watermarks by replacing words in original sentences with synonyms from handcrafted lexical resources (e.g., WordNet), but they do not consider the substitution’s impact on the overall sentence's meaning. Recently, a transformer-based network was proposed to embed watermarks by modifying the unobtrusive words (e.g., function words), which also impair the sentence's logical and semantic coherence. Besides, one well-trained network fails on other different types of text content.   To address the limitations mentioned above, we propose a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, we employ BERT to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Based on this, a selection strategy in terms of synchronicity and substitutability is further designed to test whether a word is exactly suitable for carrying the watermark signal. Extensive experiments demonstrate that, under both objective and subjective metrics, our watermarking scheme can well preserve the semantic integrity of original sentences and has a better transferability than existing methods. Besides, the proposed LS approach outperforms the state-of-the-art approach on the Stanford Word Substitution Benchmark",
    "volume": "main",
    "checked": true,
    "id": "9b79eb8d21c8a832daedbfc6d8c31bebe0da3ed5",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21416": {
    "title": "Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents",
    "abstract": "The goal of building intelligent dialogue systems has largely been separately pursued under two paradigms: task-oriented dialogue (TOD) systems, which perform task-specific functions, and open-domain dialogue (ODD) systems, which focus on non-goal-oriented chitchat. The two dialogue modes can potentially be intertwined together seamlessly in the same conversation, as easily done by a friendly human assistant. Such ability is desirable in conversational agents, as the integration makes them more accessible and useful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn dialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting the existing TOD turns and adding new ODD turns. This procedure constructs conversation sessions containing exchanges from both dialogue modes. It features inter-mode contextual dependency, i.e., the dialogue turns from the two modes depend on each other. Rich dependency patterns such as co-reference and ellipsis are included. The new dataset, with 60k new human-written ODD turns and 5k re-written TOD turns, offers a benchmark to test a dialogue model's ability to perform inter-mode conversations. This is a more challenging task since the model has to determine the appropriate dialogue mode and generate the response based on the inter-mode context. However, such models would better mimic human-level conversation capabilities. We evaluate two baseline models on this task, including the classification-based two-stage models and the two-in-one fused models. We publicly release FusedChat and the baselines to propel future work on inter-mode dialogue systems",
    "volume": "main",
    "checked": true,
    "id": "937c037176c33b97131d9e92823119442a416993",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21417": {
    "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding",
    "abstract": "Knowledge graphs (KGs) contain rich information about world knowledge, entities, and relations. Thus, they can be great supplements to existing pre-trained language models. However, it remains a challenge to efficiently integrate information from KG into language modeling. And the understanding of a knowledge graph requires related context. We propose a novel joint pre-training framework, JAKET, to model both the knowledge graph and language. The knowledge module and language module provide essential information to mutually assist each other: the knowledge module produces embeddings for entities in text while the language module generates context-aware initial embeddings for entities and relations in the graph. Our design enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains. Experiment results on several knowledge-aware NLP tasks show that our proposed framework achieves superior performance by effectively leveraging knowledge in language understanding",
    "volume": "main",
    "checked": true,
    "id": "37bf0bf34603145246c3311df19e2afdf6e0270a",
    "citation_count": 36
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21418": {
    "title": "KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training",
    "abstract": "The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an automated system, which is not meant to replace humans, but rather providing a first-pass draft for a machine-assisted human review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the Kid-Review better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training. We make all code, relevant dataset available: https://github.com/Anonymous4nlp233/KIDReview as well as the Kid-Review system: http://nlpeer.reviews",
    "volume": "main",
    "checked": true,
    "id": "24a95ff7a4f37d7d05f30e60dae40a576f49eeda",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21419": {
    "title": "Reference-Based Speech Enhancement via Feature Alignment and Fusion Network",
    "abstract": "Speech enhancement aims at recovering a clean speech from a noisy input, which can be classified into single speech enhancement and personalized speech enhancement. Personalized speech enhancement usually utilizes the speaker identity extracted from the noisy speech itself (or a clean reference speech) as a global embedding to guide the enhancement process. Different from them, we observe that the speeches of the same speaker are correlated in terms of frame-level short-time Fourier Transform (STFT) spectrogram. Therefore, we propose reference-based speech enhancement via a feature alignment and fusion network (FAF-Net). Given a noisy speech and a clean reference speech spoken by the same speaker, we first propose a feature level alignment strategy to warp the clean reference with the noisy speech in frame level. Then, we fuse the reference feature with the noisy feature via a similarity-based fusion strategy. Finally, the fused features are skipped connected to the decoder, which generates the enhanced results. Experimental results demonstrate that the performance of the proposed FAF-Net is close to state-of-the-art speech enhancement methods on both DNS and Voice Bank+DEMAND datasets. Our code is available at https://github.com/HieDean/FAF-Net",
    "volume": "main",
    "checked": true,
    "id": "87a13e6d7aec847d521c97dbfac9382b6c9f090a",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21420": {
    "title": "MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation",
    "abstract": "Chatbots are designed to carry out human-like conversations across different domains, such as general chit-chat, knowledge exchange, and persona-grounded conversations. To measure the quality of such conversational agents, a dialogue evaluator is expected to conduct assessment across domains as well. However, most of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are not designed for multi-domain evaluation. We are motivated to design a general and robust framework, MDD-Eval, to address the problem. Specifically, we first train a teacher evaluator with human-annotated data to acquire a rating skill to tell good dialogue responses from bad ones in a particular domain and then, adopt a self-training strategy to train a new evaluator with teacher-annotated multi-domain data, that helps the new evaluator to generalize across multiple domains. MDD-Eval is extensively assessed on six dialogue evaluation benchmarks. Empirical results show that the MDD-Eval framework achieves a strong performance with an absolute improvement of 7% over the state-of-the-art ADMs in terms of mean Spearman correlation scores across all the evaluation benchmarks",
    "volume": "main",
    "checked": true,
    "id": "0ba23c847d2ca087887b60ea92ce56c71f0425b2",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21421": {
    "title": "Efficient Dialog Policy Learning by Reasoning with Contextual Knowledge",
    "abstract": "Goal-oriented dialog policy learning algorithms aim to learn a dialog policy for selecting language actions based on the current dialog state. Deep reinforcement learning methods have been used for dialog policy learning. This work is motivated by the observation that, although dialog is a domain with rich contextual knowledge, reinforcement learning methods are ill-equipped to incorporate such knowledge into the dialog policy learning process. In this paper, we develop a deep reinforcement learning framework for goal-oriented dialog policy learning that learns user preferences from user goal data, while leveraging commonsense knowledge from people. The developed framework has been evaluated using a realistic dialog simulation platform. Compared with baselines from the literature and the ablations of our approach, we see significant improvements in learning efficiency and the quality of the computed action policies",
    "volume": "main",
    "checked": true,
    "id": "c1da0bb8db38642a28bea077604897237bf67946",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21422": {
    "title": "Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization",
    "abstract": "Multimodal summarization with multimodal output (MSMO) generates a summary with both textual and visual content. Multimodal news report contains heterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed that different modalities of data in the news report correlate hierarchically. Traditional MSMO methods indistinguishably handle different modalities of data by learning a representation for the whole data, which is not directly adaptable to the heterogeneous contents and hierarchical correlation. In this paper, we propose a hierarchical cross-modality semantic correlation learning model (HCSCL) to learn the intra- and inter-modal correlation existing in the multimodal data. HCSCL adopts a graph network to encode the intra-modal correlation. Then, a hierarchical fusion framework is proposed to learn the hierarchical correlation between text and images. Furthermore, we construct a new dataset with relevant image annotation and image object label information to provide the supervision information for the learning procedure. Extensive experiments on the dataset show that HCSCL significantly outperforms the baseline methods in automatic summarization metrics and fine-grained diversity tests",
    "volume": "main",
    "checked": true,
    "id": "30675cc0f831ccc418c02a9f7f9f2d2bdaaf68de",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21423": {
    "title": "Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers",
    "abstract": "Deep and large pre-trained language models (e.g., BERT, GPT-3) are state-of-the-art for various natural language processing tasks. However, the huge size of these models brings challenges to fine-tuning and online deployment due to latency and cost constraints. Existing knowledge distillation methods reduce the model size, but they may encounter difficulties transferring knowledge from the teacher model to the student model due to the limited data from the downstream tasks. In this work, we propose AD^2, a novel and effective data augmentation approach to improving the task-specific knowledge transfer when compressing large pre-trained transformer models. Different from prior methods, AD^2 performs distillation by using an enhanced training set that contains both original inputs and adversarially perturbed samples that mimic the output distribution from the teacher. Experimental results show that this method allows better transfer of knowledge from the teacher to the student during distillation, producing student models that retain 99.6\\% accuracy of the teacher model while outperforming existing task-specific knowledge distillation baselines by 1.2 points on average over a variety of natural language understanding tasks. Moreover, compared with alternative data augmentation methods, such as text-editing-based approaches, AD^2 is up to 28 times faster while achieving comparable or higher accuracy. In addition, when AD^2 is combined with more advanced task-agnostic distillation, we can advance the state-of-the-art performance even more. On top of the encouraging performance, this paper also provides thorough ablation studies and analysis. The discovered interplay between KD and adversarial data augmentation for compressing pre-trained Transformers may further inspire more advanced KD algorithms for compressing even larger scale models",
    "volume": "main",
    "checked": true,
    "id": "8dd49f94b213834e580eeea18098f402b1be1226",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21424": {
    "title": "Text-Based Interactive Recommendation via Offline Reinforcement Learning",
    "abstract": "Interactive recommendation with natural-language feedback can provide richer user feedback and has demonstrated advantages over traditional recommender systems. However, the classical online paradigm involves iteratively collecting experience via interaction with users, which is expensive and risky. We consider an offline interactive recommendation to exploit arbitrary experience collected by multiple unknown policies. A direct application of policy learning with such fixed experience suffers from the distribution shift. To tackle this issue, we develop a behavior-agnostic off-policy correction framework to make offline interactive recommendation possible. Specifically, we leverage the   conservative Q-function to perform off-policy evaluation, which enables learning effective policies from fixed datasets without further interactions. Empirical results on the simulator derived from real-world datasets demonstrate the effectiveness of our proposed offline training framework",
    "volume": "main",
    "checked": true,
    "id": "b88c84a90160393a175c608e10eef9222ff69990",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21425": {
    "title": "DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding",
    "abstract": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained models with relation triples injecting from knowledge graphs to improve language understanding abilities.Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. To guarantee effective knowledge injection, previous studies integrate models with knowledge encoders for representing knowledge retrieved from knowledge graphs. The operations for knowledge retrieval and encoding bring significant computational burdens, restricting the usage of such models in real-world applications that require high inference speed. In this paper, we propose a novel KEPLM named DKPLM that   decomposes knowledge injection process of the pre-trained language models in pre-training, fine-tuning and inference stages, which facilitates the applications of KEPLMs in real-world scenarios. Specifically, we first detect knowledge-aware long-tail entities as the target for knowledge injection, enhancing the KEPLMs' semantic understanding abilities and avoiding injecting redundant information.   The embeddings of long-tail entities are replaced by ``pseudo token representations'' formed by relevant knowledge triples. We further design the relational knowledge decoding task for pre-training to force the models to truly understand the injected knowledge by relation triple reconstruction. Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. We further show that DKPLM has a higher inference speed than other competing models due to the decomposing mechanism",
    "volume": "main",
    "checked": true,
    "id": "2fab75cfd8394de70bca365572bc5bb04a1b1eb5",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21426": {
    "title": "Frequency-Aware Contrastive Learning for Neural Machine Translation",
    "abstract": "Low-frequency word prediction remains a challenge in modern neural machine translation (NMT) systems. Recent adaptive training methods promote the output of infrequent words by emphasizing their weights in the overall training objectives. Despite the improved recall of low-frequency words, their prediction precision is unexpectedly hindered by the adaptive objectives. Inspired by the observation that low-frequency words form a more compact embedding space, we tackle this challenge from a representation learning perspective. Specifically, we propose a frequency-aware token-level contrastive learning method, in which the hidden state of each decoding step is pushed away from the counterparts of other target words, in a soft contrastive way based on the corresponding word frequencies. We conduct experiments on widely used NIST Chinese-English and WMT14 English-German translation tasks. Empirical results show that our proposed methods can not only significantly improve the translation quality but also enhance lexical diversity and optimize word representation space. Further investigation reveals that, comparing with related adaptive training strategies, the superiority of our method on low-frequency word prediction lies in the robustness of token-level recall across different frequencies without sacrificing precision",
    "volume": "main",
    "checked": true,
    "id": "a821453809f36a23402ce93b6042c76d85a720a6",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21427": {
    "title": "Probing Word Syntactic Representations in the Brain by a Feature Elimination Method",
    "abstract": "Neuroimaging studies have identified multiple brain regions that are associated with semantic and syntactic processing when comprehending language. However, existing methods cannot explore the neural correlates of fine-grained word syntactic features, such as part-of-speech and dependency relations. This paper proposes an alternative framework to study how different word syntactic features are represented in the brain.  To separate each syntactic feature, we propose a feature elimination method, called Mean Vector Null space Projection (MVNP). This method can remove a specific feature from word representations, resulting in one-feature-removed representations. Then we respectively associate one-feature-removed and the original word vectors with brain imaging data to explore how the brain represents the removed feature.  This paper for the first time studies the cortical representations of multiple fine-grained syntactic features simultaneously and suggests some possible contributions of several brain regions to the complex division of syntactic processing. These findings indicate that the brain foundations of syntactic information processing might be broader than those suggested by classical studies",
    "volume": "main",
    "checked": true,
    "id": "c9bd83a75817a5aef817a704b398ea920bf3bfce",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21428": {
    "title": "Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives",
    "abstract": "Unsupervised sentence representation learning is a fundamental problem in natural language processing. Recently, contrastive learning has made great success on this task. Existing constrastive learning based models usually apply random sampling to select negative examples for training. Previous work in computer vision has shown that hard negative examples help contrastive learning to achieve faster convergency and better optimization for representation learning. However, the importance of hard negatives in contrastive learning for sentence representation is yet to be explored. In this study, we prove that hard negatives are essential for maintaining strong gradient signals in the training process while random sampling negative examples is ineffective for sentence representation. Accordingly, we present a contrastive model, MixCSE, that extends the current state-of-the-art SimCSE by continually constructing hard negatives via mixing both positive and negative features. The superior performance of the proposed approach is demonstrated via empirical studies on Semantic Textual Similarity datasets and Transfer task datasets",
    "volume": "main",
    "checked": true,
    "id": "ff5c0e3e23a79fbcca95aa6dba1ec7ba71baf204",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21429": {
    "title": "RetGen: A Joint Framework for Retrieval and Grounded Text Generation Modeling",
    "abstract": "Recent advances in large-scale pre-training such as GPT-3 allow seemingly high quality text to be generated from a given prompt. However, such generation systems often suffer from problems of hallucinated facts, and are not inherently designed to incorporate useful external information. Grounded generation models appear to offer remedies, but their training typically relies on rarely-available parallel data where information-relevant documents are provided for context. We propose a framework that alleviates this data constraint by jointly training a grounded generator and document retriever on the language model signal. The model learns to reward retrieval of the documents with the highest utility in generation, and attentively combines them using a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We demonstrate that both generator and retriever can take advantage of this joint training and work synergistically to produce more informative and relevant text in both prose and dialogue generation",
    "volume": "main",
    "checked": true,
    "id": "39a9f750c9b79fba4a0404179fdac6a7cb922838",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21430": {
    "title": "BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles",
    "abstract": "A riddle is a question or statement with double or veiled meanings, followed by an unexpected answer. Solving riddle is a challenging task for both machine and human, testing the capability of understanding figurative, creative natural language and reasoning with commonsense knowledge. We introduce BiRdQA, a bilingual multiple-choice question answering dataset with 6614 English riddles and 8751 Chinese riddles. For each riddle-answer pair, we provide four distractors with additional information from Wikipedia. The distractors are automatically generated at scale with minimal bias. Existing monolingual and multilingual QA models fail to perform well on our dataset, indicating that there is a long way to go before machine can beat human on solving tricky riddles. The dataset is publicly available at https://forms.gle/NvT7DfWhAPhvoFvH7",
    "volume": "main",
    "checked": true,
    "id": "4f7c4a9d73f6d18f4bbb4424c7f8a16df45df474",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21431": {
    "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
    "abstract": "With the rapid increase of multimedia data, a large body of literature has emerged to work on multimodal summarization, the majority of which target at refining salient information from textual and image modalities to output a pictorial summary with the most relevant images. Existing methods mostly focus on either extractive or abstractive summarization and rely on the presence and quality of image captions to build image references. We are the first to propose a Unified framework for Multimodal Summarization grounding on BART, UniMS, that integrates extractive and abstractive objectives, as well as selecting the image output. Specially, we adopt knowledge distillation from a vision-language pretrained model to improve image selection, which avoids any requirement on the existence and quality of image captions. Besides, we introduce a visual guided decoder to better integrate textual and visual modalities in guiding abstractive text generation. Results show that our best model achieves a new state-of-the-art result on a large-scale benchmark dataset. The newly involved extractive objective as well as the knowledge distillation technique are proven to bring a noticeable improvement to the multimodal summarization task",
    "volume": "main",
    "checked": true,
    "id": "cfeb9181a23c13d1d3ab85985038110a14f97c7d",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21432": {
    "title": "DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization",
    "abstract": "Dialogue is an essential part of human communication and cooperation. Existing research mainly focuses on short dialogue scenarios in a one-on-one fashion. However, multi-person interactions in the real world, such as meetings or interviews, are frequently over a few thousand words. There is still a lack of corresponding research and powerful tools to understand and process such long dialogues. Therefore, in this work, we present a pre-training framework for long dialogue understanding and summarization. Considering the nature of long conversations, we propose a window-based denoising approach for generative pre-training. For a dialogue, it corrupts a window of text with dialogue-inspired noise, and guides the model to reconstruct this window based on the content of the remaining conversation. Furthermore, to process longer input, we augment the model with sparse attention which is combined with conventional attention in a hybrid manner. We conduct extensive experiments on five datasets of long dialogues, covering tasks of dialogue summarization, abstractive question answering and topic segmentation. Experimentally, we show that our pre-trained model DialogLM significantly surpasses the state-of-the-art models across datasets and tasks. Source code and all the pre-trained models are available on our GitHub repository (https://github.com/microsoft/DialogLM)",
    "volume": "main",
    "checked": true,
    "id": "ac95a18762133d4065ac8af518c33084d83c5582",
    "citation_count": 24
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21433": {
    "title": "Idiomatic Expression Paraphrasing without Strong Supervision",
    "abstract": "Idiomatic expressions (IEs) play an essential role in natural language. In this paper, we study the task of idiomatic sentence paraphrasing (ISP), which aims to paraphrase a sentence with an IE by replacing the IE with its literal paraphrase. The lack of large-scale corpora with idiomatic-literal parallel sentences is a primary challenge for this task, for which we consider two separate solutions. First, we propose an unsupervised approach to ISP, which leverages an IE's contextual information and definition and does not require a parallel sentence training set. Second, we propose a weakly supervised approach using back-translation to jointly perform paraphrasing and generation of sentences with IEs to enlarge the small-scale parallel sentence training dataset. Other significant derivatives of the study include a model that replaces a literal phrase in a sentence with an IE to generate an idiomatic expression and a large scale parallel dataset with idiomatic/literal sentence pairs. The effectiveness of the proposed solutions compared to competitive baselines is seen in the relative gains of over 5.16 points in BLEU, over 8.75 points in METEOR, and over 19.57 points in SARI when the generated sentences are empirically validated on a parallel dataset using automatic and manual evaluations. We demonstrate the practical utility of ISP as a preprocessing step in En-De machine translation",
    "volume": "main",
    "checked": true,
    "id": "c1ce047eb0911babdfd160071981bb82a41664fb",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21434": {
    "title": "Multilingual Code Snippets Training for Program Translation",
    "abstract": "Program translation aims to translate source code from one programming language to another. It is particularly useful in applications such as multiple-platform adaptation and legacy code migration. Traditional rule-based program translation methods usually rely on meticulous manual rule-crafting, which is costly both in terms of time and effort. Recently, neural network based methods have been developed to address this problem. However, the absence of high-quality parallel code data is one of the main bottlenecks which impedes the development of program translation models. In this paper, we introduce CoST, a new multilingual Code Snippet Translation dataset that contains parallel data from 7 commonly used programming languages. The dataset is parallel at the level of code snippets, which provides much more fine-grained alignments between different languages than the existing translation datasets. We also propose a new program translation model that leverages multilingual snippet denoising auto-encoding and Multilingual Snippet Translation (MuST) pre-training. Extensive experiments show that the multilingual snippet training is effective in improving program translation performance, especially for low-resource languages. Moreover, our training method shows good generalizability and consistently improves the translation performance of a number of baseline models. The proposed model outperforms the baselines on both snippet-level and program-level translation, and achieves state-of-the-art performance on CodeXGLUE translation task. The code, data, and appendix for this paper can be found at https://github.com/reddy-lab-code-research/MuST-CoST",
    "volume": "main",
    "checked": true,
    "id": "62851a515ea0ee3a547d94e8a493d978c22d0be9",
    "citation_count": 5
  }
}