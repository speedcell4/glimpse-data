{
  "https://ojs.aaai.org/index.php/AAAI/article/view/19873": {
    "title": "Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders",
    "volume": "main",
    "abstract": "We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial autoencoder-based representation learning that correlates 3D motion-captured gesture sequences with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of 58.43%. We see an improvement in performance compared to current state-of-the-art algorithms for generalized zero-shot learning by an absolute 25-27%. We also demonstrate our approach on publicly available online videos and movie scenes, where the actors' pose has been extracted and map to their respective emotive states",
    "checked": true,
    "id": "b836524d1d86e8612dff9fa5108000c6f78728fb",
    "semantic_title": "learning unseen emotions from gestures via semantically-conditioned zero-shot perception with adversarial autoencoders",
    "citation_count": 11,
    "authors": [
      "Abhishek Banerjee",
      "Uttaran Bhattacharya",
      "Aniket Bera"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19874": {
    "title": "Optimized Potential Initialization for Low-Latency Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10 dataset and CIFAR-100 dataset and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small",
    "checked": true,
    "id": "f791cf3dbd0a1c97d340869988370fe086bfca03",
    "semantic_title": "optimized potential initialization for low-latency spiking neural networks",
    "citation_count": 28,
    "authors": [
      "Tong Bu",
      "Jianhao Ding",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19875": {
    "title": "Planning with Biological Neurons and Synapses",
    "volume": "main",
    "abstract": "We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as \"put the top block in stack 1 on the table\". The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic --- if idealized --- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs",
    "checked": true,
    "id": "862396645566e310579f5cea6bdf4da995fedde8",
    "semantic_title": "planning with biological neurons and synapses",
    "citation_count": 3,
    "authors": [
      "Francesco d'Amore",
      "Daniel Mitropolsky",
      "Pierluigi Crescenzi",
      "Emanuele Natale",
      "Christos H. Papadimitriou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19876": {
    "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding",
    "volume": "main",
    "abstract": "In humans, perceptual awareness facilitates the fast recognition and extraction of information from sensory input. This awareness largely depends on how the human agent interacts with the environment. In this work, we propose active neural generative coding, a computational framework for learning action-driven generative models without backpropagation of errors (backprop) in dynamic environments. Specifically, we develop an intelligent agent that operates even with sparse rewards, drawing inspiration from the cognitive theory of planning as inference. We demonstrate on several simple control problems that our framework performs competitively with deep Q-learning. The robust performance of our agent offers promising evidence that a backprop-free approach for neural inference and learning can drive goal-directed behavior",
    "checked": true,
    "id": "09897d26367a644811dc3be2109fe48778300d0a",
    "semantic_title": "backprop-free reinforcement learning with active neural generative coding",
    "citation_count": 12,
    "authors": [
      "Alexander G. Ororbia",
      "Ankur Mali"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19877": {
    "title": "VECA: A New Benchmark and Toolkit for General Cognitive Development",
    "volume": "main",
    "abstract": "The developmental approach, simulating a cognitive development of a human, arises as a way to nurture a human-level commonsense and overcome the limitations of data-driven approaches. However, neither a virtual environment nor an evaluation platform exists for the overall development of core cognitive skills. We present the VECA(Virtual Environment for Cognitive Assessment), which consists of two main components: (i) a first benchmark to assess the overall cognitive development of an AI agent, and (ii) a novel toolkit to generate diverse and distinct cognitive tasks. VECA benchmark virtually implements the cognitive scale of Bayley Scales of Infant and Toddler Development-IV(Bayley-4), the gold-standard developmental assessment for human infants and toddlers. Our VECA toolkit provides a human toddler-like embodied agent with various human-like perceptual features crucial to human cognitive development, e.g., binocular vision, 3D-spatial audio, and tactile receptors. We compare several modern RL algorithms on our VECA benchmark and seek their limitations in modeling human-like cognitive development. We further analyze the validity of the VECA benchmark, as well as the effect of human-like sensory characteristics on cognitive skills",
    "checked": true,
    "id": "4bc58f0754829a24da39de6cdfda17a2bb3507fc",
    "semantic_title": "veca: a new benchmark and toolkit for general cognitive development",
    "citation_count": 0,
    "authors": [
      "Kwanyoung Park",
      "Hyunseok Oh",
      "Youngki Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19878": {
    "title": "Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network",
    "volume": "main",
    "abstract": "Cognitive processing signals can be used to improve natural language processing (NLP) tasks. However, it is not clear how these signals correlate with linguistic information. Bridging between human language processing and linguistic features has been widely studied in neurolinguistics, usually via single-variable controlled experiments with highly-controlled stimuli. Such methods not only compromises the authenticity of natural reading, but also are time-consuming and expensive. In this paper, we propose a data-driven method to investigate the relationship between cognitive processing signals and linguistic features. Specifically, we present a unified attentional framework that is composed of embedding, attention, encoding and predicting layers to selectively map cognitive processing signals to linguistic features. We define the mapping procedure as a bridging task and develop 12 bridging tasks for lexical, syntactic and semantic features. The proposed framework only requires cognitive processing signals recorded under natural reading as inputs, and can be used to detect a wide range of linguistic features with a single cognitive dataset. Observations from experiment results resonate with previous neuroscience findings. In addition to this, our experiments also reveal a number of interesting findings, such as the correlation between contextual eye-tracking features and tense of sentence",
    "checked": true,
    "id": "b31502cc80b808da383bcb3fdc6ee7c5079bdabc",
    "semantic_title": "bridging between cognitive processing signals and linguistic features via a unified attentional network",
    "citation_count": 1,
    "authors": [
      "Yuqi Ren",
      "Deyi Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19879": {
    "title": "Multi-Sacle Dynamic Coding Improved Spiking Actor Network for Reinforcement Learning",
    "volume": "main",
    "abstract": "With the help of deep neural networks (DNNs), deep reinforcement learning (DRL) has achieved great success on many complex tasks, from games to robotic control. Compared to DNNs with partial brain-inspired structures and functions, spiking neural networks (SNNs) consider more biological features, including spiking neurons with complex dynamics and learning paradigms with biologically plausible plasticity principles. Inspired by the efficient computation of cell assembly in the biological brain, whereby memory-based coding is much more complex than readout, we propose a multiscale dynamic coding improved spiking actor network (MDC-SAN) for reinforcement learning to achieve effective decision-making. The population coding at the network scale is integrated with the dynamic neurons coding (containing 2nd-order neuronal dynamics) at the neuron scale towards a powerful spatial-temporal state representation. Extensive experimental results show that our MDC-SAN performs better than its counterpart deep actor network (based on DNNs) on four continuous control tasks from OpenAI gym. We think this is a significant attempt to improve SNNs from the perspective of efficient coding towards effective decision-making, just like that in biological networks",
    "checked": true,
    "id": "7b8e746ef2c6c7d0fd1262a6905bf99acc5a1498",
    "semantic_title": "multi-sacle dynamic coding improved spiking actor network for reinforcement learning",
    "citation_count": 13,
    "authors": [
      "Duzhen Zhang",
      "Tielin Zhang",
      "Shuncheng Jia",
      "Bo Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19880": {
    "title": "Joint Human Pose Estimation and Instance Segmentation with PosePlusSeg",
    "volume": "main",
    "abstract": "Despite the advances in multi-person pose estimation, state-of-the-art techniques only deliver the human pose structure.Yet, they do not leverage the keypoints of human pose to deliver whole-body shape information for human instance segmentation. This paper presents PosePlusSeg, a joint model designed for both human pose estimation and instance segmentation. For pose estimation, PosePlusSeg first takes a bottom-up approach to detect the soft and hard keypoints of individuals by producing a strong keypoint heat map, then improves the keypoint detection confidence score by producing a body heat map. For instance segmentation, PosePlusSeg generates a mask offset where keypoint is defined as a centroid for the pixels in the embedding space, enabling instance-level segmentation for the human class. Finally, we propose a new pose and instance segmentation algorithm that enables PosePlusSeg to determine the joint structure of the human pose and instance segmentation. Experiments using the COCO challenging dataset demonstrate that PosePlusSeg copes better with challenging scenarios, like occlusions, en-tangled limbs, and overlapped people. PosePlusSeg outperforms state-of-the-art detection-based approaches achieving a 0.728 mAP for human pose estimation and a 0.445 mAP for instance segmentation. Code has been made available at: https://github.com/RaiseLab/PosePlusSeg",
    "checked": true,
    "id": "ed50d44540518b7f3b96a39b84e2d5d8d3dbb649",
    "semantic_title": "joint human pose estimation and instance segmentation with poseplusseg",
    "citation_count": 2,
    "authors": [
      "Niaz Ahmad",
      "Jawad Khan",
      "Jeremy Yuhyun Kim",
      "Youngmoon Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19881": {
    "title": "Logic Rule Guided Attribution with Dynamic Ablation",
    "volume": "main",
    "abstract": "With the increasing demands for understanding the internal behaviors of deep networks, Explainable AI (XAI) has been made remarkable progress in interpreting the model's decision. A family of attribution techniques has been proposed, highlighting whether the input pixels are responsible for the model's prediction. However, the existing attribution methods suffer from the lack of rule guidance and require further human interpretations. In this paper, we construct the 'if-then' logic rules that are sufficiently precise locally. Moreover, a novel rule-guided method, dynamic ablation (DA), is proposed to find a minimal bound sufficient in an input image to justify the network's prediction and aggregate iteratively to reach a complete attribution. Both qualitative and quantitative experiments are conducted to evaluate the proposed DA. We demonstrate the advantages of our method in providing clear and explicit explanations that are also easy for human experts to understand. Besides, through the attribution on a series of trained networks with different architectures, we show that more complex networks require less information to make a specific prediction",
    "checked": true,
    "id": "24420650c05f8260a315c156a20de8e79e152e19",
    "semantic_title": "logic rule guided attribution with dynamic ablation",
    "citation_count": 0,
    "authors": [
      "Jianqiao An",
      "Yuandu Lai",
      "Yahong Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19882": {
    "title": "Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video",
    "volume": "main",
    "abstract": "We present Neural Marionette, an unsupervised approach that discovers the skeletal structure from a dynamic sequence and learns to generate diverse motions that are consistent with the observed motion dynamics. Given a video stream of point cloud observation of an articulated body under arbitrary motion, our approach discovers the unknown low-dimensional skeletal relationship that can effectively represent the movement. Then the discovered structure is utilized to encode the motion priors of dynamic sequences in a latent structure, which can be decoded to the relative joint rotations to represent the full skeletal motion. Our approach works without any prior knowledge of the underlying motion or skeletal structure, and we demonstrate that the discovered structure is even comparable to the hand-labeled ground truth skeleton in representing a 4D sequence of motion. The skeletal structure embeds the general semantics of possible motion space that can generate motions for diverse scenarios. We verify that the learned motion prior is generalizable to the multi-modal sequence generation, interpolation of two poses, and motion retargeting to a different skeletal structure",
    "checked": true,
    "id": "fc4742f32fd906a2627201d6170c33ed73965908",
    "semantic_title": "neural marionette: unsupervised learning of motion skeleton and latent dynamics from volumetric video",
    "citation_count": 2,
    "authors": [
      "Jinseok Bae",
      "Hojun Jang",
      "Cheol-Hui Min",
      "Hyungun Choi",
      "Young Min Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19883": {
    "title": "Deformable Part Region Learning for Object Detection",
    "volume": "main",
    "abstract": "In a convolutional object detector, the detection accuracy can be degraded often due to the low feature discriminability caused by geometric variation or transformation of an object. In this paper, we propose a deformable part region learning in order to allow decomposed part regions to be deformable according to geometric transformation of an object. To this end, we introduce trainable geometric parameters for the location of each part model. Because the ground truth of the part models is not available, we design classification and mask losses for part models, and learn the geometric parameters by minimizing an integral loss including those part losses. As a result, we can train a deformable part region network without extra super-vision and make each part model deformable according to object scale variation. Furthermore, for improving cascade object detection and instance segmentation, we present a Cascade deformable part region architecture which can refine whole and part detections iteratively in the cascade manner. Without bells and whistles, our implementation of a Cascade deformable part region detector achieves better detection and segmentation mAPs on COCO and VOC datasets, compared to the recent cascade and other state-of-the-art detectors",
    "checked": true,
    "id": "60c2529f36bf43c1eef90fbb4d4496c263b0e2e1",
    "semantic_title": "deformable part region learning for object detection",
    "citation_count": 3,
    "authors": [
      "Seung-Hwan Bae"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19884": {
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "volume": "main",
    "abstract": "We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks",
    "checked": true,
    "id": "cb09a44988c8f5ef7ff8bcfe1eb4509f9384cbbb",
    "semantic_title": "towards end-to-end image compression and analysis with transformers",
    "citation_count": 17,
    "authors": [
      "Yuanchao Bai",
      "Xu Yang",
      "Xianming Liu",
      "Junjun Jiang",
      "Yaowei Wang",
      "Xiangyang Ji",
      "Wen Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19885": {
    "title": "Handwritten Mathematical Expression Recognition via Attention Aggregation Based Bi-directional Mutual Learning",
    "volume": "main",
    "abstract": "Handwritten mathematical expression recognition aims to automatically generate LaTeX sequences from given images. Currently, attention-based encoder-decoder models are widely used in this task. They typically generate target sequences in a left-to-right (L2R) manner, leaving the right-to-left (R2L) contexts unexploited. In this paper, we propose an Attention aggregation based Bi-directional Mutual learning Network (ABM) which consists of one shared encoder and two parallel inverse decoders (L2R and R2L). The two decoders are enhanced via mutual distillation, which involves one-to-one knowledge transfer at each training step, making full use of the complementary information from two inverse directions. Moreover, in order to deal with mathematical symbols in diverse scales, an Attention Aggregation Module (AAM) is proposed to effectively integrate multi-scale coverage attentions. Notably, in the inference phase, given that the model already learns knowledge from two inverse directions, we only use the L2R branch for inference, keeping the original parameter size and inference speed. Extensive experiments demonstrate that our proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014, 52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation and model ensembling, substantially outperforming the state-of-the-art methods. The source code is available in https://github.com/XH-B/ABM",
    "checked": true,
    "id": "bfa08d26a484df40b5de6dcade2565af91418174",
    "semantic_title": "handwritten mathematical expression recognition via attention aggregation based bi-directional mutual learning",
    "citation_count": 11,
    "authors": [
      "Xiaohang Bian",
      "Bo Qin",
      "Xiaozhe Xin",
      "Jianwu Li",
      "Xuefeng Su",
      "Yanfeng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19886": {
    "title": "ADD: Frequency Attention and Multi-View Based Knowledge Distillation to Detect Low-Quality Compressed Deepfake Images",
    "volume": "main",
    "abstract": "Despite significant advancements of deep learning-based forgery detectors for distinguishing manipulated deepfake images, most detection approaches suffer from moderate to significant performance degradation with low-quality compressed deepfake images. Because of the limited information in low-quality images, detecting low-quality deepfake remains an important challenge. In this work, we apply frequency domain learning and optimal transport theory in knowledge distillation (KD) to specifically improve the detection of low-quality compressed deepfake images. We explore transfer learning capability in KD to enable a student network to learn discriminative features from low-quality images effectively. In particular, we propose the Attention-based Deepfake detection Distiller (ADD), which consists of two novel distillations: 1) frequency attention distillation that effectively retrieves the removed high-frequency components in the student network, and 2) multi-view attention distillation that creates multiple attention vectors by slicing the teacher's and student's tensors under different views to transfer the teacher tensor's distribution to the student more efficiently. Our extensive experimental results demonstrate that our approach outperforms state-of-the-art baselines in detecting low-quality compressed deepfake images",
    "checked": true,
    "id": "3c233de0f1bf308019b52920bae654043579491e",
    "semantic_title": "add: frequency attention and multi-view based knowledge distillation to detect low-quality compressed deepfake images",
    "citation_count": 31,
    "authors": [
      "Le  Minh Binh",
      "Simon Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19887": {
    "title": "LUNA: Localizing Unfamiliarity Near Acquaintance for Open-Set Long-Tailed Recognition",
    "volume": "main",
    "abstract": "The predefined artificially-balanced training classes in object recognition have limited capability in modeling real-world scenarios where objects are imbalanced-distributed with unknown classes. In this paper, we discuss a promising solution to the Open-set Long-Tailed Recognition (OLTR) task utilizing metric learning. Firstly, we propose a distribution-sensitive loss, which weighs more on the tail classes to decrease the intra-class distance in the feature space. Building upon these concentrated feature clusters, a local-density-based metric is introduced, called Localizing Unfamiliarity Near Acquaintance (LUNA), to measure the novelty of a testing sample. LUNA is flexible with different cluster sizes and is reliable on the cluster boundary by considering neighbors of different properties. Moreover, contrary to most of the existing works that alleviate the open-set detection as a simple binary decision, LUNA is a quantitative measurement with interpretable meanings. Our proposed method exceeds the state-of-the-art algorithm by 4-6% in the closed-set recognition accuracy and 4% in F-measure under the open-set on the public benchmark datasets, including our own newly introduced fine-grained OLTR dataset about marine species (MS-LT), which is the first naturally-distributed OLTR dataset revealing the genuine genetic relationships of the classes",
    "checked": true,
    "id": "b0a38249dbf08536c95844f3c716a273a97194b7",
    "semantic_title": "luna: localizing unfamiliarity near acquaintance for open-set long-tailed recognition",
    "citation_count": 4,
    "authors": [
      "Jiarui Cai",
      "Yizhou Wang",
      "Hung-Min Hsu",
      "Jenq-Neng Hwang",
      "Kelsey Magrane",
      "Craig S Rose"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19888": {
    "title": "Prior Gradient Mask Guided Pruning-Aware Fine-Tuning",
    "volume": "main",
    "abstract": "We proposed a Prior Gradient Mask Guided Pruning-aware Fine-Tuning (PGMPF) framework to accelerate deep Convolutional Neural Networks (CNNs). In detail, the proposed PGMPF selectively suppresses the gradient of those \"unimportant\" parameters via a prior gradient mask generated by the pruning criterion during fine-tuning. PGMPF has three charming characteristics over previous works: (1) Pruning-aware network fine-tuning. A typical pruning pipeline consists of training, pruning and fine-tuning, which are relatively independent, while PGMPF utilizes a variant of the pruning mask as a prior gradient mask to guide fine-tuning, without complicated pruning criteria. (2) An excellent tradeoff between large model capacity during fine-tuning and stable convergence speed to obtain the final compact model. Previous works preserve more training information of pruned parameters during fine-tuning to pursue better performance, which would incur catastrophic non-convergence of the pruned model for relatively large pruning rates, while our PGMPF greatly stabilizes the fine-tuning phase by gradually constraining the learning rate of those \"unimportant\" parameters. (3) Channel-wise random dropout of the prior gradient mask to impose some gradient noise to fine-tuning to further improve the robustness of final compact model. Experimental results on three image classification benchmarks CIFAR10/ 100 and ILSVRC-2012 demonstrate the effectiveness of our method for various CNN architectures, datasets and pruning rates. Notably, on ILSVRC-2012, PGMPF reduces 53.5% FLOPs on ResNet-50 with only 0.90% top-1 accuracy drop and 0.52% top-5 accuracy drop, which has advanced the state-of-the-art with negligible extra computational cost",
    "checked": true,
    "id": "4cbfa720b09abc0e1832cdb72ce67d1f10416c89",
    "semantic_title": "prior gradient mask guided pruning-aware fine-tuning",
    "citation_count": 14,
    "authors": [
      "Linhang Cai",
      "Zhulin An",
      "Chuanguang Yang",
      "Yangchun Yan",
      "Yongjun Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19889": {
    "title": "Context-Aware Transfer Attacks for Object Detection",
    "volume": "main",
    "abstract": "Blackbox transfer attacks for image classifiers have been extensively studied in recent years. In contrast, little progress has been made on transfer attacks for object detectors. Object detectors take a holistic view of the image and the detection of one object (or lack thereof) often depends on other objects in the scene. This makes such detectors inherently context-aware and adversarial attacks in this space are more challenging than those targeting image classifiers. In this paper, we present a new approach to generate context-aware attacks for object detectors. We show that by using co-occurrence of objects and their relative locations and sizes as context information, we can successfully generate targeted mis-categorization attacks that achieve higher transfer success rates on blackbox object detectors than the state-of-the-art. We test our approach on a variety of object detectors with images from PASCAL VOC and MS COCO datasets and demonstrate up to 20 percentage points improvement in performance compared to the other state-of-the-art methods",
    "checked": true,
    "id": "415e98ed66f9a0badc8f7dcafffdda1630557c2c",
    "semantic_title": "context-aware transfer attacks for object detection",
    "citation_count": 11,
    "authors": [
      "Zikui Cai",
      "Xinxin Xie",
      "Shasha Li",
      "Mingjun Yin",
      "Chengyu Song",
      "Srikanth V. Krishnamurthy",
      "Amit K. Roy-Chowdhury",
      "M. Salman Asif"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19890": {
    "title": "OoDHDR-Codec: Out-of-Distribution Generalization for HDR Image Compression",
    "volume": "main",
    "abstract": "Recently, deep learning has been proven to be a promising approach in standard dynamic range (SDR) image compression. However, due to the wide luminance distribution of high dynamic range (HDR) images and the lack of large standard datasets, developing a deep model for HDR image compression is much more challenging. To tackle this issue, we view HDR data as distributional shifts of SDR data and the HDR image compression can be modeled as an out-of-distribution generalization (OoD) problem. Herein, we propose a novel out-of-distribution (OoD) HDR image compression framework (OoDHDR-codec). It learns the general representation across HDR and SDR environments, and allows the model to be trained effectively using a large set of SDR datases supplemented with much fewer HDR samples. Specifically, OoDHDR-codec consists of two branches to process the data from two environments. The SDR branch is a standard blackbox network. For the HDR branch, we develop a hybrid system that models luminance masking and tone mapping with white-box modules and performs content compression with black-box neural networks. To improve the generalization from SDR training data on HDR data, we introduce an invariance regularization term to learn the common representation for both SDR and HDR compression. Extensive experimental results show that the OoDHDR codec achieves strong competitive in-distribution performance and state-of-the-art OoD performance. To the best of our knowledge, our proposed approach is the first work to model HDR compression as OoD generalization problems and our OoD generalization algorithmic framework can be applied to any deep compression model in addition to the network architectural choice demonstrated in the paper. Code available at https://github.com/caolinfeng/OoDHDR-codec",
    "checked": true,
    "id": "cfbfe331d4438c080b5dc5b8308d2ac633967baf",
    "semantic_title": "oodhdr-codec: out-of-distribution generalization for hdr image compression",
    "citation_count": 4,
    "authors": [
      "Linfeng Cao",
      "Aofan Jiang",
      "Wei Li",
      "Huaying Wu",
      "Nanyang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19891": {
    "title": "Visual Consensus Modeling for Video-Text Retrieval",
    "volume": "main",
    "abstract": "In this paper, we propose a novel method to mine the commonsense knowledge shared between the video and text modalities for video-text retrieval, namely visual consensus modeling. Different from the existing works, which learn the video and text representations and their complicated relationships solely based on the pairwise video-text data, we make the first attempt to model the visual consensus by mining the visual concepts from videos and exploiting their co-occurrence patterns within the video and text modalities with no reliance on any additional concept annotations. Specifically, we build a shareable and learnable graph as the visual consensus, where the nodes denoting the mined visual concepts and the edges connecting the nodes representing the co-occurrence relationships between the visual concepts. Extensive experimental results on the public benchmark datasets demonstrate that our proposed method, with the ability to effectively model the visual consensus, achieves state-of-the-art performances on the bidirectional video-text retrieval task. Our code is available at https://github.com/sqiangcao99/VCM",
    "checked": true,
    "id": "72acaae80826d27ad8eeed33c23dbae369aa14ea",
    "semantic_title": "visual consensus modeling for video-text retrieval",
    "citation_count": 15,
    "authors": [
      "Shuqiang Cao",
      "Bairui Wang",
      "Wei Zhang",
      "Lin Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19892": {
    "title": "Proximal PanNet: A Model-Based Deep Network for Pansharpening",
    "volume": "main",
    "abstract": "Recently, deep learning techniques have been extensively studied for pansharpening, which aims to generate a high resolution multispectral (HRMS) image by fusing a low resolution multispectral (LRMS) image with a high resolution panchromatic (PAN) image. However, existing deep learning-based pansharpening methods directly learn the mapping from LRMS and PAN to HRMS. These network architectures always lack sufficient interpretability, which limits further performance improvements. To alleviate this issue, we propose a novel deep network for pansharpening by combining the model-based methodology with the deep learning method. Firstly, we build an observation model for pansharpening using the convolutional sparse coding (CSC) technique and design a proximal gradient algorithm to solve this model. Secondly, we unfold the iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning the proximal operators using convolutional neural networks. Finally, all the learnable modules can be automatically learned in an end-to-end manner. Experimental results on some benchmark datasets show that our network performs better than other advanced methods both quantitatively and qualitatively",
    "checked": true,
    "id": "04788b110c555bc63a49f0166bfe572ba1edb828",
    "semantic_title": "proximal pannet: a model-based deep network for pansharpening",
    "citation_count": 7,
    "authors": [
      "Xiangyong Cao",
      "Yang Chen",
      "Wenfei Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19893": {
    "title": "CF-DETR: Coarse-to-Fine Transformers for End-to-End Object Detection",
    "volume": "main",
    "abstract": "The recently proposed DEtection TRansformer (DETR) achieves promising performance for end-to-end object detection. However, it has relatively lower detection performance on small objects and suffers from slow convergence. This paper observed that DETR performs surprisingly well even on small objects when measuring Average Precision (AP) at decreased Intersection-over-Union (IoU) thresholds. Motivated by this observation, we propose a simple way to improve DETR by refining the coarse features and predicted locations. Specifically, we propose a novel Coarse-to-Fine (CF) decoder layer constituted of a coarse layer and a carefully designed fine layer. Within each CF decoder layer, the extracted local information (region of interest feature) is introduced into the flow of global context information from the coarse layer to refine and enrich the object query features via the fine layer. In the fine layer, the multi-scale information can be fully explored and exploited via the Adaptive Scale Fusion(ASF) module and Local Cross-Attention (LCA) module. The multi-scale information can also be enhanced by another proposed Transformer Enhanced FPN (TEF) module to further improve the performance. With our proposed framework (named CF-DETR), the localization accuracy of objects (especially for small objects) can be largely improved. As a byproduct, the slow convergence issue of DETR can also be addressed. The effectiveness of CF-DETR is validated via extensive experiments on the coco benchmark. CF-DETR achieves state-of-the-art performance among end-to-end detectors, e.g., achieving 47.8 AP using ResNet-50 with 36 epochs in the standard 3x training schedule",
    "checked": true,
    "id": "f47ccff51c9153959fcba2863dbf7fdd04283665",
    "semantic_title": "cf-detr: coarse-to-fine transformers for end-to-end object detection",
    "citation_count": 19,
    "authors": [
      "Xipeng Cao",
      "Peng Yuan",
      "Bailan Feng",
      "Kun Niu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19894": {
    "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications",
    "volume": "main",
    "abstract": "This paper starts by revealing a surprising finding: without any learning, a randomly initialized CNN can localize objects surprisingly well. That is, a CNN has an inductive bias to naturally focus on objects, named as Tobias (\"The object is at sight\") in this paper. This empirical inductive bias is further analyzed and successfully applied to self-supervised learning (SSL). A CNN is encouraged to learn representations that focus on the foreground object, by transforming every image into various versions with different backgrounds, where the foreground and background separation is guided by Tobias. Experimental results show that the proposed Tobias significantly improves downstream tasks, especially for object detection. This paper also shows that Tobias has consistent improvements on training sets of different sizes, and is more resilient to changes in image augmentations",
    "checked": true,
    "id": "317909e30c6c614fda97c1060c3a6297de8cffa1",
    "semantic_title": "a random cnn sees objects: one inductive bias of cnn and its applications",
    "citation_count": 12,
    "authors": [
      "Yun-Hao Cao",
      "Jianxin Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19895": {
    "title": "Texture Generation Using Dual-Domain Feature Flow with Multi-View Hallucinations",
    "volume": "main",
    "abstract": "We propose a dual-domain generative model to estimate a texture map from a single image for colorizing a 3D human model. When estimating a texture map, a single image is insufficient as it reveals only one facet of a 3D object. To provide sufficient information for estimating a complete texture map, the proposed model simultaneously generates multi-view hallucinations in the image domain and an estimated texture map in the texture domain. During the generating process, each domain generator exchanges features to the other by a flow-based local attention mechanism. In this manner, the proposed model can estimate a texture map utilizing abundant multi-view image features from which multiview hallucinations are generated. As a result, the estimated texture map contains consistent colors and patterns over the entire region. Experiments show the superiority of our model for estimating a directly render-able texture map, which is applicable to 3D animation rendering. Furthermore, our model also improves an overall generation quality in the image domain for pose and viewpoint transfer tasks",
    "checked": true,
    "id": "918d9522d57b4ed2bcf0996d22d610eb7605e09f",
    "semantic_title": "texture generation using dual-domain feature flow with multi-view hallucinations",
    "citation_count": 1,
    "authors": [
      "Seunggyu Chang",
      "Jungchan Cho",
      "Songhwai Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19896": {
    "title": "Resistance Training Using Prior Bias: Toward Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "Scene Graph Generation (SGG) aims to build a structured representation of a scene using objects and pairwise relationships, which benefits downstream tasks. However, current SGG methods usually suffer from sub-optimal scene graph generation because of the long-tailed distribution of training data. To address this problem, we propose Resistance Training using Prior Bias (RTPB) for the scene graph generation. Specifically, RTPB uses a distributed-based prior bias to improve models' detecting ability on less frequent relationships during training, thus improving the model generalizability on tail categories. In addition, to further explore the contextual information of objects and relationships, we design a contextual encoding backbone network, termed as Dual Transformer (DTrans). We perform extensive experiments on a very popular benchmark, VG150, to demonstrate the effectiveness of our method for the unbiased scene graph generation. In specific, our RTPB achieves an improvement of over 10% under the mean recall when applied to current SGG methods. Furthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods with a large margin. Code is available at https://github.com/ChCh1999/RTPB",
    "checked": true,
    "id": "75ccd6dd8bbb466ecfdf3e2bf7691de32ea7e87c",
    "semantic_title": "resistance training using prior bias: toward unbiased scene graph generation",
    "citation_count": 16,
    "authors": [
      "Chao Chen",
      "Yibing Zhan",
      "Baosheng Yu",
      "Liu Liu",
      "Yong Luo",
      "Bo Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19897": {
    "title": "SASA: Semantics-Augmented Set Abstraction for Point-Based 3D Object Detection",
    "volume": "main",
    "abstract": "Although point-based networks are demonstrated to be accurate for 3D point cloud modeling, they are still falling behind their voxel-based competitors in 3D detection. We observe that the prevailing set abstraction design for down-sampling points may maintain too much unimportant background information that can affect feature learning for detecting objects. To tackle this issue, we propose a novel set abstraction method named Semantics-Augmented Set Abstraction (SASA). Technically, we first add a binary segmentation module as the side output to help identify foreground points. Based on the estimated point-wise foreground scores, we then propose a semantics-guided point sampling algorithm to help retain more important foreground points during down-sampling. In practice, SASA shows to be effective in identifying valuable points related to foreground objects and improving feature learning for point-based 3D detection. Additionally, it is an easy-to-plug-in module and able to boost various point-based detectors, including single-stage and two-stage ones. Extensive experiments on the popular KITTI and nuScenes datasets validate the superiority of SASA, lifting point-based detection models to reach comparable performance to state-of-the-art voxel-based methods. Code is available at https://github.com/blakechen97/SASA",
    "checked": true,
    "id": "6c7df45080cc5ad3a8c3dea35a4c95191ce4cfe0",
    "semantic_title": "sasa: semantics-augmented set abstraction for point-based 3d object detection",
    "citation_count": 44,
    "authors": [
      "Chen Chen",
      "Zhe Chen",
      "Jing Zhang",
      "Dacheng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19898": {
    "title": "Comprehensive Regularization in a Bi-directional Predictive Network for Video Anomaly Detection",
    "volume": "main",
    "abstract": "Video anomaly detection aims to automatically identify unusual objects or behaviours by learning from normal videos. Previous methods tend to use simplistic reconstruction or prediction constraints, which leads to the insufficiency of learned representations for normal data. As such, we propose a novel bi-directional architecture with three consistency constraints to comprehensively regularize the prediction task from pixel-wise, cross-modal, and temporal-sequence levels. First, predictive consistency is proposed to consider the symmetry property of motion and appearance in forwards and backwards time, which ensures the highly realistic appearance and motion predictions at the pixel-wise level. Second, association consistency considers the relevance between different modalities and uses one modality to regularize the prediction of another one. Finally, temporal consistency utilizes the relationship of the video sequence and ensures that the predictive network generates temporally consistent frames. During inference, the pattern of abnormal frames is unpredictable and will therefore cause higher prediction errors. Experiments show that our method outperforms advanced anomaly detectors and achieves state-of-the-art results on UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets",
    "checked": true,
    "id": "e97a764c2ac3d0dc588dc363ea77721b300c0790",
    "semantic_title": "comprehensive regularization in a bi-directional predictive network for video anomaly detection",
    "citation_count": 27,
    "authors": [
      "Chengwei Chen",
      "Yuan Xie",
      "Shaohui Lin",
      "Angela Yao",
      "Guannan Jiang",
      "Wei Zhang",
      "Yanyun Qu",
      "Ruizhi Qiao",
      "Bo Ren",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19899": {
    "title": "Keypoint Message Passing for Video-Based Person Re-identification",
    "volume": "main",
    "abstract": "Video-based person re-identification~(re-ID) is an important technique in visual surveillance systems which aims to match video snippets of people captured by different cameras. Existing methods are mostly based on convolutional neural networks~(CNNs), whose building blocks either process local neighbor pixels at a time, or, when 3D convolutions are used to model temporal information, suffer from the misalignment problem caused by person movement. In this paper, we propose to overcome the limitations of normal convolutions with a human-oriented graph method. Specifically, features located at person joint keypoints are extracted and connected as a spatial-temporal graph. These keypoint features are then updated by message passing from their connected nodes with a graph convolutional network~(GCN). During training, the GCN can be attached to any CNN-based person re-ID model to assist representation learning on feature maps, whilst it can be dropped after training for better inference speed. Our method brings significant improvements over the CNN-based baseline model on the MARS dataset with generated person keypoints and a newly annotated dataset: PoseTrackReID. It also defines a new state-of-the-art method in terms of top-1 accuracy and mean average precision in comparison to prior works",
    "checked": true,
    "id": "d741ab50f814f0ef218a519c316d227acf7e28ba",
    "semantic_title": "keypoint message passing for video-based person re-identification",
    "citation_count": 9,
    "authors": [
      "Di Chen",
      "Andreas Doering",
      "Shanshan Zhang",
      "Jian Yang",
      "Juergen Gall",
      "Bernt Schiele"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19900": {
    "title": "DCAN: Improving Temporal Action Detection via Dual Context Aggregation",
    "volume": "main",
    "abstract": "Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN",
    "checked": true,
    "id": "fa85c6c09bec52f11feb6cb5b96e854d87a13ed9",
    "semantic_title": "dcan: improving temporal action detection via dual context aggregation",
    "citation_count": 32,
    "authors": [
      "Guo Chen",
      "Yin-Dong Zheng",
      "Limin Wang",
      "Tong Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19901": {
    "title": "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer",
    "volume": "main",
    "abstract": "We present a customized 3D mesh Transformer model for the pose transfer task. As the 3D pose transfer essentially is a deformation procedure dependent on the given meshes, the intuition of this work is to perceive the geometric inconsistency between the given meshes with the powerful self-attention mechanism. Specifically, we propose a novel geometry-contrastive Transformer that has an efficient 3D structured perceiving ability to the global geometric inconsistencies across the given meshes. Moreover, locally, a simple yet efficient central geodesic contrastive loss is further proposed to improve the regional geometric-inconsistency learning. At last, we present a latent isometric regularization module together with a novel semi-synthesized dataset for the cross-dataset 3D pose transfer task towards unknown spaces. The massive experimental results prove the efficacy of our approach by showing state-of-the-art quantitative performances on SMPL-NPT, FAUST and our new proposed dataset SMG-3D datasets, as well as promising qualitative results on MG-cloth and SMAL datasets. It's demonstrated that our method can achieve robust 3D pose transfer and be generalized to challenging meshes from unknown spaces on cross-dataset tasks. The code and dataset are made available. Code is available: https://github.com/mikecheninoulu/CGT",
    "checked": true,
    "id": "c2957eb6f9d26dabafff1653f2bb09d6b6d7253a",
    "semantic_title": "geometry-contrastive transformer for generalized 3d pose transfer",
    "citation_count": 8,
    "authors": [
      "Haoyu Chen",
      "Hao Tang",
      "Zitong Yu",
      "Nicu Sebe",
      "Guoying Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19902": {
    "title": "Explore Inter-contrast between Videos via Composition for Weakly Supervised Temporal Sentence Grounding",
    "volume": "main",
    "abstract": "Weakly supervised temporal sentence grounding aims to temporally localize the target segment corresponding to a given natural language query, where it provides video-query pairs without temporal annotations during training. Most existing methods use the fused visual-linguistic feature to reconstruct the query, where the least reconstruction error determines the target segment. This work introduces a novel approach that explores the inter-contrast between videos in a composed video by selecting components from two different videos and fusing them into a single video. Such a straightforward yet effective composition strategy provides the temporal annotations at multiple composed positions, resulting in numerous videos with temporal ground-truths for training the temporal sentence grounding task. A transformer framework is introduced with multi-tasks training to learn a compact but efficient visual-linguistic space. The experimental results on the public Charades-STA and ActivityNet-Caption dataset demonstrate the effectiveness of the proposed method, where our approach achieves comparable performance over the state-of-the-art weakly-supervised baselines. The code is available at https://github.com/PPjmchen/Composition_WSTG",
    "checked": true,
    "id": "d9927d77914f67645d55178707bc7ef0999d919b",
    "semantic_title": "explore inter-contrast between videos via composition for weakly supervised temporal sentence grounding",
    "citation_count": 9,
    "authors": [
      "Jiaming Chen",
      "Weixin Luo",
      "Wei Zhang",
      "Lin Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19903": {
    "title": "Adaptive Image-to-Video Scene Graph Generation via Knowledge Reasoning and Adversarial Learning",
    "volume": "main",
    "abstract": "Scene graph in a video conveys a wealth of information about objects and their relationships in the scene, thus benefiting many downstream tasks such as video captioning and visual question answering. Existing methods of scene graph generation require large-scale training videos annotated with objects and relationships in each frame to learn a powerful model. However, such comprehensive annotation is time-consuming and labor-intensive. On the other hand, it is much easier and less cost to annotate images with scene graphs, so we investigate leveraging annotated images to facilitate training a scene graph generation model for unannotated videos, namely image-to-video scene graph generation. This task presents two challenges: 1) infer unseen dynamic relationships in videos from static relationships in images due to the absence of motion information in images; 2) adapt objects and static relationships from images to video frames due to the domain shift between them. To address the first challenge, we exploit external commonsense knowledge to infer the unseen dynamic relationship from the temporal evolution of static relationships. We tackle the second challenge by hierarchical adversarial learning to reduce the data distribution discrepancy between images and video frames. Extensive experiment results on two benchmark video datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "700f19964b9a9cc164dd815f840466685ef4bb53",
    "semantic_title": "adaptive image-to-video scene graph generation via knowledge reasoning and adversarial learning",
    "citation_count": 1,
    "authors": [
      "Jin Chen",
      "Xiaofeng Ji",
      "Xinxiao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19904": {
    "title": "Text Gestalt: Stroke-Aware Scene Text Image Super-resolution",
    "volume": "main",
    "abstract": "In the last decade, the blossom of deep learning has witnessed the rapid development of scene text recognition. However, the recognition of low-resolution scene text images remains a challenge. Even though some super-resolution methods have been proposed to tackle this problem, they usually treat text images as general images while ignoring the fact that the visual quality of strokes (the atomic unit of text) plays an essential role for text recognition. According to Gestalt Psychology, humans are capable of composing parts of details into the most similar objects guided by prior knowledge. Likewise, when humans observe a low-resolution text image, they will inherently use partial stroke-level details to recover the appearance of holistic characters. Inspired by Gestalt Psychology, we put forward a Stroke-Aware Scene Text Image Super-Resolution method containing a Stroke-Focused Module (SFM) to concentrate on stroke-level internal structures of characters in text images. Specifically, we attempt to design rules for decomposing English characters and digits at stroke-level, then pre-train a text recognizer to provide stroke-level attention maps as positional clues with the purpose of controlling the consistency between the generated super-resolution image and high-resolution ground truth. The extensive experimental results validate that the proposed method can indeed generate more distinguishable images on TextZoom and manually constructed Chinese character dataset Degraded-IC13. Furthermore, since the proposed SFM is only used to provide stroke-level guidance when training, it will not bring any time overhead during the test phase. Code is available at https://github.com/FudanVI/FudanOCR/tree/main/text-gestalt",
    "checked": true,
    "id": "103b86754729c532db733602810bc4030e7c9f78",
    "semantic_title": "text gestalt: stroke-aware scene text image super-resolution",
    "citation_count": 21,
    "authors": [
      "Jingye Chen",
      "Haiyang Yu",
      "Jianqi Ma",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19905": {
    "title": "Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion",
    "volume": "main",
    "abstract": "Face self-occlusions are inevitable due to the 3D nature of the human face and the loss of information in the projection process from 3D to 2D images. While recovering face self-occlusions based on 3D face reconstruction, e.g., 3D Morphable Model (3DMM) and its variants provides an effective solution, most of the existing methods show apparent limitations in expressing high-fidelity, natural, and diverse facial details. To overcome these limitations, we propose in this paper a new generative adversarial network (MvInvert) for natural face self-occlusion recovery without using paired image-texture data. We design a coarse-to-fine generator for photorealistic texture generation. A coarse texture is computed by inpainting the invisible areas in the photorealistic but incomplete texture sampled directly from the 2D image using the unrealistic but complete statistical texture from 3DMM. Then, we design a multi-view Residual-based GAN Inversion, which re-renders and refines multi-view 2D images, which are used for extracting multiple high-fidelity textures. Finally, these high-fidelity textures are fused based on their visibility maps via Poisson blending. To perform adversarial learning to assure the quality of the recovered texture, we design a discriminator consisting of two heads, i.e., one for global and local discrimination between the recovered texture and a small set of real textures in UV space, and the other for discrimination between the input image and the re-rendered 2D face images via pixel-wise, identity, and adversarial losses. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in face self-occlusion recovery under unconstrained scenarios",
    "checked": true,
    "id": "e2cd959758141683cef915540292e839e7edca0e",
    "semantic_title": "towards high-fidelity face self-occlusion recovery via multi-view residual-based gan inversion",
    "citation_count": 3,
    "authors": [
      "Jinsong Chen",
      "Hu Han",
      "Shiguang Shan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19906": {
    "title": "ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation",
    "volume": "main",
    "abstract": "Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting apparent motion of objects with microsecond resolution, and shows great application potential in monitoring and other fields. However, the output event stream of existing DVS inevitably contains background activity noise (BA noise) due to dark current and junction leakage current, which will affect the temporal correlation of objects, resulting in deteriorated motion estimation performance. Particularly, the existing filter-based denoising methods cannot be directly applied to suppress the noise in event stream, since there is no spatial correlation. To address this issue, this paper presents a novel progressive framework, in which a Motion Estimation (ME) module and an Event Denoising (ED) module are jointly optimized in a mutually reinforced manner. Specifically, based on the maximum sharpness criterion, ME module divides the input event into several segments by adaptive clustering in a motion compensating warp field, and captures the temporal correlation of event stream according to the clustered motion parameters. Taking temporal correlation as guidance, ED module calculates the confidence that each event belongs to real activity events, and transmits it to ME module to update energy function of motion segmentation for noise suppression. The two steps are iteratively updated until stable motion segmentation results are obtained. Extensive experimental results on both synthetic and real datasets demonstrate the superiority of our proposed approaches against the State-Of-The-Art (SOTA) methods",
    "checked": true,
    "id": "3360cb3348d0db8251a4d5d70f56b6ee37ec8ff7",
    "semantic_title": "progressivemotionseg: mutually reinforced framework for event-based motion segmentation",
    "citation_count": 4,
    "authors": [
      "Jinze Chen",
      "Yang Wang",
      "Yang Cao",
      "Feng Wu",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19907": {
    "title": "Attacking Video Recognition Models with Bullet-Screen Comments",
    "volume": "main",
    "abstract": "Recent research has demonstrated that Deep Neural Networks (DNNs) are vulnerable to adversarial patches which introduce perceptible but localized changes to the input. Nevertheless, existing approaches have focused on generating adversarial patches on images, their counterparts in videos have been less explored. Compared with images, attacking videos is much more challenging as it needs to consider not only spatial cues but also temporal cues. To close this gap, we introduce a novel adversarial attack in this paper, the bullet-screen comment (BSC) attack, which attacks video recognition models with BSCs. Specifically, adversarial BSCs are generated with a Reinforcement Learning (RL) framework, where the environment is set as the target model and the agent plays the role of selecting the position and transparency of each BSC. By continuously querying the target models and receiving feedback, the agent gradually adjusts its selection strategies in order to achieve a high fooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of meaningful patch, adding it to a clean video will not affect people's understanding of the video content, nor will arouse people's suspicion. We conduct extensive experiments to verify the effectiveness of the proposed method. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve about 90% fooling rate when attacking three mainstream video recognition models, while only occluding < 8% areas in the video. Our code is available at https://github.com/kay-ck/BSC-attack",
    "checked": true,
    "id": "3474b9eb70e8b0feb8ef72e7b8c222b8bd5209d6",
    "semantic_title": "attacking video recognition models with bullet-screen comments",
    "citation_count": 10,
    "authors": [
      "Kai Chen",
      "Zhipeng Wei",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19908": {
    "title": "VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Invariance to diverse types of image corruption, such as noise, blurring, or colour shifts, is essential to establish robust models in computer vision. Data augmentation has been the major approach in improving the robustness against common corruptions. However, the samples produced by popular augmentation strategies deviate significantly from the underlying data manifold. As a result, performance is skewed toward certain types of corruption. To address this issue, we propose a multi-source vicinal transfer augmentation (VITA) method for generating diverse on-manifold samples. The proposed VITA consists of two complementary parts: tangent transfer and integration of multi-source vicinal samples. The tangent transfer creates initial augmented samples for improving corruption robustness. The integration employs a generative model to characterize the underlying manifold built by vicinal samples, facilitating the generation of on-manifold samples. Our proposed VITA significantly outperforms the current state-of-the-art augmentation methods, demonstrated in extensive experiments on corruption benchmarks",
    "checked": true,
    "id": "4113c49ff42b92c8a1192bc58412d7b646a9aeda",
    "semantic_title": "vita: a multi-source vicinal transfer augmentation method for out-of-distribution generalization",
    "citation_count": 2,
    "authors": [
      "Minghui Chen",
      "Cheng Wen",
      "Feng Zheng",
      "Fengxiang He",
      "Ling Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19909": {
    "title": "TransZero: Attribute-Guided Transformer for Zero-Shot Learning",
    "volume": "main",
    "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is learned from attribute descriptions shared between different classes, which are strong prior for localization of object attribute for representing discriminative region features enabling significant visual-semantic interaction. Although few attention-based models have attempted to learn such region features in a single image, the transferability and discriminative attribute localization of visual features are typically neglected. In this paper, we propose an attribute-guided Transformer network to learn the attribute localization for discriminative visual-semantic embedding representations in ZSL, termed TransZero. Specifically, TransZero takes a feature augmentation encoder to alleviate the cross-dataset bias between ImageNet and ZSL benchmarks and improve the transferability of visual features by reducing the entangled relative geometry relationships among region features. To learn locality-augmented visual features, TransZero employs a visual-semantic decoder to localize the most relevant image regions to each attributes from a given image under the guidance of attribute semantic information. Then, the locality-augmented visual features and semantic vectors are used for conducting effective visual-semantic interaction in a visual-semantic embedding network. Extensive experiments show that TransZero achieves a new state-of-the-art on three ZSL benchmarks. The codes are available at: https://github.com/shiming-chen/TransZero",
    "checked": true,
    "id": "e54cb48aac19d8c7714fdc1af4c328dca49bb7f3",
    "semantic_title": "transzero: attribute-guided transformer for zero-shot learning",
    "citation_count": 43,
    "authors": [
      "Shiming Chen",
      "Ziming Hong",
      "Yang Liu",
      "Guo-Sen Xie",
      "Baigui Sun",
      "Hao Li",
      "Qinmu Peng",
      "Ke Lu",
      "Xinge You"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19910": {
    "title": "Structured Semantic Transfer for Multi-Label Recognition with Partial Labels",
    "volume": "main",
    "abstract": "Multi-label image recognition is a fundamental yet practical task because real-world images inherently possess multiple semantic labels. However, it is difficult to collect large-scale multi-label annotations due to the complexity of both the input images and output label spaces. To reduce the annotation cost, we propose a structured semantic transfer (SST) framework that enables training multi-label recognition models with partial labels, i.e., merely some labels are known while other labels are missing (also called unknown labels) per image. The framework consists of two complementary transfer modules that explore within-image and cross-image semantic correlations to transfer knowledge of known labels to generate pseudo labels for unknown labels. Specifically, an intra-image semantic transfer module learns image-specific label co-occurrence matrix and maps the known labels to complement unknown labels based on this matrix. Meanwhile, a cross-image transfer module learns category-specific feature similarities and helps complement unknown labels with high similarities. Finally, both known and generated labels are used to train the multi-label recognition models. Extensive experiments on the Microsoft COCO, Visual Genome and Pascal VOC datasets show that the proposed SST framework obtains superior performance over current state-of-the-art algorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL",
    "checked": true,
    "id": "1fe030dd8c8f0dcb35d74f2255d1e2baa166e812",
    "semantic_title": "structured semantic transfer for multi-label recognition with partial labels",
    "citation_count": 23,
    "authors": [
      "Tianshui Chen",
      "Tao Pu",
      "Hefeng Wu",
      "Yuan Xie",
      "Liang Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19911": {
    "title": "SJDL-Vehicle: Semi-supervised Joint Defogging Learning for Foggy Vehicle Re-identification",
    "volume": "main",
    "abstract": "Vehicle re-identification (ReID) has attracted considerable attention in computer vision. Although several methods have been proposed to achieve state-of-the-art performance on this topic, re-identifying vehicle in foggy scenes remains a great challenge due to the degradation of visibility. To our knowledge, this problem is still not well-addressed so far. In this paper, to address this problem, we propose a novel training framework called Semi-supervised Joint Defogging Learning (SJDL) framework. First, the fog removal branch and the re-identification branch are integrated to perform simultaneous training. With the collaborative training scheme, defogged features generated by the defogging branch from input images can be shared to learn better representation for the re-identification branch. However, since the fog-free image of real-world data is intractable, this architecture can only be trained on the synthetic data, which may cause the domain gap problem between real-world and synthetic scenarios. To solve this problem, we design a semi-supervised defogging training scheme that can train two kinds of data alternatively in each iteration. Due to the lack of a dataset specialized for vehicle ReID in the foggy weather, we construct a dataset called FVRID which consists of real-world and synthetic foggy images to train and evaluate the performance. Experimental results show that the proposed method is effective and outperforms other existing vehicle ReID methods in the foggy weather. The code and dataset are available in https://github.com/Cihsaing/SJDL-Foggy-Vehicle-Re-Identification--AAAI2022",
    "checked": true,
    "id": "3c67010cd7bc5021926332cfd0064f201668d0a6",
    "semantic_title": "sjdl-vehicle: semi-supervised joint defogging learning for foggy vehicle re-identification",
    "citation_count": 5,
    "authors": [
      "Wei-Ting Chen",
      "I-Hsiang Chen",
      "Chih-Yuan Yeh",
      "Hao-Hsiang Yang",
      "Jian-Jiun Ding",
      "Sy-Yen Kuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19912": {
    "title": "Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification",
    "volume": "main",
    "abstract": "Real-world data often follows a long-tailed distribution, which makes the performance of existing classification algorithms degrade heavily. A key issue is that the samples in tail categories fail to depict their intra-class diversity. Humans can imagine a sample in new poses, scenes and view angles with their prior knowledge even if it is the first time to see this category. Inspired by this, we propose a novel reasoning-based implicit semantic data augmentation method to borrow transformation directions from other classes. Since the covariance matrix of each category represents the feature transformation directions, we can sample new directions from similar categories to generate definitely different instances. Specifically, the long-tailed distributed data is first adopted to train a backbone and a classifier. Then, a covariance matrix for each category is estimated, and a knowledge graph is constructed to store the relations of any two categories. Finally, tail samples are adaptively enhanced via propagating information from all the similar categories in the knowledge graph. Experimental results on CIFAR-LT-100, ImageNet-LT, and iNaturalist 2018 have demonstrated the effectiveness of our proposed method compared with the state-of-the-art methods",
    "checked": true,
    "id": "ec17b999b4c61309e68442a751043239b5599cc8",
    "semantic_title": "imagine by reasoning: a reasoning-based implicit semantic data augmentation for long-tailed classification",
    "citation_count": 11,
    "authors": [
      "Xiaohua Chen",
      "Yucan Zhou",
      "Dayan Wu",
      "Wanqian Zhang",
      "Yu Zhou",
      "Bo Li",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19913": {
    "title": "Guide Local Feature Matching by Overlap Estimation",
    "volume": "main",
    "abstract": "Local image feature matching under large appearance, viewpoint, and distance changes is challenging yet important. Conventional methods detect and match tentative local features across the whole images, with heuristic consistency checks to guarantee reliable matches. In this paper, we introduce a novel Overlap Estimation method conditioned on image pairs with TRansformer, named OETR, to constrain local feature matching in the commonly visible region. OETR performs overlap estimation in a two step process of feature correlation and then overlap regression. As a preprocessing module, OETR can be plugged into any existing local feature detection and matching pipeline, to mitigate potential view angle or scale variance. Intensive experiments show that OETR can boost state of the art local feature matching performance substantially, especially for image pairs with small shared regions. The code will be publicly available at https://github.com/AbyssGaze/OETR",
    "checked": true,
    "id": "d2859386b9a6de476c35358ca2c707895d90ac7f",
    "semantic_title": "guide local feature matching by overlap estimation",
    "citation_count": 11,
    "authors": [
      "Ying Chen",
      "Dihe Huang",
      "Shang Xu",
      "Jianlin Liu",
      "Yong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19914": {
    "title": "Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition",
    "volume": "main",
    "abstract": "Subject-invariant facial action unit (AU) recognition remains challenging for the reason that the data distribution varies among subjects. In this paper, we propose a causal inference framework for subject-invariant facial action unit recognition. To illustrate the causal effect existing in AU recognition task, we formulate the causalities among facial images, subjects, latent AU semantic relations, and estimated AU occurrence probabilities via a structural causal model. By constructing such a causal diagram, we clarify the causal-effect among variables and propose a plug-in causal intervention module, CIS, to deconfound the confounder Subject in the causal diagram. Extensive experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of our CIS, and the model with CIS inserted, CISNet, has achieved state-of-the-art performance",
    "checked": true,
    "id": "09bf883a92c4744c2615b978a1c62c1bb053a6f7",
    "semantic_title": "causal intervention for subject-deconfounded facial action unit recognition",
    "citation_count": 11,
    "authors": [
      "Yingjie Chen",
      "Diqi Chen",
      "Tao Wang",
      "Yizhou Wang",
      "Yun Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19915": {
    "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor",
    "volume": "main",
    "abstract": "One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description. Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples. To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples. The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training. The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples. This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description. In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models. IGD also shows better robustness in problems with small or contaminated training sets",
    "checked": true,
    "id": "fa1aa090d09dc5d420d0cce37cea72c21e99d36b",
    "semantic_title": "deep one-class classification via interpolated gaussian descriptor",
    "citation_count": 44,
    "authors": [
      "Yuanhong Chen",
      "Yu Tian",
      "Guansong Pang",
      "Gustavo Carneiro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19916": {
    "title": "Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization",
    "volume": "main",
    "abstract": "We present an extremely simple Ultra-Resolution Style Transfer framework, termed URST, to flexibly process arbitrary high-resolution images (e.g., 10000x10000 pixels) style transfer for the first time. Most of the existing state-of-the-art methods would fall short due to massive memory cost and small stroke size when processing ultra-high resolution images. URST completely avoids the memory problem caused by ultra-high resolution images by (1) dividing the image into small patches and (2) performing patch-wise style transfer with a novel Thumbnail Instance Normalization (TIN). Specifically, TIN can extract thumbnail features' normalization statistics and apply them to small patches, ensuring the style consistency among different patches. Overall, the URST framework has three merits compared to prior arts. (1) We divide input image into small patches and adopt TIN, successfully transferring image style with arbitrary high-resolution. (2) Experiments show that our URST surpasses existing SOTA methods on ultra-high resolution images benefiting from the effectiveness of the proposed stroke perceptual loss in enlarging the stroke size. (3) Our URST can be easily plugged into most existing style transfer methods and directly improve their performance even without training. Code is available at https://git.io/URST",
    "checked": true,
    "id": "c2793f9d9a555b6bbfa22d2b15d127ff01efd7d2",
    "semantic_title": "towards ultra-resolution neural style transfer via thumbnail instance normalization",
    "citation_count": 13,
    "authors": [
      "Zhe Chen",
      "Wenhai Wang",
      "Enze Xie",
      "Tong Lu",
      "Ping Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19917": {
    "title": "DeTarNet: Decoupling Translation and Rotation by Siamese Network for Point Cloud Registration",
    "volume": "main",
    "abstract": "Point cloud registration is a fundamental step for many tasks. In this paper, we propose a neural network named DetarNet to decouple the translation t and rotation R, so as to overcome the performance degradation due to their mutual interference in point cloud registration. First, a Siamese Network based Progressive and Coherent Feature Drift (PCFD) module is proposed to align the source and target points in high-dimensional feature space, and accurately recover translation from the alignment process. Then we propose a Consensus Encoding Unit (CEU) to construct more distinguishable features for a set of putative correspondences. After that, a Spatial and Channel Attention (SCA) block is adopted to build a classification network for finding good correspondences. Finally, the rotation is obtained by Singular Value Decomposition (SVD). In this way, the proposed network decouples the estimation of translation and rotation, resulting in better performance for both of them. Experimental results demonstrate that the proposed DetarNet improves registration performance on both indoor and outdoor scenes. Our code will be available in https://github.com/ZhiChen902/DetarNet",
    "checked": true,
    "id": "ad488e939cb9d31a25435d98987f570ef1f6a9c0",
    "semantic_title": "detarnet: decoupling translation and rotation by siamese network for point cloud registration",
    "citation_count": 14,
    "authors": [
      "Zhi Chen",
      "Fan Yang",
      "Wenbing Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19918": {
    "title": "LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "Weakly supervised object localization (WSOL) aims to learn object localizer solely by using image-level labels. The convolution neural network (CNN) based techniques often result in highlighting the most discriminative part of objects while ignoring the entire object extent. Recently, the transformer architecture has been deployed to WSOL to capture the long-range feature dependencies with self-attention mechanism and multilayer perceptron structure. Nevertheless, transformers lack the locality inductive bias inherent to CNNs and therefore may deteriorate local feature details in WSOL. In this paper, we propose a novel framework built upon the transformer, termed LCTR (Local Continuity TRansformer), which targets at enhancing the local perception capability of global features among long-range feature dependencies. To this end, we propose a relational patch-attention module (RPAM), which considers cross-patch information on a global basis. We further design a cue digging module (CDM), which utilizes local features to guide the learning trend of the model for highlighting the weak local responses. Finally, comprehensive experiments are carried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify the effectiveness of our method",
    "checked": true,
    "id": "a522ddc865ea6ba15923c3ad84882c1a3bf92f8f",
    "semantic_title": "lctr: on awakening the local continuity of transformer for weakly supervised object localization",
    "citation_count": 22,
    "authors": [
      "Zhiwei Chen",
      "Changan Wang",
      "Yabiao Wang",
      "Guannan Jiang",
      "Yunhang Shen",
      "Ying Tai",
      "Chengjie Wang",
      "Wei Zhang",
      "Liujuan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19919": {
    "title": "Efficient Virtual View Selection for 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "3D hand pose estimation from single depth is a fundamental problem in computer vision, and has wide applications. However, the existing methods still can not achieve satisfactory hand pose estimation results due to view variation and occlusion of human hand. In this paper, we propose a new virtual view selection and fusion module for 3D hand pose estimation from single depth. We propose to automatically select multiple virtual viewpoints for pose estimation and fuse the results of all and find this empirically delivers accurate and robust pose estimation. In order to select most effective virtual views for pose fusion, we evaluate the virtual views based on the confidence of virtual views using a light-weight network via network distillation. Experiments on three main benchmark datasets including NYU, ICVL and Hands2019 demonstrate that our method outperforms the state-of-the-arts on NYU and ICVL, and achieves very competitive performance on Hands2019-Task1, and our proposed virtual view selection and fusion module is both effective for 3D hand pose estimation",
    "checked": true,
    "id": "85d5d0d7a48092243d383b7af7aa12d8c9357e34",
    "semantic_title": "efficient virtual view selection for 3d hand pose estimation",
    "citation_count": 11,
    "authors": [
      "Jian Cheng",
      "Yanguang Wan",
      "Dexin Zuo",
      "Cuixia Ma",
      "Jian Gu",
      "Ping Tan",
      "Hongan Wang",
      "Xiaoming Deng",
      "Yinda Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19920": {
    "title": "Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction",
    "volume": "main",
    "abstract": "We present a pose adaptive few-shot learning procedure and a two-stage data interpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for single-image 3D reconstruction. While augmentations via interpolating feature-label pairs are effective in classification tasks, they fall short in shape predictions potentially due to inconsistencies between interpolated products of two images and volumes when rendering viewpoints are unknown. PADMix targets this issue with two sets of mixup procedures performed sequentially. We first perform an input mixup which, combined with a pose adaptive learning procedure, is helpful in learning 2D feature extraction and pose adaptive latent encoding. The stagewise training allows us to build upon the pose invariant representations to perform a follow-up latent mixup under one-to-one correspondences between features and ground-truth volumes. PADMix significantly outperforms previous literature on few-shot settings over the ShapeNet dataset and sets new benchmarks on the more challenging real-world Pix3D dataset",
    "checked": true,
    "id": "ff689f3d74f843a118cab38deac2c670e7f9d06f",
    "semantic_title": "pose adaptive dual mixup for few-shot single-view 3d reconstruction",
    "citation_count": 6,
    "authors": [
      "Ta-Ying Cheng",
      "Hsuan-Ru Yang",
      "Niki Trigoni",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19921": {
    "title": "PureGaze: Purifying Gaze Feature for Generalizable Gaze Estimation",
    "volume": "main",
    "abstract": "Gaze estimation methods learn eye gaze from facial features. However, among rich information in the facial image, real gaze-relevant features only correspond to subtle changes in eye region, while other gaze-irrelevant features like illumination, personal appearance and even facial expression may affect the learning in an unexpected way. This is a major reason why existing methods show significant performance degradation in cross-domain/dataset evaluation. In this paper, we tackle the cross-domain problem in gaze estimation. Different from common domain adaption methods, we propose a domain generalization method to improve the cross-domain performance without touching target samples. The domain generalization is realized by gaze feature purification. We eliminate gaze-irrelevant factors such as illumination and identity to improve the cross-domain performance. We design a plug-and-play self-adversarial framework for the gaze feature purification. The framework enhances not only our baseline but also existing gaze estimation methods directly and significantly. To the best of our knowledge, we are the first to propose domain generalization methods in gaze estimation. Our method achieves not only state-of-the-art performance among typical gaze estimation methods but also competitive results among domain adaption methods. The code is released in https://github.com/yihuacheng/PureGaze",
    "checked": true,
    "id": "9beb13adf7c97dea56b37dead91c0665cc5c40bd",
    "semantic_title": "puregaze: purifying gaze feature for generalizable gaze estimation",
    "citation_count": 31,
    "authors": [
      "Yihua Cheng",
      "Yiwei Bao",
      "Feng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19922": {
    "title": "(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering",
    "volume": "main",
    "abstract": "Spatio-temporal scene-graph approaches to video-based reasoning tasks, such as video question-answering (QA), typically construct such graphs for every video frame. These approaches often ignore the fact that videos are essentially sequences of 2D ``views'' of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame. Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos. Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it. Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world. The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes. Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity. To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art",
    "checked": true,
    "id": "f959acbc5b554801800a50c9a6eacdf896119920",
    "semantic_title": "(2.5+1)d spatio-temporal scene graphs for video question answering",
    "citation_count": 18,
    "authors": [
      "Anoop Cherian",
      "Chiori Hori",
      "Tim K. Marks",
      "Jonathan Le Roux"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19923": {
    "title": "Event-Image Fusion Stereo Using Cross-Modality Feature Propagation",
    "volume": "main",
    "abstract": "Event cameras asynchronously output the polarity values of pixel-level log intensity alterations. They are robust against motion blur and can be adopted in challenging light conditions. Owing to these advantages, event cameras have been employed in various vision tasks such as depth estimation, visual odometry, and object detection. In particular, event cameras are effective in stereo depth estimation to ﬁnd correspondence points between two cameras under challenging illumination conditions and/or fast motion. However, because event cameras provide spatially sparse event stream data, it is difﬁcult to obtain a dense disparity map. Although it is possible to estimate disparity from event data at the edge of a structure where intensity changes are likely to occur, estimating the disparity in a region where event occurs rarely is challenging. In this study, we propose a deep network that combines the features of an image with the features of an event to generate a dense disparity map. The proposed network uses images to obtain spatially dense features that are lacking in events. In addition, we propose a spatial multi-scale correlation between two fused feature maps for an accurate disparity map. To validate our method, we conducted experiments using synthetic and real-world datasets",
    "checked": true,
    "id": "405771deaaef87ba5b5ea9c3be0667f014abdf05",
    "semantic_title": "event-image fusion stereo using cross-modality feature propagation",
    "citation_count": 4,
    "authors": [
      "Hoonhee Cho",
      "Kuk-Jin Yoon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19924": {
    "title": "Style-Guided and Disentangled Representation for Robust Image-to-Image Translation",
    "volume": "main",
    "abstract": "Recently, various image-to-image translation (I2I) methods have improved mode diversity and visual quality in terms of neural networks or regularization terms. However, conventional I2I methods relies on a static decision boundary and the encoded representations in those methods are entangled with each other, so they often face with ‘mode collapse' phenomenon. To mitigate mode collapse, 1) we design a so-called style-guided discriminator that guides an input image to the target image style based on the strategy of flexible decision boundary. 2) Also, we make the encoded representations include independent domain attributes. Based on two ideas, this paper proposes Style-Guided and Disentangled Representation for Robust Image-to-Image Translation (SRIT). SRIT showed outstanding FID by 8%, 22.8%, and 10.1% for CelebA-HQ, AFHQ, and Yosemite datasets, respectively. The translated images of SRIT reflect the styles of target domain successfully. This indicates that SRIT shows better mode diversity than previous works",
    "checked": true,
    "id": "e13310fcdd5ee12342a48ed880e7bf8c5abda3bc",
    "semantic_title": "style-guided and disentangled representation for robust image-to-image translation",
    "citation_count": 3,
    "authors": [
      "Jaewoong Choi",
      "Daeha Kim",
      "Byung Cheol Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19925": {
    "title": "Denoised Maximum Classiﬁer Discrepancy for Source-Free Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Source-Free Unsupervised Domain Adaptation(SFUDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to the original labeled source domain samples. Many existing SFUDA approaches apply the self-training strategy, which involves iteratively selecting confidently predicted target samples as pseudo-labeled samples used to train the model to fit the target domain. However, the self-training strategy may also suffer from sample selection bias and be impacted by the label noise of the pseudo-labeled samples. In this work, we provide a rigorous theoretical analysis on how these two issues affect the model generalization ability when applying the self-training strategy for the SFUDA problem. Based on this theoretical analysis, we then propose a new Denoised Maximum Classifier Discrepancy (D-MCD) method for SFUDA to effectively address these two issues. In particular, we first minimize the distribution mismatch between the selected pseudo-labeled samples and the remaining target domain samples to alleviate the sample selection bias. Moreover, we design a strong-weak self-training paradigm to denoise the selected pseudo-labeled samples, where the strong network is used to select pseudo-labeled samples while the weak network helps the strong network to filter out hard samples to avoid incorrect labels. In this way, we are able to ensure both the quality of the pseudo-labels and the generalization ability of the trained model on the target domain. We achieve state-of-the-art results on three domain adaptation benchmark datasets, which clearly validates the effectiveness of our proposed approach. Full code is available at https://github.com/kkkkkkon/D-MCD",
    "checked": false,
    "id": "c9ad8f3c495c952a5f68ef78d0becda092e4fda3",
    "semantic_title": "denoised maximum classifier discrepancy for source-free unsupervised domain adaptation",
    "citation_count": 20,
    "authors": [
      "Tong Chu",
      "Yahao Liu",
      "Jinhong Deng",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19926": {
    "title": "Model-Based Image Signal Processors via Learnable Dictionaries",
    "volume": "main",
    "abstract": "Digital cameras transform sensor RAW readings into RGB images by means of their Image Signal Processor (ISP). Computational photography tasks such as image denoising and colour constancy are commonly performed in the RAW domain, in part due to the inherent hardware design, but also due to the appealing simplicity of noise statistics that result from the direct sensor readings. Despite this, the availability of RAW images is limited in comparison with the abundance and diversity of available RGB data. Recent approaches have attempted to bridge this gap by estimating the RGB to RAW mapping: handcrafted model-based methods that are interpretable and controllable usually require manual parameter fine-tuning, while end-to-end learnable neural networks require large amounts of training data, at times with complex training procedures, and generally lack interpretability and parametric control. Towards addressing these existing limitations, we present a novel hybrid model-based and data-driven ISP that builds on canonical ISP operations and is both learnable and interpretable. Our proposed invertible model, capable of bidirectional mapping between RAW and RGB domains, employs end-to-end learning of rich parameter representations, i.e. dictionaries, that are free from direct parametric supervision and additionally enable simple and plausible data augmentation. We evidence the value of our data generation process by extensive experiments under both RAW image reconstruction and RAW image denoising tasks, obtaining state-of-the-art performance in both. Additionally, we show that our ISP can learn meaningful mappings from few data samples, and that denoising models trained with our dictionary-based data augmentation are competitive despite having only few or zero ground-truth labels",
    "checked": true,
    "id": "bf7b34376518140379c84cd0dd8e71a4b8d888a6",
    "semantic_title": "model-based image signal processors via learnable dictionaries",
    "citation_count": 19,
    "authors": [
      "Marcos V. Conde",
      "Steven McDonagh",
      "Matteo Maggioni",
      "Ales Leonardis",
      "Eduardo Pérez-Pellitero"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19927": {
    "title": "MMA: Multi-Camera Based Global Motion Averaging",
    "volume": "main",
    "abstract": "In order to fully perceive the surrounding environment, many intelligent robots and self-driving cars are equipped with a multi-camera system. Based on this system, the structure-from-motion (SfM) technology is used to realize scene reconstruction, but the fixed relative poses between cameras in the multi-camera system are usually not considered. This paper presents a tailor-made multi-camera based motion averaging system, where the fixed relative poses are utilized to improve the accuracy and robustness of SfM. Our approach starts by dividing the images into reference images and non-reference images, and edges in view-graph are divided into four categories accordingly. Then, a multi-camera based rotating averaging problem is formulated and solved in two stages, where an iterative re-weighted least squares scheme is used to deal with outliers. Finally, a multi-camera based translation averaging problem is formulated and a l1-norm based optimization scheme is proposed to compute the relative translations of multi-camera system and reference camera positions simultaneously. Experiments demonstrate that our algorithm achieves superior accuracy and robustness on various data sets compared to the state-of-the-art methods",
    "checked": true,
    "id": "479b97b03381ebf827a644746879797e33d7889c",
    "semantic_title": "mma: multi-camera based global motion averaging",
    "citation_count": 2,
    "authors": [
      "Hainan Cui",
      "Shuhan Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19928": {
    "title": "GenCo: Generative Co-training for Generative Adversarial Networks with Limited Data",
    "volume": "main",
    "abstract": "Training effective Generative Adversarial Networks (GANs) requires large amounts of training data, without which the trained models are usually sub-optimal with discriminator over-fitting. Several prior studies address this issue by expanding the distribution of the limited training data via massive and hand-crafted data augmentation. We handle data-limited image generation from a very different perspective. Specifically, we design GenCo, a Generative Co-training network that mitigates the discriminator over-fitting issue by introducing multiple complementary discriminators that provide diverse supervision from multiple distinctive views in training. We instantiate the idea of GenCo in two ways. The first way is Weight-Discrepancy Co-training (WeCo) which co-trains multiple distinctive discriminators by diversifying their parameters. The second way is Data-Discrepancy Co-training (DaCo) which achieves co-training by feeding discriminators with different views of the input images. Extensive experiments over multiple benchmarks show that GenCo achieves superior generation with limited training data. In addition, GenCo also complements the augmentation approach with consistent and clear performance gains when combined",
    "checked": true,
    "id": "3fe1eb2c788a6fbc2642f6b43e7915fe93fb318b",
    "semantic_title": "genco: generative co-training for generative adversarial networks with limited data",
    "citation_count": 11,
    "authors": [
      "Kaiwen Cui",
      "Jiaxing Huang",
      "Zhipeng Luo",
      "Gongjie Zhang",
      "Fangneng Zhan",
      "Shijian Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19929": {
    "title": "Unbiased IoU for Spherical Image Object Detection",
    "volume": "main",
    "abstract": "As one of the fundamental components of object detection, intersection-over-union (IoU) calculations between two bounding boxes play an important role in samples selection, NMS operation and evaluation of object detection algorithms. This procedure is well-defined and solved for planar images, while it is challenging for spherical ones. Some existing methods utilize planar bounding boxes to represent spherical objects. However, they are biased due to the distortions of spherical objects. Others use spherical rectangles as unbiased representations, but they adopt excessive approximate algorithms when computing the IoU. In this paper, we propose an unbiased IoU as a novel evaluation criterion for spherical image object detection, which is based on the unbiased representations and utilize unbiased analytical method for IoU calculation. This is the first time that the absolutely accurate IoU calculation is applied to the evaluation criterion, thus object detection algorithms can be correctly evaluated for spherical images. With the unbiased representation and calculation, we also present Spherical CenterNet, an anchor free object detection algorithm for spherical images. The experiments show that our unbiased IoU gives accurate results and the proposed Spherical CenterNet achieves better performance on one real-world and two synthetic spherical object detection datasets than existing methods",
    "checked": true,
    "id": "612b01f46729c3449add53b4c8ae2bdbbdd56510",
    "semantic_title": "unbiased iou for spherical image object detection",
    "citation_count": 7,
    "authors": [
      "Feng Dai",
      "Bin Chen",
      "Hang Xu",
      "Yike Ma",
      "Xiaodong Li",
      "Bailan Feng",
      "Peng Yuan",
      "Chenggang Yan",
      "Qiang Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19930": {
    "title": "InsCLR: Improving Instance Retrieval with Self-Supervision",
    "volume": "main",
    "abstract": "This work aims at improving instance retrieval with self-supervision. We find that fine-tuning using the recently developed self-supervised learning (SSL) methods, such as SimCLR and MoCo, fails to improve the performance of instance retrieval. In this work, we identify that the learnt representations for instance retrieval should be invariant to large variations in viewpoint and background etc., whereas self-augmented positives applied by the current SSL methods can not provide strong enough signals for learning robust instance-level representations. To overcome this problem, we propose InsCLR, a new SSL method that builds on the instance-level contrast, to learn the intra-class invariance by dynamically mining meaningful pseudo positive samples from both mini-batches and a memory bank during training. Extensive experiments demonstrate that InsCLR achieves similar or even better performance than the state-of-the-art SSL methods on instance retrieval. Code is available at https://github.com/zeludeng/insclr",
    "checked": true,
    "id": "8aa0161174f5a6c9b4d6eb019bf306a4cceb7f62",
    "semantic_title": "insclr: improving instance retrieval with self-supervision",
    "citation_count": 7,
    "authors": [
      "Zelu Deng",
      "Yujie Zhong",
      "Sheng Guo",
      "Weilin Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19931": {
    "title": "Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation",
    "volume": "main",
    "abstract": "Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin",
    "checked": true,
    "id": "37486b4948d9b31632c2ca030ca6d9d268af4423",
    "semantic_title": "spatio-temporal recurrent networks for event-based optical flow estimation",
    "citation_count": 27,
    "authors": [
      "Ziluo Ding",
      "Rui Zhao",
      "Jiyuan Zhang",
      "Tianxiao Gao",
      "Ruiqin Xiong",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19932": {
    "title": "Construct Effective Geometry Aware Feature Pyramid Network for Multi-Scale Object Detection",
    "volume": "main",
    "abstract": "Feature Pyramid Network (FPN) has been widely adopted to exploit multi-scale features for scale variation in object detection. However, intrinsic defects in most of the current methods with FPN make it difficult to adapt to the feature of different geometric objects. To address this issue, we introduce geometric prior into FPN to obtain more discriminative features. In this paper, we propose Geometry-aware Feature Pyramid Network (GaFPN), which mainly consists of the novel Geometry-aware Mapping Module and Geometry-aware Predictor Head.The Geometry-aware Mapping Module is proposed to make full use of all pyramid features to obtain better proposal features by the weight-generation subnetwork. The weights generation subnetwork generates fusion weight for each layer proposal features by using the geometric information of the proposal. The Geometry-aware Predictor Head introduces geometric prior into predictor head by the embedding generation network to strengthen feature representation for classification and regression. Our GaFPN can be easily extended to other two-stage object detectors with feature pyramid and applied to instance segmentation task. The proposed GaFPN significantly improves detection performance compared to baseline detectors with ResNet-50-FPN: +1.9, +2.0, +1.7, +1.3, +0.8 points Average Precision (AP) on Faster-RCNN, Cascade R-CNN, Dynamic R-CNN, SABL, and AugFPN respectively on MS COCO dataset",
    "checked": true,
    "id": "032e0be06d5c52ac228fa0cb8aa1764c447f2d9e",
    "semantic_title": "construct effective geometry aware feature pyramid network for multi-scale object detection",
    "citation_count": 2,
    "authors": [
      "Jinpeng Dong",
      "Yuhao Huang",
      "Songyi Zhang",
      "Shitao Chen",
      "Nanning Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19933": {
    "title": "Complementary Attention Gated Network for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Pedestrian trajectory prediction is crucial in many practical applications due to the diversity of pedestrian movements, such as social interactions and individual motion behaviors. With similar observable trajectories and social environments, different pedestrians may make completely different future decisions. However, most existing methods only focus on the frequent modal of the trajectory and thus are difficult to generalize to the peculiar scenario, which leads to the decline of the multimodal fitting ability when facing similar scenarios. In this paper, we propose a complementary attention gated network (CAGN) for pedestrian trajectory prediction, in which a dual-path architecture including normal and inverse attention is proposed to capture both frequent and peculiar modals in spatial and temporal patterns, respectively. Specifically, a complementary block is proposed to guide normal and inverse attention, which are then be summed with learnable weights to get attention features by a gated network. Finally, multiple trajectory distributions are estimated based on the fused spatio-temporal attention features due to the multimodality of future trajectory. Experimental results on benchmark datasets, i.e., the ETH, and the UCY, demonstrate that our method outperforms state-of-the-art methods by 13.8% in Average Displacement Error (ADE) and 10.4% in Final Displacement Error (FDE). Code will be available at https://github.com/jinghaiD/CAGN",
    "checked": true,
    "id": "d269ee705c57c8576cb5c29858087764287f8603",
    "semantic_title": "complementary attention gated network for pedestrian trajectory prediction",
    "citation_count": 11,
    "authors": [
      "Jinghai Duan",
      "Le Wang",
      "Chengjiang Long",
      "Sanping Zhou",
      "Fang Zheng",
      "Liushuai Shi",
      "Gang Hua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19934": {
    "title": "SVT-Net: Super Light-Weight Sparse Voxel Transformer for Large Scale Place Recognition",
    "volume": "main",
    "abstract": "Simultaneous Localization and Mapping (SLAM) and Autonomous Driving are becoming increasingly more important in recent years. Point cloud-based large scale place recognition is the spine of them. While many models have been proposed and have achieved acceptable performance by learning short-range local features, they always skip long-range contextual properties. Moreover, the model size also becomes a serious shackle for their wide applications. To overcome these challenges, we propose a super light-weight network model termed SVT-Net. On top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based Sparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer (CSVT) are proposed respectively to learn both short-range local features and long-range contextual features. Consisting of ASVT and CSVT, SVT-Net can achieve state-of-the-art performance in terms of both recognition accuracy and running speed with a super-light model size (0.9M parameters). Meanwhile, for the purpose of further boosting efficiency, we introduce two simplified versions, which also achieve state-of-the-art performance and further reduce the model size to 0.8M and 0.4M respectively",
    "checked": true,
    "id": "684088badaa01b7de3afc214ca2839a4bc212f52",
    "semantic_title": "svt-net: super light-weight sparse voxel transformer for large scale place recognition",
    "citation_count": 30,
    "authors": [
      "Zhaoxin Fan",
      "Zhenbo Song",
      "Hongyan Liu",
      "Zhiwu Lu",
      "Jun He",
      "Xiaoyong Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19935": {
    "title": "Backdoor Attacks on the DNN Interpretation System",
    "volume": "main",
    "abstract": "Interpretability is crucial to understand the inner workings of deep neural networks (DNNs). Many interpretation methods help to understand the decision-making of DNNs by generating saliency maps that highlight parts of the input image that contribute the most to the prediction made by the DNN. In this paper we design a backdoor attack that alters the saliency map produced by the network for an input image with a specific trigger pattern while not losing the prediction performance significantly. The saliency maps are incorporated in the penalty term of the objective function that is used to train a deep model and its influence on model training is conditioned upon the presence of a trigger. We design two types of attacks: a targeted attack that enforces a specific modification of the saliency map and a non-targeted attack when the importance scores of the top pixels from the original saliency map are significantly reduced. We perform empirical evaluations of the proposed backdoor attacks on gradient-based interpretation methods, Grad-CAM and SimpleGrad, and a gradient-free scheme, VisualBackProp, for a variety of deep learning architectures. We show that our attacks constitute a serious security threat to the reliability of the interpretation methods when deploying models developed by untrusted sources. We furthermore show that existing backdoor defense mechanisms are ineffective in detecting our attacks. Finally, we demonstrate that the proposed methodology can be used in an inverted setting, where the correct saliency map can be obtained only in the presence of a trigger (key), effectively making the interpretation system available only to selected users",
    "checked": true,
    "id": "51ebe054d93bcde2de3f53c913216d891917b846",
    "semantic_title": "backdoor attacks on the dnn interpretation system",
    "citation_count": 9,
    "authors": [
      "Shihong Fang",
      "Anna Choromanska"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19936": {
    "title": "Learning to Learn Transferable Attack",
    "volume": "main",
    "abstract": "Transfer adversarial attack is a non-trivial black-box adversarial attack that aims to craft adversarial perturbations on the surrogate model and then apply such perturbations to the victim model. However, the transferability of perturbations from existing methods is still limited, since the adversarial perturbations are easily overﬁtting with a single surrogate model and speciﬁc data pattern. In this paper, we propose a Learning to Learn Transferable Attack (LLTA) method, which makes the adversarial perturbations more generalized via learning from both data and model augmentation. For data augmentation, we adopt simple random resizing and padding. For model augmentation, we randomly alter the back propagation instead of the forward propagation to eliminate the effect on the model prediction. By treating the attack of both speciﬁc data and a modiﬁed model as a task, we expect the adversarial perturbations to adopt enough tasks for generalization. To this end, the meta-learning algorithm is further introduced during the iteration of perturbation generation. Empirical results on the widely-used dataset demonstrate the effectiveness of our attack method with a 12.85% higher success rate of transfer attack compared with the state-of-the-art methods. We also evaluate our method on the real-world online system, i.e., Google Cloud Vision API, to further show the practical potentials of our method",
    "checked": true,
    "id": "11b981f8c34c848ebf67ccdfcebbd87e45ee2d8e",
    "semantic_title": "learning to learn transferable attack",
    "citation_count": 4,
    "authors": [
      "Shuman Fang",
      "Jie Li",
      "Xianming Lin",
      "Rongrong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19937": {
    "title": "Perceptual Quality Assessment of Omnidirectional Images",
    "volume": "main",
    "abstract": "Omnidirectional images, also called 360◦images, have attracted extensive attention in recent years, due to the rapid development of virtual reality (VR) technologies. During omnidirectional image processing including capture, transmission, consumption, and so on, measuring the perceptual quality of omnidirectional images is highly desired, since it plays a great role in guaranteeing the immersive quality of experience (IQoE). In this paper, we conduct a comprehensive study on the perceptual quality of omnidirectional images from both subjective and objective perspectives. Specifically, we construct the largest so far subjective omnidirectional image quality database, where we consider several key influential elements, i.e., realistic non-uniform distortion, viewing condition, and viewing behavior, from the user view. In addition to subjective quality scores, we also record head and eye movement data. Besides, we make the first attempt by using the proposed database to train a convolutional neural network (CNN) for blind omnidirectional image quality assessment. To be consistent with the human viewing behavior in the VR device, we extract viewports from each omnidirectional image and incorporate the user viewing conditions naturally in the proposed model. The proposed model is composed of two parts, including a multi-scale CNN-based feature extraction module and a perceptual quality prediction module. The feature extraction module is used to incorporate the multi-scale features, and the perceptual quality prediction module is designed to regress them to perceived quality scores. The experimental results on our database verify that the proposed model achieves the competing performance compared with the state-of-the-art methods",
    "checked": true,
    "id": "cb5bcf48443c4809c578c5e3d489f61eb16f1d6b",
    "semantic_title": "perceptual quality assessment of omnidirectional images",
    "citation_count": 89,
    "authors": [
      "Yuming Fang",
      "Liping Huang",
      "Jiebin Yan",
      "Xuelin Liu",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19938": {
    "title": "PatchUp: A Feature-Space Block-Level Regularization Technique for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Large capacity deep learning models are often prone to a high generalization gap when trained with a limited amount of labeled training data. A recent class of methods to address this problem uses various ways to construct a new training sample by mixing a pair (or more) of training samples. We propose PatchUp, a hidden state block-level regularization technique for Convolutional Neural Networks (CNNs), that is applied on selected contiguous blocks of feature maps from a random pair of samples. Our approach improves the robustness of CNN models against the manifold intrusion problem that may occur in other state-of-the-art mixing approaches. Moreover, since we are mixing the contiguous block of features in the hidden space, which has more dimensions than the input space, we obtain more diverse samples for training towards different dimensions. Our experiments on CIFAR10/100, SVHN, Tiny-ImageNet, and ImageNet using ResNet architectures including PreActResnet18/34, WRN-28-10, ResNet101/152 models show that PatchUp improves upon, or equals, the performance of current state-of-the-art regularizers for CNNs. We also show that PatchUp can provide a better generalization to deformed samples and is more robust against adversarial attacks",
    "checked": true,
    "id": "8244fc710bce3c1fd023fb74a01949d71ceb5405",
    "semantic_title": "patchup: a feature-space block-level regularization technique for convolutional neural networks",
    "citation_count": 8,
    "authors": [
      "Mojtaba Faramarzi",
      "Mohammad Amini",
      "Akilesh Badrinaaraayanan",
      "Vikas Verma",
      "Sarath Chandar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19939": {
    "title": "DuMLP-Pin: A Dual-MLP-Dot-Product Permutation-Invariant Network for Set Feature Extraction",
    "volume": "main",
    "abstract": "Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one. Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer. It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters. In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets. We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold. We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions. The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets. The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets. On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively. The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin",
    "checked": true,
    "id": "12066543704c24062d056236c5b8e3ca5f70148b",
    "semantic_title": "dumlp-pin: a dual-mlp-dot-product permutation-invariant network for set feature extraction",
    "citation_count": 2,
    "authors": [
      "Jiajun Fei",
      "Ziyu Zhu",
      "Wenlei Liu",
      "Zhidong Deng",
      "Mingyang Li",
      "Huanjun Deng",
      "Shuo Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19940": {
    "title": "Attention-Aligned Transformer for Image Captioning",
    "volume": "main",
    "abstract": "Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance. However, some researchers have argued \"deviated focus\" problem of existing attention mechanisms in determining the effective and influential image features. In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead. Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation. We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight. Then, we propose four aligned strategies to use this information to refine attention weight distribution. Under such a pattern, image regions are attended correctly with the output words. Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation. Trained models and code for reproducing the experiments are publicly available",
    "checked": true,
    "id": "977f49f96ec7b532c29576e468adbd140c502810",
    "semantic_title": "attention-aligned transformer for image captioning",
    "citation_count": 13,
    "authors": [
      "Zhengcong Fei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19941": {
    "title": "Model Doctor: A Simple Gradient Aggregation Strategy for Diagnosing and Treating CNN Classifiers",
    "volume": "main",
    "abstract": "Recently, Convolutional Neural Network (CNN) has achieved excellent performance in the classification task. It is widely known that CNN is deemed as a 'blackbox', which is hard for understanding the prediction mechanism and debugging the wrong prediction. Some model debugging and explanation works are developed for solving the above drawbacks. However, those methods focus on explanation and diagnosing possible causes for model prediction, based on which the researchers handle the following optimization of models manually. In this paper, we propose the first completely automatic model diagnosing and treating tool, termed as Model Doctor. Based on two discoveries that 1) each category is only correlated with sparse and specific convolution kernels, and 2) adversarial samples are isolated while normal samples are successive in the feature space, a simple aggregate gradient constraint is devised for effectively diagnosing and optimizing CNN classifiers. The aggregate gradient strategy is a versatile module for mainstream CNN classifiers. Extensive experiments demonstrate that the proposed Model Doctor applies to all existing CNN classifiers, and improves the accuracy of 16 mainstream CNN classifiers by 1%~5%",
    "checked": true,
    "id": "caf543516034e4daf875efd37152145972dbb5b6",
    "semantic_title": "model doctor: a simple gradient aggregation strategy for diagnosing and treating cnn classifiers",
    "citation_count": 10,
    "authors": [
      "Zunlei Feng",
      "Jiacong Hu",
      "Sai Wu",
      "XiaoTian Yu",
      "Jie Song",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19942": {
    "title": "OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression",
    "volume": "main",
    "abstract": "In point cloud compression, sufficient contexts are significant for modeling the point cloud distribution. However, the contexts gathered by the previous voxel-based methods decrease when handling sparse point clouds. To address this problem, we propose a multiple-contexts deep learning framework called OctAttention employing the octree structure, a memory-efficient representation for point clouds. Our approach encodes octree symbol sequences in a lossless way by gathering the information of sibling and ancestor nodes. Expressly, we first represent point clouds with octree to reduce spatial redundancy, which is robust for point clouds with different resolutions. We then design a conditional entropy model with a large receptive field that models the sibling and ancestor contexts to exploit the strong dependency among the neighboring nodes and employ an attention mechanism to emphasize the correlated nodes in the context. Furthermore, we introduce a mask operation during training and testing to make a trade-off between encoding time and performance. Compared to the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset (e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based baseline. The code is available at https://github.com/zb12138/OctAttention",
    "checked": true,
    "id": "7eebdc034ef539c55257e09c65f6cb8b94c07a99",
    "semantic_title": "octattention: octree-based large-scale contexts model for point cloud compression",
    "citation_count": 32,
    "authors": [
      "Chunyang Fu",
      "Ge Li",
      "Rui Song",
      "Wei Gao",
      "Shan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19943": {
    "title": "DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents",
    "volume": "main",
    "abstract": "Creating presentation materials requires complex multimodal reasoning skills to summarize key concepts and arrange them in a logical and visually pleasing manner. Can machines learn to emulate this laborious process? We present a novel task and approach for document-to-slide generation. Solving this involves document summarization, image and text retrieval, slide structure and layout prediction to arrange key elements in a form suitable for presentation. We propose a hierarchical sequence-to-sequence approach to tackle our task in an end-to-end manner. Our approach exploits the inherent structures within documents and slides and incorporates paraphrasing and layout prediction modules to generate slides. To help accelerate research in this domain, we release a dataset about 6K paired documents and slide decks used in our experiments. We show that our approach outperforms strong baselines and produces slides with rich content and aligned imagery",
    "checked": true,
    "id": "d503245e82984c17e36801259b9cee485c258437",
    "semantic_title": "doc2ppt: automatic presentation slides generation from scientific documents",
    "citation_count": 18,
    "authors": [
      "Tsu-Jui Fu",
      "William Yang Wang",
      "Daniel McDuff",
      "Yale Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19944": {
    "title": "Unsupervised Underwater Image Restoration: From a Homology Perspective",
    "volume": "main",
    "abstract": "Underwater images suffer from degradation due to light scattering and absorption. It remains challenging to restore such degraded images using deep neural networks since real-world paired data is scarcely available while synthetic paired data cannot approximate real-world data perfectly. In this paper, we propose an UnSupervised Underwater Image Restoration method (USUIR) by leveraging the homology property between a raw underwater image and a re-degraded image. Specifically, USUIR first estimates three latent components of the raw underwater image, i.e., the global background light, the transmission map, and the scene radiance (the clean image). Then, a re-degraded image is generated by randomly mixing up the estimated scene radiance and the raw underwater image. We demonstrate that imposing a homology constraint between the raw underwater image and the re-degraded image is equivalent to minimizing the restoration error and hence can be used for the unsupervised restoration. Extensive experiments show that USUIR achieves promising performance in both inference time and restoration quality",
    "checked": true,
    "id": "b70c70e99449b199575917e8940f6b4abb65ed82",
    "semantic_title": "unsupervised underwater image restoration: from a homology perspective",
    "citation_count": 11,
    "authors": [
      "Zhenqi Fu",
      "Huangxing Lin",
      "Yan Yang",
      "Shu Chai",
      "Liyan Sun",
      "Yue Huang",
      "Xinghao Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19945": {
    "title": "Playing Lottery Tickets with Vision and Language",
    "volume": "main",
    "abstract": "Large-scale pre-training has recently revolutionized vision-and-language (VL) research. Models such as LXMERT and UNITER have significantly lifted the state of the art over a wide range of VL tasks. However, the large number of parameters in such models hinders their application in practice. In parallel, work on the lottery ticket hypothesis (LTH) has shown that deep neural networks contain small matching subnetworks that can achieve on par or even better performance than the dense networks when trained in isolation. In this work, we perform the first empirical study to assess whether such trainable subnetworks also exist in pre-trained VL models. We use UNITER as the main testbed (also test on LXMERT and ViLT), and consolidate 7 representative VL tasks for experiments, including visual question answering, visual commonsense reasoning, visual entailment, referring expression comprehension, image-text retrieval, GQA, and NLVR2. Through comprehensive analysis, we summarize our main findings as follows. (i) It is difficult to find subnetworks that strictly match the performance of the full model. However, we can find relaxed winning tickets at 50%-70% sparsity that maintain 99% of the full accuracy. (ii) Subnetworks found by task-specific pruning transfer reasonably well to the other tasks, while those found on the pre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96% of the full accuracy on average over all the tasks. (iii) Besides UNITER, other models such as LXMERT and ViLT can also play lottery tickets. However, the highest sparsity we can achieve for ViLT is far lower than LXMERT and UNITER (30% vs. 70%). (iv) LTH also remains relevant when using other training methods (e.g., adversarial training)",
    "checked": true,
    "id": "22299b440277b4bc887168a669408d5547c1461a",
    "semantic_title": "playing lottery tickets with vision and language",
    "citation_count": 36,
    "authors": [
      "Zhe Gan",
      "Yen-Chun Chen",
      "Linjie Li",
      "Tianlong Chen",
      "Yu Cheng",
      "Shuohang Wang",
      "Jingjing Liu",
      "Lijuan Wang",
      "Zicheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19946": {
    "title": "Feature Distillation Interaction Weighting Network for Lightweight Image Super-resolution",
    "volume": "main",
    "abstract": "Convolutional neural networks based single-image superresolution (SISR) has made great progress in recent years. However, it is difficult to apply these methods to real-world scenarios due to the computational and memory cost. Meanwhile, how to take full advantage of the intermediate features under the constraints of limited parameters and calculations is also a huge challenge. To alleviate these issues, we propose a lightweight yet efficient Feature Distillation Interaction Weighted Network (FDIWN). Specifically, FDIWN utilizes a series of specially designed Feature Shuffle Weighted Groups (FSWG) as the backbone, and several novel mutual Wide-residual Distillation Interaction Blocks (WDIB) form an FSWG. In addition, Wide Identical Residual Weighting (WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are introduced into WDIB for better feature distillation. Moreover, a Wide-Residual Distillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF) unit are proposed to interact features with different scales more flexibly and efficiently. Extensive experiments show that our FDIWN is superior to other models to strike a good balance between model performance and efficiency. The code is available at https://github.com/IVIPLab/FDIWN",
    "checked": true,
    "id": "3c0d3173b7b74913f6b1895788501f51096aa183",
    "semantic_title": "feature distillation interaction weighting network for lightweight image super-resolution",
    "citation_count": 34,
    "authors": [
      "Guangwei Gao",
      "Wenjie Li",
      "Juncheng Li",
      "Fei Wu",
      "Huimin Lu",
      "Yi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19947": {
    "title": "Weakly-Supervised Salient Object Detection Using Point Supervision",
    "volume": "main",
    "abstract": "Current state-of-the-art saliency detection models rely heavily on large datasets of accurate pixel-wise annotations, but manually labeling pixels is time-consuming and labor-intensive. There are some weakly supervised methods developed for alleviating the problem, such as image label, bounding box label, and scribble label, while point label still has not been explored in this field. In this paper, we propose a novel weakly-supervised salient object detection method using point supervision. To infer the saliency map, we first design an adaptive masked flood filling algorithm to generate pseudo labels. Then we develop a transformer-based point-supervised saliency detection model to produce the first round of saliency maps. However, due to the sparseness of the label, the weakly supervised model tends to degenerate into a general foreground detection model. To address this issue, we propose a Non-Salient Suppression (NSS) method to optimize the erroneous saliency maps generated in the first round and leverage them for the second round of training. Moreover, we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS dataset. In P-DUTS, there is only one labeled point for each salient object. Comprehensive experiments on five largest benchmark datasets demonstrate our method outperforms the previous state-of-the-art methods trained with the stronger supervision and even surpass several fully supervised state-of-the-art models. The code is available at: https://github.com/shuyonggao/PSOD",
    "checked": false,
    "id": "6df10a508e679c03f713a347190bbf5cab07f3ef",
    "semantic_title": "weakly-supervised salient object detection using point supervison",
    "citation_count": 17,
    "authors": [
      "Shuyong Gao",
      "Wei Zhang",
      "Yan Wang",
      "Qianyu Guo",
      "Chenglong Zhang",
      "Yangji He",
      "Wenqiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19948": {
    "title": "Latent Space Explanation by Intervention",
    "volume": "main",
    "abstract": "The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output. While this property serves to fit the training data well, it also obscures the mechanism that drives prediction. This study aims to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders. An explanatory model then visualizes the encoded information from any hidden layer and its corresponding intervened representation. By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class, hence providing interpretability. We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations for bias in the data and suggest different interventions to reveal and change bias",
    "checked": true,
    "id": "4799ea215e65e66a9a170303861763662d04e8b1",
    "semantic_title": "latent space explanation by intervention",
    "citation_count": 9,
    "authors": [
      "Itai Gat",
      "Guy Lorberbom",
      "Idan Schwartz",
      "Tamir Hazan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19949": {
    "title": "Lifelong Person Re-identification by Pseudo Task Knowledge Preservation",
    "volume": "main",
    "abstract": "In real world, training data for person re-identification (Re-ID) is collected discretely with spatial and temporal variations, which requires a model to incrementally learn new knowledge without forgetting old knowledge. This problem is called lifelong person re-identification (LReID). Variations of illumination and background for images of each task exhibit task-specific image style and lead to task-wise domain gap. In addition to missing data from the old tasks, task-wise domain gap is a key factor for catastrophic forgetting in LReID, which is ignored in existing approaches for LReID. The model tends to learn task-specific knowledge with task-wise domain gap, which results in stability and plasticity dilemma. To overcome this problem, we cast LReID as a domain adaptation problem and propose a pseudo task knowledge preservation framework to alleviate the domain gap. Our framework is based on a pseudo task transformation module which maps the features of the new task into the feature space of the old tasks to complement the limited saved exemplars of the old tasks. With extra transformed features in the task-specific feature space, we propose a task-specific domain consistency loss to implicitly alleviate the task-wise domain gap for learning task-shared knowledge instead of task-specific one. Furthermore, to guide knowledge preservation with the feature distributions of the old tasks, we propose to preserve knowledge on extra pseudo tasks which jointly distills knowledge and discriminates identity, in order to achieve a better trade-off between stability and plasticity for lifelong learning with task-wise domain gap. Extensive experiments demonstrate the superiority of our method as compared with the state-of-the-art lifelong learning and LReID methods",
    "checked": true,
    "id": "4f0ba24e1793859b222a77209cf1276b45df279e",
    "semantic_title": "lifelong person re-identification by pseudo task knowledge preservation",
    "citation_count": 5,
    "authors": [
      "Wenhang Ge",
      "Junlong Du",
      "Ancong Wu",
      "Yuqiao Xian",
      "Ke Yan",
      "Feiyue Huang",
      "Wei-Shi Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19950": {
    "title": "Adversarial Robustness in Multi-Task Learning: Promises and Illusions",
    "volume": "main",
    "abstract": "Vulnerability to adversarial attacks is a well-known weakness of Deep Neural networks. While most of the studies focus on single-task neural networks with computer vision datasets, very little research has considered complex multi-task models that are common in real applications. In this paper, we evaluate the design choices that impact the robustness of multi-task deep learning networks. We provide evidence that blindly adding auxiliary tasks, or weighing the tasks provides a false sense of robustness. Thereby, we tone down the claim made by previous research and study the different factors which may affect robustness. In particular, we show that the choice of the task to incorporate in the loss function are important factors that can be leveraged to yield more robust models. We provide the appendix, all our algorithms, models, and open source-code at https://github.com/yamizi/taskaugment",
    "checked": true,
    "id": "940bfecc757404e10b837867ec0c531c96c61d85",
    "semantic_title": "adversarial robustness in multi-task learning: promises and illusions",
    "citation_count": 10,
    "authors": [
      "Salah Ghamizi",
      "Maxime Cordy",
      "Mike Papadakis",
      "Yves Le Traon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19951": {
    "title": "Deep Confidence Guided Distance for 3D Partial Shape Registration",
    "volume": "main",
    "abstract": "We present a novel non-iterative learnable method for partial-to-partial 3D shape registration. The partial alignment task is extremely complex, as it jointly tries to match between points, and identify which points do not appear in the corresponding shape, causing the solution to be non-unique and ill-posed in most cases. Until now, two main methodologies have been suggested to solve this problem: sample a subset of points that are likely to have correspondences, or perform soft alignment between the point clouds and try to avoid a match to an occluded part. These heuristics work when the partiality is mild or when the transformation is small but fails for severe occlusions, or when outliers are present. We present a unique approach named Confidence Guided Distance Network (CGD-net), where we fuse learnable similarity between point embeddings and spatial distance between point clouds, inducing an optimized solution for the overlapping points while ignoring parts that only appear in one of the shapes. The point feature generation is done by a self-supervised architecture that repels far points to have different embeddings, therefore succeeds to align partial views of shapes, even with excessive internal symmetries, or acute rotations. We compare our network to recently presented learning-based and axiomatic methods and report a fundamental boost in performance",
    "checked": true,
    "id": "8ae3477ed2f33449d62397839b96a9a58ec761fb",
    "semantic_title": "deep confidence guided distance for 3d partial shape registration",
    "citation_count": 1,
    "authors": [
      "Dvir Ginzburg",
      "Dan Raviv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19952": {
    "title": "Predicting Physical World Destinations for Commands Given to Self-Driving Cars",
    "volume": "main",
    "abstract": "In recent years, we have seen significant steps taken in the development of self-driving cars. Multiple companies are starting to roll out impressive systems that work in a variety of settings. These systems can sometimes give the impression that full self-driving is just around the corner and that we would soon build cars without even a steering wheel. The increase in the level of autonomy and control given to an AI provides an opportunity for new modes of human-vehicle interaction. However, surveys have shown that giving more control to an AI in self-driving cars is accompanied by a degree of uneasiness by passengers. In an attempt to alleviate this issue, recent works have taken a natural language-oriented approach by allowing the passenger to give commands that refer to specific objects in the visual scene. Nevertheless, this is only half the task as the car should also understand the physical destination of the command, which is what we focus on in this paper. We propose an extension in which we annotate the 3D destination that the car needs to reach after executing the given command and evaluate multiple different baselines on predicting this destination location. Additionally, we introduce a model that outperforms the prior works adapted for this particular setting",
    "checked": true,
    "id": "2ad60ff300a91f0e312a77f70c52ccc09fc945e7",
    "semantic_title": "predicting physical world destinations for commands given to self-driving cars",
    "citation_count": 1,
    "authors": [
      "Dusan Grujicic",
      "Thierry Deruyttere",
      "Marie-Francine Moens",
      "Matthew B. Blaschko"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19953": {
    "title": "Towards Light-Weight and Real-Time Line Segment Detection",
    "volume": "main",
    "abstract": "Previous deep learning-based line segment detection (LSD) suffers from the immense model size and high computational cost for line prediction. This constrains them from real-time inference on computationally restricted environments. In this paper, we propose a real-time and light-weight line segment detector for resource-constrained environments named Mobile LSD (M-LSD). We design an extremely efficient LSD architecture by minimizing the backbone network and removing the typical multi-module process for line prediction found in previous methods. To maintain competitive performance with a light-weight network, we present novel training schemes: Segments of Line segment (SoL) augmentation, matching and geometric loss. SoL augmentation splits a line segment into multiple subparts, which are used to provide auxiliary line data during the training process. Moreover, the matching and geometric loss allow a model to capture additional geometric cues. Compared with TP-LSD-Lite, previously the best real-time LSD method, our model (M-LSD-tiny) achieves competitive performance with 2.5% of model size and an increase of 130.5% in inference speed on GPU. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices, respectively. To the best of our knowledge, this is the first real-time deep LSD available on mobile devices",
    "checked": true,
    "id": "0cceb0393b87d3ff65a1f0beea696ce40e889597",
    "semantic_title": "towards light-weight and real-time line segment detection",
    "citation_count": 17,
    "authors": [
      "Geonmo Gu",
      "Byungsoo Ko",
      "SeoungHyun Go",
      "Sung-Hyun Lee",
      "Jingeun Lee",
      "Minchul Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19954": {
    "title": "Exploiting Fine-Grained Face Forgery Clues via Progressive Enhancement Learning",
    "volume": "main",
    "abstract": "With the rapid development of facial forgery techniques, forgery detection has attracted more and more attention due to security concerns. Existing approaches attempt to use frequency information to mine subtle artifacts under high-quality forged faces. However, the exploitation of frequency information is coarse-grained, and more importantly, their vanilla learning process struggles to extract fine-grained forgery traces. To address this issue, we propose a progressive enhancement learning framework to exploit both the RGB and fine-grained frequency clues. Specifically, we perform a fine-grained decomposition of RGB images to completely decouple the real and fake traces in the frequency space. Subsequently, we propose a progressive enhancement learning framework based on a two-branch network, combined with self-enhancement and mutual-enhancement modules. The self-enhancement module captures the traces in different input spaces based on spatial noise enhancement and channel attention. The Mutual-enhancement module concurrently enhances RGB and frequency features by communicating in the shared spatial dimension. The progressive enhancement process facilitates the learning of discriminative features with fine-grained face forgery clues. Extensive experiments on several datasets show that our method outperforms the state-of-the-art face forgery detection methods",
    "checked": true,
    "id": "64266a2bea58937799631bfecda250f85b44bed1",
    "semantic_title": "exploiting fine-grained face forgery clues via progressive enhancement learning",
    "citation_count": 41,
    "authors": [
      "Qiqi Gu",
      "Shen Chen",
      "Taiping Yao",
      "Yang Chen",
      "Shouhong Ding",
      "Ran Yi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19955": {
    "title": "Delving into the Local: Dynamic Inconsistency Learning for DeepFake Video Detection",
    "volume": "main",
    "abstract": "The rapid development of facial manipulation techniques has aroused public concerns in recent years. Existing deepfake video detection approaches attempt to capture the discrim- inative features between real and fake faces based on tem- poral modelling. However, these works impose supervisions on sparsely sampled video frames but overlook the local mo- tions among adjacent frames, which instead encode rich in- consistency information that can serve as an efficient indica- tor for DeepFake video detection. To mitigate this issue, we delves into the local motion and propose a novel sampling unit named snippet which contains a few successive videos frames for local temporal inconsistency learning. Moreover, we elaborately design an Intra-Snippet Inconsistency Module (Intra-SIM) and an Inter-Snippet Interaction Module (Inter- SIM) to establish a dynamic inconsistency modelling frame- work. Specifically, the Intra-SIM applies bi-directional tem- poral difference operations and a learnable convolution ker- nel to mine the short-term motions within each snippet. The Inter-SIM is then devised to promote the cross-snippet infor- mation interaction to form global representations. The Intra- SIM and Inter-SIM work in an alternate manner and can be plugged into existing 2D CNNs. Our method outperforms the state of the art competitors on four popular benchmark dataset, i.e., FaceForensics++, Celeb-DF, DFDC and Wild- Deepfake. Besides, extensive experiments and visualizations are also presented to further illustrate its effectiveness",
    "checked": true,
    "id": "0f138a9b3d90f3874ca5b2b0dd25db13bc3ce32b",
    "semantic_title": "delving into the local: dynamic inconsistency learning for deepfake video detection",
    "citation_count": 31,
    "authors": [
      "Zhihao Gu",
      "Yang Chen",
      "Taiping Yao",
      "Shouhong Ding",
      "Jilin Li",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19956": {
    "title": "Assessing a Single Image in Reference-Guided Image Synthesis",
    "volume": "main",
    "abstract": "Assessing the performance of Generative Adversarial Networks (GANs) has been an important topic due to its practical significance. Although several evaluation metrics have been proposed, they generally assess the quality of the whole generated image distribution. For Reference-guided Image Synthesis (RIS) tasks, i.e., rendering a source image in the style of another reference image, where assessing the quality of a single generated image is crucial, these metrics are not applicable. In this paper, we propose a general learning-based framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively evaluate the quality of a single generated image. Notably, the training of RISA does not require human annotations. In specific, the training data for RISA are acquired by the intermediate models from the training procedure in RIS, and weakly annotated by the number of models' iterations, based on the positive correlation between image quality and iterations. As this annotation is too coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise interpolation scheme to refine the coarse labels, and 2) multiple binary classifiers to replace a naïve regressor. In addition, an unsupervised contrastive loss is introduced to effectively capture the style similarity between a generated image and its reference image. Empirical results on various datasets demonstrate that RISA is highly consistent with human preference and transfers well across models",
    "checked": true,
    "id": "819724765844c625b7ab701783770ffd102a1ad2",
    "semantic_title": "assessing a single image in reference-guided image synthesis",
    "citation_count": 1,
    "authors": [
      "Jiayi Guo",
      "Chaoqun Du",
      "Jiangshan Wang",
      "Huijuan Huang",
      "Pengfei Wan",
      "Gao Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19957": {
    "title": "Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-Supervised Action Recognition",
    "volume": "main",
    "abstract": "In recent years, self-supervised representation learning for skeleton-based action recognition has been developed with the advance of contrastive learning methods. The existing contrastive learning methods use normal augmentations to construct similar positive samples, which limits the ability to explore novel movement patterns. In this paper, to make better use of the movement patterns introduced by extreme augmentations, a Contrastive Learning framework utilizing Abundant Information Mining for self-supervised action Representation (AimCLR) is proposed. First, the extreme augmentations and the Energy-based Attention-guided Drop Module (EADM) are proposed to obtain diverse positive samples, which bring novel movement patterns to improve the universality of the learned representations. Second, since directly using extreme augmentations may not be able to boost the performance due to the drastic changes in original identity, the Dual Distributional Divergence Minimization Loss (D3M Loss) is proposed to minimize the distribution divergence in a more gentle way. Third, the Nearest Neighbors Mining (NNM) is proposed to further expand positive samples to make the abundant information mining process more reasonable. Exhaustive experiments on NTU RGB+D 60, PKU-MMD, NTU RGB+D 120 datasets have verified that our AimCLR can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols with observed higher quality action representations. Our code is available at https://github.com/Levigty/AimCLR",
    "checked": true,
    "id": "8a660d62fb5383a2ddd7088f869eeafd2bbb7c4c",
    "semantic_title": "contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition",
    "citation_count": 58,
    "authors": [
      "Tianyu Guo",
      "Hong Liu",
      "Zhan Chen",
      "Mengyuan Liu",
      "Tao Wang",
      "Runwei Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19958": {
    "title": "Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition",
    "volume": "main",
    "abstract": "Modern Convolutional Neural Network (CNN) architectures, despite their superiority in solving various problems, are generally too large to be deployed on resource constrained edge devices. In this paper, we reduce memory usage and floating-point operations required by convolutional layers in CNNs. We compress these layers by generalizing the Kronecker Product Decomposition to apply to multidimensional tensors, leading to the Generalized Kronecker Product Decomposition (GKPD). Our approach yields a plug-and-play module that can be used as a drop-in replacement for any convolutional layer. Experimental results for image classification on CIFAR-10 and ImageNet datasets using ResNet, MobileNetv2 and SeNet architectures substantiate the effectiveness of our proposed approach. We find that GKPD outperforms state-of-the-art decomposition methods including Tensor-Train and Tensor-Ring as well as other relevant compression methods such as pruning and knowledge distillation",
    "checked": true,
    "id": "b7ae93cbec5c7cb2c96adf8c4524a09e4dde7f2f",
    "semantic_title": "convolutional neural network compression through generalized kronecker product decomposition",
    "citation_count": 10,
    "authors": [
      "Marawan Gamal Abdel Hameed",
      "Marzieh S. Tahaei",
      "Ali Mosleh",
      "Vahid Partovi Nia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19959": {
    "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment",
    "volume": "main",
    "abstract": "Few-shot object detection (FSOD) aims to detect objects using only a few examples. How to adapt state-of-the-art object detectors to the few-shot domain remains challenging. Object proposal is a key ingredient in modern object detectors. However, the quality of proposals generated for few-shot classes using existing methods is far worse than that of many-shot classes, e.g., missing boxes for few-shot classes due to misclassification or inaccurate spatial locations with respect to true objects. To address the noisy proposal problem, we propose a novel meta-learning based FSOD model by jointly optimizing the few-shot proposal generation and fine-grained few-shot proposal classification. To improve proposal generation for few-shot classes, we propose to learn a lightweight metric-learning based prototype matching network, instead of the conventional simple linear object/nonobject classifier, e.g., used in RPN. Our non-linear classifier with the feature fusion network could improve the discriminative prototype matching and the proposal recall for few-shot classes. To improve the fine-grained few-shot proposal classification, we propose a novel attentive feature alignment method to address the spatial misalignment between the noisy proposals and few-shot classes, thus improving the performance of few-shot object detection. Meanwhile we learn a separate Faster R-CNN detection head for many-shot base classes and show strong performance of maintaining base-classes knowledge. Our model achieves state-of-the-art performance on multiple FSOD benchmarks over most of the shots and metrics",
    "checked": true,
    "id": "70e7b914b76f32e312878534737e7e0372b93da4",
    "semantic_title": "meta faster r-cnn: towards accurate few-shot object detection with attentive feature alignment",
    "citation_count": 70,
    "authors": [
      "Guangxing Han",
      "Shiyuan Huang",
      "Jiawei Ma",
      "Yicheng He",
      "Shih-Fu Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19960": {
    "title": "Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-identification",
    "volume": "main",
    "abstract": "Clustering-based unsupervised domain adaptive (UDA) person re-identification (ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature embedding and imperfect clustering, pseudo labels for target domain data inherently contain an unknown proportion of wrong ones, which would mislead feature learning. In this paper, we propose an approach named probabilistic uncertainty guided progressive label refinery (P2LR) for domain adaptive person re-identification. First, we propose to model the labeling uncertainty with the probabilistic distance along with ideal single-peak distributions. A quantitative criterion is established to measure the uncertainty of pseudo labels and facilitate the network training. Second, we explore a progressive strategy for refining pseudo labels. With the uncertainty-guided alternative optimization, we balance between the exploration of target domain data and the negative effects of noisy labeling. On top of a strong baseline, we obtain significant improvements and achieve the state-of-the-art performance on four UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5% mAP on the Duke2Market task, while surpassing the state-of-the-art method by 2.5% mAP on the Market2MSMT task. Code is available at: https://github.com/JeyesHan/P2LR",
    "checked": true,
    "id": "1d230bd379e11c142f42e3cbb114389ecee2b83a",
    "semantic_title": "delving into probabilistic uncertainty for unsupervised domain adaptive person re-identification",
    "citation_count": 14,
    "authors": [
      "Jian Han",
      "Ya-Li Li",
      "Shengjin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19961": {
    "title": "Laneformer: Object-Aware Row-Column Transformers for Lane Detection",
    "volume": "main",
    "abstract": "We present Laneformer, a conceptually simple yet powerful transformer-based architecture tailored for lane detection that is a long-standing research topic for visual perception in autonomous driving. The dominant paradigms rely on purely CNN-based architectures which often fail in incorporating relations of long-range lane points and global contexts induced by surrounding objects (e.g., pedestrians, vehicles). Inspired by recent advances of the transformer encoder-decoder architecture in various vision tasks, we move forwards to design a new end-to-end Laneformer architecture that revolutionizes the conventional transformers into better capturing the shape and semantic characteristics of lanes, with minimal overhead in latency. First, coupling with deformable pixel-wise self-attention in the encoder, Laneformer presents two new row and column self-attention operations to efficiently mine point context along with the lane shapes. Second, motivated by the appearing objects would affect the decision of predicting lane segments, Laneformer further includes the detected object instances as extra inputs of multi-head attention blocks in the encoder and decoder to facilitate the lane point detection by sensing semantic contexts. Specifically, the bounding box locations of objects are added into Key module to provide interaction with each pixel and query while the ROI-aligned features are inserted into Value module. Extensive experiments demonstrate our Laneformer achieves state-of-the-art performances on CULane benchmark, in terms of 77.1% F1 score. We hope our simple and effective Laneformer will serve as a strong baseline for future research in self-attention models for lane detection",
    "checked": true,
    "id": "74db14977f55a01cb11b5e5a9367669acc46570d",
    "semantic_title": "laneformer: object-aware row-column transformers for lane detection",
    "citation_count": 12,
    "authors": [
      "Jianhua Han",
      "Xiajun Deng",
      "Xinyue Cai",
      "Zhen Yang",
      "Hang Xu",
      "Chunjing Xu",
      "Xiaodan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19962": {
    "title": "Modify Self-Attention via Skeleton Decomposition for Effective Point Cloud Transformer",
    "volume": "main",
    "abstract": "Although considerable progress has been achieved regarding the transformers in recent years, the large number of parameters, quadratic computational complexity, and memory cost conditioned on long sequences make the transformers hard to train and implement, especially in edge computing configurations. In this case, a dizzying number of works have sought to make improvements around computational and memory efficiency upon the original transformer architecture. Nevertheless, many of them restrict the context in the attention to seek a trade-off between cost and performance with prior knowledge of orderly stored data. It is imperative to dig deep into an efficient feature extractor for point clouds due to their irregularity and a large number of points. In this paper, we propose a novel skeleton decomposition-based self-attention (SD-SA) which has no sequence length limit and exhibits favorable scalability in long-sequence models. Due to the numerical low-rank nature of self-attention, we approximate it by the skeleton decomposition method while maintaining its effectiveness. At this point, we have shown that the proposed method works for the proposed approach on point cloud classification, segmentation, and detection tasks on the ModelNet40, ShapeNet, and KITTI datasets, respectively. Our approach significantly improves the efficiency of the point cloud transformer and exceeds other efficient transformers on point cloud tasks in terms of the speed at comparable performance",
    "checked": true,
    "id": "c9288cfc84953ee77d5390fb718106ac5caea2af",
    "semantic_title": "modify self-attention via skeleton decomposition for effective point cloud transformer",
    "citation_count": 1,
    "authors": [
      "Jiayi Han",
      "Longbin Zeng",
      "Liang Du",
      "Xiaoqing Ye",
      "Weiyang Ding",
      "Jianfeng Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19963": {
    "title": "Generalizable Person Re-identification via Self-Supervised Batch Norm Test-Time Adaption",
    "volume": "main",
    "abstract": "In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain. As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods. However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated. To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively. Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain. This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively. To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods",
    "checked": true,
    "id": "d53c7c436feb7ff0ee29ad9b55e7be5b0cd2e309",
    "semantic_title": "generalizable person re-identification via self-supervised batch norm test-time adaption",
    "citation_count": 4,
    "authors": [
      "Ke Han",
      "Chenyang Si",
      "Yan Huang",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19964": {
    "title": "RRL: Regional Rotate Layer in Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) perform very well in image classification and object detection in recent years, but even the most advanced models have limited rotation invariance. Known solutions include the enhancement of training data and the increase of rotation invariance by globally merging the rotation equivariant features. These methods either increase the workload of training or increase the number of model parameters. To address this problem, this paper proposes a module that can be inserted into the existing networks, and directly incorporates the rotation invariance into the feature extraction layers of the CNNs. This module does not have learnable parameters and will not increase the complexity of the model. At the same time, only by training the upright data, it can perform well on the rotated testing set. These ad-vantages will be suitable for fields such as biomedicine and astronomy where it is difficult to obtain upright samples or the target has no directionality. Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we get impressive results",
    "checked": true,
    "id": "cc2ad0a41725792402aed2f432e724fb9add5671",
    "semantic_title": "rrl: regional rotate layer in convolutional neural networks",
    "citation_count": 1,
    "authors": [
      "Zongbo Hao",
      "Tao Zhang",
      "Mingwang Chen",
      "Zou Kaixu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19965": {
    "title": "QueryProp: Object Query Propagation for High-Performance Video Object Detection",
    "volume": "main",
    "abstract": "Video object detection has been an important yet challenging topic in computer vision. Traditional methods mainly focus on designing the image-level or box-level feature propagation strategies to exploit temporal information. This paper argues that with a more effective and efficient feature propagation framework, video object detectors can gain improvement in terms of both accuracy and speed. For this purpose, this paper studies object-level feature propagation, and proposes an object query propagation (QueryProp) framework for high-performance video object detection. The proposed QueryProp contains two propagation strategies: 1) query propagation is performed from sparse key frames to dense non-key frames to reduce the redundant computation on non-key frames; 2) query propagation is performed from previous key frames to the current key frame to improve feature representation by temporal context modeling. To further facilitate query propagation, an adaptive propagation gate is designed to achieve flexible key frame selection. We conduct extensive experiments on the ImageNet VID dataset. QueryProp achieves comparable accuracy with state-of-the-art methods and strikes a decent accuracy/speed trade-off",
    "checked": true,
    "id": "74e2b895227dd800ab3b8c47d6ee70293e314849",
    "semantic_title": "queryprop: object query propagation for high-performance video object detection",
    "citation_count": 12,
    "authors": [
      "Fei He",
      "Naiyu Gao",
      "Jian Jia",
      "Xin Zhao",
      "Kaiqi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19966": {
    "title": "Flow-Based Unconstrained Lip to Speech Generation",
    "volume": "main",
    "abstract": "Unconstrained lip-to-speech aims to generate corresponding speeches based on silent facial videos with no restriction to head pose or vocabulary. It is desirable to generate intelligible and natural speech with a fast speed in unconstrained settings. Currently, to handle the more complicated scenarios, most existing methods adopt the autoregressive architecture, which is optimized with the MSE loss. Although these methods have achieved promising performance, they are prone to bring issues including high inference latency and mel-spectrogram over-smoothness. To tackle these problems, we propose a novel flow-based non-autoregressive lip-to-speech model (GlowLTS) to break autoregressive constraints and achieve faster inference. Concretely, we adopt a flow-based decoder which is optimized by maximizing the likelihood of the training data and is capable of more natural and fast speech generation. Moreover, we devise a condition module to improve the intelligibility of generated speech. We demonstrate the superiority of our proposed method through objective and subjective evaluation on Lip2Wav-Chemistry-Lectures and Lip2Wav-Chess-Analysis datasets. Our demo video can be found at https://glowlts.github.io/",
    "checked": true,
    "id": "279a6f09f7255fd6d18226f215d12cbc777ced8b",
    "semantic_title": "flow-based unconstrained lip to speech generation",
    "citation_count": 10,
    "authors": [
      "Jinzheng He",
      "Zhou Zhao",
      "Yi Ren",
      "Jinglin Liu",
      "Baoxing Huai",
      "Nicholas Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19967": {
    "title": "TransFG: A Transformer Architecture for Fine-Grained Recognition",
    "volume": "main",
    "abstract": "Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model",
    "checked": true,
    "id": "860e24025c67487b9dd87b442c7b44e5bbf5a054",
    "semantic_title": "transfg: a transformer architecture for fine-grained recognition",
    "citation_count": 172,
    "authors": [
      "Ju He",
      "Jie-Neng Chen",
      "Shuai Liu",
      "Adam Kortylewski",
      "Cheng Yang",
      "Yutong Bai",
      "Changhu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19968": {
    "title": "Self-Supervised Robust Scene Flow Estimation via the Alignment of Probability Density Functions",
    "volume": "main",
    "abstract": "In this paper, we present a new self-supervised scene flow estimation approach for a pair of consecutive point clouds. The key idea of our approach is to represent discrete point clouds as continuous probability density functions using Gaussian mixture models. Scene flow estimation is therefore converted into the problem of recovering motion from the alignment of probability density functions, which we achieve using a closed-form expression of the classic Cauchy-Schwarz divergence. Unlike existing nearest-neighbor-based approaches that use hard pairwise correspondences, our proposed approach establishes soft and implicit point correspondences between point clouds and generates more robust and accurate scene flow in the presence of missing correspondences and outliers. Comprehensive experiments show that our method makes noticeable gains over the Chamfer Distance and the Earth Mover's Distance in real-world environments and achieves state-of-the-art performance among self-supervised learning methods on FlyingThings3D and KITTI, even outperforming some supervised methods with ground truth annotations",
    "checked": true,
    "id": "3b073f2d04fc758cf72ae5a64541fef21d12aab7",
    "semantic_title": "self-supervised robust scene flow estimation via the alignment of probability density functions",
    "citation_count": 6,
    "authors": [
      "Pan He",
      "Patrick Emami",
      "Sanjay Ranka",
      "Anand Rangarajan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19969": {
    "title": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds",
    "volume": "main",
    "abstract": "Accurate 3D object detection from point clouds has become a crucial component in autonomous driving. However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets. In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels. The local and global graphs serve as the attention mechanism to enhance the extracted features. In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels. Experiments on KITTI detection benchmark and Waymo Open dataset demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy",
    "checked": true,
    "id": "002b12ad36d3b19a78d0754e6949f791987830dd",
    "semantic_title": "svga-net: sparse voxel-graph attention network for 3d object detection from point clouds",
    "citation_count": 49,
    "authors": [
      "Qingdong He",
      "Zhengning Wang",
      "Hao Zeng",
      "Yi Zeng",
      "Yijun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19970": {
    "title": "SECRET: Self-Consistent Pseudo Label Refinement for Unsupervised Domain Adaptive Person Re-identification",
    "volume": "main",
    "abstract": "Unsupervised domain adaptive person re-identification aims at learning on an unlabeled target domain with only labeled data in source domain. Currently, the state-of-the-arts usually solve this problem by pseudo-label-based clustering and fine-tuning in target domain. However, the reason behind the noises of pseudo labels is not sufficiently explored, especially for the popular multi-branch models. We argue that the consistency between different feature spaces is the key to the pseudo labels' quality. Then a SElf-Consistent pseudo label RefinEmenT method, termed as SECRET, is proposed to improve consistency by mutually refining the pseudo labels generated from different feature spaces. The proposed SECRET gradually encourages the improvement of pseudo labels' quality during training process, which further leads to better cross-domain Re-ID performance. Extensive experiments on benchmark datasets show the superiority of our method. Specifically, our method outperforms the state-of-the-arts by 6.3% in terms of mAP on the challenging dataset MSMT17. In the purely unsupervised setting, our method also surpasses existing works by a large margin. Code is available at https://github.com/LunarShen/SECRET",
    "checked": true,
    "id": "37e509b067cee3ab98e497ba84ed679999eabfbc",
    "semantic_title": "secret: self-consistent pseudo label refinement for unsupervised domain adaptive person re-identification",
    "citation_count": 19,
    "authors": [
      "Tao He",
      "Leqi Shen",
      "Yuchen Guo",
      "Guiguang Ding",
      "Zhenhua Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19971": {
    "title": "Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition",
    "volume": "main",
    "abstract": "Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text. To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper. Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity. Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph. Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning. Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning. S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets. Code is available at https://github.com/adeline-cs/GTR",
    "checked": true,
    "id": "99730b6be5721157ec3b93332a22da3410539e9a",
    "semantic_title": "visual semantics allow for textual reasoning better in scene text recognition",
    "citation_count": 28,
    "authors": [
      "Yue He",
      "Chen Chen",
      "Jing Zhang",
      "Juhua Liu",
      "Fengxiang He",
      "Chaoyue Wang",
      "Bo Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19972": {
    "title": "Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives",
    "volume": "main",
    "abstract": "This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a new member in the family of InfoNCE losses that preserves a ranked ordering of positive samples. In contrast to the standard InfoNCE loss, which requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE can exploit information about a similarity ranking for learning a corresponding embedding space. We show that the proposed loss function learns favorable embeddings compared to the standard InfoNCE whenever at least noisy ranking information can be obtained or when the definition of positives and negatives is blurry. We demonstrate this for a supervised classification task with additional superclass labels and noisy similarity scores. Furthermore, we show that RINCE can also be applied to unsupervised training with experiments on unsupervised representation learning from videos. In particular, the embedding yields higher classification accuracy, retrieval rates and performs better on out-of-distribution detection than the standard InfoNCE loss",
    "checked": true,
    "id": "b74b9ad4d58670acbace98cf4fa19d24c365ca44",
    "semantic_title": "ranking info noise contrastive estimation: boosting contrastive learning via ranked positives",
    "citation_count": 13,
    "authors": [
      "David T. Hoffmann",
      "Nadine Behrmann",
      "Juergen Gall",
      "Thomas Brox",
      "Mehdi Noroozi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19973": {
    "title": "Uncertainty-Driven Dehazing Network",
    "volume": "main",
    "abstract": "Deep learning has made remarkable achievements for single image haze removal. However, existing deep dehazing models only give deterministic results without discussing the uncertainty of them. There exist two types of uncertainty in the dehazing models: aleatoric uncertainty that comes from noise inherent in the observations and epistemic uncertainty that accounts for uncertainty in the model. In this paper, we propose a novel uncertainty-driven dehazing network (UDN) that improves the dehazing results by exploiting the relationship between the uncertain and confident representations. We first introduce an Uncertainty Estimation Block (UEB) to predict the aleatoric and epistemic uncertainty together. Then, we propose an Uncertainty-aware Feature Modulation (UFM) block to adaptively enhance the learned features. UFM predicts a convolution kernel and channel-wise modulation cofficients conitioned on the uncertainty weighted representation. Moreover, we develop an uncertainty-driven self-distillation loss to improve the uncertain representation by transferring the knowledge from the confident one. Extensive experimental results on synthetic datasets and real-world images show that UDN achieves significant quantitative and qualitative improvements, outperforming the state-of-the-arts",
    "checked": true,
    "id": "e075410a978ab6cdfeb7564ce8762f0c0efac767",
    "semantic_title": "uncertainty-driven dehazing network",
    "citation_count": 15,
    "authors": [
      "Ming Hong",
      "Jianzhuang Liu",
      "Cuihua Li",
      "Yanyun Qu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19974": {
    "title": "Shadow Generation for Composite Image in Real-World Scenes",
    "volume": "main",
    "abstract": "Image composition targets at inserting a foreground object into a background image. Most previous image composition methods focus on adjusting the foreground to make it compatible with background while ignoring the shadow effect of foreground on the background. In this work, we focus on generating plausible shadow for the foreground object in the composite image. First, we contribute a real-world shadow generation dataset DESOBA by generating synthetic composite images based on paired real images and deshadowed images. Then, we propose a novel shadow generation network SGRNet, which consists of a shadow mask prediction stage and a shadow filling stage. In the shadow mask prediction stage, foreground and background information are thoroughly interacted to generate foreground shadow mask. In the shadow filling stage, shadow parameters are predicted to fill the shadow area. Extensive experiments on our DESOBA dataset and real composite images demonstrate the effectiveness of our proposed method. Our dataset and code are available at https://github.com/bcmi/Object-Shadow-Generation- Dataset-DESOBA",
    "checked": true,
    "id": "7e02430d27bcb2ee91ba200d29211e64e83def49",
    "semantic_title": "shadow generation for composite image in real-world scenes",
    "citation_count": 21,
    "authors": [
      "Yan Hong",
      "Li Niu",
      "Jianfu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19975": {
    "title": "Shape-Adaptive Selection and Measurement for Oriented Object Detection",
    "volume": "main",
    "abstract": "The development of detection methods for oriented object detection remains a challenging task. A considerable obstacle is the wide variation in the shape (e.g., aspect ratio) of objects. Sample selection in general object detection has been widely studied as it plays a crucial role in the performance of the detection method and has achieved great progress. However, existing sample selection strategies still overlook some issues: (1) most of them ignore the object shape information; (2) they do not make a potential distinction between selected positive samples; and (3) some of them can only be applied to either anchor-free or anchor-based methods and cannot be used for both of them simultaneously. In this paper, we propose novel flexible shape-adaptive selection (SA-S) and shape-adaptive measurement (SA-M) strategies for oriented object detection, which comprise an SA-S strategy for sample selection and SA-M strategy for the quality estimation of positive samples. Specifically, the SA-S strategy dynamically selects samples according to the shape information and characteristics distribution of objects. The SA-M strategy measures the localization potential and adds quality information on the selected positive samples. The experimental results on both anchor-free and anchor-based baselines and four publicly available oriented datasets (DOTA, HRSC2016, UCAS-AOD, and ICDAR2015) demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "3bcc436f59b301bde1808f62202787d6938cd88f",
    "semantic_title": "shape-adaptive selection and measurement for oriented object detection",
    "citation_count": 52,
    "authors": [
      "Liping Hou",
      "Ke Lu",
      "Jian Xue",
      "Yuqiu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19976": {
    "title": "H^2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "Current representation learning methods for whole slide image (WSI) with pyramidal resolutions are inherently homogeneous and flat, which cannot fully exploit the multiscale and heterogeneous diagnostic information of different structures for comprehensive analysis. This paper presents a novel graph neural network-based multiple instance learning framework (i.e., H^2-MIL) to learn hierarchical representation from a heterogeneous graph with different resolutions for WSI analysis. A heterogeneous graph with the \"resolution\" attribute is constructed to explicitly model the feature and spatial-scaling relationship of multi-resolution patches. We then design a novel resolution-aware attention convolution (RAConv) block to learn compact yet discriminative representation from the graph, which tackles the heterogeneity of node neighbors with different resolutions and yields more reliable message passing. More importantly, to explore the task-related structured information of WSI pyramid, we elaborately design a novel iterative hierarchical pooling (IHPool) module to progressively aggregate the heterogeneous graph based on scaling relationships of different nodes. We evaluated our method on two public WSI datasets from the TCGA project, i.e., esophageal cancer and kidney cancer. Experimental results show that our method clearly outperforms the state-of-the-art methods on both tumor typing and staging tasks",
    "checked": true,
    "id": "b1ef7a10061c97c4771fc7da0ff1915dbeec06e2",
    "semantic_title": "h^2-mil: exploring hierarchical representation with heterogeneous multiple instance learning for whole slide image analysis",
    "citation_count": 18,
    "authors": [
      "Wentai Hou",
      "Lequan Yu",
      "Chengxuan Lin",
      "Helong Huang",
      "Rongshan Yu",
      "Jing Qin",
      "Liansheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19977": {
    "title": "Elastic-Link for Binarized Neural Networks",
    "volume": "main",
    "abstract": "Recent work has shown that Binarized Neural Networks (BNNs) are able to greatly reduce computational costs and memory footprints, facilitating model deployment on resource-constrained devices. However, in comparison to their full-precision counterparts, BNNs suffer from severe accuracy degradation. Research aiming to reduce this accuracy gap has thus far largely focused on specific network architectures with few or no 1 × 1 convolutional layers, for which standard binarization methods do not work well. Because 1 × 1 convolutions are common in the design of modern architectures (e.g. GoogleNet, ResNet, DenseNet), it is crucial to develop a method to binarize them effectively for BNNs to be more widely adopted. In this work, we propose an \"Elastic-Link\" (EL) module to enrich information flow within a BNN by adaptively adding real-valued input features to the subsequent convolutional output features. The proposed EL module is easily implemented and can be used in conjunction with other methods for BNNs. We demonstrate that adding EL to BNNs produces a significant improvement on the challenging large-scale ImageNet dataset. For example, we raise the top-1 accuracy of binarized ResNet26 from 57.9% to 64.0%. EL also aids con-vergence in the training of binarized MobileNet, for which a top-1 accuracy of 56.4% is achieved. Finally, with the integration of ReActNet, it yields a new state-of-the-art result of 71.9% top-1 accuracy",
    "checked": false,
    "id": "23f5ec58ffe808904838cfae8ad8f3c0aabcec13",
    "semantic_title": "elastic-link for binarized neural network",
    "citation_count": 4,
    "authors": [
      "Jie Hu",
      "Ziheng Wu",
      "Vince Tan",
      "Zhilin Lu",
      "Mengze Zeng",
      "Enhua Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19978": {
    "title": "FInfer: Frame Inference-Based Deepfake Detection for High-Visual-Quality Videos",
    "volume": "main",
    "abstract": "Deepfake has ignited hot research interests in both academia and industry due to its potential security threats. Many countermeasures have been proposed to mitigate such risks. Current Deepfake detection methods achieve superior performances in dealing with low-visual-quality Deepfake media which can be distinguished by the obvious visual artifacts. However, with the development of deep generative models, the realism of Deepfake media has been significantly improved and becomes tough challenging to current detection models. In this paper, we propose a frame inference-based detection framework (FInfer) to solve the problem of high-visual-quality Deepfake detection. Specifically, we first learn the referenced representations of the current and future frames' faces. Then, the current frames' facial representations are utilized to predict the future frames' facial representations by using an autoregressive model. Finally, a representation-prediction loss is devised to maximize the discriminability of real videos and fake videos. We demonstrate the effectiveness of our FInfer framework through information theory analyses. The entropy and mutual information analyses indicate the correlation between the predicted representations and referenced representations in real videos is higher than that of high-visual-quality Deepfake videos. Extensive experiments demonstrate the performance of our method is promising in terms of in-dataset detection performance, detection efficiency, and cross-dataset detection performance in high-visual-quality Deepfake videos",
    "checked": true,
    "id": "114bdf03192f1bcf1c3711867feaf9e3be6d8389",
    "semantic_title": "finfer: frame inference-based deepfake detection for high-visual-quality videos",
    "citation_count": 23,
    "authors": [
      "Juan Hu",
      "Xin Liao",
      "Jinwen Liang",
      "Wenbo Zhou",
      "Zheng Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19979": {
    "title": "Bi-volution: A Static and Dynamic Coupled Filter",
    "volume": "main",
    "abstract": "Dynamic convolution has achieved significant gain in performance and computational complexity, thanks to its powerful representation capability given limited filter number/layers. However, SOTA dynamic convolution operators are sensitive to input noises (e.g., Gaussian noise, shot noise, e.t.c.) and lack sufficient spatial contextual information in filter generation. To alleviate this inherent weakness, we propose a lightweight and heterogeneous-structure (i.e., static and dynamic) operator, named Bi-volution. On the one hand, Bi-volution is designed as a dual-branch structure to fully leverage complementary properties of static/dynamic convolution, which endows Bi-volution more robust properties and higher performance. On the other hand, the Spatial Augmented Kernel Generation module is proposed to improve the dynamic convolution, realizing the learning of spatial context information with negligible additional computational complexity. Extensive experiments illustrate that the ResNet-50 equipped with Bi-volution achieves a highly competitive boost in performance (+2.8% top-1 accuracy on ImageNet classification, +2.4% box AP and +2.2% mask AP on COCO detection and instance segmentation) while maintaining extremely low FLOPs (i.e., ResNet50@2.7 GFLOPs). Furthermore, our Bi-volution shows better robustness than dynamic convolution against various noise and input corruptions. Our code is available at https://github.com/neuralchen/Bivolution",
    "checked": true,
    "id": "e556ce96eabeda4acd200960b17100e0c8ce0a32",
    "semantic_title": "bi-volution: a static and dynamic coupled filter",
    "citation_count": 3,
    "authors": [
      "Xiwei Hu",
      "Xuanhong Chen",
      "Bingbing Ni",
      "Teng Li",
      "Yutian Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19980": {
    "title": "AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds",
    "volume": "main",
    "abstract": "There have been two streams in the 3D detection from point clouds: single-stage methods and two-stage methods. While the former is more computationally efficient, the latter usually provides better detection accuracy. By carefully examining the two-stage approaches, we have found that if appropriately designed, the first stage can produce accurate box regression. In this scenario, the second stage mainly rescores the boxes such that the boxes with better localization get selected. From this observation, we have devised a single-stage anchor-free network that can fulfill these requirements. This network, named AFDetV2, extends the previous work by incorporating a self-calibrated convolution block in the backbone, a keypoint auxiliary supervision, and an IoU prediction branch in the multi-task head. We take a simple product of the predicted IoU score with the classification heatmap to form the final classification confidence. The enhanced backbone strengthens the box localization capability, and the rescoring approach effectively joins the object presence confidence and the box regression accuracy. As a result, the detection accuracy is drastically boosted in the single-stage. To evaluate our approach, we have conducted extensive experiments on the Waymo Open Dataset and the nuScenes Dataset. We have observed that our AFDetV2 achieves the state-of-the-art results on these two datasets, superior to all the prior arts, including both the single-stage and the two-stage 3D detectors. AFDetV2 won the 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge 2021. In addition, a variant of our model AFDetV2-Base was entitled the \"Most Efficient Model\" by the Challenge Sponsor, showing a superior computational efficiency. To demonstrate the generality of this single-stage method, we have also applied it to the first stage of the two-stage networks. Without exception, the results show that with the strengthened backbone and the rescoring approach, the second stage refinement is no longer needed",
    "checked": true,
    "id": "69cc6392972664e0e8a8dfab8b39d9fe3a934122",
    "semantic_title": "afdetv2: rethinking the necessity of the second stage for object detection from point clouds",
    "citation_count": 75,
    "authors": [
      "Yihan Hu",
      "Zhuangzhuang Ding",
      "Runzhou Ge",
      "Wenxin Shao",
      "Li Huang",
      "Kun Li",
      "Qiang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19981": {
    "title": "Divide-and-Regroup Clustering for Domain Adaptive Person Re-identification",
    "volume": "main",
    "abstract": "Clustering is important for domain adaptive person re-identification(re-ID). A majority of unsupervised domain adaptation (UDA) methods conduct clustering on the target domain and then use the generated pseudo labels for adaptive training. Albeit important, the clustering pipeline adopted by current literature is quite standard and lacks consideration for two characteristics of re-ID, i.e., 1) a single person has various feature distribution in multiple cameras. 2) a person's occurrence in the same camera are usually temporally continuous. We argue that the multi-camera distribution hinders clustering because it enlarges the intra-class distances. In contrast, the temporal continuity prior is beneficial, because it offers clue for distinguishing some look-alike person (who are temporally far away from each other). These two insight motivate us to propose a novel Divide-And-Regroup Clustering (DARC) pipeline for re-ID UDA. Specifically, DARC divides the unlabeled data into multiple camera-specific groups and conducts local clustering within each camera. Afterwards, it regroups those local clusters potentially belonging to the same person into a unity. Through this divide-and-regroup pipeline, DARC avoids directly clustering across multiple cameras and focuses on the feature distribution within each individual camera. Moreover, during the local clustering, DARC uses the temporal continuity prior to distinguish some look-alike person and thus reduces false positive pseudo labels. Consequentially, DARC effectively reduces clustering errors and improves UDA. Importantly, we show that DARC is compatible to many pseudo label-based UDA methods and brings general improvement. Based on a recent UDA method, DARC advances the state of the art (e.g, 85.1% mAP on MSMT-to-Market and 83.1% mAP on PersonX-to-Market)",
    "checked": true,
    "id": "1248e2b5a8fff3210edfcbe22fb7881bc416e4e5",
    "semantic_title": "divide-and-regroup clustering for domain adaptive person re-identification",
    "citation_count": 5,
    "authors": [
      "Zhengdong Hu",
      "Yifan Sun",
      "Yi Yang",
      "Jianguang Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19982": {
    "title": "CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes",
    "volume": "main",
    "abstract": "Malicious applications of deepfakes (i.e., technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security. To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs. Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model. To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models. Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively. Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models. Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict. Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones. Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods. Our code is available at https://github.com/VDIGPKU/CMUA-Watermark",
    "checked": true,
    "id": "c1bb6400bf88b929467bb8cc411d72c5e9eda56d",
    "semantic_title": "cmua-watermark: a cross-model universal adversarial watermark for combating deepfakes",
    "citation_count": 33,
    "authors": [
      "Hao Huang",
      "Yongtao Wang",
      "Zhaoyu Chen",
      "Yuze Zhang",
      "Yuheng Li",
      "Zhi Tang",
      "Wei Chu",
      "Jingdong Chen",
      "Weisi Lin",
      "Kai-Kuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19983": {
    "title": "Deconfounded Visual Grounding",
    "volume": "main",
    "abstract": "We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial languagelocation association without visual reasoning, e.g., grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code is available at: https://github.com/JianqiangH/Deconfounded_VG",
    "checked": true,
    "id": "a07c55085bc02daa7e05857532a4a58939f9b14d",
    "semantic_title": "deconfounded visual grounding",
    "citation_count": 15,
    "authors": [
      "Jianqiang Huang",
      "Yu Qin",
      "Jiaxin Qi",
      "Qianru Sun",
      "Hanwang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19984": {
    "title": "Learning to Model Pixel-Embedded Affinity for Homogeneous Instance Segmentation",
    "volume": "main",
    "abstract": "Homogeneous instance segmentation aims to identify each instance in an image where all interested instances belong to the same category, such as plant leaves and microscopic cells. Recently, proposal-free methods, which straightforwardly generate instance-aware information to group pixels into different instances, have received increasing attention due to their efficient pipeline. However, they often fail to distinguish adjacent instances due to similar appearances, dense distribution and ambiguous boundaries of instances in homogeneous images. In this paper, we propose a pixel-embedded affinity modeling method for homogeneous instance segmentation, which is able to preserve the semantic information of instances and improve the distinguishability of adjacent instances. Instead of predicting affinity directly, we propose a self-correlation module to explicitly model the pairwise relationships between pixels, by estimating the similarity between embeddings generated from the input image through CNNs. Based on the self-correlation module, we further design a cross-correlation module to maintain the semantic consistency between instances. Specifically, we map the transformed input images with different views and appearances into the same embedding space, and then mutually estimate the pairwise relationships of embeddings generated from the original input and its transformed variants. In addition, to integrate the global instance information, we introduce an embedding pyramid module to model affinity on different scales. Extensive experiments demonstrate the versatile and superior performance of our method on three representative datasets. Code and models are available at https://github.com/weih527/Pixel-Embedded-Affinity",
    "checked": true,
    "id": "eb8348f95373bb6c391ef55417fb32b348b892c1",
    "semantic_title": "learning to model pixel-embedded affinity for homogeneous instance segmentation",
    "citation_count": 11,
    "authors": [
      "Wei Huang",
      "Shiyu Deng",
      "Chang Chen",
      "Xueyang Fu",
      "Zhiwei Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19985": {
    "title": "Channelized Axial Attention – considering Channel Relation within Spatial Attention for Semantic Segmentation",
    "volume": "main",
    "abstract": "Spatial and channel attentions, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial and channel attentions separately sometimes causes errors, especially for those difficult cases. In this paper, we propose Channelized Axial Attention (CAA) to seamlessly integrate channel attention and spatial attention into a single operation with negligible computation overhead. Specifically, we break down the dot-product operation of the spatial attention into two parts and insert channel relation in between, allowing for independently optimized channel attention on each spatial location. We further develop grouped vectorization, which allows our model to run with very little memory consumption without slowing down the running speed. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate that our CAA outperforms many state-of-the-art segmentation models (including dual attention) on all tested datasets",
    "checked": false,
    "id": "9afcacef80e4961227439eedf5a3f06e5319b9b7",
    "semantic_title": "channelized axial attention - considering channel relation within spatial attention for semantic segmentation",
    "citation_count": 15,
    "authors": [
      "Ye Huang",
      "Di Kang",
      "Wenjing Jia",
      "Liu Liu",
      "Xiangjian He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19986": {
    "title": "UFPMP-Det:Toward Accurate and Efficient Object Detection on Drone Imagery",
    "volume": "main",
    "abstract": "This paper proposes a novel approach to object detection on drone imagery, namely Multi-Proxy Detection Network with Unified Foreground Packing (UFPMP-Det). To deal with the numerous instances of very small scales, different from the common solution that divides the high-resolution input image into quite a number of chips with low foreground ratios to perform detection on them each, the Unified Foreground Packing (UFP) module is designed, where the sub-regions given by a coarse detector are initially merged through clustering to suppress background and the resulting ones are subsequently packed into a mosaic for a single inference, thus significantly reducing overall time cost. Furthermore, to address the more serious confusion between inter-class similarities and intra-class variations of instances, which deteriorates detection performance but is rarely discussed, the Multi-Proxy Detection Network (MP-Det) is presented to model object distributions in a fine-grained manner by employing multiple proxy learning, and the proxies are enforced to be diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport loss. By such means, UFPMP-Det largely promotes both the detection accuracy and efficiency. Extensive experiments are carried out on the widely used VisDrone and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much higher speed, highlighting its advantages. The code is available at https://github.com/PuAnysh/UFPMP-Det",
    "checked": false,
    "id": "cbd89606487ddbeae73bf423d5f70b22bdccbac2",
    "semantic_title": "ufpmp-det: toward accurate and efficient object detection on drone imagery",
    "citation_count": 24,
    "authors": [
      "Yecheng Huang",
      "Jiaxin Chen",
      "Di Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19987": {
    "title": "Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "RGB-infrared person re-identification is an emerging cross-modality re-identification task, which is very challenging due to significant modality discrepancy between RGB and infrared images. In this work, we propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations. MID designs a modality-adaptive mixup scheme to generate suitable mixed modality images between RGB and infrared images for mitigating the inherent modality discrepancy at the pixel-level. It formulates modality mixup procedure as Markov decision process, where an actor-critic agent learns dynamical and local linear interpolation policy between different regions of cross-modality images under a deep reinforcement learning framework. Such policy guarantees modality-invariance in a more continuous latent space and avoids manifold intrusion by the corrupted mixed modality samples. Moreover, to further counter modality discrepancy and enforce invariant visual semantics at the feature-level, MID employs modality-adaptive convolution decomposition to disassemble a regular convolution layer into modality-specific basis layers and a modality-shared coefficient layer. Extensive experimental results on two challenging benchmarks demonstrate superior performance of MID over state-of-the-art methods",
    "checked": true,
    "id": "c7dbb5ada2c66911ae727763d9831c469bd48261",
    "semantic_title": "modality-adaptive mixup and invariant decomposition for rgb-infrared person re-identification",
    "citation_count": 14,
    "authors": [
      "Zhipeng Huang",
      "Jiawei Liu",
      "Liang Li",
      "Kecheng Zheng",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19988": {
    "title": "MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion",
    "volume": "main",
    "abstract": "Multimodal sensors (visual, non-visual, and wearable) can provide complementary information to develop robust perception systems for recognizing activities accurately. However, it is challenging to extract robust multimodal representations due to the heterogeneous characteristics of data from multimodal sensors and disparate human activities, especially in the presence of noisy and misaligned sensor data. In this work, we propose a cooperative multitask learning-based guided multimodal fusion approach, MuMu, to extract robust multimodal representations for human activity recognition (HAR). MuMu employs an auxiliary task learning approach to extract features specific to each set of activities with shared characteristics (activity-group). MuMu then utilizes activity-group-specific features to direct our proposed Guided Multimodal Fusion Approach (GM-Fusion) for extracting complementary multimodal representations, designed as the target task. We evaluated MuMu by comparing its performance to state-of-the-art multimodal HAR approaches on three activity datasets. Our extensive experimental results suggest that MuMu outperforms all the evaluated approaches across all three datasets. Additionally, the ablation study suggests that MuMu significantly outperforms the baseline models (p<0.05), which do not use our guided multimodal fusion. Finally, the robust performance of MuMu on noisy and misaligned sensor data posits that our approach is suitable for HAR in real-world settings",
    "checked": true,
    "id": "abde8a122570cd7111a1d1335ddb89a3ed38f99b",
    "semantic_title": "mumu: cooperative multitask learning-based guided multimodal fusion",
    "citation_count": 20,
    "authors": [
      "Md Mofijul Islam",
      "Tariq Iqbal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19989": {
    "title": "An Unsupervised Way to Understand Artifact Generating Internal Units in Generative Neural Networks",
    "volume": "main",
    "abstract": "Despite significant improvements on the image generation performance of Generative Adversarial Networks (GANs), generations with low visual fidelity still have been observed. As widely used metrics for GANs focus more on the overall performance of the model, evaluation on the quality of individual generations or detection of defective generations is challenging. While recent studies try to detect featuremap units that cause artifacts and evaluate individual samples, these approaches require additional resources such as external networks or a number of training data to approximate the real data manifold. In this work, we propose the concept of local activation, and devise a metric on the local activation to detect artifact generations without additional supervision. We empirically verify that our approach can detect and correct artifact generations from GANs with various datasets. Finally, we discuss a geometrical analysis to partially reveal the relation between the proposed concept and low visual fidelity",
    "checked": true,
    "id": "a5f31f5f46f4eec87fde33a9b856d52ac1ea2018",
    "semantic_title": "an unsupervised way to understand artifact generating internal units in generative neural networks",
    "citation_count": 1,
    "authors": [
      "Haedong Jeong",
      "Jiyeon Han",
      "Jaesik Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19990": {
    "title": "FrePGAN: Robust Deepfake Detection Using Frequency-Level Perturbations",
    "volume": "main",
    "abstract": "Various deepfake detectors have been proposed, but challenges still exist to detect images of unknown categories or GAN models outside of the training settings. Such issues arise from the overfitting issue, which we discover from our own analysis and the previous studies to originate from the frequency-level artifacts in generated images. We find that ignoring the frequency-level artifacts can improve the detector's generalization across various GAN models, but it can reduce the model's performance for the trained GAN models. Thus, we design a framework to generalize the deepfake detector for both the known and unseen GAN models. Our framework generates the frequency-level perturbation maps to make the generated images indistinguishable from the real images. By updating the deepfake detector along with the training of the perturbation generator, our model is trained to detect the frequency-level artifacts at the initial iterations and consider the image-level irregularities at the last iterations. For experiments, we design new test scenarios varying from the training settings in GAN models, color manipulations, and object categories. Numerous experiments validate the state-of-the-art performance of our deepfake detector",
    "checked": true,
    "id": "51cd05dc52aa348f988d91a549c292a653dcdaca",
    "semantic_title": "frepgan: robust deepfake detection using frequency-level perturbations",
    "citation_count": 13,
    "authors": [
      "Yonghyun Jeong",
      "Doyeon Kim",
      "Youngmin Ro",
      "Jongwon Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19991": {
    "title": "Learning Disentangled Attribute Representations for Robust Pedestrian Attribute Recognition",
    "volume": "main",
    "abstract": "Although various methods have been proposed for pedestrian attribute recognition, most studies follow the same feature learning mechanism, \\ie, learning a shared pedestrian image feature to classify multiple attributes. However, this mechanism leads to low-confidence predictions and non-robustness of the model in the inference stage. In this paper, we investigate why this is the case. We mathematically discover that the central cause is that the optimal shared feature cannot maintain high similarities with multiple classifiers simultaneously in the context of minimizing classification loss. In addition, this feature learning mechanism ignores the spatial and semantic distinctions between different attributes. To address these limitations, we propose a novel disentangled attribute feature learning (DAFL) framework to learn a disentangled feature for each attribute, which exploits the semantic and spatial characteristics of attributes. The framework mainly consists of learnable semantic queries, a cascaded semantic-spatial cross-attention (SSCA) module, and a group attention merging (GAM) module. Specifically, based on learnable semantic queries, the cascaded SSCA module iteratively enhances the spatial localization of attribute-related regions and aggregates region features into multiple disentangled attribute features, used for classification and updating learnable semantic queries. The GAM module splits attributes into groups based on spatial distribution and utilizes reliable group attention to supervise query attention maps. Experiments on PETA, RAPv1, PA100k, and RAPv2 show that the proposed method performs favorably against state-of-the-art methods",
    "checked": true,
    "id": "c0b7cdd0e2249f9bc90b4d0218839f36c3965453",
    "semantic_title": "learning disentangled attribute representations for robust pedestrian attribute recognition",
    "citation_count": 10,
    "authors": [
      "Jian Jia",
      "Naiyu Gao",
      "Fei He",
      "Xiaotang Chen",
      "Kaiqi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19992": {
    "title": "Degrade Is Upgrade: Learning Degradation for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Low-light image enhancement aims to improve an image's visibility while keeping its visual naturalness. Different from existing methods, which tend to accomplish the relighting task directly, we investigate the intrinsic degradation and relight the low-light image while refining the details and color in two steps. Inspired by the color image formulation (diffuse illumination color plus environment illumination color), we first estimate the degradation from low-light inputs to simulate the distortion of environment illumination color, and then refine the content to recover the loss of diffuse illumination color. To this end, we propose a novel Degradation-to-Refinement Generation Network (DRGN). Its distinctive features can be summarized as 1) A novel two-step generation network for degradation learning and content refinement. It is not only superior to one-step methods, but also capable of synthesizing sufficient paired samples to benefit the model training; 2) A multi-resolution fusion network to represent the target information (degradation or contents) in a multi-scale cooperative manner, which is more effective to address the complex unmixing problems. Extensive experiments on both the enhancement task and the joint detection task have verified the effectiveness and efficiency of our proposed method, surpassing the SOTA by 1.59dB on average and 3.18\\% in mAP on the ExDark dataset. The code will be available soon",
    "checked": true,
    "id": "50b0c71a317d450330a36dac8ed962da10022399",
    "semantic_title": "degrade is upgrade: learning degradation for low-light image enhancement",
    "citation_count": 14,
    "authors": [
      "Kui Jiang",
      "Zhongyuan Wang",
      "Zheng Wang",
      "Chen Chen",
      "Peng Yi",
      "Tao Lu",
      "Chia-Wen Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19993": {
    "title": "HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images",
    "volume": "main",
    "abstract": "Multiple medical institutions collaboratively training a model using federated learning (FL) has become a promising solution for maximizing the potential of data-driven models, yet the non-independent and identically distributed (non-iid) data in medical images is still an outstanding challenge in real-world practice. The feature heterogeneity caused by diverse scanners or protocols introduces a drift in the learning process, in both local (client) and global (server) optimizations, which harms the convergence as well as model performance. Many previous works have attempted to address the non-iid issue by tackling the drift locally or globally, but how to jointly solve the two essentially coupled drifts is still unclear. In this work, we concentrate on handling both local and global drifts and introduce a new harmonizing framework called HarmoFL. First, we propose to mitigate the local update drift by normalizing amplitudes of images transformed into the frequency domain to mimic a unified imaging setting, in order to generate a harmonized feature space across local clients. Second, based on harmonized features, we design a client weight perturbation guiding each local model to reach a flat optimum, where a neighborhood area of the local optimal solution has a uniformly low loss. Without any extra communication cost, the perturbation assists the global model to optimize towards a converged optimal solution by aggregating several local flat optima. We have theoretically analyzed the proposed method and empirically conducted extensive experiments on three medical image classification and segmentation tasks, showing that HarmoFL outperforms a set of recent state-of-the-art methods with promising convergence behavior. Code is available at: https://github.com/med-air/HarmoFL",
    "checked": true,
    "id": "62c35d9f6ffaf3dad56b1407292de47527ae72f5",
    "semantic_title": "harmofl: harmonizing local and global drifts in federated learning on heterogeneous medical images",
    "citation_count": 39,
    "authors": [
      "Meirui Jiang",
      "Zirui Wang",
      "Qi Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19994": {
    "title": "Coarse-to-Fine Generative Modeling for Graphic Layouts",
    "volume": "main",
    "abstract": "Even though graphic layout generation has attracted growing attention recently, it is still challenging to synthesis realistic and diverse layouts, due to the complicated element relationships and varied element arrangements. In this work, we seek to improve the performance of layout generation by incorporating the concept of regions, which consist of a smaller number of elements and appears like a simple layout, into the generation process. Specifically, we leverage Variational Autoencoder (VAE) as the overall architecture and decompose the decoding process into two stages. The first stage predicts representations for regions, and the second stage fills in the detailed position for each element within the region based on the predicted region representation. Compared to prior studies that merely abstract the layout into a list of elements and generate all the element positions in one go, our approach has at least two advantages. First, by the two-stage decoding, our approach decouples the complex layout generation task into several simple layout generation tasks, which reduces the problem difficulty. Second, the predicted regions can help the model roughly know what the graphic layout looks like and serve as global context to improve the generation of detailed element positions. Qualitative and quantitative experiments demonstrate that our approach significantly outperforms the existing methods, especially on the complex graphic layouts",
    "checked": true,
    "id": "bfa186e680d02ab0c161d9400c0f151b5f7c73ac",
    "semantic_title": "coarse-to-fine generative modeling for graphic layouts",
    "citation_count": 16,
    "authors": [
      "Zhaoyun Jiang",
      "Shizhao Sun",
      "Jihua Zhu",
      "Jian-Guang Lou",
      "Dongmei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19995": {
    "title": "DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior",
    "volume": "main",
    "abstract": "RGB-NIR fusion is a promising method for low-light imaging. However, high-intensity noise in low-light images amplifies the effect of structure inconsistency between RGB-NIR images, which fails existing algorithms. To handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net (DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior (DIP). The Deep Structure extracts clear structure details in deep multiscale feature space rather than raw input space, which is more robust to noisy inputs. Based on the deep structures from both RGB and NIR domains, we introduce the DIP to leverage the structure inconsistency to guide the fusion of RGB-NIR. Benefits from this, the proposed DVN obtains high-quality low-light images without the visual artifacts. We also propose a new dataset called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as the first public RGB-NIR fusion benchmark. Quantitative and qualitative results on the proposed benchmark show that DVN significantly outperforms other comparison algorithms in PSNR and SSIM, especially in extremely low light conditions",
    "checked": true,
    "id": "dee897c638f77086c46644e3e214c5a7de49b37c",
    "semantic_title": "darkvisionnet: low-light imaging via rgb-nir fusion with deep inconsistency prior",
    "citation_count": 10,
    "authors": [
      "Shuangping Jin",
      "Bingbing Yu",
      "Minhao Jing",
      "Yi Zhou",
      "Jiajun Liang",
      "Renhe Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19996": {
    "title": "LAGConv: Local-Context Adaptive Convolution Kernels with Global Harmonic Bias for Pansharpening",
    "volume": "main",
    "abstract": "Pansharpening is a critical yet challenging low-level vision task that aims to obtain a higher-resolution image by fusing a multispectral (MS) image and a panchromatic (PAN) image. While most pansharpening methods are based on convolutional neural network (CNN) architectures with standard convolution operations, few attempts have been made with context-adaptive/dynamic convolution, which delivers impressive results on high-level vision tasks. In this paper, we propose a novel strategy to generate local-context adaptive (LCA) convolution kernels and introduce a new global harmonic (GH) bias mechanism, exploiting image local specificity as well as integrating global information, dubbed LAGConv. The proposed LAGConv can replace the standard convolution that is context-agnostic to fully perceive the particularity of each pixel for the task of remote sensing pansharpening. Furthermore, by applying the LAGConv, we provide an image fusion network architecture, which is more effective than conventional CNN-based pansharpening approaches. The superiority of the proposed method is demonstrated by extensive experiments implemented on a wide range of datasets compared with state-of-the-art pansharpening methods. Besides, more discussions testify that the proposed LAGConv outperforms recent adaptive convolution techniques for pansharpening",
    "checked": true,
    "id": "c3dec38d79c888cbd5b369390af2f65ccc685ec1",
    "semantic_title": "lagconv: local-context adaptive convolution kernels with global harmonic bias for pansharpening",
    "citation_count": 16,
    "authors": [
      "Zi-Rong Jin",
      "Tian-Jing Zhang",
      "Tai-Xiang Jiang",
      "Gemine Vivone",
      "Liang-Jian Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19997": {
    "title": "Learning the Dynamics of Visual Relational Reasoning via Reinforced Path Routing",
    "volume": "main",
    "abstract": "Reasoning is a dynamic process. In cognitive theories, the dynamics of reasoning refers to reasoning states over time after successive state transitions. Modeling the cognitive dynamics is of utmost importance to simulate human reasoning capability. In this paper, we propose to learn the reasoning dynamics of visual relational reasoning by casting it as a path routing task. We present a reinforced path routing method that represents an input image via a structured visual graph and introduces a reinforcement learning based model to explore paths (sequences of nodes) over the graph based on an input sentence to infer reasoning results. By exploring such paths, the proposed method represents reasoning states clearly and characterizes state transitions explicitly to fully model the reasoning dynamics for accurate and transparent visual relational reasoning. Extensive experiments on referring expression comprehension and visual question answering demonstrate the effectiveness of our method",
    "checked": true,
    "id": "6bb962d694f6be9a5c74cd9a18d0cf2a6f6319d7",
    "semantic_title": "learning the dynamics of visual relational reasoning via reinforced path routing",
    "citation_count": 2,
    "authors": [
      "Chenchen Jing",
      "Yunde Jia",
      "Yuwei Wu",
      "Chuanhao Li",
      "Qi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19998": {
    "title": "Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "Graph Convolutional Networks (GCNs) have been widely used to model the high-order dynamic dependencies for skeleton-based action recognition. Most existing approaches do not explicitly embed the high-order spatio-temporal importance to joints' spatial connection topology and intensity, and they do not have direct objectives on their attention module to jointly learn when and where to focus on in the action sequence. To address these problems, we propose the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition framework that utilizes the spatio-temporal gradient to focus on relevant spatio-temporal features. We first propose the STF modules with learnable gradient-enforced and instance-dependent adjacency matrices to model the high-order spatio-temporal dynamics. Second, we propose three loss terms defined on the gradient-based spatio-temporal focus to explicitly guide the classifier when and where to look at, distinguish confusing classes, and optimize the stacked STF modules. STF outperforms the state-of-the-art methods on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all 15 settings over different views, subjects, setups, and input modalities, and STF also shows better accuracy on scarce data and dataset shifting settings",
    "checked": true,
    "id": "1bf21ec7f4371b28f8222cfa7e05e00893fc8961",
    "semantic_title": "towards to-a-t spatio-temporal focus for skeleton-based action recognition",
    "citation_count": 11,
    "authors": [
      "Lipeng Ke",
      "Kuan-Chuan Peng",
      "Siwei Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/19999": {
    "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition",
    "volume": "main",
    "abstract": "Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications. In this work, we present a light-weight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image. The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints. In addition, MODNet includes two novel techniques for improving model efficiency and robustness. First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation. Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MODNet to real-world data to address the domain shift problem common to trimap-free methods. MODNet is easy to be trained in an end-to-end manner. It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us. Further, MODNet achieves remarkable results on daily photos and videos",
    "checked": true,
    "id": "9d953680e9231c1bf88b890f6276169859349d28",
    "semantic_title": "modnet: real-time trimap-free portrait matting via objective decomposition",
    "citation_count": 62,
    "authors": [
      "Zhanghan Ke",
      "Jiayu Sun",
      "Kaican Li",
      "Qiong Yan",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20000": {
    "title": "Learning Mixture of Domain-Specific Experts via Disentangled Factors for Autonomous Driving",
    "volume": "main",
    "abstract": "Since human drivers only consider the driving-related factors that affect vehicle control depending on the situation, they can drive safely even in diverse driving environments. To mimic this behavior, we propose an autonomous driving framework based on the two-stage representation learning that initially splits the latent features as domain-specific features and domain-general features. Subsequently, the dynamic-object features, which contain information of dynamic objects, are disentangled from latent features using mutual information estimator. In this study, the problem in behavior cloning is divided into several domain-specific subspaces, with experts becoming specialized on each domain-specific policy. The proposed mixture of domain-specific experts (MoDE) model predicts the final control values through the cooperation of experts using a gating function. The domain-specific features are used to calculate the importance weight of the domain-specific experts, and the disentangled domain-general and dynamic-object features are applied in estimating the control values. To validate the proposed MoDE model, we conducted several experiments and achieved a higher success rate on the CARLA benchmarks under several conditions and tasks than state-of-the-art approaches",
    "checked": true,
    "id": "03aaf8d61bb6ee4e5887e5839487de22e46f71bd",
    "semantic_title": "learning mixture of domain-specific experts via disentangled factors for autonomous driving",
    "citation_count": 1,
    "authors": [
      "Inhan Kim",
      "Joonyeong Lee",
      "Daijin Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20001": {
    "title": "Towards Versatile Pedestrian Detector with Multisensory-Matching and Multispectral Recalling Memory",
    "volume": "main",
    "abstract": "Recently, automated surveillance cameras can change a visible sensor and a thermal sensor for all-day operation. However, existing single-modal pedestrian detectors mainly focus on detecting pedestrians in only one specific modality (i.e., visible or thermal), so they cannot cope with other modal inputs. In addition, recent multispectral pedestrian detectors have shown remarkable performance by adopting multispectral modalities, but they also have limitations in practical applications (e.g., different Field-of-View (FoV) and frame rate). In this paper, we introduce a versatile pedestrian detector that shows robust detection performance in any single modality. We propose a multisensory-matching contrastive loss to reduce the difference between the visual representation of pedestrians in the visible and thermal modalities. Moreover, for the robust detection on a single modality, we design a Multispectral Recalling (MSR) Memory. The MSR Memory enhances the visual representation of the single modal features by recalling that of the multispectral modalities. To guide the MSR Memory to store the multispectral modal contexts, we introduce a multispectral recalling loss. It enables the pedestrian detector to encode more discriminative features with a single input modality. We believe our method is a step forward detector that can be applied to a variety of real-world applications. The comprehensive experimental results verify the effectiveness of the proposed method",
    "checked": true,
    "id": "622be6e7b8402659fb0ecf3ed3ddd7a30fd4ed8e",
    "semantic_title": "towards versatile pedestrian detector with multisensory-matching and multispectral recalling memory",
    "citation_count": 8,
    "authors": [
      "Jung Uk Kim",
      "Sungjune Park",
      "Yong Man Ro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20002": {
    "title": "Semantic Feature Extraction for Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the attribute. In this paper, we put forth a new GZSL technique that improves the GZSL classification performance greatly. Key idea of the proposed approach, henceforth referred to as semantic feature extraction-based GZSL (SE-GZSL), is to use the semantic feature containing only attribute-related information in learning the relationship between the image and the attribute. In doing so, we can remove the interference, if any, caused by the attribute-irrelevant information contained in the image feature. To train a network extracting the semantic feature, we present two novel loss functions, 1) mutual information-based loss to capture all the attribute-related information in the image feature and 2) similarity-based loss to remove unwanted attribute-irrelevant information. From extensive experiments using various datasets, we show that the proposed SE-GZSL technique outperforms conventional GZSL approaches by a large margin",
    "checked": true,
    "id": "ebeae49c085faf85548079c9f1af9cafdce81037",
    "semantic_title": "semantic feature extraction for generalized zero-shot learning",
    "citation_count": 8,
    "authors": [
      "Junhan Kim",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20003": {
    "title": "Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading",
    "volume": "main",
    "abstract": "Recognizing speech from silent lip movement, which is called lip reading, is a challenging task due to 1) the inherent information insufficiency of lip movement to fully represent the speech, and 2) the existence of homophenes that have similar lip movement with different pronunciations. In this paper, we try to alleviate the aforementioned two challenges in lip reading by proposing a Multi-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual datasets and remembers audio representations by modelling the inter-relationships of paired audio-visual representations. At the inference stage, visual input alone can extract the saved audio representation from the memory by examining the learned inter-relationships. Therefore, the lip reading model can complement the insufficient visual information with the extracted audio representations. Secondly, MVM is composed of multi-head key memories for saving visual features and one value memory for saving audio knowledge, which is designed to distinguish the homophenes. With the multi-head key memories, MVM extracts possible candidate audio features from the memory, which allows the lip reading model to consider the possibility of which pronunciations can be represented from the input lip movement. This also can be viewed as an explicit implementation of the one-to-many mapping of viseme-to-phoneme. Moreover, MVM is employed in multi-temporal levels to consider the context when retrieving the memory and distinguish the homophenes. Extensive experimental results verify the effectiveness of the proposed method in lip reading and in distinguishing the homophenes",
    "checked": true,
    "id": "56d80a28d2a1750511ed9d4f5d9b89e0f1bc0c1c",
    "semantic_title": "distinguishing homophenes using multi-head visual-audio memory for lip reading",
    "citation_count": 29,
    "authors": [
      "Minsu Kim",
      "Jeong Hun Yeo",
      "Yong Man Ro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20004": {
    "title": "Deep Translation Prior: Test-Time Training for Photorealistic Style Transfer",
    "volume": "main",
    "abstract": "Recent techniques to solve photorealistic style transfer within deep convolutional neural networks (CNNs) generally require intensive training from large-scale datasets, thus having limited applicability and poor generalization ability to unseen images or styles. To overcome this, we propose a novel framework, dubbed Deep Translation Prior (DTP), to accomplish photorealistic style transfer through test-time training on given input image pair with untrained networks, which learns an image pair-specific translation prior and thus yields better performance and generalization. Tailored for such test-time training for style transfer, we present novel network architectures, with two sub-modules of correspondence and generation modules, and loss functions consisting of contrastive content, style, and cycle consistency losses. Our framework does not require offline training phase for style transfer, which has been one of the main challenges in existing methods, but the networks are to be solely learned during test time. Experimental results prove that our framework has a better generalization ability to unseen image pairs and even outperforms the state-of-the-art methods",
    "checked": true,
    "id": "f1ee79f49040dd9279d6a8173430c865b8027f39",
    "semantic_title": "deep translation prior: test-time training for photorealistic style transfer",
    "citation_count": 8,
    "authors": [
      "Sunwoo Kim",
      "Soohyun Kim",
      "Seungryong Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20005": {
    "title": "PrivateSNN: Privacy-Preserving Spiking Neural Networks",
    "volume": "main",
    "abstract": "How can we bring both privacy and energy-efficiency to a neural system? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage is caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using the generated images. However, converted SNNs remain vulnerable to class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes SNNs difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN eliminates data and class leakage issues with a slight performance drop (less than ~2%) and significant energy-efficiency gain (about 55x) compared to the standard ANN. We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training",
    "checked": true,
    "id": "6b0867a5d61f66f594c31968d38a6e1901f612c7",
    "semantic_title": "privatesnn: privacy-preserving spiking neural networks",
    "citation_count": 10,
    "authors": [
      "Youngeun Kim",
      "Yeshwanth Venkatesha",
      "Priyadarshini Panda"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20006": {
    "title": "NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency",
    "volume": "main",
    "abstract": "We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data. In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further. With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 and show that our images are more consistent with original data distribution than prior works by visualization and additional analysis. Furthermore, our synthesized images outperform prior works on various applications such as knowledge distillation and pruning, demonstrating the effectiveness of our proposed method",
    "checked": true,
    "id": "6b1ac12dde02e4894c08fa7f4c340bd4911dc979",
    "semantic_title": "naturalinversion: data-free image synthesis improving real-world consistency",
    "citation_count": 2,
    "authors": [
      "Yujin Kim",
      "Dogyun Park",
      "Dohee Kim",
      "Suhyun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20007": {
    "title": "Joint 3D Object Detection and Tracking Using Spatio-Temporal Representation of Camera Image and LiDAR Point Clouds",
    "volume": "main",
    "abstract": "In this paper, we propose a new joint object detection and tracking (JoDT) framework for 3D object detection and tracking based on camera and LiDAR sensors. The proposed method, referred to as 3D DetecTrack, enables the detector and tracker to cooperate to generate a spatio-temporal representation of the camera and LiDAR data, with which 3D object detection and tracking are then performed. The detector constructs the spatio-temporal features via the weighted temporal aggregation of the spatial features obtained by the camera and LiDAR fusion. Then, the detector reconfigures the initial detection results using information from the tracklets maintained up to the previous time step. Based on the spatio-temporal features generated by the detector, the tracker associates the detected objects with previously tracked objects using a graph neural network (GNN). We devise a fully-connected GNN facilitated by a combination of rule-based edge pruning and attention-based edge gating, which exploits both spatial and temporal object contexts to improve tracking performance. The experiments conducted on both KITTI and nuScenes benchmarks demonstrate that the proposed 3D DetecTrack achieves significant improvements in both detection and tracking performances over baseline methods and achieves state-of-the-art performance among existing methods through collaboration between the detector and tracker",
    "checked": true,
    "id": "455df8920cf3037d022a1b9e3b2eb07918a0c15a",
    "semantic_title": "joint 3d object detection and tracking using spatio-temporal representation of camera image and lidar point clouds",
    "citation_count": 5,
    "authors": [
      "Junho Koh",
      "Jaekyum Kim",
      "Jin Hyeok Yoo",
      "Yecheol Kim",
      "Dongsuk Kum",
      "Jun Won Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20008": {
    "title": "Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains. We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA. Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output. However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST). During adaptation, we employ the AST auto-encoder for two purposes. First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent. Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype. Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment. We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks",
    "checked": true,
    "id": "e59343ecddcbb10edcbb03c6c3fe3b3bb7fa5856",
    "semantic_title": "amplitude spectrum transformation for open compound domain adaptive semantic segmentation",
    "citation_count": 7,
    "authors": [
      "Jogendra Nath Kundu",
      "Akshay R Kulkarni",
      "Suvaansh Bhambri",
      "Varun Jampani",
      "Venkatesh Babu Radhakrishnan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20009": {
    "title": "Siamese Network with Interactive Transformer for Video Object Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised video object segmentation (VOS) refers to segmenting the target object in remaining frames given its annotation in the first frame, which has been actively studied in recent years. The key challenge lies in finding effective ways to exploit the spatio-temporal context of past frames to help learn discriminative target representation of current frame. In this paper, we propose a novel Siamese network with a specifically designed interactive transformer, called SITVOS, to enable effective context propagation from historical to current frames. Technically, we use the transformer encoder and decoder to handle the past frames and current frame separately, i.e., the encoder encodes robust spatio-temporal context of target object from the past frames, while the decoder takes the feature embedding of current frame as the query to retrieve the target from the encoder output. To further enhance the target representation, a feature interaction module (FIM) is devised to promote the information flow between the encoder and decoder. Moreover, we employ the Siamese architecture to extract backbone features of both past and current frames, which enables feature reuse and is more efficient than existing methods. Experimental results on three challenging benchmarks validate the superiority of SITVOS over state-of-the-art methods. Code is available at https://github.com/LANMNG/SITVOS",
    "checked": true,
    "id": "a5d61a0f974f82872b8c17257badf4850faba40d",
    "semantic_title": "siamese network with interactive transformer for video object segmentation",
    "citation_count": 17,
    "authors": [
      "Meng Lan",
      "Jing Zhang",
      "Fengxiang He",
      "Lefei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20010": {
    "title": "Adversarial Attack for Asynchronous Event-Based Data",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model",
    "checked": true,
    "id": "fc1fc9461506b470380e92ccb1853af5c8504916",
    "semantic_title": "adversarial attack for asynchronous event-based data",
    "citation_count": 2,
    "authors": [
      "Wooju Lee",
      "Hyun Myung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20011": {
    "title": "Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier",
    "volume": "main",
    "abstract": "Unsupervised video object segmentation (UVOS) is a per-pixel binary labeling problem which aims at separating the foreground object from the background in the video without using the ground truth (GT) mask of the foreground object. Most of the previous UVOS models use the first frame or the entire video as a reference frame to specify the mask of the foreground object. Our question is why the first frame should be selected as a reference frame or why the entire video should be used to specify the mask. We believe that we can select a better reference frame to achieve the better UVOS performance than using only the first frame or the entire video as a reference frame. In our paper, we propose Easy Frame Selector (EFS). The EFS enables us to select an \"easy\" reference frame that makes the subsequent VOS become easy, thereby improving the VOS performance. Furthermore, we propose a new framework named as Iterative Mask Prediction (IMP). In the framework, we repeat applying EFS to the given video and selecting an \"easier\" reference frame from the video than the previous iteration, increasing the VOS performance incrementally. The IMP consists of EFS, Bi-directional Mask Prediction (BMP), and Temporal Information Updating (TIU). From the proposed framework, we achieve state-of-the-art performance in three UVOS benchmark sets: DAVIS16, FBMS, and SegTrack-V2",
    "checked": true,
    "id": "1b94586911f6db447c6c19ea0fd8735feba64180",
    "semantic_title": "iteratively selecting an easy reference frame makes unsupervised video object segmentation easier",
    "citation_count": 18,
    "authors": [
      "Youngjo Lee",
      "Hongje Seong",
      "Euntai Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20012": {
    "title": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation",
    "volume": "main",
    "abstract": "We propose a novel scene flow estimation approach to capture and infer 3D motions from point clouds. Estimating 3D motions for point clouds is challenging, since a point cloud is unordered and its density is significantly non-uniform. Such unstructured data poses difficulties in matching corresponding points between point clouds, leading to inaccurate flow estimation. We propose a novel architecture named Sparse Convolution-Transformer Network (SCTN) that equips the sparse convolution with the transformer. Specifically, by leveraging the sparse convolution, SCTN transfers irregular point cloud into locally consistent flow features for estimating spatially consistent motions within an object/local object part. We further propose to explicitly learn point relations using a point transformer module, different from exiting methods. We show that the learned relation-based contextual information is rich and helpful for matching corresponding points, benefiting scene flow estimation. In addition, a novel loss function is proposed to adaptively encourage flow consistency according to feature similarity. Extensive experiments demonstrate that our proposed approach achieves a new state of the art in scene flow estimation. Our approach achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene Flow respectively, which significantly outperforms previous methods by large margins",
    "checked": true,
    "id": "df9dfbe775df0c66a57308ec52900a590a92c9f7",
    "semantic_title": "sctn: sparse convolution-transformer network for scene flow estimation",
    "citation_count": 20,
    "authors": [
      "Bing Li",
      "Cheng Zheng",
      "Silvio Giancola",
      "Bernard Ghanem"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20013": {
    "title": "Shrinking Temporal Attention in Transformers for Video Action Recognition",
    "volume": "main",
    "abstract": "Spatiotemporal modeling in an unified architecture is key for video action recognition. This paper proposes a Shrinking Temporal Attention Transformer (STAT), which efficiently builts spatiotemporal attention maps considering the attenuation of spatial attention in short and long temporal sequences. Specifically, for short-term temporal tokens, query token interacts with them in a fine-grained manner in dealing with short-range motion. It then shrinks to a coarse attention in neighborhood for long-term tokens, to provide larger receptive field for long-range spatial aggregation. Both of them are composed in a short-long temporal integrated block to build visual appearances and temporal structure concurrently with lower costly in computation. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple action recognition benchmarks including Kinetics400 and Something-Something v2, outperforming prior methods with 50% less FLOPs and without any pretrained model",
    "checked": true,
    "id": "339fc2130705f4d3d5fb59fb87dc0e1c9cacfb15",
    "semantic_title": "shrinking temporal attention in transformers for video action recognition",
    "citation_count": 5,
    "authors": [
      "Bonan Li",
      "Pengfei Xiong",
      "Congying Han",
      "Tiande Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20014": {
    "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer",
    "volume": "main",
    "abstract": "Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, i.e., a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer. Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively. Moreover, the proposed DanceFormer, together with the PhantomDance dataset, are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications",
    "checked": true,
    "id": "b5ca56d9bab64445ddf66e724616e4b09f3cb110",
    "semantic_title": "danceformer: music conditioned 3d dance generation with parametric motion transformer",
    "citation_count": 45,
    "authors": [
      "Buyu Li",
      "Yongchi Zhao",
      "Shi Zhelun",
      "Lu Sheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20015": {
    "title": "Interpretable Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs). This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts. Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images. The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts. The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer. Our method can be broadly applied to different types of GANs. Experiments have demonstrated the effectiveness of our method",
    "checked": true,
    "id": "e16f5b41141355051c60b1ce0c4ba06ac96241dd",
    "semantic_title": "interpretable generative adversarial networks",
    "citation_count": 7,
    "authors": [
      "Chao Li",
      "Kelu Yao",
      "Jin Wang",
      "Boyu Diao",
      "Yongjun Xu",
      "Quanshi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20016": {
    "title": "Cross-Modal Object Tracking: Modality-Aware Representations and a Unified Benchmark",
    "volume": "main",
    "abstract": "In many visual systems, visual tracking often bases on RGB image sequences, in which some targets are invalid in low-light conditions, and tracking performance is thus affected significantly. Introducing other modalities such as depth and infrared data is an effective way to handle imaging limitations of individual sources, but multi-modal imaging platforms usually require elaborate designs and cannot be applied in many real-world applications at present. Near-infrared (NIR) imaging becomes an essential part of many surveillance cameras, whose imaging is switchable between RGB and NIR based on the light intensity. These two modalities are heterogeneous with very different visual properties and thus bring big challenges for visual tracking. However, existing works have not studied this challenging problem. In this work, we address the cross-modal object tracking problem and contribute a new video dataset, including 654 cross-modal image sequences with over 481K frames in total, and the average video length is more than 735 frames. To promote the research and development of cross-modal object tracking, we propose a new algorithm, which learns the modality-aware target representation to mitigate the appearance gap between RGB and NIR modalities in the tracking process. It is plug-and-play and could thus be flexibly embedded into different tracking frameworks. Extensive experiments on the dataset are conducted, and we demonstrate the effectiveness of the proposed algorithm in two representative tracking frameworks against 19 state-of-the-art tracking methods. Dataset, code, model and results are available at https://github.com/mmic-lcl/source-code",
    "checked": true,
    "id": "e3f52c0976f1e4492a875527e4292f4f1d399672",
    "semantic_title": "cross-modal object tracking: modality-aware representations and a unified benchmark",
    "citation_count": 0,
    "authors": [
      "Chenglong Li",
      "Tianhao Zhu",
      "Lei Liu",
      "Xiaonan Si",
      "Zilin Fan",
      "Sulan Zhai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20017": {
    "title": "You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "We present YOFO (You Only inFer Once), a new paradigm for referring video object segmentation (RVOS) that operates in an one-stage manner. Our key insight is that the language descriptor should serve as target-specific guidance to identify the target object, while a direct feature fusion of image and language can increase feature complexity and thus may be sub-optimal for RVOS. To this end, we propose a meta-transfer module, which is trained in a learning-to-learn fashion and aims to transfer the target-specific information from the language domain to the image domain, while discarding the uncorrelated complex variations of language description. To bridge the gap between the image and language domains, we develop a multi-scale cross-modal feature mining block that aggregates all the essential features required by RVOS from both domains and generates regression labels for the meta-transfer module. The whole system can be trained in an end-to-end manner and shows competitive performance against state-of-the-art two-stage approaches",
    "checked": true,
    "id": "c5dfab50eaecf6e2fde7bb8858e90733515b8ce1",
    "semantic_title": "you only infer once: cross-modal meta-transfer for referring video object segmentation",
    "citation_count": 16,
    "authors": [
      "Dezhuang Li",
      "Ruoqi Li",
      "Lijun Wang",
      "Yifan Wang",
      "Jinqing Qi",
      "Lu Zhang",
      "Ting Liu",
      "Qingquan Xu",
      "Huchuan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20018": {
    "title": "Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-Guided Feature Imitation",
    "volume": "main",
    "abstract": "Knowledge Distillation (KD) is a widely-used technology to inherit information from cumbersome teacher models to compact student models, consequently realizing model compression and acceleration. Compared with image classification, object detection is a more complex task, and designing specific KD methods for object detection is non-trivial. In this work, we elaborately study the behaviour difference between the teacher and student detection models, and obtain two intriguing observations: First, the teacher and student rank their detected candidate boxes quite differently, which results in their precision discrepancy. Second, there is a considerable gap between the feature response differences and prediction differences between teacher and student, indicating that equally imitating all the feature maps of the teacher is the sub-optimal choice for improving the student's accuracy. Based on the two observations, we propose Rank Mimicking (RM) and Prediction-guided Feature Imitation (PFI) for distilling one-stage detectors, respectively. RM takes the rank of candidate boxes from teachers as a new form of knowledge to distill, which consistently outperforms the traditional soft label distillation. PFI attempts to correlate feature differences with prediction differences, making feature imitation directly help to improve the student's accuracy. On MS COCO and PASCAL VOC benchmarks, extensive experiments are conducted on various detectors with different backbones to validate the effectiveness of our method. Specifically, RetinaNet with ResNet50 achieves 40.4% mAP on MS COCO, which is 3.5% higher than its baseline, and also outperforms previous KD methods",
    "checked": true,
    "id": "998603aeedb725924fe980f6f6d8044a72fe08dc",
    "semantic_title": "knowledge distillation for object detection via rank mimicking and prediction-guided feature imitation",
    "citation_count": 29,
    "authors": [
      "Gang Li",
      "Xiang Li",
      "Yujie Wang",
      "Shanshan Zhang",
      "Yichao Wu",
      "Ding Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20019": {
    "title": "Rethinking Pseudo Labels for Semi-supervised Object Detection",
    "volume": "main",
    "abstract": "Recent advances in semi-supervised object detection (SSOD) are largely driven by consistency-based pseudo-labeling methods for image classification tasks, producing pseudo labels as supervisory signals. However, when using pseudo labels, there is a lack of consideration in localization precision and amplified class imbalance, both of which are critical for detection tasks. In this paper, we introduce certainty-aware pseudo labels tailored for object detection, which can effectively estimate the classification and localization quality of derived pseudo labels. This is achieved by converting conventional localization as a classification task followed by refinement. Conditioned on classification and localization quality scores, we dynamically adjust the thresholds used to generate pseudo labels and reweight loss functions for each category to alleviate the class imbalance problem. Extensive experiments demonstrate that our method improves state-of-the-art SSOD performance by 1-2% AP on COCO and PASCAL VOC while being orthogonal and complementary to most existing methods. In the limited-annotation regime, our approach improves supervised baselines by up to 10% AP using only 1-10% labeled data from COCO",
    "checked": true,
    "id": "1143a3c855a6536448352754b6908db2fd87985f",
    "semantic_title": "rethinking pseudo labels for semi-supervised object detection",
    "citation_count": 41,
    "authors": [
      "Hengduo Li",
      "Zuxuan Wu",
      "Abhinav Shrivastava",
      "Larry S. Davis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20020": {
    "title": "Action-Aware Embedding Enhancement for Image-Text Retrieval",
    "volume": "main",
    "abstract": "Image-text retrieval plays a central role in bridging vision and language, which aims to reduce the semantic discrepancy between images and texts. Most of existing works rely on refined words and objects representation through the data-oriented method to capture the word-object cooccurrence. Such approaches are prone to ignore the asymmetric action relation between images and texts, that is, the text has explicit action representation (i.e., verb phrase) while the image only contains implicit action information. In this paper, we propose Action-aware Memory-Enhanced embedding (AME) method for image-text retrieval, which aims to emphasize the action information when mapping the images and texts into a shared embedding space. Specifically, we integrate action prediction along with an action-aware memory bank to enrich the image and text features with action-similar text features. The effectiveness of our proposed AME method is verified by comprehensive experimental results on two benchmark datasets",
    "checked": true,
    "id": "09504b7989824c5ecca84f0a06ac39f55bfc656d",
    "semantic_title": "action-aware embedding enhancement for image-text retrieval",
    "citation_count": 5,
    "authors": [
      "Jiangtong Li",
      "Li Niu",
      "Liqing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20021": {
    "title": "Retinomorphic Object Detection in Asynchronous Visual Streams",
    "volume": "main",
    "abstract": "Due to high-speed motion blur and challenging illumination, conventional frame-based cameras have encountered an important challenge in object detection tasks. Neuromorphic cameras that output asynchronous visual streams instead of intensity frames, by taking the advantage of high temporal resolution and high dynamic range, have brought a new perspective to address the challenge. In this paper, we propose a novel problem setting, retinomorphic object detection, which is the first trial that integrates foveal-like and peripheral-like visual streams. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-Vidar-DVS) over 215.5k spatio-temporal synchronized labels. Then, we design temporal aggregation representations to preserve the spatio-temporal information from asynchronous visual streams. Finally, we present a novel bio-inspired unifying framework to fuse two sensing modalities via a dynamic interaction mechanism. Our experimental evaluation shows that our approach has significant improvements over the state-of-the-art methods with the single-modality, especially in high-speed motion and low-light scenarios. We hope that our work will attract further research into this newly identified, yet crucial research direction. Our dataset can be available at https://www.pkuml.org/resources/pku-vidar-dvs.html",
    "checked": true,
    "id": "df446afcb25605d5f176c2dba4e2e6032dfa8652",
    "semantic_title": "retinomorphic object detection in asynchronous visual streams",
    "citation_count": 6,
    "authors": [
      "Jianing Li",
      "Xiao Wang",
      "Lin Zhu",
      "Jia Li",
      "Tiejun Huang",
      "Yonghong Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20022": {
    "title": "Learning from Weakly-Labeled Web Videos via Exploring Sub-concepts",
    "volume": "main",
    "abstract": "Learning visual knowledge from massive weakly-labeled web videos has attracted growing research interests thanks to the large corpus of easily accessible video data on the Internet. However, for video action recognition, the action of interest might only exist in arbitrary clips of untrimmed web videos, resulting in high label noises in the temporal space. To address this challenge, we introduce a new method for pre-training video action recognition models using queried web videos. Instead of trying to filter out potential noises, we propose to provide fine-grained supervision signals by defining the concept of Sub-Pseudo Label (SPL). Specifically, SPL spans out a new set of meaningful \"middle ground\" label space constructed by extrapolating the original weak labels during video querying and the prior knowledge distilled from a teacher model. Consequently, SPL provides enriched supervision for video models to learn better representations and improves data utilization efficiency of untrimmed videos. We validate the effectiveness of our method on four video action recognition datasets and a weakly-labeled image dataset. Experiments show that SPL outperforms several existing pre-training strategies and the learned representations lead to competitive results on several benchmarks",
    "checked": true,
    "id": "acc94eae52e80bfadf3296ada569fad6427d4e5f",
    "semantic_title": "learning from weakly-labeled web videos via exploring sub-concepts",
    "citation_count": 1,
    "authors": [
      "Kunpeng Li",
      "Zizhao Zhang",
      "Guanhang Wu",
      "Xuehan Xiong",
      "Chen-Yu Lee",
      "Zhichao Lu",
      "Yun Fu",
      "Tomas Pfister"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20023": {
    "title": "Learning Universal Adversarial Perturbation by Adversarial Example",
    "volume": "main",
    "abstract": "Deep learning models have shown to be susceptible to universal adversarial perturbation (UAP), which has aroused wide concerns in the community. Compared with the conventional adversarial attacks that generate adversarial samples at the instance level, UAP can fool the target model for different instances with only a single perturbation, enabling us to evaluate the robustness of the model from a more effective and accurate perspective. The existing universal attack methods fail to exploit the differences and connections between the instance and universal levels to produce dominant perturbations. To address this challenge, we propose a new universal attack method that unifies instance-specific and universal attacks from a feature perspective to generate a more dominant UAP. Specifically, we reformulate the UAP generation task as a minimax optimization problem and then utilize the instance-specific attack method to solve the minimization problem thereby obtaining better training data for generating UAP. At the same time, we also introduce a consistency regularizer to explore the relationship between training data, thus further improving the dominance of the generated UAP. Furthermore, our method is generic with no additional assumptions about the training data and hence can be applied to both data-dependent (supervised) and data-independent (unsupervised) manners. Extensive experiments demonstrate that the proposed method improves the performance by a significant margin over the existing methods in both data-dependent and data-independent settings. Code is available at https://github.com/lisenxd/AT-UAP",
    "checked": true,
    "id": "1935668faf67ed68c6320d45144785ce4256fdf0",
    "semantic_title": "learning universal adversarial perturbation by adversarial example",
    "citation_count": 6,
    "authors": [
      "Maosen Li",
      "Yanhua Yang",
      "Kun Wei",
      "Xu Yang",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20024": {
    "title": "Logit Perturbation",
    "volume": "main",
    "abstract": "Features, logits, and labels are the three primary data when a sample passes through a deep neural network. Feature perturbation and label perturbation receive increasing attention in recent years. They have been proven to be useful in various deep learning approaches. For example, (adversarial) feature perturbation can improve the robustness or even generalization capability of learned models. However, limited studies have explicitly explored for the perturbation of logit vectors. This work discusses several existing methods related to logit perturbation. Based on a unified viewpoint between positive/negative data augmentation and loss variations incurred by logit perturbation, a new method is proposed to explicitly learn to perturb logits. A comparative analysis is conducted for the perturbations used in our and existing methods. Extensive experiments on benchmark image classification data sets and their long-tail versions indicated the competitive performance of our learning method. In addition, existing methods can be further improved by utilizing our method",
    "checked": true,
    "id": "69f2e76490f94b5a6433f7513f92e52115a9a2ec",
    "semantic_title": "logit perturbation",
    "citation_count": 3,
    "authors": [
      "Mengyang Li",
      "Fengguang Su",
      "Ou Wu",
      "Ji Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20025": {
    "title": "Neighborhood-Adaptive Structure Augmented Metric Learning",
    "volume": "main",
    "abstract": "Most metric learning techniques typically focus on sample embedding learning, while implicitly assume a homogeneous local neighborhood around each sample, based on the metrics used in training ( e.g., hypersphere for Euclidean distance or unit hyperspherical crown for cosine distance). As real-world data often lies on a low-dimensional manifold curved in a high-dimensional space, it is unlikely that everywhere of the manifold shares the same local structures in the input space. Besides, considering the non-linearity of neural networks, the local structure in the output embedding space may not be homogeneous as assumed. Therefore, representing each sample simply with its embedding while ignoring its individual neighborhood structure would have limitations in Embedding-Based Retrieval (EBR). By exploiting the heterogeneity of local structures in the embedding space, we propose a Neighborhood-Adaptive Structure Augmented metric learning framework (NASA), where the neighborhood structure is realized as a structure embedding, and learned along with the sample embedding in a self-supervised manner. In this way, without any modifications, most indexing techniques can be used to support large-scale EBR with NASA embeddings. Experiments on six standard benchmarks with two kinds of embeddings, i.e., binary embeddings and real-valued embeddings, show that our method significantly improves and outperforms the state-of-the-art methods",
    "checked": true,
    "id": "9ad3694613eaa36582d812c002ee0ad7548c970a",
    "semantic_title": "neighborhood-adaptive structure augmented metric learning",
    "citation_count": 14,
    "authors": [
      "Pandeng Li",
      "Yan Li",
      "Hongtao Xie",
      "Lei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20026": {
    "title": "Stereo Neural Vernier Caliper",
    "volume": "main",
    "abstract": "We propose a new object-centric framework for learning-based stereo 3D object detection. Previous studies build scene-centric representations that do not consider the significant variation among outdoor instances and thus lack the flexibility and functionalities that an instance-level model can offer. We build such an instance-level model by formulating and tackling a local update problem, i.e., how to predict a refined update given an initial 3D cuboid guess. We demonstrate how solving this problem can complement scene-centric approaches in (i) building a coarse-to-fine multi-resolution system, (ii) performing model-agnostic object location refinement, and (iii) conducting stereo 3D tracking-by-detection. Extensive experiments demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on the KITTI benchmark. Code and pre-trained models are available at https://github.com/Nicholasli1995/SNVC",
    "checked": true,
    "id": "d941a13a9d8bba6a3ec25d115019145f74d767e8",
    "semantic_title": "stereo neural vernier caliper",
    "citation_count": 4,
    "authors": [
      "Shichao Li",
      "Zechun Liu",
      "Zhiqiang Shen",
      "Kwang-Ting Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20027": {
    "title": "EditVAE: Unsupervised Parts-Aware Controllable 3D Point Cloud Shape Generation",
    "volume": "main",
    "abstract": "This paper tackles the problem of parts-aware point cloud generation. Unlike existing works which require the point cloud to be segmented into parts a priori, our parts-aware editing and generation are performed in an unsupervised manner. We achieve this with a simple modification of the Variational Auto-Encoder which yields a joint model of the point cloud itself along with a schematic representation of it as a combination of shape primitives. In particular, we introduce a latent representation of the point cloud which can be decomposed into a disentangled representation for each part of the shape. These parts are in turn disentangled into both a shape primitive and a point cloud representation, along with a standardising transformation to a canonical coordinate system. The dependencies between our standardising transformations preserve the spatial dependencies between the parts in a manner that allows meaningful parts-aware point cloud generation and shape editing. In addition to the flexibility afforded by our disentangled representation, the inductive bias introduced by our joint modeling approach yields state-of-the-art experimental results on the ShapeNet dataset",
    "checked": false,
    "id": "88fef11717f22d9a2c2c353297ff87f3df253d16",
    "semantic_title": "editvae: unsupervised part-aware controllable 3d point cloud shape generation",
    "citation_count": 11,
    "authors": [
      "Shidi Li",
      "Miaomiao Liu",
      "Christian Walder"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20028": {
    "title": "Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Weakly supervised Video Anomaly Detection (VAD) using Multi-Instance Learning (MIL) is usually based on the fact that the anomaly score of an abnormal snippet is higher than that of a normal snippet. In the beginning of training, due to the limited accuracy of the model, it is easy to select the wrong abnormal snippet. In order to reduce the probability of selection errors, we first propose a Multi-Sequence Learning (MSL) method and a hinge-based MSL ranking loss that uses a sequence composed of multiple snippets as an optimization unit. We then design a Transformer-based MSL network to learn both video-level anomaly probability and snippet-level anomaly scores. In the inference stage, we propose to use the video-level anomaly probability to suppress the fluctuation of snippet-level anomaly scores. Finally, since VAD needs to predict the snippet-level anomaly scores, by gradually reducing the length of selected sequence, we propose a self-training strategy to gradually refine the anomaly scores. Experimental results show that our method achieves significant improvements on ShanghaiTech, UCF-Crime, and XD-Violence",
    "checked": true,
    "id": "84f3290630be8dff726ca43195ed6c82bb65d0d9",
    "semantic_title": "self-training multi-sequence learning with transformer for weakly supervised video anomaly detection",
    "citation_count": 50,
    "authors": [
      "Shuo Li",
      "Fang Liu",
      "Licheng Jiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20029": {
    "title": "TA2N: Two-Stage Action Alignment Network for Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support). The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos. In this paper, we arrest this problem from two distinct aspects -- action duration misalignment and action evolution misalignment. We address them sequentially through a Two-stage Action Alignment Network (TA2N). The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (e.g. background). Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition",
    "checked": true,
    "id": "6a0a75c1c7f2e6bf48a78f01ad7129c45154ce96",
    "semantic_title": "ta2n: two-stage action alignment network for few-shot action recognition",
    "citation_count": 28,
    "authors": [
      "Shuyuan Li",
      "Huabin Liu",
      "Rui Qian",
      "Yuxi Li",
      "John See",
      "Mengjuan Fei",
      "Xiaoyuan Yu",
      "Weiyao Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20030": {
    "title": "Best-Buddy GANs for Highly Detailed Image Super-resolution",
    "volume": "main",
    "abstract": "We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the ill-posed SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the rigid one-to-one constraint, we allow the estimated patches to dynamically seek trustworthy surrogates of supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research",
    "checked": true,
    "id": "ca8e418b42b46c024cf417912fc0a6f2f98dcfba",
    "semantic_title": "best-buddy gans for highly detailed image super-resolution",
    "citation_count": 25,
    "authors": [
      "Wenbo Li",
      "Kun Zhou",
      "Lu Qi",
      "Liying Lu",
      "Jiangbo Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20031": {
    "title": "SCAN: Cross Domain Object Detection with Semantic Conditioned Adaptation",
    "volume": "main",
    "abstract": "The domain gap severely limits the transferability and scalability of object detectors trained in a specific domain when applied to a novel one. Most existing works bridge the domain gap by minimizing the domain discrepancy in the category space and aligning category-agnostic global features. Though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level distribution. Besides, the category-agnostic alignment leads to the disagreement of class-specific distributions in the two domains, further causing inevitable classification errors. To overcome these two challenges, we propose a novel Semantic Conditioned AdaptatioN (SCAN) framework such that well-modeled unbiased semantics can support semantic conditioned adaptation for precise domain adaptive object detection. Specifically, class-specific semantics crossing different images in the source domain are graphically aggregated as the input to learn an unbiased semantic paradigm incrementally. The paradigm is then sent to a lightweight manifestation module to obtain conditional kernels to serve as the role of extracting semantics from the target domain for better adaptation. Subsequently, conditional kernels are integrated into global alignment to support the class-specific adaptation in a well-designed Conditional Kernel guided Alignment (CKA) module. Meanwhile, rich knowledge of the unbiased paradigm is transferred to the target domain with a novel Graph-based Semantic Transfer (GST) mechanism, yielding the adaptation in the category-based feature space. Comprehensive experiments conducted on three adaptation benchmarks demonstrate that SCAN outperforms existing works by a large margin",
    "checked": true,
    "id": "95ce8550d84281be6d19783b727f1e1fefe2e19c",
    "semantic_title": "scan: cross domain object detection with semantic conditioned adaptation",
    "citation_count": 19,
    "authors": [
      "Wuyang Li",
      "Xinyu Liu",
      "Xiwen Yao",
      "Yixuan Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20032": {
    "title": "Hybrid Instance-Aware Temporal Fusion for Online Video Instance Segmentation",
    "volume": "main",
    "abstract": "Recently, transformer-based image segmentation methods have achieved notable success against previous solutions. While for video domains, how to effectively model temporal context with the attention of object instances across frames remains an open problem. In this paper, we propose an online video instance segmentation framework with a novel instance-aware temporal fusion method. We first leverage the representation, \\ie, a latent code in the global context (instance code) and CNN feature maps to represent instance- and pixel-level features. Based on this representation, we introduce a cropping-free temporal fusion approach to model the temporal consistency between video frames. Specifically, we encode global instance-specific information in the instance code and build up inter-frame contextual fusion with hybrid attentions between the instance codes and CNN feature maps. Inter-frame consistency between the instance codes is further enforced with order constraints. By leveraging the learned hybrid temporal consistency, we are able to directly retrieve and maintain instance identities across frames, eliminating the complicated frame-wise instance matching in prior methods. Extensive experiments have been conducted on popular VIS datasets, i.e. Youtube-VIS-19/21. Our model achieves the best performance among all online VIS methods. Notably, our model also eclipses all offline methods when using the ResNet-50 backbone",
    "checked": true,
    "id": "038a4a2e8ced15154c356a74dbff76521daf3822",
    "semantic_title": "hybrid instance-aware temporal fusion for online video instance segmentation",
    "citation_count": 9,
    "authors": [
      "Xiang Li",
      "Jinglu Wang",
      "Xiao Li",
      "Yan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20033": {
    "title": "Close the Loop: A Unified Bottom-Up and Top-Down Paradigm for Joint Image Deraining and Segmentation",
    "volume": "main",
    "abstract": "In this work, we focus on a very practical problem: image segmentation under rain conditions. Image deraining is a classic low-level restoration task, while image segmentation is a typical high-level understanding task. Most of the existing methods intuitively employ the bottom-up paradigm by taking deraining as a preprocessing step for subsequent segmentation. However, our statistical analysis indicates that not only deraining would benefit segmentation (bottom-up), but also segmentation would further improve deraining performance (top-down) in turn. This motivates us to solve the rainy image segmentation task within a novel top-down and bottom-up unified paradigm, in which two sub-tasks are alternatively performed and collaborated with each other. Specifically, the bottom-up procedure yields both clearer images and rain-robust features from both image and feature domains, so as to ease the segmentation ambiguity caused by rain streaks. The top-down procedure adopts semantics to adaptively guide the restoration for different contents via a novel multi-path semantic attentive module (SAM). Thus the deraining and segmentation could boost the performance of each other cooperatively and progressively. Extensive experiments and ablations demonstrate that the proposed method outperforms the state-of-the-art on rainy image segmentation",
    "checked": true,
    "id": "f34890b8aa91ca9ab6830409bae18f6b98308458",
    "semantic_title": "close the loop: a unified bottom-up and top-down paradigm for joint image deraining and segmentation",
    "citation_count": 11,
    "authors": [
      "Yi Li",
      "Yi Chang",
      "Changfeng Yu",
      "Luxin Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20034": {
    "title": "Uncertainty Estimation via Response Scaling for Pseudo-Mask Noise Mitigation in Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly-Supervised Semantic Segmentation (WSSS) segments objects without heavy burden of dense annotation. While as a price, generated pseudo-masks exist obvious noisy pixels, which result in sub-optimal segmentation models trained over these pseudo-masks. But rare studies notice or work on this problem, even these noisy pixels are inevitable after their improvements on pseudo-mask. So we try to improve WSSS in the aspect of noise mitigation. And we observe that many noisy pixels are of high confidences, especially when the response range is too wide or narrow, presenting an uncertain status. Thus, in this paper, we simulate noisy variations of response by scaling the prediction map in multiple times for uncertainty estimation. The uncertainty is then used to weight the segmentation loss to mitigate noisy supervision signals. We call this method URN, abbreviated from Uncertainty estimation via Response scaling for Noise mitigation. Experiments validate the benefits of URN, and our method achieves state-of-the-art results at 71.2% and 41.5% on PASCAL VOC 2012 and MS COCO 2014 respectively, without extra models like saliency detection. Code is available at https://github.com/XMed-Lab/URN",
    "checked": true,
    "id": "47969582b6f61674da0fa538d36a211803fdc1a2",
    "semantic_title": "uncertainty estimation via response scaling for pseudo-mask noise mitigation in weakly-supervised semantic segmentation",
    "citation_count": 28,
    "authors": [
      "Yi Li",
      "Yiqun Duan",
      "Zhanghui Kuang",
      "Yimin Chen",
      "Wayne Zhang",
      "Xiaomeng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20035": {
    "title": "Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking",
    "volume": "main",
    "abstract": "Multi-modal fusion is proven to be an effective method to improve the accuracy and robustness of speaker tracking, especially in complex scenarios. However, how to combine the heterogeneous information and exploit the complementarity of multi-modal signals remains a challenging issue. In this paper, we propose a novel Multi-modal Perception Tracker (MPT) for speaker tracking using both audio and visual modalities. Specifically, a novel acoustic map based on spatial-temporal Global Coherence Field (stGCF) is first constructed for heterogeneous signal fusion, which employs a camera model to map audio cues to the localization space consistent with the visual cues. Then a multi-modal perception attention network is introduced to derive the perception weights that measure the reliability and effectiveness of intermittent audio and video streams disturbed by noise. Moreover, a unique cross-modal self-supervised learning method is presented to model the confidence of audio and visual observations by leveraging the complementarity and consistency between different modalities. Experimental results show that the proposed MPT achieves 98.6% and 78.3% tracking accuracy on the standard and occluded datasets, respectively, which demonstrates its robustness under adverse conditions and outperforms the current state-of-the-art methods",
    "checked": true,
    "id": "eb948a57c9e334797199513e7bfe26bad309c115",
    "semantic_title": "multi-modal perception attention network with self-supervised learning for audio-visual speaker tracking",
    "citation_count": 6,
    "authors": [
      "Yidi Li",
      "Hong Liu",
      "Hao Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20036": {
    "title": "Defending against Model Stealing via Verifying Embedded External Features",
    "volume": "main",
    "abstract": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification)",
    "checked": true,
    "id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
    "semantic_title": "defending against model stealing via verifying embedded external features",
    "citation_count": 19,
    "authors": [
      "Yiming Li",
      "Linghui Zhu",
      "Xiaojun Jia",
      "Yong Jiang",
      "Shu-Tao Xia",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20037": {
    "title": "Towards an Effective Orthogonal Dictionary Convolution Strategy",
    "volume": "main",
    "abstract": "Orthogonality regularization has proven effective in improving the precision, convergence speed and the training stability of CNNs. Here, we propose a novel Orthogonal Dictionary Convolution Strategy (ODCS) on CNNs to improve orthogonality effect by optimizing the network architecture and changing the regularized object. Specifically, we remove the nonlinear layer in typical convolution block \"Conv(BN) + Nonlinear + Pointwise Conv(BN)\", and only impose orthogonal regularization on the front Conv. The structure, \"Conv(BN) + Pointwise Conv(BN)\", is then equivalent to a pair of dictionary and encoding, defined in sparse dictionary learning. Thanks to the exact and efficient representation of signal with dictionaries in low-dimensional projections, our strategy could reduce the superfluous information in dictionary Conv kernels. Meanwhile, the proposed strategy relieves the too strict orthogonality regularization in training, which makes hyper-parameters tuning of model to be more flexible. In addition, our ODCS can modify the state-of-the-art models easily without any extra consumption in inference phase. We evaluate it on a variety of CNNs in small-scale (CIFAR), large-scale (ImageNet) and fine-grained (CUB-200-2011) image classification tasks, respectively. The experimental results show that our method achieve a stable and superior improvement",
    "checked": true,
    "id": "cb037f709612a51107c9b74b1a6389b37d0c4c23",
    "semantic_title": "towards an effective orthogonal dictionary convolution strategy",
    "citation_count": 1,
    "authors": [
      "Yishi Li",
      "Kunran Xu",
      "Rui Lai",
      "Lin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20038": {
    "title": "ELMA: Energy-Based Learning for Multi-Agent Activity Forecasting",
    "volume": "main",
    "abstract": "This paper describes an energy-based learning method that predicts the activities of multiple agents simultaneously. It aims to forecast both upcoming actions and paths of all agents in a scene based on their past activities, which can be jointly formulated by a probabilistic model over time. Learning this model is challenging because: 1) it has a large number of time-dependent variables that must scale with the forecast horizon and the number of agents; 2) distribution functions have to contain multiple modes in order to capture the spatio-temporal complexities of each agent's activities. To address these challenges, we put forth a novel Energy-based Learning approach for Multi-Agent activity forecasting (ELMA) to estimate this complex model via maximum log-likelihood estimation. Specifically, by sampling from a sequence of factorized marginalized multi-model distributions, ELMA generates most possible future actions efficiently. Moreover, by graph-based representations, ELMA also explicitly resolves the spatio-temporal dependencies of all agents' activities in a single pass. Our experiments on two large-scale datasets prove that ELMA outperforms recent leading studies by an obvious margin",
    "checked": true,
    "id": "69e89edeadd896b2f7966ccd7c888b44fd032fb4",
    "semantic_title": "elma: energy-based learning for multi-agent activity forecasting",
    "citation_count": 3,
    "authors": [
      "Yuke Li",
      "Pin Wang",
      "Lixiong Chen",
      "Zheng Wang",
      "Ching-Yao Chan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20039": {
    "title": "Equal Bits: Enforcing Equally Distributed Binary Network Weights",
    "volume": "main",
    "abstract": "Binary networks are extremely efficient as they use only two symbols to define the network: {+1, −1}. One can make the prior distribution of these symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a Bernoulli distribution with equal priors (equal bit ratios) over the binary weights leads to maximum entropy and thus minimizes information loss. However, prior work cannot precisely control the binary weight distribution during training, and therefore cannot guarantee maximum entropy. Here, we show that quantizing using optimal transport can guarantee any bit ratio, including equal ratios. We investigate experimentally that equal bit ratios are indeed preferable and show that our method leads to optimization benefits. We show that our quantization method is effective when compared to state-of-the-art binarization methods, even when using binary weight pruning. Our code is available at https://github.com/liyunqianggyn/Equal-Bits-BNN",
    "checked": true,
    "id": "686193a33d56d6cc0f9df299f92f3f0bbf555810",
    "semantic_title": "equal bits: enforcing equally distributed binary network weights",
    "citation_count": 9,
    "authors": [
      "Yunqiang Li",
      "Silvia-Laura Pintea",
      "Jan C van Gemert"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20040": {
    "title": "SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations",
    "volume": "main",
    "abstract": "Pre-training has become a standard paradigm in many computer vision tasks. However, most of the methods are generally designed on the RGB image domain. Due to the discrepancy between the two-dimensional image plane and the three-dimensional space, such pre-trained models fail to perceive spatial information and serve as sub-optimal solutions for 3D-related tasks. To bridge this gap, we aim to learn a spatial-aware visual representation that can describe the three-dimensional space and is more suitable and effective for these tasks. To leverage point clouds, which are much more superior in providing spatial information compared to images, we propose a simple yet effective 2D Image and 3D Point cloud Unsupervised pre-training strategy, called SimIPU. Specifically, we develop a multi-modal contrastive learning framework that consists of an intra-modal spatial perception module to learn a spatial-aware representation from point clouds and an inter-modal feature interaction module to transfer the capability of perceiving spatial information from the point cloud encoder to the image encoder, respectively. Positive pairs for contrastive losses are established by the matching algorithm and the projection matrix. The whole framework is trained in an unsupervised end-to-end fashion. To the best of our knowledge, this is the first study to explore contrastive learning pre-training strategies for outdoor multi-modal datasets, containing paired camera images and LIDAR point clouds",
    "checked": true,
    "id": "88e119f047fa9327215124e5872a7392ba421181",
    "semantic_title": "simipu: simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware visual representations",
    "citation_count": 32,
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang",
      "Bolei Zhou",
      "Hang Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20041": {
    "title": "Improving Human-Object Interaction Detection via Phrase Learning and Label Composition",
    "volume": "main",
    "abstract": "Human-Object Interaction (HOI) detection is a fundamental task in high-level human-centric scene understanding. We propose PhraseHOI, containing a HOI branch and a novel phrase branch, to leverage language prior and improve relation expression. Specifically, the phrase branch is supervised by semantic embeddings, whose ground truths are automatically converted from the original HOI annotations without extra human efforts. Meanwhile, a novel label composition method is proposed to deal with the long-tailed problem in HOI, which composites novel phrase labels by semantic neighbors. Further, to optimize the phrase branch, a loss composed of a distilling loss and a balanced triplet loss is proposed. Extensive experiments are conducted to prove the effectiveness of the proposed PhraseHOI, which achieves significant improvement over the baseline and surpasses previous state-of-the-art methods on Full and NonRare on the challenging HICO-DET benchmark",
    "checked": true,
    "id": "d0c5dfe88928a77232b1d91eb45009b45d2a5a18",
    "semantic_title": "improving human-object interaction detection via phrase learning and label composition",
    "citation_count": 11,
    "authors": [
      "Zhimin Li",
      "Cheng Zou",
      "Yu Zhao",
      "Boxun Li",
      "Sheng Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20042": {
    "title": "Rethinking the Optimization of Average Precision: Only Penalizing Negative Instances before Positive Ones Is Enough",
    "volume": "main",
    "abstract": "Optimising the approximation of Average Precision (AP) has been widely studied for image retrieval. Limited by the definition of AP, such methods consider both negative and positive instances ranking before each positive instance. However, we claim that only penalizing negative instances before positive ones is enough, because the loss only comes from these negative instances. To this end, we propose a novel loss, namely Penalizing Negative instances before Positive ones (PNP), which can directly minimize the number of negative instances before each positive one. In addition, AP-based methods adopt a fixed and sub-optimal gradient assignment strategy. Therefore, we systematically investigate different gradient assignment solutions via constructing derivative functions of the loss, resulting in PNP-I with increasing derivative functions and PNP-D with decreasing ones. PNP-I focuses more on the hard positive instances by assigning larger gradients to them and tries to make all relevant instances closer. In contrast, PNP-D pays less attention to such instances and slowly corrects them. For most real-world data, one class usually contains several local clusters. PNP-I blindly gathers these clusters while PNP-D keeps them as they were. Therefore, PNP-D is more superior. Experiments on three standard retrieval datasets show consistent results with the above analysis. Extensive evaluations demonstrate that PNP-D achieves the state-of-the-art performance. Code is available at https://github.com/interestingzhuo/PNPloss",
    "checked": true,
    "id": "fe38ad10078fae9ca2e13d977ced9c0307e10c47",
    "semantic_title": "rethinking the optimization of average precision: only penalizing negative instances before positive ones is enough",
    "citation_count": 6,
    "authors": [
      "Zhuo Li",
      "Weiqing Min",
      "Jiajun Song",
      "Yaohui Zhu",
      "Liping Kang",
      "Xiaoming Wei",
      "Xiaolin Wei",
      "Shuqiang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20043": {
    "title": "Reliability Exploration with Self-Ensemble Learning for Domain Adaptive Person Re-identification",
    "volume": "main",
    "abstract": "Person re-identifcation (Re-ID) based on unsupervised domain adaptation (UDA) aims to transfer the pre-trained model from one labeled source domain to an unlabeled target domain. Existing methods tackle this problem by using clustering methods to generate pseudo labels. However, pseudo labels produced by these techniques may be unstable and noisy, substantially deteriorating models' performance. In this paper, we propose a Reliability Exploration with Self-ensemble Learning (RESL) framework for domain adaptive person ReID. First, to increase the feature diversity, multiple branches are presented to extract features from different data augmentations. Taking the temporally average model as a mean teacher model, online label refning is conducted by using its dynamic ensemble predictions from different branches as soft labels. Second, to combat the adverse effects of unreliable samples in clusters, sample reliability is estimated by evaluating the consistency of different clusters' results, followed by selecting reliable instances for training and re-weighting sample contribution within Re-ID losses. A contrastive loss is also utilized with cluster-level memory features which are updated by the mean feature. The experiments demonstrate that our method can signifcantly surpass the state-of-the-art performance on the unsupervised domain adaptive person ReID",
    "checked": true,
    "id": "5244b86942d9ba20480cccc28c2548faecc3784c",
    "semantic_title": "reliability exploration with self-ensemble learning for domain adaptive person re-identification",
    "citation_count": 10,
    "authors": [
      "Zongyi Li",
      "Yuxuan Shi",
      "Hefei Ling",
      "Jiazhong Chen",
      "Qian Wang",
      "Fengfan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20044": {
    "title": "Deconfounding Physical Dynamics with Global Causal Relation and Confounder Transmission for Counterfactual Prediction",
    "volume": "main",
    "abstract": "Discovering the underneath causal relations is the fundamental ability for reasoning about the surrounding environment and predicting the future states in the physical world. Counterfactual prediction from visual input, which requires simulating future states based on unrealized situations in the past, is a vital component in causal relation tasks. In this paper, we work on the confounders that have effect on the physical dynamics, including masses, friction coefficients, etc., to bridge relations between the intervened variable and the affected variable whose future state may be altered. We propose a neural network framework combining Global Causal Relation Attention (GCRA) and Confounder Transmission Structure (CTS). The GCRA looks for the latent causal relations between different variables and estimates the confounders by capturing both spatial and temporal information. The CTS integrates and transmits the learnt confounders in a residual way, so that the estimated confounders can be encoded into the network as a constraint for object positions when performing counterfactual prediction. Without any access to ground truth information about confounders, our model outperforms the state-of-the-art method on various benchmarks by fully utilizing the constraints of confounders. Extensive experiments demonstrate that our model can generalize to unseen environments and maintain good performance",
    "checked": true,
    "id": "9c1cea9e9738a746fadb6cb7d565390d08dadfd3",
    "semantic_title": "deconfounding physical dynamics with global causal relation and confounder transmission for counterfactual prediction",
    "citation_count": 1,
    "authors": [
      "Zongzhao Li",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Zhaoxiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20045": {
    "title": "One More Check: Making \"Fake Background\" Be Tracked Again",
    "volume": "main",
    "abstract": "The one-shot multi-object tracking, which integrates object detection and ID embedding extraction into a unified network, has achieved groundbreaking results in recent years. However, current one-shot trackers solely rely on single-frame detections to predict candidate bounding boxes, which may be unreliable when facing disastrous visual degradation, e.g., motion blur, occlusions. Once a target bounding box is mistakenly classified as background by the detector, the temporal consistency of its corresponding tracklet will be no longer maintained. In this paper, we set out to restore the bounding boxes misclassified as ``fake background'' by proposing a re-check network. The re-check network innovatively expands the role of ID embedding from data association to motion forecasting by effectively propagating previous tracklets to the current frame with a small overhead. Note that the propagation results are yielded by an independent and efficient embedding search, preventing the model from over-relying on detection results. Eventually, it helps to reload the ``fake background'' and repair the broken tracklets. Building on a strong baseline CSTrack, we construct a new one-shot tracker and achieve favorable gains by 70.7 ➡ 76.4, 70.6 ➡ 76.3 MOTA on MOT16 and MOT17, respectively. It also reaches a new state-of-the-art MOTA and IDF1 performance. Code is released at https://github.com/JudasDie/SOTS",
    "checked": true,
    "id": "b8692df72ff4207810aa39d1de4f1b97e4685722",
    "semantic_title": "one more check: making \"fake background\" be tracked again",
    "citation_count": 36,
    "authors": [
      "Chao Liang",
      "Zhipeng Zhang",
      "Xue Zhou",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20046": {
    "title": "Semantically Contrastive Learning for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Low-light image enhancement (LLE) remains challenging due to the unfavorable prevailing low-contrast and weak-visibility problems of single RGB images. In this paper, we respond to the intriguing learning-related question -- if leveraging both accessible unpaired over/underexposed images and high-level semantic guidance, can improve the performance of cutting-edge LLE models? Here, we propose an effective semantically contrastive learning paradigm for LLE (namely SCL-LLE). Beyond the existing LLE wisdom, it casts the image enhancement task as multi-task joint learning, where LLE is converted into three constraints of contrastive learning, semantic brightness consistency, and feature preservation for simultaneously ensuring the exposure, texture, and color consistency. SCL-LLE allows the LLE model to learn from unpaired positives (normal-light)/negatives (over/underexposed), and enables it to interact with the scene semantics to regularize the image enhancement network, yet the interaction of high-level semantic knowledge and the low-level signal prior is seldom investigated in previous methods. Training on readily available open data, extensive experiments demonstrate that our method surpasses the state-of-the-arts LLE models over six independent cross-scenes datasets. Moreover, SCL-LLE's potential to benefit the downstream semantic segmentation under extremely dark conditions is discussed. Source Code: https://github.com/LingLIx/SCL-LLE",
    "checked": true,
    "id": "0c39317191711aa9d20134b9851491d77a4b885f",
    "semantic_title": "semantically contrastive learning for low-light image enhancement",
    "citation_count": 35,
    "authors": [
      "Dong Liang",
      "Ling Li",
      "Mingqiang Wei",
      "Shuo Yang",
      "Liyan Zhang",
      "Wenhan Yang",
      "Yun Du",
      "Huiyu Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20047": {
    "title": "Self-Supervised Spatiotemporal Representation Learning by Exploiting Video Continuity",
    "volume": "main",
    "abstract": "Recent self-supervised video representation learning methods have found significant success by exploring essential properties of videos, e.g. speed, temporal order, etc. This work exploits an essential yet under-explored property of videos, the \\textit{video continuity}, to obtain supervision signals for self-supervised representation learning. Specifically, we formulate three novel continuity-related pretext tasks, i.e. continuity justification, discontinuity localization, and missing section approximation, that jointly supervise a shared backbone for video representation learning. This self-supervision approach, termed as Continuity Perception Network (CPNet), solves the three tasks altogether and encourages the backbone network to learn local and long-ranged motion and context representations. It outperforms prior arts on multiple downstream tasks, such as action recognition, video retrieval, and action localization. Additionally, the video continuity can be complementary to other coarse-grained video properties for representation learning, and integrating the proposed pretext task to prior arts can yield much performance gains",
    "checked": true,
    "id": "f92ff2143ac8f9796cffecdb71f3a0191a9e83ec",
    "semantic_title": "self-supervised spatiotemporal representation learning by exploiting video continuity",
    "citation_count": 14,
    "authors": [
      "Hanwen Liang",
      "Niamul Quader",
      "Zhixiang Chi",
      "Lizhe Chen",
      "Peng Dai",
      "Juwei Lu",
      "Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20048": {
    "title": "Inharmonious Region Localization by Magnifying Domain Discrepancy",
    "volume": "main",
    "abstract": "Inharmonious region localization aims to localize the region in a synthetic image which is incompatible with surrounding background. The inharmony issue is mainly attributed to the color and illumination inconsistency produced by image editing techniques. In this work, we tend to transform the input image to another color space to magnify the domain discrepancy between inharmonious region and background, so that the model can identify the inharmonious region more easily. To this end, we present a novel framework consisting of a color mapping module and an inharmonious region localization network, in which the former is equipped with a novel domain discrepancy magnification loss and the latter could be an arbitrary localization network. Extensive experiments on image harmonization dataset show the superiority of our designed framework",
    "checked": true,
    "id": "e4adffa920024e112f7920724f1cbed96e04dbf1",
    "semantic_title": "inharmonious region localization by magnifying domain discrepancy",
    "citation_count": 3,
    "authors": [
      "Jing Liang",
      "Li Niu",
      "Penghao Wu",
      "Fengjun Guo",
      "Teng Long"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20049": {
    "title": "Distribution Aware VoteNet for 3D Object Detection",
    "volume": "main",
    "abstract": "Occlusion is common in the actual 3D scenes, causing the boundary ambiguity of the targeted object. This uncertainty brings difficulty for labeling and learning. Current 3D detectors predict the bounding box directly, regarding it as Dirac delta distribution. However, it does not fully consider such ambiguity. To deal with it, distribution learning is used to efficiently represent the boundary ambiguity. In this paper, we revise the common regression method by predicting the distribution of the 3D box and then present a distribution-aware regression (DAR) module for box refinement and localization quality estimation. It contains scale adaptive (SA) encoder and joint localization quality estimator (JLQE). With the adaptive receptive field, SA encoder refines discriminative features for precise distribution learning. JLQE provides a reliable location score by further leveraging the distribution statistics, correlating with the localization quality of the targeted object. Combining DAR module and the baseline VoteNet, we propose a novel 3D detector called DAVNet. Extensive experiments on both ScanNet V2 and SUN RGB-D datasets demonstrate that the proposed DAVNet achieves significant improvement and outperforms state-of-the-art 3D detectors",
    "checked": true,
    "id": "205a7ae22ecd3afbdd7aedf705426313397e83ae",
    "semantic_title": "distribution aware votenet for 3d object detection",
    "citation_count": 1,
    "authors": [
      "Junxiong Liang",
      "Pei An",
      "Jie Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20050": {
    "title": "Contrastive Instruction-Trajectory Learning for Vision-Language Navigation",
    "volume": "main",
    "abstract": "The vision-language navigation (VLN) task requires an agent to reach a target with the guidance of natural language instruction. Previous works learn to navigate step-by-step following an instruction. However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the temporal continuity of sub-instructions. These problems hinder agents from learning distinctive vision-and-language representations, harming the robustness and generalizability of the navigation policy. In this paper, we propose a Contrastive Instruction-Trajectory Learning (CITL) framework that explores invariance across similar data samples and variance across different ones to learn distinctive representations for robust navigation. Specifically, we propose: (1) a coarse-grained contrastive learning objective to enhance vision-and-language representations by contrasting semantics of full trajectory observations and instructions, respectively; (2) a fine-grained contrastive learning objective to perceive instructions by leveraging the temporal information of the sub-instructions; (3) a pairwise sample-reweighting mechanism for contrastive learning to mine hard samples and hence mitigate the influence of data sampling bias in contrastive learning. Our CITL can be easily integrated with VLN backbones to form a new learning paradigm and achieve better generalizability in unseen environments. Extensive experiments show that the model with CITL surpasses the previous state-of-the-art methods on R2R, R4R, and RxR",
    "checked": true,
    "id": "9149931ce9dde23f573396b159bf8f82b8433a3e",
    "semantic_title": "contrastive instruction-trajectory learning for vision-language navigation",
    "citation_count": 11,
    "authors": [
      "Xiwen Liang",
      "Fengda Zhu",
      "Yi Zhu",
      "Bingqian Lin",
      "Bing Wang",
      "Xiaodan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20051": {
    "title": "Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction",
    "volume": "main",
    "abstract": "When applying multi-instance learning (MIL) to make predictions for bags of instances, the prediction accuracy of an instance often depends on not only the instance itself but also its context in the corresponding bag. From the viewpoint of causal inference, such bag contextual prior works as a confounder and may result in model robustness and interpretability issues. Focusing on this problem, we propose a novel interventional multi-instance learning (IMIL) framework to achieve deconfounded instance-level prediction. Unlike traditional likelihood-based strategies, we design an Expectation-Maximization (EM) algorithm based on causal intervention, providing a robust instance selection in the training phase and suppressing the bias caused by the bag contextual prior. Experiments on pathological image analysis demonstrate that our IMIL method substantially reduces false positives and outperforms state-of-the-art MIL methods",
    "checked": true,
    "id": "b66145e04fe54ea9ebcf644ca97e82d2a55b9404",
    "semantic_title": "interventional multi-instance learning with deconfounded instance-level prediction",
    "citation_count": 8,
    "authors": [
      "Tiancheng Lin",
      "Hongteng Xu",
      "Canqian Yang",
      "Yi Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20052": {
    "title": "A Causal Debiasing Framework for Unsupervised Salient Object Detection",
    "volume": "main",
    "abstract": "Unsupervised Salient Object Detection (USOD) is a promising yet challenging task that aims to learn a salient object detection model without any ground-truth labels. Self-supervised learning based methods have achieved remarkable success recently and have become the dominant approach in USOD. However, we observed that two distribution biases of salient objects limit further performance improvement of the USOD methods, namely, contrast distribution bias and spatial distribution bias. Concretely, contrast distribution bias is essentially a confounder that makes images with similar high-level semantic contrast and/or low-level visual appearance contrast spuriously dependent, thus forming data-rich contrast clusters and leading the training process biased towards the data-rich contrast clusters in the data. Spatial distribution bias means that the position distribution of all salient objects in a dataset is concentrated on the center of the image plane, which could be harmful to off-center objects prediction. This paper proposes a causal based debiasing framework to disentangle the model from the impact of such biases. Specifically, we use causal intervention to perform de-confounded model training to minimize the contrast distribution bias and propose an image-level weighting strategy that softly weights each image's importance according to the spatial distribution bias map. Extensive experiments on 6 benchmark datasets show that our method significantly outperforms previous unsupervised state-of-the-art methods and even surpasses some of the supervised methods, demonstrating our debiasing framework's effectiveness",
    "checked": true,
    "id": "f9adf73144d5afecb8c09a5866fd0956e8a849d6",
    "semantic_title": "a causal debiasing framework for unsupervised salient object detection",
    "citation_count": 11,
    "authors": [
      "Xiangru Lin",
      "Ziyi Wu",
      "Guanqi Chen",
      "Guanbin Li",
      "Yizhou Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20053": {
    "title": "A Causal Inference Look at Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Unsupervised video anomaly detection, a task that requires no labeled normal/abnormal training data in any form, is challenging yet of great importance to both industrial applications and academic research. Existing methods typically follow an iterative pseudo label generation process. However, they lack a principled analysis of the impact of such pseudo label generation on training. Furthermore, the long-range temporal dependencies also has been overlooked, which is unreasonable since the definition of an abnormal event depends on the long-range temporal context. To this end, first, we propose a causal graph to analyze the confounding effect of the pseudo label generation process. Then, we introduce a simple yet effective causal inference based framework to disentangle the noisy pseudo label's impact. Finally, we perform counterfactual based model ensemble that blends long-range temporal context with local image context in inference to make final anomaly detection. Extensive experiments on six standard benchmark datasets show that our proposed method significantly outperforms previous state-of-the-art methods, demonstrating our framework's effectiveness",
    "checked": true,
    "id": "2caeea7cb81973ba56bba6daf46137a6b7b279d8",
    "semantic_title": "a causal inference look at unsupervised video anomaly detection",
    "citation_count": 9,
    "authors": [
      "Xiangru Lin",
      "Yuyang Chen",
      "Guanbin Li",
      "Yizhou Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20054": {
    "title": "Unpaired Multi-Domain Stain Transfer for Kidney Histopathological Images",
    "volume": "main",
    "abstract": "As an essential step in the pathological diagnosis, histochemical staining can show specific tissue structure information and, consequently, assist pathologists in making accurate diagnoses. Clinical kidney histopathological analyses usually employ more than one type of staining: H&E, MAS, PAS, PASM, etc. However, due to the interference of colors among multiple stains, it is not easy to perform multiple staining simultaneously on one biological tissue. To address this problem, we propose a network based on unpaired training data to virtually generate multiple types of staining from one staining. Our method can preserve the content of input images while transferring them to multiple target styles accurately. To efficiently control the direction of stain transfer, we propose a style guided normalization (SGN). Furthermore, a multiple style encoding (MSE) is devised to represent the relationship among different staining styles dynamically. An improved one-hot label is also proposed to enhance the generalization ability and extendibility of our method. Vast experiments have demonstrated that our model can achieve superior performance on a tiny dataset. The results exhibit not only good performance but also great visualization and interpretability. Especially, our method also achieves satisfactory results over cross-tissue, cross-staining as well as cross-task. We believe that our method will significantly influence clinical stain transfer and reduce the workload greatly for pathologists. Our code and Supplementary materials are available at https://github.com/linyiyang98/UMDST",
    "checked": true,
    "id": "1e2b44f44db69f18e4aecac55cc3887d30bf5940",
    "semantic_title": "unpaired multi-domain stain transfer for kidney histopathological images",
    "citation_count": 8,
    "authors": [
      "Yiyang Lin",
      "Bowei Zeng",
      "Yifeng Wang",
      "Yang Chen",
      "Zijie Fang",
      "Jian Zhang",
      "Xiangyang Ji",
      "Haoqian Wang",
      "Yongbing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20055": {
    "title": "Dynamic Spatial Propagation Network for Depth Completion",
    "volume": "main",
    "abstract": "Image-guided depth completion aims to generate dense depth maps with sparse depth measurements and corresponding RGB images. Currently, spatial propagation networks (SPNs) are the most popular affinity-based methods in depth completion, but they still suffer from the representation limitation of the fixed affinity and the over smoothing during iterations. Our solution is to estimate independent affinity matrices in each SPN iteration, but it is over-parameterized and heavy calculation.This paper introduces an efficient model that learns the affinity among neighboring pixels with an attention-based, dynamic approach. Specifically, the Dynamic Spatial Propagation Network (DySPN) we proposed makes use of a non-linear propagation model (NLPM). It decouples the neighborhood into parts regarding to different distances and recursively generates independent attention maps to refine these parts into adaptive affinity matrices. Furthermore, we adopt a diffusion suppression (DS) operation so that the model converges at an early stage to prevent over-smoothing of dense depth. Finally, in order to decrease the computational cost required, we also introduce three variations that reduce the amount of neighbors and attentions needed while still retaining similar accuracy. In practice, our method requires less iteration to match the performance of other SPNs and yields better results overall. DySPN outperforms other state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation by the time of submission and is able to yield SoTA performance in NYU Depth v2 dataset as well",
    "checked": true,
    "id": "0ea1dece34d1c6e8da5cc2c4dd6ff8cc08048c5a",
    "semantic_title": "dynamic spatial propagation network for depth completion",
    "citation_count": 44,
    "authors": [
      "Yuankai Lin",
      "Tao Cheng",
      "Qi Zhong",
      "Wending Zhou",
      "Hua Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20056": {
    "title": "Local Similarity Pattern and Cost Self-Reassembling for Deep Stereo Matching Networks",
    "volume": "main",
    "abstract": "Although convolutional neural network based stereo matching architectures have made impressive achievements, there are still some limitations: 1) Convolutional Feature (CF) tends to capture appearance information, which is inadequate for accurate matching. 2) Due to the static filters, current convolution based disparity refinement modules often produce over-smooth results. In this paper, we present two schemes to address these issues, where some traditional wisdoms are integrated. Firstly, we introduce a pairwise feature for deep stereo matching networks, named LSP (Local Similarity Pattern). Through explicitly revealing the neighbor relationships, LSP contains rich structural information, which can be leveraged to aid CF for more discriminative feature description. Secondly, we design a dynamic self-reassembling refinement strategy and apply it to the cost distribution and the disparity map respectively. The former could be equipped with the unimodal distribution constraint to alleviate the over-smoothing problem, and the latter is more practical. The effectiveness of the proposed methods is demonstrated via incorporating them into two well-known basic architectures, GwcNet and GANet-deep. Experimental results on the SceneFlow and KITTI benchmarks show that our modules significantly improve the performance of the model. Code is available at https://github.com/SpadeLiu/Lac-GwcNet",
    "checked": true,
    "id": "654849305b95c6ee30b91130f69b9b09a6327602",
    "semantic_title": "local similarity pattern and cost self-reassembling for deep stereo matching networks",
    "citation_count": 18,
    "authors": [
      "Biyang Liu",
      "Huimin Yu",
      "Yangqi Long"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20057": {
    "title": "FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition",
    "volume": "main",
    "abstract": "Current state-of-the-art deep learning based face recognition (FR) models require a large number of face identities for central training. However, due to the growing privacy awareness, it is prohibited to access the face images on user devices to continually improve face recognition models. Federated Learning (FL) is a technique to address the privacy issue, which can collaboratively optimize the model without sharing the data between clients. In this work, we propose a FL based framework called FedFR to improve the generic face representation in a privacy-aware manner. Besides, the framework jointly optimizes personalized models for the corresponding clients via the proposed Decoupled Feature Customization module. The client-specific personalized model can serve the need of optimized face recognition experience for registered identities at the local device. To the best of our knowledge, we are the first to explore the personalized face recognition in FL setup. The proposed framework is validated to be superior to previous approaches on several generic and personalized face recognition benchmarks with diverse FL scenarios. The source codes and our proposed personalized FR benchmark under FL setup are available at https://github.com/jackie840129/FedFR",
    "checked": true,
    "id": "9270a3b676b4f87aa5b9576ddf48d7a91a79d39b",
    "semantic_title": "fedfr: joint optimization federated framework for generic and personalized face recognition",
    "citation_count": 8,
    "authors": [
      "Chih-Ting Liu",
      "Chien-Yi Wang",
      "Shao-Yi Chien",
      "Shang-Hong Lai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20058": {
    "title": "Memory-Guided Semantic Learning Network for Temporal Sentence Grounding",
    "volume": "main",
    "abstract": "Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Although existing methods train well-designed deep networks with large amount of data, we find that they can easily forget the rarely appeared cases during training due to the off-balance data distribution, which influences the model generalization and leads to unsatisfactory performance. To tackle this issue, we propose a memory-augmented network, called Memory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes the rarely appeared content in TSG task. Specifically, our proposed model consists of three main parts: cross-modal interaction module, memory augmentation module, and heterogeneous attention module. We first align the given video-query pair by a cross-modal graph convolutional network, and then utilize memory module to record the cross-modal shared semantic features in the domain-specific persistent memory. During training, the memory slots are dynamically associated with both common and rare cases, alleviating the forgetting issue. In testing, the rare cases can thus be enhanced by retrieving the stored memories, leading to better generalization. At last, the heterogeneous attention module is utilized to integrate the enhanced multi-modal features in both video and query domains. Experimental results on three benchmarks show the superiority of our method on both effectiveness and efficiency, which substantially improves the accuracy not only on the entire dataset but also on the rare cases",
    "checked": true,
    "id": "7fc933a06db0cc54a540478a62bf0b370e631aa9",
    "semantic_title": "memory-guided semantic learning network for temporal sentence grounding",
    "citation_count": 29,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Xing Di",
      "Yu Cheng",
      "Zichuan Xu",
      "Pan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20059": {
    "title": "Exploring Motion and Appearance Information for Temporal Sentence Grounding",
    "volume": "main",
    "abstract": "This paper addresses temporal sentence grounding. Previous works typically solve this task by learning frame-level video features and align them with the textual information. A major limitation of these works is that they fail to distinguish ambiguous video frames with subtle appearance differences due to frame-level feature extraction. Recently, a few methods adopt Faster R-CNN to extract detailed object features in each frame to differentiate the fine-grained appearance similarities. However, the object-level features extracted by Faster R-CNN suffer from missing motion analysis since the object detection model lacks temporal modeling. To solve this issue, we propose a novel Motion-Appearance Reasoning Network (MARN), which incorporates both motion-aware and appearance-aware object features to better reason object relations for modeling the activity among successive frames. Specifically, we first introduce two individual video encoders to embed the video into corresponding motion-oriented and appearance-aspect object representations. Then, we develop separate motion and appearance branches to learn motion-guided and appearance-guided object relations, respectively. At last, both motion and appearance information from two branches are associated to generate more representative features for final grounding. Extensive experiments on two challenging datasets (Charades-STA and TACoS) show that our proposed MARN significantly outperforms previous state-of-the-art methods by a large margin",
    "checked": true,
    "id": "f3aff0341bdf52cc45286fc5818075a5ec70c762",
    "semantic_title": "exploring motion and appearance information for temporal sentence grounding",
    "citation_count": 24,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Pan Zhou",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20060": {
    "title": "Unsupervised Temporal Video Grounding with Deep Semantic Clustering",
    "volume": "main",
    "abstract": "Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that our DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches",
    "checked": true,
    "id": "b760ffcb002d4bea7cb2c131c63a0e964ae0e314",
    "semantic_title": "unsupervised temporal video grounding with deep semantic clustering",
    "citation_count": 28,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Yinzhen Wang",
      "Xing Di",
      "Kai Zou",
      "Yu Cheng",
      "Zichuan Xu",
      "Pan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20061": {
    "title": "SpikeConverter: An Efficient Conversion Framework Zipping the Gap between Artificial Neural Networks and Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have recently attracted enormous research interest since their event-driven and brain-inspired structure enables low-power computation. In image recognition tasks, the best results are achieved by SNN so far utilizing ANN-SNN conversion methods that replace activation functions in artificial neural networks~(ANNs) with integrate-and-fire neurons. Compared to source ANNs, converted SNNs usually suffer from accuracy loss and require a considerable number of time steps to achieve competitive accuracy. We find that the performance degradation of converted SNN stems from the fact that the information capacity of spike trains in transferred networks is smaller than that of activation values in source ANN, resulting in less information being passed during SNN inference. To better correlate ANN and SNN for better performance, we propose a conversion framework to mitigate the gap between the activation value of source ANN and the generated spike train of target SNN. The conversion framework originates from exploring an identical relation in the conversion and exploits temporal separation scheme and novel neuron model for the relation to hold. We demonstrate almost lossless ANN-SNN conversion using SpikeConverter for VGG-16, ResNet-20/34, and MobileNet-v2 SNNs on challenging datasets including CIFAR-10, CIFAR-100, and ImageNet. Our results also show that SpikeConverter achieves the abovementioned accuracy across different network architectures and datasets using 32X - 512X fewer inference time-steps than state-of-the-art ANN-SNN conversion methods",
    "checked": true,
    "id": "f8df151ec0ed755c153b7d58bea0bc3c6c6eb986",
    "semantic_title": "spikeconverter: an efficient conversion framework zipping the gap between artificial neural networks and spiking neural networks",
    "citation_count": 15,
    "authors": [
      "Fangxin Liu",
      "Wenbo Zhao",
      "Yongbiao Chen",
      "Zongwu Wang",
      "Li Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20062": {
    "title": "Perceiving Stroke-Semantic Context: Hierarchical Contrastive Learning for Robust Scene Text Recognition",
    "volume": "main",
    "abstract": "We introduce Perceiving Stroke-Semantic Context (PerSec), a new approach to self-supervised representation learning tailored for Scene Text Recognition (STR) task. Considering scene text images carry both visual and semantic properties, we equip our PerSec with dual context perceivers which can contrast and learn latent representations from low-level stroke and high-level semantic contextual spaces simultaneously via hierarchical contrastive learning on unlabeled text image data. Experiments in un- and semi-supervised learning settings on STR benchmarks demonstrate our proposed framework can yield a more robust representation for both CTC-based and attention-based decoders than other contrastive learning methods. To fully investigate the potential of our method, we also collect a dataset of 100 million unlabeled text images, named UTI-100M, covering 5 scenes and 4 languages. By leveraging hundred-million-level unlabeled data, our PerSec shows significant performance improvement when fine-tuning the learned representation on the labeled data. Furthermore, we observe that the representation learned by PerSec presents great generalization, especially under few labeled data scenes",
    "checked": true,
    "id": "de4b03e5ed2a1b352f243a3ff68ee9ebf98afe09",
    "semantic_title": "perceiving stroke-semantic context: hierarchical contrastive learning for robust scene text recognition",
    "citation_count": 17,
    "authors": [
      "Hao Liu",
      "Bin Wang",
      "Zhimin Bao",
      "Mobai Xue",
      "Sheng Kang",
      "Deqiang Jiang",
      "Yinsong Liu",
      "Bo Ren"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20063": {
    "title": "AnchorFace: Boosting TAR@FAR for Practical Face Recognition",
    "volume": "main",
    "abstract": "Within the field of face recognition (FR), it is widely accepted that the key objective is to optimize the entire feature space in the training process and acquire robust feature representations. However, most real-world FR systems tend to operate at a pre-defined False Accept Rate (FAR), and the corresponding True Accept Rate (TAR) represents the performance of the FR systems, which indicates that the optimization on the pre-defined FAR is more meaningful and important in the practical evaluation process. In this paper, we call the predefined FAR as Anchor FAR, and we argue that the existing FR loss functions cannot guarantee the optimal TAR under the Anchor FAR, which impedes further improvements of FR systems. To this end, we propose AnchorFace to bridge the aforementioned gap between the training and practical evaluation process for FR. Given the Anchor FAR, AnchorFace can boost the performance of FR systems by directly optimizing the non-differentiable FR evaluation metrics. Specifically, in AnchorFace, we first calculate the similarities of the positive and negative pairs based on both the features of the current batch and the stored features in the maintained online-updating set. Then, we generate the differentiable TAR loss and FAR loss using a soften strategy. Our AnchorFace can be readily integrated into most existing FR loss functions, and extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of AnchorFace",
    "checked": true,
    "id": "1ac1e7acde18299acc2e4ef70af07abae6d51b68",
    "semantic_title": "anchorface: boosting tar@far for practical face recognition",
    "citation_count": 5,
    "authors": [
      "Jiaheng Liu",
      "Haoyu Qin",
      "Yichao Wu",
      "Ding Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20064": {
    "title": "Memory-Based Jitter: Improving Visual Recognition on Long-Tailed Data with Diversity in Memory",
    "volume": "main",
    "abstract": "This paper considers deep visual recognition on long-tailed data. To make our method general, we tackle two applied scenarios, i.e. , deep classification and deep metric learning. Under the long-tailed data distribution, the most classes (i.e., tail classes) only occupy relatively few samples and are prone to lack of within-class diversity. A radical solution is to augment the tail classes with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ). We observe that during training, the deep model constantly changes its parameters after every iteration, yielding the phenomenon of weight jitters. Consequentially, given a same image as the input, two historical editions of the model generate two different features in the deeply-embedded space, resulting in feature jitters. Using a memory bank, we collect these (model or feature) jitters across multiple training iterations and get the so-called Memory-based Jitter. The accumulated jitters enhance the within-class diversity for the tail classes and consequentially improves long-tailed visual recognition. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, i.e., deep image classification and deep metric learning (on long-tailed data). Extensive experiments on five long-tailed classification benchmarks and two deep metric learning benchmarks demonstrate significant improvement. Moreover, the achieved performance are on par with the state of the art on both tasks",
    "checked": true,
    "id": "380493918881ab21f19f9ac68846f77090e28598",
    "semantic_title": "memory-based jitter: improving visual recognition on long-tailed data with diversity in memory",
    "citation_count": 18,
    "authors": [
      "Jialun Liu",
      "Wenhui Li",
      "Yifan Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20065": {
    "title": "Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-identification",
    "volume": "main",
    "abstract": "Generalizable person re-identification aims to learn a model with only several labeled source domains that can perform well on unseen domains. Without access to the unseen domain, the feature statistics of the batch normalization (BN) layer learned from a limited number of source domains is doubtlessly biased for unseen domain. This would mislead the feature representation learning for unseen domain and deteriorate the generalizaiton ability of the model. In this paper, we propose a novel Debiased Batch Normalization via Gaussian Process approach (GDNorm) for generalizable person re-identification, which models the feature statistic estimation from BN layers as a dynamically self-refining Gaussian process to alleviate the bias to unseen domain for improving the generalization. Specifically, we establish a lightweight model with multiple set of domain-specific BN layers to capture the discriminability of individual source domain, and learn the corresponding parameters of the domain-specific BN layers. These parameters of different source domains are employed to deduce a Gaussian process. We randomly sample several paths from this Gaussian process served as the BN estimations of potential new domains outside of existing source domains, which can further optimize these learned parameters from source domains, and estimate more accurate Gaussian process by them in return, tending to real data distribution. Even without a large number of source domains, GDNorm can still provide debiased BN estimation by using the mean path of the Gaussian process, while maintaining low computational cost during testing. Extensive experiments demonstrate that our GDNorm effectively improves the generalization ability of the model on unseen domain",
    "checked": true,
    "id": "26750aea7f0573b47ea04d34e09cf9ae7bc80d6c",
    "semantic_title": "debiased batch normalization via gaussian process for generalizable person re-identification",
    "citation_count": 6,
    "authors": [
      "Jiawei Liu",
      "Zhipeng Huang",
      "Liang Li",
      "Kecheng Zheng",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20066": {
    "title": "Parallel and High-Fidelity Text-to-Lip Generation",
    "volume": "main",
    "abstract": "As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a parallel decoding model for fast and high-fidelity text-to-lip generation (ParaLip). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets demonstrate the superiority of proposed methods",
    "checked": true,
    "id": "4623dd252bc7cfc8bda622a12163ed22ddfac211",
    "semantic_title": "parallel and high-fidelity text-to-lip generation",
    "citation_count": 3,
    "authors": [
      "Jinglin Liu",
      "Zhiying Zhu",
      "Yi Ren",
      "Wencan Huang",
      "Baoxing Huai",
      "Nicholas Yuan",
      "Zhou Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20067": {
    "title": "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-trained Siamese Transformers",
    "volume": "main",
    "abstract": "We propose a novel zero-shot multi-frame image restoration method for removing unwanted obstruction elements (such as rains, snow, and moire patterns) that vary in successive frames. It has three stages: transformer pre-training, zero-shot restoration, and hard patch refinement. Using the pre-trained transformers, our model is able to tell the motion difference between the true image information and the obstructing elements. For zero-shot image restoration, we design a novel model, termed SiamTrans, which is constructed by Siamese transformers, encoders, and decoders. Each transformer has a temporal attention layer and several self-attention layers, to capture both temporal and spatial information of multiple frames. Only self-supervisedly pre-trained on the denoising task, SiamTrans is tested on three different low-level vision tasks (deraining, demoireing, and desnowing). Compared with related methods, SiamTrans achieves the best performances, even outperforming those with supervised learning",
    "checked": true,
    "id": "53fd31cf9d1ed810a182afc1af8bbf5bae570d76",
    "semantic_title": "siamtrans: zero-shot multi-frame image restoration with pre-trained siamese transformers",
    "citation_count": 7,
    "authors": [
      "Lin Liu",
      "Shanxin Yuan",
      "Jianzhuang Liu",
      "Xin Guo",
      "Youliang Yan",
      "Qi Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20068": {
    "title": "Single-Domain Generalization in Medical Image Segmentation via Test-Time Adaptation from Shape Dictionary",
    "volume": "main",
    "abstract": "Domain generalization typically requires data from multiple source domains for model learning. However, such strong assumption may not always hold in practice, especially in medical field where the data sharing is highly concerned and sometimes prohibitive due to privacy issue. This paper studies the important yet challenging single domain generalization problem, in which a model is learned under the worst-case scenario with only one source domain to directly generalize to different unseen target domains. We present a novel approach to address this problem in medical image segmentation, which extracts and integrates the semantic shape prior information of segmentation that are invariant across domains and can be well-captured even from single domain data to facilitate segmentation under distribution shifts. Besides, a test-time adaptation strategy with dual-consistency regularization is further devised to promote dynamic incorporation of these shape priors under each unseen domain to improve model generalizability. Extensive experiments on two medical image segmentation tasks demonstrate the consistent improvements of our method across various unseen domains, as well as its superiority over state-of-the-art approaches in addressing domain generalization under the worst-case scenario",
    "checked": true,
    "id": "d754649f661eb29e0648ada875a35fd0985fbe4d",
    "semantic_title": "single-domain generalization in medical image segmentation via test-time adaptation from shape dictionary",
    "citation_count": 9,
    "authors": [
      "Quande Liu",
      "Cheng Chen",
      "Qi Dou",
      "Pheng-Ann Heng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20069": {
    "title": "Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints",
    "volume": "main",
    "abstract": "Detecting 3D lanes from the camera is a rising problem for autonomous vehicles. In this task, the correct camera pose is the key to generating accurate lanes, which can transform an image from perspective-view to the top-view. With this transformation, we can get rid of the perspective effects so that 3D lanes would look similar and can accurately be fitted by low-order polynomials. However, mainstream 3D lane detectors rely on perfect camera poses provided by other sensors, which is expensive and encounters multi-sensor calibration issues. To overcome this problem, we propose to predict 3D lanes by estimating camera pose from a single image with a two-stage framework. The first stage aims at the camera pose task from perspective-view images. To improve pose estimation, we introduce an auxiliary 3D lane task and geometry constraints to benefit from multi-task learning, which enhances consistencies between 3D and 2D, as well as compatibility in the above two tasks. The second stage targets the 3D lane task. It uses previously estimated pose to generate top-view images containing distance-invariant lane appearances for predicting accurate 3D lanes. Experiments demonstrate that, without ground truth camera pose, our method outperforms the state-of-the-art perfect-camera-pose-based methods and has the fewest parameters and computations. Codes are available at https://github.com/liuruijin17/CLGo",
    "checked": true,
    "id": "c6c9750d9c45a4ebc2c5071a6ca866597ab7ed08",
    "semantic_title": "learning to predict 3d lane shape and camera pose from a single image via geometry constraints",
    "citation_count": 18,
    "authors": [
      "Ruijin Liu",
      "Dapeng Chen",
      "Tie Liu",
      "Zhiliang Xiong",
      "Zejian Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20070": {
    "title": "OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning",
    "volume": "main",
    "abstract": "We introduce the task of open-vocabulary visual instance search (OVIS). Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database. The term ``open vocabulary'' means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query. We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA). ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query. To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words. Experimental results show that ViSA achieves an mAP@50 of 27.8% on OVIS40 and achieves a recall@30 of 21.3% on OVIS1400 dataset under the most challenging settings",
    "checked": true,
    "id": "27d36a848d0c34a17373e64bcade441f47753b45",
    "semantic_title": "ovis: open-vocabulary visual instance search via visual-semantic aligned representation learning",
    "citation_count": 2,
    "authors": [
      "Sheng Liu",
      "Kevin Lin",
      "Lijuan Wang",
      "Junsong Yuan",
      "Zicheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20071": {
    "title": "Feature Generation and Hypothesis Verification for Reliable Face Anti-spoofing",
    "volume": "main",
    "abstract": "Although existing face anti-spoofing (FAS) methods achieve high accuracy in intra-domain experiments, their effects drop severely in cross-domain scenarios because of poor generalization. Recently, multifarious techniques have been explored, such as domain generalization and representation disentanglement. However, the improvement is still limited by two issues: 1) It is difficult to perfectly map all faces to a shared feature space. If faces from unknown domains are not mapped to the known region in the shared feature space, accidentally inaccurate predictions will be obtained. 2) It is hard to completely consider various spoof traces for disentanglement. In this paper, we propose a Feature Generation and Hypothesis Verification framework to alleviate the two issues. Above all, feature generation networks which generate hypotheses of real faces and known attacks are introduced for the first time in the FAS task. Subsequently, two hypothesis verification modules are applied to judge whether the input face comes from the real-face space and the real-face distribution respectively. Furthermore, some analyses of the relationship between our framework and Bayesian uncertainty estimation are given, which provides theoretical support for reliable defense in unknown domains. Experimental results show our framework achieves promising results and outperforms the state-of-the-art approaches on extensive public datasets",
    "checked": true,
    "id": "dc4c28b752c0b3acf1a74027381d0c3b3b610db9",
    "semantic_title": "feature generation and hypothesis verification for reliable face anti-spoofing",
    "citation_count": 17,
    "authors": [
      "Shice Liu",
      "Shitao Lu",
      "Hongyi Xu",
      "Jing Yang",
      "Shouhong Ding",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20072": {
    "title": "Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions",
    "volume": "main",
    "abstract": "Though deep learning-based object detection methods have achieved promising results on the conventional datasets, it is still challenging to locate objects from the low-quality images captured in adverse weather conditions. The existing methods either have difficulties in balancing the tasks of image enhancement and object detection, or often ignore the latent information beneficial for detection. To alleviate this problem, we propose a novel Image-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively enhanced for better detection performance. Specifically, a differentiable image processing (DIP) module is presented to take into account the adverse weather conditions for YOLO detector, whose parameters are predicted by a small convolutional neural network (CNN-PP). We learn CNN-PP and YOLOv3 jointly in an end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP to enhance the image for detection in a weakly supervised manner. Our proposed IA-YOLO approach can adaptively process images in both normal and adverse weather conditions. The experimental results are very encouraging, demonstrating the effectiveness of our proposed IA-YOLO method in both foggy and low-light scenarios. The source code can be found at https://github.com/wenyyu/Image-Adaptive-YOLO",
    "checked": true,
    "id": "8c332bdf0be56ecc96281c6d31e7b64544e78a78",
    "semantic_title": "image-adaptive yolo for object detection in adverse weather conditions",
    "citation_count": 85,
    "authors": [
      "Wenyu Liu",
      "Gaofeng Ren",
      "Runsheng Yu",
      "Shi Guo",
      "Jianke Zhu",
      "Lei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20073": {
    "title": "Visual Sound Localization in the Wild by Cross-Modal Interference Erasing",
    "volume": "main",
    "abstract": "The task of audiovisual sound source localization has been well studied under constrained scenes, where the audio recordings are clean. However, in real world scenarios, audios are usually contaminated by off screen sound and background noise. They will interfere with the procedure of identifying desired sources and building visual sound connections, making previous studies nonapplicable. In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audiovisual sound source localization in the wild. The key idea is to eliminate the interference by redefining and carving discriminative audio representations. Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals. We thus extend the audio representation with our Audio Instance Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed. Then we erase the influence of the audible but off screen sounds and the silent but visible objects by a Cross modal Referrer module with cross modality distillation. Quantitative and qualitative evaluations demonstrate that our framework achieves superior results on sound localization tasks, especially under real world scenarios",
    "checked": true,
    "id": "1ef85a73bdc6bd1f2865a9067e383610d608eec9",
    "semantic_title": "visual sound localization in the wild by cross-modal interference erasing",
    "citation_count": 17,
    "authors": [
      "Xian Liu",
      "Rui Qian",
      "Hang Zhou",
      "Di Hu",
      "Weiyao Lin",
      "Ziwei Liu",
      "Bolei Zhou",
      "Xiaowei Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20074": {
    "title": "Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image. It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference. This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection. The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training. The proposed MonoCon is motivated by the Cramer–Wold theorem in measure theory at a high level. In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, which consists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded for better inference efficiency. In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist). It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons. Our code is released at https://git.io/MonoCon",
    "checked": true,
    "id": "2ac06626c66aa7c1b0e7a5b31ee51a088c630d88",
    "semantic_title": "learning auxiliary monocular contexts helps monocular 3d object detection",
    "citation_count": 42,
    "authors": [
      "Xianpeng Liu",
      "Nan Xue",
      "Tianfu Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20075": {
    "title": "Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "Human-Object Interaction (HOI) detection plays a core role in activity understanding. As a compositional learning problem (human-verb-object), studying its generalization matters. However, widely-used metric mean average precision (mAP) fails to model the compositional generalization well. Thus, we propose a novel metric, mPD (mean Performance Degradation), as a complementary of mAP to evaluate the performance gap among compositions of different objects and the same verb. Surprisingly, mPD reveals that previous methods usually generalize poorly. With mPD as a cue, we propose Object Category (OC) Immunity to boost HOI generalization. The idea is to prevent model from learning spurious object-verb correlations as a short-cut to over-fit the train set. To achieve OC-immunity, we propose an OC-immune network that decouples the inputs from OC, extracts OC-immune representations, and leverages uncertainty quantification to generalize to unseen objects. In both conventional and zero-shot experiments, our method achieves decent improvements. To fully evaluate the generalization, we design a new and more difficult benchmark, on which we present significant advantage. The code is available at https://github.com/Foruck/OC-Immunity",
    "checked": true,
    "id": "bc0acf5baa46df39dd39f588f4e22e6f06f1c84a",
    "semantic_title": "highlighting object category immunity for the generalization of human-object interaction detection",
    "citation_count": 6,
    "authors": [
      "Xinpeng Liu",
      "Yong-Lu Li",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20076": {
    "title": "DMN4: Few-Shot Learning via Discriminative Mutual Nearest Neighbor Neural Network",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) aims to classify images under low-data regimes, where the conventional pooled global feature is likely to lose useful local characteristics. Recent work has achieved promising performances by using deep descriptors. They generally take all deep descriptors from neural networks into consideration while ignoring that some of them are useless in classification due to their limited receptive field, e.g., task-irrelevant descriptors could be misleading and multiple aggregative descriptors from background clutter could even overwhelm the object's presence. In this paper, we argue that a Mutual Nearest Neighbor (MNN) relation should be established to explicitly select the query descriptors that are most relevant to each task and discard less relevant ones from aggregative clutters in FSL. Specifically, we propose Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive experiments demonstrate that our method outperforms the existing state-of-the-arts on both fine-grained and generalized datasets",
    "checked": true,
    "id": "4ee1d92eacfc04f9713d187cc3772a574610bd36",
    "semantic_title": "dmn4: few-shot learning via discriminative mutual nearest neighbor neural network",
    "citation_count": 8,
    "authors": [
      "Yang Liu",
      "Tu Zheng",
      "Jie Song",
      "Deng Cai",
      "Xiaofei He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20077": {
    "title": "Multi-Knowledge Aggregation and Transfer for Semantic Segmentation",
    "volume": "main",
    "abstract": "As a popular deep neural networks (DNN) compression technique, knowledge distillation (KD) has attracted increasing attentions recently. Existing KD methods usually utilize one kind of knowledge in an intermediate layer of DNN for classification tasks to transfer useful information from cumbersome teacher networks to compact student networks. However, this paradigm is not very suitable for semantic segmentation, a comprehensive vision task based on both pixel-level and contextual information, since it cannot provide rich information for distillation. In this paper, we propose a novel multi-knowledge aggregation and transfer (MKAT) framework to comprehensively distill knowledge within an intermediate layer for semantic segmentation. Specifically, the proposed framework consists of three parts: Independent Transformers and Encoders module (ITE), Auxiliary Prediction Branch (APB), and Mutual Label Calibration (MLC) mechanism, which can take advantage of abundant knowledge from intermediate features. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on three segmentation datasets: Pascal VOC, Cityscapes, and CamVid, showing that MKAT outperforms the other KD methods",
    "checked": true,
    "id": "8525f452188a9eb5880fc680b3cb960f152a1d88",
    "semantic_title": "multi-knowledge aggregation and transfer for semantic segmentation",
    "citation_count": 3,
    "authors": [
      "Yuang Liu",
      "Wei Zhang",
      "Jun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20078": {
    "title": "Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency",
    "volume": "main",
    "abstract": "In recent years, creative content generations like style transfer and neural photo editing have attracted more and more attention. Among these, cartoonization of real-world scenes has promising applications in entertainment and industry. Different from image translations focusing on improving the style effect of generated images, video cartoonization has additional requirements on the temporal consistency. In this paper, we propose a spatially-adaptive semantic alignment framework with perceptual motion consistency for coherent video cartoonization in an unsupervised manner. The semantic alignment module is designed to restore deformation of semantic structure caused by spatial information lost in the encoder-decoder architecture. Furthermore, we introduce the spatio-temporal correlative map as a style-independent, global-aware regularization on perceptual motion consistency. Deriving from similarity measurement of high-level features in photo and cartoon frames, it captures global semantic information beyond raw pixel-value of optical flow. Besides, the similarity measurement disentangles temporal relationship from domain-specific style properties, which helps regularize the temporal consistency without hurting style effects of cartoon images. Qualitative and quantitative experiments demonstrate our method is able to generate highly stylistic and temporal consistent cartoon videos",
    "checked": true,
    "id": "482411d98c2ee57cbf8e9acf2f75d50987f68484",
    "semantic_title": "unsupervised coherent video cartoonization with perceptual motion consistency",
    "citation_count": 2,
    "authors": [
      "Zhenhuan Liu",
      "Liang Li",
      "Huajie Jiang",
      "Xin Jin",
      "Dandan Tu",
      "Shuhui Wang",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20079": {
    "title": "Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL), especially contrastive methods, has raised attraction recently as it learns effective transferable representations without semantic annotations. A common practice for self-supervised pre-training is to use as much data as possible. For a specific downstream task, however, involving irrelevant data in pre-training may degenerate the downstream performance, observed from our extensive experiments. On the other hand, for existing SSL methods, it is burdensome and infeasible to use different downstream-task-customized datasets in pre-training for different tasks. To address this issue, we propose a novel SSL paradigm called Scalable Dynamic Routing (SDR), which can be trained once and deployed efficiently to different downstream tasks with task-customized pre-trained models. Specifically, we construct the SDRnet with various sub-nets and train each sub-net with only one subset of the data by data-aware progressive training. When a downstream task arrives, we route among all the pre-trained sub-nets to get the best along with its corresponding weights. Experiment results show that our SDR can train 256 sub-nets on ImageNet simultaneously, which provides better transfer performance than a unified model trained on the full ImageNet, achieving state-of-the-art (SOTA) averaged accuracy over 11 downstream classification tasks and AP on PASCAL VOC detection task",
    "checked": true,
    "id": "9176415ed922f8a7e7bb9ab7c5974a60c1ec6309",
    "semantic_title": "task-customized self-supervised pre-training with scalable dynamic routing",
    "citation_count": 11,
    "authors": [
      "Zhili LIU",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Kai Chen",
      "Chunjing Xu",
      "Zhenguo Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20080": {
    "title": "Pose Guided Image Generation from Misaligned Sources via Residual Flow Based Correction",
    "volume": "main",
    "abstract": "Generating new images with desired properties (e.g. new view/poses) from source images has been enthusiastically pursued recently, due to its wide range of potential applications. One way to ensure high-quality generation is to use multiple sources with complementary information such as different views of the same object. However, as source images are often misaligned due to the large disparities among the camera settings, strong assumptions have been made in the past with respect to the camera(s) or/and the object in interest, limiting the application of such techniques. Therefore, we propose a new general approach which models multiple types of variations among sources, such as view angles, poses, facial expressions, in a unified framework, so that it can be employed on datasets of vastly different nature. We verify our approach on a variety of data including humans bodies, faces, city scenes and 3D objects. Both the qualitative and quantitative results demonstrate the better performance of our method than the state of the art",
    "checked": true,
    "id": "5d46a99f72e9e7710a750b99723b3930c05aab00",
    "semantic_title": "pose guided image generation from misaligned sources via residual flow based correction",
    "citation_count": 1,
    "authors": [
      "Jiawei Lu",
      "He Wang",
      "Tianjia Shao",
      "Yin Yang",
      "Kun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20081": {
    "title": "PMAL: Open Set Recognition via Robust Prototype Mining",
    "volume": "main",
    "abstract": "Open Set Recognition (OSR) has been an emerging topic. Besides recognizing predefined classes, the system needs to reject the unknowns. Prototype learning is a potential manner to handle the problem, as its ability to improve intra-class compactness of representations is much needed in discrimination between the known and the unknowns. In this work, we propose a novel Prototype Mining And Learning (PMAL) framework. It has a prototype mining mechanism before the phase of optimizing embedding space, explicitly considering two crucial properties, namely high-quality and diversity of the prototype set. Concretely, a set of high-quality candidates are firstly extracted from training samples based on data uncertainty learning, avoiding the interference from unexpected noise. Considering the multifarious appearance of objects even in a single category, a diversity-based strategy for prototype set filtering is proposed. Accordingly, the embedding space can be better optimized to discriminate therein the predefined classes and between known and unknowns. Extensive experiments verify the two good characteristics (i.e., high-quality and diversity) embraced in prototype mining, and show the remarkable performance of the proposed framework compared to state-of-the-arts",
    "checked": true,
    "id": "95a676640a57cd0c3d98d38f3e645241a3af96ed",
    "semantic_title": "pmal: open set recognition via robust prototype mining",
    "citation_count": 22,
    "authors": [
      "Jing Lu",
      "Yunlu Xu",
      "Hao Li",
      "Zhanzhan Cheng",
      "Yi Niu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20082": {
    "title": "Barely-Supervised Learning: Semi-supervised Learning with Very Few Labeled Images",
    "volume": "main",
    "abstract": "This paper tackles the problem of semi-supervised learning when the set of labeled samples is limited to a small number of images per class, typically less than 10, problem that we refer to as barely-supervised learning. We analyze in depth the behavior of a state-of-the-art semi-supervised method, FixMatch, which relies on a weakly-augmented version of an image to obtain supervision signal for a more strongly-augmented version. We show that it frequently fails in barely-supervised scenarios, due to a lack of training signal when no pseudo-label can be predicted with high confidence. We propose a method to leverage self-supervised methods that provides training signal in the absence of confident pseudo-labels. We then propose two methods to refine the pseudo-label selection process which lead to further improvements.The first one relies on a per-sample history of the model predictions, akin to a voting scheme. The second iteratively up-dates class-dependent confidence thresholds to better explore classes that are under-represented in the pseudo-labels. Our experiments show that our approach performs significantly better on STL-10 in the barely-supervised regime,e.g. with 4 or 8 labeled images per class",
    "checked": true,
    "id": "2bd5c2840ee9d499137895197b93cef129c22b60",
    "semantic_title": "barely-supervised learning: semi-supervised learning with very few labeled images",
    "citation_count": 13,
    "authors": [
      "Thomas Lucas",
      "Philippe Weinzaepfel",
      "Gregory Rogez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20083": {
    "title": "Learning Optical Flow with Adaptive Graph Reasoning",
    "volume": "main",
    "abstract": "Estimating per-pixel motion between video frames, known as optical flow, is a long-standing problem in video understanding and analysis. Most contemporary optical flow techniques largely focus on addressing the cross-image matching with feature similarity, with few methods considering how to explicitly reason over the given scene for achieving a holistic motion understanding. In this work, taking a fresh perspective, we introduce a novel graph-based approach, called adaptive graph reasoning for optical flow (AGFlow), to emphasize the value of scene/context information in optical flow. Our key idea is to decouple the context reasoning from the matching procedure, and exploit scene information to effectively assist motion estimation by learning to reason over the adaptive graph. The proposed AGFlow can effectively exploit the context information and incorporate it within the matching procedure, producing more robust and accurate results. On both Sintel clean and final passes, our AGFlow achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming state-of-the-art approaches by 11.2% and 13.6%, respectively. Code is publicly available at https://github.com/megvii-research/AGFlow",
    "checked": true,
    "id": "3ae69ec5a9fd4f28dfc495ddf1259e936b140000",
    "semantic_title": "learning optical flow with adaptive graph reasoning",
    "citation_count": 21,
    "authors": [
      "Ao Luo",
      "Fan Yang",
      "Kunming Luo",
      "Xin Li",
      "Haoqiang Fan",
      "Shuaicheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20084": {
    "title": "A Fusion-Denoising Attack on InstaHide with Data Augmentation",
    "volume": "main",
    "abstract": "InstaHide is a state-of-the-art mechanism for protecting private training images, by mixing multiple private images and modifying them such that their visual features are indistinguishable to the naked eye. In recent work, however, Carlini et al. show that it is possible to reconstruct private images from the encrypted dataset generated by InstaHide. Nevertheless, we demonstrate that Carlini et al.'s attack can be easily defeated by incorporating data augmentation into InstaHide. This leads to a natural question: is InstaHide with data augmentation secure? In this paper, we provide a negative answer to this question, by devising an attack for recovering private images from the outputs of InstaHide even when data augmentation is present. The basic idea is to use a comparative network to identify encrypted images that are likely to correspond to the same private image, and then employ a fusion-denoising network for restoring the private image from the encrypted ones, taking into account the effects of data augmentation. Extensive experiments demonstrate the effectiveness of the proposed attack in comparison to Carlini et al.'s attack",
    "checked": true,
    "id": "61483fedb6d1cfc4b2786cc043c8f1c15ad5dfb7",
    "semantic_title": "a fusion-denoising attack on instahide with data augmentation",
    "citation_count": 3,
    "authors": [
      "Xinjian Luo",
      "Xiaokui Xiao",
      "Yuncheng Wu",
      "Juncheng Liu",
      "Beng Chin Ooi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20085": {
    "title": "Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation",
    "volume": "main",
    "abstract": "How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs",
    "checked": true,
    "id": "49898d5e12569415d13d39b30313e4c6a4f1b1d8",
    "semantic_title": "deep neural networks learn meta-structures from noisy labels in semantic segmentation",
    "citation_count": 5,
    "authors": [
      "Yaoru Luo",
      "Guole Liu",
      "Yuanhao Guo",
      "Ge Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20086": {
    "title": "Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration",
    "volume": "main",
    "abstract": "Large deformations of organs, caused by diverse shapes and nonlinear shape changes, pose a significant challenge for medical image registration. Traditional registration methods need to iteratively optimize an objective function via a specific deformation model along with meticulous parameter tuning, but which have limited capabilities in registering images with large deformations. While deep learning-based methods can learn the complex mapping from input images to their respective deformation field, it is regression-based and is prone to be stuck at local minima, particularly when large deformations are involved. To this end, we present Stochastic Planner-Actor-Critic (spac), a novel reinforcement learning-based framework that performs step-wise registration. The key notion is warping a moving image successively by each time step to finally align to a fixed image. Considering that it is challenging to handle high dimensional continuous action and state spaces in the conventional reinforcement learning (RL) framework, we introduce a new concept `Plan' to the standard Actor-Critic model, which is of low dimension and can facilitate the actor to generate a tractable high dimensional action. The entire framework is based on unsupervised training and operates in an end-to-end manner. We evaluate our method on several 2D and 3D medical image datasets, some of which contain large deformations. Our empirical results highlight that our work achieves consistent, significant gains and outperforms state-of-the-art methods",
    "checked": true,
    "id": "9a4fb2cca17f777650e8de7bb135e9d6ce992485",
    "semantic_title": "stochastic planner-actor-critic for unsupervised deformable image registration",
    "citation_count": 5,
    "authors": [
      "Ziwei Luo",
      "Jing Hu",
      "Xin Wang",
      "Shu Hu",
      "Bin Kong",
      "Youbing Yin",
      "Qi Song",
      "Xi Wu",
      "Siwei Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20087": {
    "title": "Adaptive Poincaré Point to Set Distance for Few-Shot Classification",
    "volume": "main",
    "abstract": "Learning and generalizing from limited examples, i.e., few-shot learning, is of core importance to many real-world vision applications. A principal way of achieving few-shot learning is to realize an embedding where samples from different classes are distinctive. Recent studies suggest that embedding via hyperbolic geometry enjoys low distortion for hierarchical and structured data, making it suitable for few-shot learning. In this paper, we propose to learn a context-aware hyperbolic metric to characterize the distance between a point and a set associated with a learned set to set distance. To this end, we formulate the metric as a weighted sum on the tangent bundle of the hyperbolic space and develop a mechanism to obtain the weights adaptively, based on the constellation of the points. This not only makes the metric local but also dependent on the task in hand, meaning that the metric will adapt depending on the samples that it compares. We empirically show that such metric yields robustness in the presence of outliers and achieves a tangible improvement over baseline models. This includes the state-of-the-art results on five popular few-shot classification benchmarks, namely mini-ImageNet, tiered-ImageNet, Caltech-UCSD Birds-200-2011(CUB), CIFAR-FS, and FC100",
    "checked": true,
    "id": "8fcf2c89df04c9cf55d2a8d50567f7716b08e006",
    "semantic_title": "adaptive poincaré point to set distance for few-shot classification",
    "citation_count": 25,
    "authors": [
      "Rongkai Ma",
      "Pengfei Fang",
      "Tom Drummond",
      "Mehrtash Harandi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20088": {
    "title": "Generative Adaptive Convolutions for Real-World Noisy Image Denoising",
    "volume": "main",
    "abstract": "Recently, deep learning techniques are soaring and have shown dramatic improvements in real-world noisy image denoising. However, the statistics of real noise generally vary with different camera sensors and in-camera signal processing pipelines. This will induce problems of most deep denoisers for the overfitting or degrading performance due to the noise discrepancy between the training and test sets. To remedy this issue, we propose a novel flexible and adaptive denoising network, coined as FADNet. Our FADNet is equipped with a plane dynamic filter module, which generates weight filters with flexibility that can adapt to the specific input and thereby impedes the FADNet from overfitting to the training data. Specifically, we exploit the advantage of the spatial and channel attention, and utilize this to devise a decoupling filter generation scheme. The generated filters are conditioned on the input and collaboratively applied to the decoded features for representation capability enhancement. We additionally introduce the Fourier transform and its inverse to guide the predicted weight filters to adapt to the noisy input with respect to the image contents. Experimental results demonstrate the superior denoising performances of the proposed FADNet versus the state-of-the-art. In contrast to the existing deep denoisers, our FADNet is not only flexible and efficient, but also exhibits a compelling generalization capability, enjoying tremendous potential for practical usage",
    "checked": true,
    "id": "e6919b4e35a0565c438828b039c3dbed6e364e50",
    "semantic_title": "generative adaptive convolutions for real-world noisy image denoising",
    "citation_count": 9,
    "authors": [
      "Ruijun Ma",
      "Shuyi Li",
      "Bob Zhang",
      "Zhengming Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20089": {
    "title": "REMOTE: Reinforced Motion Transformation Network for Semi-supervised 2D Pose Estimation in Videos",
    "volume": "main",
    "abstract": "Existing approaches for 2D pose estimation in videos often require a large number of dense annotations, which are costly and labor intensive to acquire. In this paper, we propose a semi-supervised REinforced MOtion Transformation nEtwork (REMOTE) to leverage a few labeled frames and temporal pose variations in videos, which enables effective learning of 2D pose estimation in sparsely annotated videos. Specifically, we introduce a Motion Transformer (MT) module to perform cross frame reconstruction, aiming to learn motion dynamic knowledge in videos. Besides, a novel reinforcement learning-based Frame Selection Agent (FSA) is designed within our framework, which is able to harness informative frame pairs on the fly to enhance the pose estimator under our cross reconstruction mechanism. We conduct extensive experiments that show the efficacy of our proposed REMOTE framework",
    "checked": true,
    "id": "09f5470f779ddf25e99d4fd3c9078c1fe1782368",
    "semantic_title": "remote: reinforced motion transformation network for semi-supervised 2d pose estimation in videos",
    "citation_count": 3,
    "authors": [
      "Xianzheng Ma",
      "Hossein Rahmani",
      "Zhipeng Fan",
      "Bin Yang",
      "Jun Chen",
      "Jun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20090": {
    "title": "Learning from the Target: Dual Prototype Network for Few Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "Due to the scarcity of annotated samples, the diversity between support set and query set becomes the main obstacle for few shot semantic segmentation. Most existing prototype-based approaches only exploit the prototype from the support feature and ignore the information from the query sample, failing to remove this obstacle.In this paper, we proposes a dual prototype network (DPNet) to dispose of few shot semantic segmentation from a new perspective. Along with the prototype extracted from the support set, we propose to build the pseudo-prototype based on foreground features in the query image. To achieve this goal, the cycle comparison module is developed to select reliable foreground features and generate the pseudo-prototype with them. Then, a prototype interaction module is utilized to integrate the information of the prototype and the pseudo-prototype based on their underlying correlation. Finally, a multi-scale fusion module is introduced to capture contextual information during the dense comparison between prototype (pseudo-prototype) and query feature. Extensive experiments conducted on two benchmarks demonstrate that our method exceeds previous state-of-the-arts with a sizable margin, verifying the effectiveness of the proposed method",
    "checked": true,
    "id": "755e0a666b0ae37206eb2387462e9f9e59c2331e",
    "semantic_title": "learning from the target: dual prototype network for few shot semantic segmentation",
    "citation_count": 6,
    "authors": [
      "Binjie Mao",
      "Xinbang Zhang",
      "Lingfeng Wang",
      "Qian Zhang",
      "Shiming Xiang",
      "Chunhong Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20091": {
    "title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
    "volume": "main",
    "abstract": "Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis. While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way. Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN. In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design. Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view",
    "checked": true,
    "id": "e3e133722951392bb3c26b397e723f892ea98a6e",
    "semantic_title": "most-gan: 3d morphable stylegan for disentangled face image manipulation",
    "citation_count": 16,
    "authors": [
      "Safa C. Medin",
      "Bernhard Egger",
      "Anoop Cherian",
      "Ye Wang",
      "Joshua B. Tenenbaum",
      "Xiaoming Liu",
      "Tim K. Marks"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20092": {
    "title": "Towards Bridging Sample Complexity and Model Capacity",
    "volume": "main",
    "abstract": "In this paper, we give a new definition for sample complexity, and further develop a theoretical analysis to bridge the gap between sample complexity and model capacity. In contrast to previous works which study on some toy samples, we conduct our analysis on more general data space, and build a qualitative relationship from sample complexity to model capacity required to achieve comparable performance. Besides, we introduce a simple indicator to evaluate the sample complexity based on continuous mapping. Moreover, we further analysis the relationship between sample complexity and data distribution, which paves the way to understand the present representation learning. Extensive experiments on several datasets well demonstrate the effectiveness of our evaluation method",
    "checked": true,
    "id": "c3ee33a96fabb4888f5e4368044e7bbede2160db",
    "semantic_title": "towards bridging sample complexity and model capacity",
    "citation_count": 1,
    "authors": [
      "Shibin Mei",
      "Chenglong Zhao",
      "Shengchao Yuan",
      "Bingbing Ni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20093": {
    "title": "Towards Accurate Facial Motion Retargeting with Identity-Consistent and Expression-Exclusive Constraints",
    "volume": "main",
    "abstract": "We address the problem of facial motion retargeting that aims to transfer facial motion from a 2D face image to 3D characters. Existing methods often formulate this problem as a 3D face reconstruction problem, which estimates the face attributes such as face identity and expression from face images. However, due to the lack of ground-truth labels for both identity and expression, most 3D-face reconstruction-based methods fail to capture the facial identity and expression accurately. As a result, these methods may not achieve promising performance. To address this, we propose an identity-consistent constraint to learn accurate identities by encouraging consistent identity prediction across multiple frames. Based on a more accurate identity, we are able to obtain a more accurate facial expression. Moreover, we further propose an expression-exclusive constraint to improve performance by avoiding the co-occurrence of contradictory expression units (e.g., ``brow lower'' vs. ``brow raise''). Extensive experiments on facial motion retargeting and 3D face reconstruction tasks demonstrate the superiority of the proposed method over existing methods. Our code and supplementary materials are available at https://github.com/deepmo24/CPEM",
    "checked": true,
    "id": "d4dff3dd33f9ab1642928b9a02faf9c7b2f6c075",
    "semantic_title": "towards accurate facial motion retargeting with identity-consistent and expression-exclusive constraints",
    "citation_count": 1,
    "authors": [
      "Langyuan Mo",
      "Haokun Li",
      "Chaoyang Zou",
      "Yubing Zhang",
      "Ming Yang",
      "Yihong Yang",
      "Mingkui Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20094": {
    "title": "Can Vision Transformers Learn without Natural Images?",
    "volume": "main",
    "abstract": "Is it possible to complete Vision Transformer (ViT) pre-training without natural images and human-annotated labels? This question has become increasingly relevant in recent months because while current ViT pre-training tends to rely heavily on a large number of natural images and human-annotated labels, the recent use of natural images has resulted in problems related to privacy violation, inadequate fairness protection, and the need for labor-intensive annotations. In this paper, we experimentally verify that the results of formula-driven supervised learning (FDSL) framework are comparable with, and can even partially outperform, sophisticated self-supervised learning (SSL) methods like SimCLRv2 and MoCov2 without using any natural images in the pre-training phase. We also consider ways to reorganize FractalDB generation based on our tentative conclusion that there is room for configuration improvements in the iterated function system (IFS) parameter settings of such databases. Moreover, we show that while ViTs pre-trained without natural images produce visualizations that are somewhat different from ImageNet pre-trained ViTs, they can still interpret natural image datasets to a large extent. Finally, in experiments using the CIFAR-10 dataset, we show that our model achieved a performance rate of 97.8, which is comparable to the rate of 97.4 achieved with SimCLRv2 and 98.0 achieved with ImageNet",
    "checked": true,
    "id": "b9ce9fea4634d6bfed5af2f4de410822295b3630",
    "semantic_title": "can vision transformers learn without natural images?",
    "citation_count": 18,
    "authors": [
      "Kodai Nakashima",
      "Hirokatsu Kataoka",
      "Asato Matsumoto",
      "Kenji Iwata",
      "Nakamasa Inoue",
      "Yutaka Satoh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20095": {
    "title": "Federated Learning for Face Recognition with Gradient Correction",
    "volume": "main",
    "abstract": "With increasing appealing to privacy issues in face recognition, federated learning has emerged as one of the most prevalent approaches to study the unconstrained face recognition problem with private decentralized data. However, conventional decentralized federated algorithm sharing whole parameters of networks among clients suffers from privacy leakage in face recognition scene. In this work, we introduce a framework, FedGC, to tackle federated learning for face recognition and guarantees higher privacy. We explore a novel idea of correcting gradients from the perspective of backward propagation and propose a softmax-based regularizer to correct gradients of class embeddings by precisely injecting a cross-client gradient term. Theoretically, we show that FedGC constitutes a valid loss function similar to standard softmax. Extensive experiments have been conducted to validate the superiority of FedGC which can match the performance of conventional centralized methods utilizing full training dataset on several popular benchmark datasets",
    "checked": true,
    "id": "ebd34a9c319d66922f4fe61ffb6efea8811be6db",
    "semantic_title": "federated learning for face recognition with gradient correction",
    "citation_count": 15,
    "authors": [
      "Yifan Niu",
      "Weihong Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20096": {
    "title": "Restorable Image Operators with Quasi-Invertible Networks",
    "volume": "main",
    "abstract": "Image operators have been extensively applied to create visually attractive photos for users to share processed images on social media. However, most image operators often smooth out details or generate textures after the processing, which removes the original content and raises challenges for restoring the original image. To resolve this issue, we propose a quasi-invertible model that learns common image processing operators in a restorable fashion: the learned image operators can generate visually pleasing results with the original content embedded. Our model is trained on input-output pairs that represent an image processing operator's behavior and uses a network that consists of an invertible branch and a non-invertible branch to increase our model's approximation capability. We evaluate the proposed model on ten image operators, including detail enhancement, abstraction, blur, photographic style, and non-photorealistic style. Extensive experiments show that our approach outperforms relevant baselines in the restoration quality, and the learned restorable operator is fast in inference and robust to compression. Furthermore, we demonstrate that the invertible operator can be easily applied to practical applications such as restorable human face retouching and highlight preserved exposure adjustment",
    "checked": true,
    "id": "f5a30a8a0f3a3ff431687f421f114232cf8816d3",
    "semantic_title": "restorable image operators with quasi-invertible networks",
    "citation_count": 2,
    "authors": [
      "Hao Ouyang",
      "Tengfei Wang",
      "Qifeng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20097": {
    "title": "TEACh: Task-Driven Embodied Agents That Chat",
    "volume": "main",
    "abstract": "Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human-human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\", asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution",
    "checked": true,
    "id": "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424",
    "semantic_title": "teach: task-driven embodied agents that chat",
    "citation_count": 86,
    "authors": [
      "Aishwarya Padmakumar",
      "Jesse Thomason",
      "Ayush Shrivastava",
      "Patrick Lange",
      "Anjali Narayan-Chen",
      "Spandana Gella",
      "Robinson Piramuthu",
      "Gokhan Tur",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20098": {
    "title": "Label-Efficient Hybrid-Supervised Learning for Medical Image Segmentation",
    "volume": "main",
    "abstract": "Due to the lack of expertise for medical image annotation, the investigation of label-efficient methodology for medical image segmentation becomes a heated topic. Recent progresses focus on the efficient utilization of weak annotations together with few strongly-annotated labels so as to achieve comparable segmentation performance in many unprofessional scenarios. However, these approaches only concentrate on the supervision inconsistency between strongly- and weakly-annotated instances but ignore the instance inconsistency inside the weakly-annotated instances, which inevitably leads to performance degradation. To address this problem, we propose a novel label-efficient hybrid-supervised framework, which considers each weakly-annotated instance individually and learns its weight guided by the gradient direction of the strongly-annotated instances, so that the high-quality prior in the strongly-annotated instances is better exploited and the weakly-annotated instances are depicted more precisely. Specially, our designed dynamic instance indicator (DII) realizes the above objectives, and is adapted to our dynamic co-regularization (DCR) framework further to alleviate the erroneous accumulation from distortions of weak annotations. Extensive experiments on two hybrid-supervised medical segmentation datasets demonstrate that with only 10% strong labels, the proposed framework can leverage the weak labels efficiently and achieve competitive performance against the 100% strong-label supervised scenario",
    "checked": true,
    "id": "e16a91912ab81b47ba07d42f0a50d632e008a65c",
    "semantic_title": "label-efficient hybrid-supervised learning for medical image segmentation",
    "citation_count": 4,
    "authors": [
      "Junwen Pan",
      "Qi Bi",
      "Yanzhan Yang",
      "Pengfei Zhu",
      "Cheng Bian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20099": {
    "title": "Less Is More: Pay Less Attention in Vision Transformers",
    "volume": "main",
    "abstract": "Transformers have become one of the dominant architectures in deep learning, particularly as a powerful alternative to convolutional neural networks (CNNs) in computer vision. However, Transformer training and inference in previous works can be prohibitively expensive due to the quadratic complexity of self-attention over a long sequence of representations, especially for high-resolution dense prediction tasks. To this end, we present a novel Less attention vIsion Transformer (LIT), building upon the fact that the early self-attention layers in Transformers still focus on local patterns and bring minor benefits in recent hierarchical vision Transformers. Specifically, we propose a hierarchical Transformer where we use pure multi-layer perceptrons (MLPs) to encode rich local patterns in the early stages while applying self-attention modules to capture longer dependencies in deeper layers. Moreover, we further propose a learned deformable token merging module to adaptively fuse informative patches in a non-uniform manner. The proposed LIT achieves promising performance on image recognition tasks, including image classification, object detection and instance segmentation, serving as a strong backbone for many vision tasks. Code is available at https://github.com/zip-group/LIT",
    "checked": true,
    "id": "a0964686d80e173529efca6377f47e6a1b2fe69a",
    "semantic_title": "less is more: pay less attention in vision transformers",
    "citation_count": 34,
    "authors": [
      "Zizheng Pan",
      "Bohan Zhuang",
      "Haoyu He",
      "Jing Liu",
      "Jianfei Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20100": {
    "title": "Unsupervised Representation for Semantic Segmentation by Implicit Cycle-Attention Contrastive Learning",
    "volume": "main",
    "abstract": "We study the unsupervised representation learning for the semantic segmentation task. Different from previous works that aim at providing unsupervised pre-trained backbones for segmentation models which need further supervised fine-tune, here, we focus on providing representation that is only trained by unsupervised methods. This means models need to directly generate pixel-level, linearly separable semantic results. We first explore and present two factors that have significant effects on segmentation under the contrastive learning framework: 1) the difficulty and diversity of the positive contrastive pairs, 2) the balance of global and local features. With the intention of optimizing these factors, we propose the cycle-attention contrastive learning (CACL). CACL makes use of semantic continuity of video frames, adopting unsupervised cycle-consistent attention mechanism to implicitly conduct contrastive learning with difficult, global-local-balanced positive pixel pairs. Compared with baseline model MoCo-v2 and other unsupervised methods, CACL demonstrates consistently superior performance on PASCAL VOC (+4.5 mIoU) and Cityscapes (+4.5 mIoU) datasets",
    "checked": true,
    "id": "4105b681f89911f3c70c7b929789f63163b6c0ac",
    "semantic_title": "unsupervised representation for semantic segmentation by implicit cycle-attention contrastive learning",
    "citation_count": 6,
    "authors": [
      "Bo Pang",
      "Yizhuo Li",
      "Yifan Zhang",
      "Gao Peng",
      "Jiajun Tang",
      "Kaiwen Zha",
      "Jiefeng Li",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20101": {
    "title": "Graph-Based Point Tracker for 3D Object Tracking in Point Clouds",
    "volume": "main",
    "abstract": "In this paper, a new deep learning network named as graph-based point tracker (GPT) is proposed for 3D object tracking in point clouds. GPT is not based on Siamese network applied to template and search area, but it is based on the transfer of target clue from the template to the search area. GPT is end-to-end trainable. GPT has two new modules: graph feature augmentation (GFA) and improved target clue (ITC) module. The key idea of GFA is to exploit one-to-many relationship between template and search area points using a bipartite graph. In GFA, edge features of the bipartite graph are generated by transferring the target clues of template points to search area points through edge convolution. It captures the relationship between template and search area points effectively from the perspective of geometry and shape of two point clouds. The second module is ITC. The key idea of ITC is to embed the information of the center of the target into the edges of the bipartite graph via Hough voting, strengthening the discriminative power of GFA. Both modules significantly contribute to the improvement of GPT by transferring geometric and shape information including target center from target template to search area effectively. Experiments on the KITTI tracking dataset show that GPT achieves state-of-the-art performance and can run in real-time",
    "checked": true,
    "id": "806c2f1ff94c937b9d0bcda11f4c76df1a996663",
    "semantic_title": "graph-based point tracker for 3d object tracking in point clouds",
    "citation_count": 1,
    "authors": [
      "Minseong Park",
      "Hongje Seong",
      "Wonje Jang",
      "Euntai Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20102": {
    "title": "SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory",
    "volume": "main",
    "abstract": "The challenge of talking face generation from speech lies in aligning two different modal information, audio and video, such that the mouth region corresponds to input audio. Previous methods either exploit audio-visual representation learning or leverage intermediate structural information such as landmarks and 3D models. However, they struggle to synthesize fine details of the lips varying at the phoneme level as they do not sufficiently provide visual information of the lips at the video synthesis step. To overcome this limitation, our work proposes Audio-Lip Memory that brings in visual information of the mouth region corresponding to input audio and enforces fine-grained audio-visual coherence. It stores lip motion features from sequential ground truth images in the value memory and aligns them with corresponding audio features so that they can be retrieved using audio input at inference time. Therefore, using the retrieved lip motion features as visual hints, it can easily correlate audio with visual dynamics in the synthesis step. By analyzing the memory, we demonstrate that unique lip features are stored in each memory slot at the phoneme level, capturing subtle lip motion based on memory addressing. In addition, we introduce visual-visual synchronization loss which can enhance lip-syncing performance when used along with audio-visual synchronization loss in our model. Extensive experiments are performed to verify that our method generates high-quality video with mouth shapes that best align with the input audio, outperforming previous state-of-the-art methods",
    "checked": true,
    "id": "da0b6558b07566b6ad9c30067562004a3ede36b1",
    "semantic_title": "synctalkface: talking face generation with precise lip-syncing via audio-lip memory",
    "citation_count": 32,
    "authors": [
      "Se Jin Park",
      "Minsu Kim",
      "Joanna Hong",
      "Jeongsoo Choi",
      "Yong Man Ro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20103": {
    "title": "Vision Transformers Are Robust Learners",
    "volume": "main",
    "abstract": "Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020). Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0",
    "checked": true,
    "id": "5e4f03f68c6867d850f457dc5cc36738e5dff6c1",
    "semantic_title": "vision transformers are robust learners",
    "citation_count": 186,
    "authors": [
      "Sayak Paul",
      "Pin-Yu Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20104": {
    "title": "Self-Supervised Category-Level 6D Object Pose Estimation with Deep Implicit Shape Representation",
    "volume": "main",
    "abstract": "Category-level 6D pose estimation can be better generalized to unseen objects in a category compared with instance-level 6D pose estimation. However, existing category-level 6D pose estimation methods usually require supervised training with a sufficient number of 6D pose annotations of objects which makes them difficult to be applied in real scenarios. To address this problem, we propose a self-supervised framework for category-level 6D pose estimation in this paper. We leverage DeepSDF as a 3D object representation and design several novel loss functions based on DeepSDF to help the self-supervised model predict unseen object poses without any 6D object pose labels and explicit 3D models in real scenarios. Experiments demonstrate that our method achieves comparable performance with the state-of-the-art fully supervised methods on the category-level NOCS benchmark",
    "checked": true,
    "id": "e8f45eadf79f53c755c98bd94ecad63d5f940370",
    "semantic_title": "self-supervised category-level 6d object pose estimation with deep implicit shape representation",
    "citation_count": 11,
    "authors": [
      "Wanli Peng",
      "Jianhang Yan",
      "Hongtao Wen",
      "Yi Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20105": {
    "title": "Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels",
    "volume": "main",
    "abstract": "Training the multi-label image recognition models with partial labels, in which merely some labels are known while others are unknown for each image, is a considerably challenging and practical task. To address this task, current algorithms mainly depend on pre-training classification or similarity models to generate pseudo labels for the unknown labels. However, these algorithms depend on sufficient multi-label annotations to train the models, leading to poor performance especially with low known label proportion. In this work, we propose to blend category-specific representation across different images to transfer information of known labels to complement unknown labels, which can get rid of pre-training models and thus does not depend on sufficient annotations. To this end, we design a unified semantic-aware representation blending (SARB) framework that exploits instance-level and prototype-level semantic representation to complement unknown labels by two complementary modules: 1) an instance-level representation blending (ILRB) module blends the representations of the known labels in an image to the representations of the unknown labels in another image to complement these unknown labels. 2) a prototype-level representation blending (PLRB) module learns more stable representation prototypes for each category and blends the representation of unknown labels with the prototypes of corresponding labels to complement these labels. Extensive experiments on the MS-COCO, Visual Genome, Pascal VOC 2007 datasets show that the proposed SARB framework obtains superior performance over current leading competitors on all known label proportion settings, i.e., with the mAP improvement of 4.6%, 4.6%, 2.2% on these three datasets when the known label proportion is 10%. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL",
    "checked": true,
    "id": "33bd9c2f77db502ac08ba1327e559b61cc19b5f8",
    "semantic_title": "semantic-aware representation blending for multi-label image recognition with partial labels",
    "citation_count": 2,
    "authors": [
      "Tao Pu",
      "Tianshui Chen",
      "Hefeng Wu",
      "Liang Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20106": {
    "title": "ReX: An Efficient Approach to Reducing Memory Cost in Image Classification",
    "volume": "main",
    "abstract": "Exiting simple samples in adaptive multi-exit networks through early modules is an effective way to achieve high computational efficiency. One can observe that deployments of multi-exit architectures on resource-constrained devices are easily limited by high memory footprint of early modules. In this paper, we propose a novel approach named recurrent aggregation operator (ReX), which uses recurrent neural networks (RNNs) to effectively aggregate intra-patch features within a large receptive field to get delicate local representations, while bypassing large early activations. The resulting model, named ReXNet, can be easily extended to dynamic inference by introducing a novel consistency-based early exit criteria, which is based on the consistency of classification decisions over several modules, rather than the entropy of the prediction distribution. Extensive experiments on two benchmark datasets, i.e., Visual Wake Words, ImageNet-1k, demonstrate that our method consistently reduces the peak RAM and average latency of a wide variety of adaptive models on low-power devices",
    "checked": true,
    "id": "9830d489e32a3e348ccc510e55cf99e8fb2c909f",
    "semantic_title": "rex: an efficient approach to reducing memory cost in image classification",
    "citation_count": 2,
    "authors": [
      "Xuwei Qian",
      "Renlong Hang",
      "Qingshan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20107": {
    "title": "CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation",
    "volume": "main",
    "abstract": "Acquiring the most representative examples via active learning (AL) can benefit many data-dependent computer vision tasks by minimizing efforts of image-level or pixel-wise annotations. In this paper, we propose a novel Collaborative Panoptic-Regional Active Learning framework (CPRAL) to address the semantic segmentation task. For a small batch of images initially sampled with pixel-wise annotations, we employ panoptic information to initially select unlabeled samples. Considering the class imbalance in the segmentation dataset, we import a Regional Gaussian Attention module (RGA) to achieve semantics-biased selection. The subset is highlighted by vote entropy and then attended by Gaussian kernels to maximize the biased regions. We also propose a Contextual Labels Extension (CLE) to boost regional annotations with contextual attention guidance. With the collaboration of semantics-agnostic panoptic matching and region-biased selection and extension, our CPRAL can strike a balance between labeling efforts and performance and compromise the semantics distribution. We perform extensive experiments on Cityscapes and BDD10K datasets and show that CPRAL outperforms the cutting-edge methods with impressive results and less labeling proportion",
    "checked": true,
    "id": "e9d12767240c615495861fac800fba68ad75e99d",
    "semantic_title": "cpral: collaborative panoptic-regional active learning for semantic segmentation",
    "citation_count": 9,
    "authors": [
      "Yu Qiao",
      "Jincheng Zhu",
      "Chengjiang Long",
      "Zeyao Zhang",
      "Yuxin Wang",
      "Zhenjun Du",
      "Xin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20108": {
    "title": "Activation Modulation and Recalibration Scheme for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Image-level weakly supervised semantic segmentation (WSSS) is a fundamental yet challenging computer vision task facilitating scene understanding and automatic driving. Most existing methods resort to classification-based Class Activation Maps (CAMs) to play as the initial pseudo labels, which tend to focus on the discriminative image regions and lack customized characteristics for the segmentation task. To alleviate this issue, we propose a novel activation modulation and recalibration (AMR) scheme, which leverages a spotlight branch and a compensation branch to obtain weighted CAMs that can provide recalibration supervision and task-specific concepts. Specifically, an attention modulation module (AMM) is employed to rearrange the distribution of feature importance from the channel-spatial sequential perspective, which helps to explicitly model channel-wise interdependencies and spatial encodings to adaptively modulate segmentation-oriented activation responses. Furthermore, we introduce a cross pseudo supervision for dual branches, which can be regarded as a semantic similar regularization to mutually refine two branches. Extensive experiments show that AMR establishes a new state-of-the-art performance on the PASCAL VOC 2012 dataset, surpassing not only current methods trained with the image-level of supervision but also some methods relying on stronger supervision, such as saliency label. Experiments also reveal that our scheme is plug-and-play and can be incorporated with other approaches to boost their performance. Our code is available at: https://github.com/jieqin-ai/AMR",
    "checked": true,
    "id": "fd711aa609221ea99f1be7c15d8beba3caca18bf",
    "semantic_title": "activation modulation and recalibration scheme for weakly supervised semantic segmentation",
    "citation_count": 41,
    "authors": [
      "Jie Qin",
      "Jie Wu",
      "Xuefeng Xiao",
      "Lujun Li",
      "Xingang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20109": {
    "title": "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework Using Self-Supervised Multi-Task Learning",
    "volume": "main",
    "abstract": "In this paper, we propose TransMEF, a transformer-based multi-exposure image fusion framework that uses self-supervised multi-task learning. The framework is based on an encoder-decoder network, which can be trained on large natural image datasets and does not require ground truth fusion images. We design three self-supervised reconstruction tasks according to the characteristics of multi-exposure images and conduct these tasks simultaneously using multi-task learning; through this process, the network can learn the characteristics of multi-exposure images and extract more generalized features. In addition, to compensate for the defect in establishing long-range dependencies in CNN-based architectures, we design an encoder that combines a CNN module with a transformer module. This combination enables the network to focus on both local and global information. We evaluated our method and compared it to 11 competitive traditional and deep learning-based methods on the latest released multi-exposure image fusion benchmark dataset, and our method achieved the best performance in both subjective and objective evaluations. Code will be available at https://github.com/miccaiif/TransMEF",
    "checked": true,
    "id": "1fc1077c0245b753269bea93b5d08cdb6e722f54",
    "semantic_title": "transmef: a transformer-based multi-exposure image fusion framework using self-supervised multi-task learning",
    "citation_count": 38,
    "authors": [
      "Linhao Qu",
      "Shaolei Liu",
      "Manning Wang",
      "Zhijian Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20110": {
    "title": "Deep Implicit Statistical Shape Models for 3D Medical Image Delineation",
    "volume": "main",
    "abstract": "3D delineation of anatomical structures is a cardinal goal in medical imaging analysis. Prior to deep learning, statistical shape models (SSMs) that imposed anatomical constraints and produced high quality surfaces were a core technology. Today's fully-convolutional networks (FCNs), while dominant, do not offer these capabilities. We present deep implicit statistical shape models (DISSMs), a new approach that marries the representation power of deep networks with the benefits of SSMs. DISSMs use an implicit representation to produce compact and descriptive deep surface embeddings that permit statistical models of anatomical variance. To reliably fit anatomically plausible shapes to an image, we introduce a novel rigid and non-rigid pose estimation pipeline that is modelled as a Markov decision process (MDP). Intra-dataset experiments on the task of pathological liver segmentation demonstrate that DISSMs can perform more robustly than four leading FCN models, including nnU-Net + an adversarial prior: reducing the mean Hausdorff distance (HD) by 7.5-14.3 mm and improving the worst case Dice-Sørensen coefficient (DSC) by 1.2-2.3%. More critically, cross-dataset experiments on an external and highly challenging clinical dataset demonstrate that DISSMs improve the mean DSC and HD by 2.1-5.9% and 9.9-24.5 mm, respectively, and the worst-case DSC by 5.4-7.3%. Supplemental validation on a highly challenging and low-contrast larynx dataset further demonstrate DISSM's improvements. These improvements are over and above any benefits from representing delineations with high-quality surfaces",
    "checked": true,
    "id": "7925d5d8676a57ea563b027141137f71e9c915ad",
    "semantic_title": "deep implicit statistical shape models for 3d medical image delineation",
    "citation_count": 18,
    "authors": [
      "Ashwin Raju",
      "Shun Miao",
      "Dakai Jin",
      "Le Lu",
      "Junzhou Huang",
      "Adam P. Harrison"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20111": {
    "title": "Decompose the Sounds and Pixels, Recompose the Events",
    "volume": "main",
    "abstract": "In this paper, we propose a framework centering around a novel architecture called the Event Decomposition Recomposition Network (EDRNet) to tackle the Audio-Visual Event (AVE) localization problem in the supervised and weakly supervised settings. AVEs in the real world exhibit common unraveling patterns (termed as Event Progress Checkpoints(EPC)), which humans can perceive through the cooperation of their auditory and visual senses. Unlike earlier methods which attempt to recognize entire event sequences, the EDRNet models EPCs and inter-EPC relationships using stacked temporal convolutions. Based on the postulation that EPC representations are theoretically consistent for an event category, we introduce the State Machine Based Video Fusion, a novel augmentation technique that blends source videos using different EPC template sequences. Additionally, we design a new loss function called the Land-Shore-Sea loss to compactify continuous foreground and background representations. Lastly, to alleviate the issue of confusing events during weak supervision, we propose a prediction stabilization method called Bag to Instance Label Correction. Experiments on the AVE dataset show that our collective framework outperforms the state-of-the-art by a sizable margin",
    "checked": true,
    "id": "0f7746dfb0be9ae82a96474855fb6bed1770aee7",
    "semantic_title": "decompose the sounds and pixels, recompose the events",
    "citation_count": 4,
    "authors": [
      "Varshanth R. Rao",
      "Md Ibrahim Khalil",
      "Haoda Li",
      "Peng Dai",
      "Juwei Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20112": {
    "title": "Learning from Label Proportions with Prototypical Contrastive Clustering",
    "volume": "main",
    "abstract": "The use of priors to avoid manual labeling for training machine learning methods has received much attention in the last few years. One of the critical subthemes in this regard is Learning from Label Proportions (LLP), where only the information about class proportions is available for training the models. While various LLP training settings verse in the literature, most approaches focus on bag-level label proportions errors, often leading to suboptimal solutions. This paper proposes a new model that jointly uses prototypical contrastive learning and bag-level cluster proportions to implement efficient LLP classification. Our proposal explicitly relaxes the equipartition constraint commonly used in prototypical contrastive learning methods and incorporates the exact cluster proportions into the optimal transport algorithm used for cluster assignments. At inference time, we compute the clusters' assignment, delivering instance-level classification. We experimented with our method on two widely used image classification benchmarks and report a new state-of-art LLP performance, achieving results close to fully supervised methods",
    "checked": true,
    "id": "222c3b3f0e43c412d82fa838d2ddedd352bd2850",
    "semantic_title": "learning from label proportions with prototypical contrastive clustering",
    "citation_count": 5,
    "authors": [
      "Laura Elena Cué La Rosa",
      "Dário Augusto Borges Oliveira"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20113": {
    "title": "Beyond Learning Features: Training a Fully-Functional Classifier with ZERO Instance-Level Labels",
    "volume": "main",
    "abstract": "We attempt to train deep neural networks for classification without using any labeled data. Existing unsupervised methods, though mine useful clusters or features, require some annotated samples to facilitate the final task-specific predictions. This defeats the true purpose of unsupervised learning and hence we envisage a paradigm of `true' self-supervision, where absolutely no annotated instances are used for training a classifier. The proposed method first pretrains a deep network through self-supervision and performs clustering on the learned features. A classifier layer is then appended to the self-supervised network and is trained by matching the distribution of the predictions to that of a predefined prior. This approach leverages the distribution of labels for supervisory signals and consequently, no image-label pair is needed. Experiments reveal that the method works on major nominal as well as ordinal classification datasets and delivers significant performance",
    "checked": true,
    "id": "811a3df8d87cdc78a42853e3fcb8b43f9cebd97c",
    "semantic_title": "beyond learning features: training a fully-functional classifier with zero instance-level labels",
    "citation_count": 0,
    "authors": [
      "Deepak Babu Sam",
      "Abhinav Agarwalla",
      "Venkatesh Babu Radhakrishnan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20114": {
    "title": "Reference-Guided Pseudo-Label Generation for Medical Semantic Segmentation",
    "volume": "main",
    "abstract": "Producing densely annotated data is a difficult and tedious task for medical imaging applications. To address this problem, we propose a novel approach to generate supervision for semi-supervised semantic segmentation. We argue that visually similar regions between labeled and unlabeled images likely contain the same semantics and therefore should share their label. Following this thought, we use a small number of labeled images as reference material and match pixels in an unlabeled image to the semantic of the best fitting pixel in a reference set. This way, we avoid pitfalls such as confirmation bias, common in purely prediction-based pseudo-labeling. Since our method does not require any architectural changes or accompanying networks, one can easily insert it into existing frameworks. We achieve the same performance as a standard fully supervised model on X-ray anatomy segmentation, albeit using 95% fewer labeled images. Aside from an in-depth analysis of different aspects of our proposed method, we further demonstrate the effectiveness of our reference-guided learning paradigm by comparing our approach against existing methods for retinal fluid segmentation with competitive performance as we improve upon recent work by up to 15% mean IoU",
    "checked": true,
    "id": "5d19cf25333250a274b5883ee08820562733d238",
    "semantic_title": "reference-guided pseudo-label generation for medical semantic segmentation",
    "citation_count": 25,
    "authors": [
      "Constantin Marc Seibold",
      "Simon Reiß",
      "Jens Kleesiek",
      "Rainer Stiefelhagen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20115": {
    "title": "Information-Theoretic Bias Reduction via Causal View of Spurious Correlation",
    "volume": "main",
    "abstract": "We propose an information-theoretic bias measurement technique through a causal interpretation of spurious correlation, which is effective to identify the feature-level algorithmic bias by taking advantage of conditional mutual information. Although several bias measurement methods have been proposed and widely investigated to achieve algorithmic fairness in various tasks such as face recognition, their accuracy- or logit-based metrics are susceptible to leading to trivial prediction score adjustment rather than fundamental bias reduction. Hence, we design a novel debiasing framework against the algorithmic bias, which incorporates a bias regularization loss derived by the proposed information-theoretic bias measurement approach. In addition, we present a simple yet effective unsupervised debiasing technique based on stochastic label noise, which does not require the explicit supervision of bias information. The proposed bias measurement and debiasing approaches are validated in diverse realistic scenarios through extensive experiments on multiple standard benchmarks",
    "checked": true,
    "id": "bea50c27597957c17f4a7085bca5d8e5881ede05",
    "semantic_title": "information-theoretic bias reduction via causal view of spurious correlation",
    "citation_count": 9,
    "authors": [
      "Seonguk Seo",
      "Joon-Young Lee",
      "Bohyung Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20116": {
    "title": "Improving Scene Graph Classification by Exploiting Knowledge from Texts",
    "volume": "main",
    "abstract": "Training scene graph classification models requires a large amount of annotated image data. Meanwhile, scene graphs represent relational knowledge that can be modeled with symbolic data from texts or knowledge graphs. While image annotation demands extensive labor, collecting textual descriptions of natural scenes requires less effort. In this work, we investigate whether textual scene descriptions can substitute for annotated image data. To this end, we employ a scene graph classification framework that is trained not only from annotated images but also from symbolic data. In our architecture, the symbolic entities are first mapped to their correspondent image-grounded representations and then fed into the relational reasoning pipeline. Even though a structured form of knowledge, such as the form in knowledge graphs, is not always available, we can generate it from unstructured texts using a transformer-based language model. We show that by fine-tuning the classification pipeline with the extracted knowledge from texts, we can achieve ~8x more accurate results in scene graph classification, ~3x in object classification, and ~1.5x in predicate classification, compared to the supervised baselines with only 1% of the annotated images",
    "checked": true,
    "id": "cd83523830c9180a7fb1fd32c9e9ca7062dcb540",
    "semantic_title": "improving scene graph classification by exploiting knowledge from texts",
    "citation_count": 6,
    "authors": [
      "Sahand Sharifzadeh",
      "Sina Moayed Baharlou",
      "Martin Schmitt",
      "Hinrich Schütze",
      "Volker Tresp"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20117": {
    "title": "Reliable Inlier Evaluation for Unsupervised Point Cloud Registration",
    "volume": "main",
    "abstract": "Unsupervised point cloud registration algorithm usually suffers from the unsatisfied registration precision in the partially overlapping problem due to the lack of effective inlier evaluation. In this paper, we propose a neighborhood consensus based reliable inlier evaluation method for robust unsupervised point cloud registration. It is expected to capture the discriminative geometric difference between the source neighborhood and the corresponding pseudo target neighborhood for effective inlier distinction. Specifically, our model consists of a matching map refinement module and an inlier evaluation module. In our matching map refinement module, we improve the point-wise matching map estimation by integrating the matching scores of neighbors into it. The aggregated neighborhood information potentially facilitates the discriminative map construction so that high-quality correspondences can be provided for generating the pseudo target point cloud. Based on the observation that the outlier has the significant structure-wise difference between its source neighborhood and corresponding pseudo target neighborhood while this difference for inlier is small, the inlier evaluation module exploits this difference to score the inlier confidence for each estimated correspondence. In particular, we construct an effective graph representation for capturing this geometric difference between the neighborhoods. Finally, with the learned correspondences and the corresponding inlier confidence, we use the weighted SVD algorithm for transformation estimation.Under the unsupervised setting, we exploit the Huber function based global alignment loss, the local neighborhood consensus loss and spatial consistency loss for model optimization. The experimental results on extensive datasets demonstrate that our unsupervised point cloud registration method can yield comparable performance",
    "checked": true,
    "id": "93eb45a3ddcfe0058e018760fda8eb0dfb4f9fa7",
    "semantic_title": "reliable inlier evaluation for unsupervised point cloud registration",
    "citation_count": 14,
    "authors": [
      "Yaqi Shen",
      "Le Hui",
      "Haobo Jiang",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20118": {
    "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
    "volume": "main",
    "abstract": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction",
    "checked": true,
    "id": "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b",
    "semantic_title": "explainable survival analysis with convolution-involved vision transformer",
    "citation_count": 4,
    "authors": [
      "Yifan Shen",
      "Li Liu",
      "Zhihao Tang",
      "Zongyi Chen",
      "Guixiang Ma",
      "Jiyan Dong",
      "Xi Zhang",
      "Lin Yang",
      "Qingfeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20119": {
    "title": "Un-mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning",
    "volume": "main",
    "abstract": "The recently advanced unsupervised learning approaches use the siamese-like framework to compare two \"views\" from the same image for learning representations. Making the two views distinctive is a core to guarantee that unsupervised methods can learn meaningful information. However, such frameworks are sometimes fragile on overfitting if the augmentations used for generating two views are not strong enough, causing the over-confident issue on the training data. This drawback hinders the model from learning subtle variance and fine-grained information. To address this, in this work we aim to involve the soft distance concept on label space in the contrastive-based unsupervised learning task and let the model be aware of the soft degree of similarity between positive or negative pairs through mixing the input data space, to further work collaboratively for the input and loss spaces. Despite its conceptual simplicity, we show empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we can learn subtler, more robust and generalized representations from the transformed input and corresponding new label space. Extensive experiments are conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet-1K with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, SwAV, etc. Our proposed image mixture and label assignment strategy can obtain consistent improvement by 1~3% following exactly the same hyperparameters and training procedures of the base methods. Code is publicly available at https://github.com/szq0214/Un-Mix",
    "checked": true,
    "id": "c96a5ee5c5018b1232b961ee2b5eaf73495a68fb",
    "semantic_title": "un-mix: rethinking image mixtures for unsupervised visual representation learning",
    "citation_count": 64,
    "authors": [
      "Zhiqiang Shen",
      "Zechun Liu",
      "Zhuang Liu",
      "Marios Savvides",
      "Trevor Darrell",
      "Eric Xing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20120": {
    "title": "On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals",
    "volume": "main",
    "abstract": "It is a consensus that small models perform quite poorly under the paradigm of self-supervised contrastive learning. Existing methods usually adopt a large off-the-shelf model to transfer knowledge to the small one via distillation. Despite their effectiveness, distillation-based methods may not be suitable for some resource-restricted scenarios due to the huge computational expenses of deploying a large model. In this paper, we study the issue of training self-supervised small models without distillation signals. We first evaluate the representation spaces of the small models and make two non-negligible observations: (i) the small models can complete the pretext task without overfitting despite their limited capacity and (ii) they universally suffer the problem of over clustering. Then we verify multiple assumptions that are considered to alleviate the over-clustering phenomenon. Finally, we combine the validated techniques and improve the baseline performances of five small architectures with considerable margins, which indicates that training small self-supervised contrastive models is feasible even without distillation signals. The code is available at https://github.com/WOWNICE/ssl-small",
    "checked": true,
    "id": "5beea6b725a73af57aa94c4520e46e97b3cd7b15",
    "semantic_title": "on the efficacy of small self-supervised contrastive models without distillation signals",
    "citation_count": 7,
    "authors": [
      "Haizhou Shi",
      "Youcai Zhang",
      "Siliang Tang",
      "Wenjie Zhu",
      "Yaqian Li",
      "Yandong Guo",
      "Yueting Zhuang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20121": {
    "title": "Social Interpretable Tree for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Understanding the multiple socially-acceptable future behaviors is an essential task for many vision applications. In this paper, we propose a tree-based method, termed as Social Interpretable Tree (SIT), to address this multi-modal prediction task, where a hand-crafted tree is built depending on the prior information of observed trajectory to model multiple future trajectories. Specifically, a path in the tree from the root to leaf represents an individual possible future trajectory. SIT employs a coarse-to-fine optimization strategy, in which the tree is first built by high-order velocity to balance the complexity and coverage of the tree and then optimized greedily to encourage multimodality. Finally, a teacher-forcing refining operation is used to predict the final fine trajectory. Compared with prior methods which leverage implicit latent variables to represent possible future trajectories, the path in the tree can explicitly explain the rough moving behaviors (e.g., go straight and then turn right), and thus provides better interpretability. Despite the hand-crafted tree, the experimental results on ETH-UCY and Stanford Drone datasets demonstrate that our method is capable of matching or exceeding the performance of state-of-the-art methods. Interestingly, the experiments show that the raw built tree without training outperforms many prior deep neural network based approaches. Meanwhile, our method presents sufficient flexibility in long-term prediction and different best-of-K predictions",
    "checked": true,
    "id": "586b26a5ef4c70e48f0d02fe49094630eda7f179",
    "semantic_title": "social interpretable tree for pedestrian trajectory prediction",
    "citation_count": 15,
    "authors": [
      "Liushuai Shi",
      "Le Wang",
      "Chengjiang Long",
      "Sanping Zhou",
      "Fang Zheng",
      "Nanning Zheng",
      "Gang Hua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20122": {
    "title": "P^3-Net: Part Mobility Parsing from Point Cloud Sequences via Learning Explicit Point Correspondence",
    "volume": "main",
    "abstract": "Understanding an articulated 3D object with its movable parts is an essential skill for an intelligent agent. This paper presents a novel approach to parse 3D part mobility from point cloud sequences. The key innovation is learning explicit point correspondence from a raw unordered point cloud sequence. We propose a novel deep network called P^3-Net to parallelize trajectory feature extraction and point correspondence establishment, performing joint optimization between them. Specifically, we design a Match-LSTM module to reaggregate point features among different frames by a point correspondence matrix, a.k.a. the matching matrix. To obtain this matrix, an attention module is proposed to calculate the point correspondence. Moreover, we implement a Gumbel-Sinkhorn module to reduce the many-to-one relationship for better point correspondence. We conduct comprehensive evaluations on public benchmarks, including the motion dataset and the PartNet dataset. Results demonstrate that our approach outperforms SOTA methods on various 3D parsing tasks of part mobility, including motion flow prediction, motion part segmentation, and motion attribute (i.e. axis & range) estimation. Moreover, we integrate our approach into a robot perception module to validate its robustness",
    "checked": true,
    "id": "36776248e0319f5247edfc7448d41f350020911d",
    "semantic_title": "p^3-net: part mobility parsing from point cloud sequences via learning explicit point correspondence",
    "citation_count": 4,
    "authors": [
      "Yahao Shi",
      "Xinyu Cao",
      "Feixiang Lu",
      "Bin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20123": {
    "title": "Improving Zero-Shot Phrase Grounding via Reasoning on External Knowledge and Spatial Relations",
    "volume": "main",
    "abstract": "Phrase grounding is a multi-modal problem that localizes a particular noun phrase in an image referred to by a text query. In the challenging zero-shot phrase grounding setting, the existing state-of-the-art grounding models have limited capacity in handling the unseen phrases. Humans, however, can ground novel types of objects in images with little effort, significantly benefiting from reasoning with commonsense. In this paper, we design a novel phrase grounding architecture that builds multi-modal knowledge graphs using external knowledge and then performs graph reasoning and spatial relation reasoning to localize the referred nouns phrases. We perform extensive experiments on different zero-shot grounding splits sub-sampled from the Flickr30K Entity and Visual Genome dataset, demonstrating that the proposed framework is orthogonal to backbone image encoders and outperforms the baselines by 2~3% in accuracy, resulting in a significant improvement under the standard evaluation metrics",
    "checked": true,
    "id": "d9693584f21834535592dfad01abcbf206ddd1bb",
    "semantic_title": "improving zero-shot phrase grounding via reasoning on external knowledge and spatial relations",
    "citation_count": 0,
    "authors": [
      "Zhan Shi",
      "Yilin Shen",
      "Hongxia Jin",
      "Xiaodan Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20124": {
    "title": "Iterative Contrast-Classify for Semi-supervised Temporal Action Segmentation",
    "volume": "main",
    "abstract": "Temporal action segmentation classifies the action of each frame in (long) video sequences. Due to the high cost of frame-wise labeling, we propose the first semi-supervised method for temporal action segmentation. Our method hinges on unsupervised representation learning, which, for temporal action segmentation, poses unique challenges. Actions in untrimmed videos vary in length and have unknown labels and start/end times. Ordering of actions across videos may also vary. We propose a novel way to learn frame-wise representations from temporal convolutional networks (TCNs) by clustering input features with added time-proximity conditions and multi-resolution similarity. By merging representation learning with conventional supervised learning, we develop an \"Iterative Contrast-Classify (ICC)'' semi-supervised learning scheme. With more labelled data, ICC progressively improves in performance; ICC semi-supervised learning, with 40% labelled videos, performs similarly to fully-supervised counterparts. Our ICC improves MoF by {+1.8, +5.6, +2.5}% on Breakfast, 50Salads, and GTEA respectively for 100% labelled videos",
    "checked": true,
    "id": "1e23b607bc0ecc598ec5ad948a12334c5e9cbf0f",
    "semantic_title": "iterative contrast-classify for semi-supervised temporal action segmentation",
    "citation_count": 9,
    "authors": [
      "Dipika Singhania",
      "Rahul Rahaman",
      "Angela Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20125": {
    "title": "JPV-Net: Joint Point-Voxel Representations for Accurate 3D Object Detection",
    "volume": "main",
    "abstract": "Voxel and point representations are widely applied in recent 3D object detection tasks from LiDAR point clouds. Voxel representations contribute to efficiently and rapidly locating objects, whereas point representations are capable of describing intra-object spatial relationship for detection refinement. In this work, we aim to exploit the strengths of both two representations, and present a novel two-stage detector, named Joint Point-Voxel Network (JPV-Net). Specifically, our framework is equipped with a Dual Encoders-Fusion Decoder, which consists of the dual encoders to extract voxel features of sketchy 3D scenes and point features rich in geometric context, respectively, and the Feature Propagation Fusion (FP-Fusion) decoder to attentively fuse them from coarse to fine. By making use of the advantages of these features, the refinement network can effectively eliminate false detection and provide better accuracy. Besides, to further develop the perception characteristics of voxel CNN and point backbone, we design two novel intersection-over-union (IoU) estimation modules for proposal generation and refinement, both of which can alleviate the misalignment between the localization and the classification confidence. Extensive experiments on the KITTI dataset and ONCE dataset demonstrate that our proposed JPV-Net outperforms other state-of-the-art methods with remarkable margins",
    "checked": true,
    "id": "835fb885a64793e5a0cb94ecbae78d60baf937d2",
    "semantic_title": "jpv-net: joint point-voxel representations for accurate 3d object detection",
    "citation_count": 2,
    "authors": [
      "Nan Song",
      "Tianyuan Jiang",
      "Jian Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20126": {
    "title": "Fully Attentional Network for Semantic Segmentation",
    "volume": "main",
    "abstract": "Recent non-local self-attention methods have proven to be effective in capturing long-range dependencies for semantic segmentation. These methods usually form a similarity map of R^(CxC) (by compressing spatial dimensions) or R^(HWxHW) (by compressing channels) to describe the feature relations along either channel or spatial dimensions, where C is the number of channels, H and W are the spatial dimensions of the input feature map. However, such practices tend to condense feature dependencies along the other dimensions, hence causing attention missing, which might lead to inferior results for small/thin categories or inconsistent segmentation inside large objects. To address this problem, we propose a new approach, namely Fully Attentional Network (FLANet), to encode both spatial and channel attentions in a single similarity map while maintaining high computational efficiency. Specifically, for each channel map, our FLANet can harvest feature responses from all other channel maps, and the associated spatial positions as well, through a novel fully attentional module. Our new method has achieved state-of-the-art performance on three challenging semantic segmentation datasets, i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes test set, the ADE20K validation set, and the PASCAL VOC test set, respectively",
    "checked": true,
    "id": "13b9bfad15dd79e5775e7b97edbd1e63a92a6fff",
    "semantic_title": "fully attentional network for semantic segmentation",
    "citation_count": 19,
    "authors": [
      "Qi Song",
      "Jie Li",
      "Chenghong Li",
      "Hao Guo",
      "Rui Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20127": {
    "title": "Self-Supervised Object Localization with Joint Graph Partition",
    "volume": "main",
    "abstract": "Object localization aims to generate a tight bounding box for the target object, which is a challenging problem that has been deeply studied in recent years. Since collecting bounding-box labels is time-consuming and laborious, many researchers focus on weakly supervised object localization (WSOL). As the recent appealing self-supervised learning technique shows its powerful function in visual tasks, in this paper, we take the early attempt to explore unsupervised object localization by self-supervision. Specifically, we adopt different geometric transformations to image and utilize their parameters as pseudo labels for self-supervised learning. Then, the class-agnostic activation map (CAAM) is used to highlight the target object potential regions. However, such attention maps merely focus on the most discriminative part of the objects, which will affect the quality of the predicted bounding box. Based on the motivation that the activation maps of different transformations of the same image should be equivariant, we further design a siamese network that encodes the paired images and propose a joint graph cluster partition mechanism in an unsupervised manner to enhance the object co-occurrent regions. To validate the effectiveness of the proposed method, extensive experiments are conducted on CUB-200-2011, Stanford Cars and FGVC-Aircraft datasets. Experimental results show that our method outperforms state-of-the-art methods using the same level of supervision, even outperforms some weakly-supervised methods",
    "checked": true,
    "id": "db888180afd09600e01e722d4e0159404350fd7b",
    "semantic_title": "self-supervised object localization with joint graph partition",
    "citation_count": 12,
    "authors": [
      "Yukun Su",
      "Guosheng Lin",
      "Yun Hao",
      "Yiwen Cao",
      "Wenjun Wang",
      "Qingyao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20128": {
    "title": "Correlation Field for Boosting 3D Object Detection in Structured Scenes",
    "volume": "main",
    "abstract": "Data augmentation is an efficient way to elevate 3D object detection performance. In this paper, we propose a simple but effective online crop-and-paste data augmentation pipeline for structured 3D point cloud scenes, named CorrelaBoost. Observing that 3D objects should have reasonable relative positions in a structured scene because of the objects' functionalities and natural relationships, we express this correlation as a kind of interactive force. An energy field called Correlation Field can be calculated correspondingly across the whole 3D space. According to the Correlation Field, we propose two data augmentation strategies to explore highly congruent positions that a designated object may be pasted to: 1) Category Consistent Exchanging and 2) Energy Optimized Transformation. We conduct exhaustive experiments on various popular benchmarks with different detection frameworks and the results illustrate that our method brings huge free-lunch improvement and significantly outperforms state-of-the-art approaches in terms of data augmentation. It is worth noting that the performance of VoteNet with mAP@0.5 is improved by 7.7 on ScanNetV2 dataset and 5.0 on SUN RGB-D dataset. Our method is simple to implement and increases few computational overhead",
    "checked": true,
    "id": "e58afcc06ab33ead9eb5baffd19221099ca14eda",
    "semantic_title": "correlation field for boosting 3d object detection in structured scenes",
    "citation_count": 5,
    "authors": [
      "Jianhua Sun",
      "Hao-Shu Fang",
      "Xianghui Zhu",
      "Jiefeng Li",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20129": {
    "title": "Boost Supervised Pretraining for Visual Transfer Learning: Implications of Self-Supervised Contrastive Representation Learning",
    "volume": "main",
    "abstract": "Unsupervised pretraining based on contrastive learning has made significant progress recently and showed comparable or even superior transfer learning performance to traditional supervised pretraining on various tasks. In this work, we first empirically investigate when and why unsupervised pretraining surpasses supervised counterparts for image classification tasks with a series of control experiments. Besides the commonly used accuracy, we further analyze the results qualitatively with the class activation maps and assess the learned representations quantitatively with the representation entropy and uniformity. Our core finding is that it is the amount of information effectively perceived by the learning model that is crucial to transfer learning, instead of absolute size of the dataset. Based on this finding, we propose Classification Activation Map guided contrastive (CAMtrast) learning which better utilizes the label supervsion to strengthen supervised pretraining, by making the networks perceive more information from the training images. CAMtrast is evaluated with three fundamental visual learning tasks: image recognition, object detection, and semantic segmentation, on various public datasets. Experimental results show that our CAMtrast effectively improves the performance of supervised pretraining, and that its performance is superior to both unsupervised counterparts and a recent related work which similarly attempted improving supervised pretraining",
    "checked": true,
    "id": "48698cba618caa06d0ba353747b86a1529827440",
    "semantic_title": "boost supervised pretraining for visual transfer learning: implications of self-supervised contrastive representation learning",
    "citation_count": 2,
    "authors": [
      "Jinghan Sun",
      "Dong Wei",
      "Kai Ma",
      "Liansheng Wang",
      "Yefeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20130": {
    "title": "Dual Contrastive Learning for General Face Forgery Detection",
    "volume": "main",
    "abstract": "With various facial manipulation techniques arising, face forgery detection has drawn growing attention due to security concerns. Previous works always formulate face forgery detection as a classiﬁcation problem based on cross-entropy loss, which emphasizes category-level differences rather than the essential discrepancies between real and fake faces, limiting model generalization in unseen domains. To address this issue, we propose a novel face forgery detection framework, named Dual Contrastive Learning (DCL), which specially constructs positive and negative paired data and performs designed contrastive learning at different granularities to learn generalized feature representation. Concretely, combined with the hard sample selection strategy, Inter-Instance Contrastive Learning (Inter-ICL) is ﬁrst proposed to promote task-related discriminative features learning by especially constructing instance pairs. Moreover, to further explore the essential discrepancies, Intra-Instance Contrastive Learning (Intra-ICL) is introduced to focus on the local content inconsistencies prevalent in the forged faces by constructing local region pairs inside instances. Extensive experiments and visualizations on several datasets demonstrate the generalization of our method against the state-of-the-art competitors. Our Code is available at https://github.com/Tencent/TFace.git",
    "checked": true,
    "id": "5d4720af01a0d3b3ff3203808e33b722322c4536",
    "semantic_title": "dual contrastive learning for general face forgery detection",
    "citation_count": 60,
    "authors": [
      "Ke Sun",
      "Taiping Yao",
      "Shen Chen",
      "Shouhong Ding",
      "Jilin Li",
      "Rongrong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20131": {
    "title": "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal",
    "volume": "main",
    "abstract": "Makeup transfer is not only to extract the makeup style of the reference image, but also to render the makeup style to the semantic corresponding position of the target image. However, most existing methods focus on the former and ignore the latter, resulting in a failure to achieve desired results. To solve the above problems, we propose a unified Symmetric Semantic-Aware Transformer (SSAT) network, which incorporates semantic correspondence learning to realize makeup transfer and removal simultaneously. In SSAT, a novel Symmetric Semantic Corresponding Feature Transfer (SSCFT) module and a weakly supervised semantic loss are proposed to model and facilitate the establishment of accurate semantic correspondence. In the generation process, the extracted makeup features are spatially distorted by SSCFT to achieve semantic alignment with the target image, then the distorted makeup features are combined with unmodified makeup irrelevant features to produce the final result. Experiments show that our method obtains more visually accurate makeup transfer results, and user study in comparison with other state-of-the-art makeup transfer methods reflects the superiority of our method. Besides, we verify the robustness of the proposed method in the difference of expression and pose, object occlusion scenes, and extend it to video makeup transfer",
    "checked": true,
    "id": "a841ef1b443ade089fef37db4d97076eaeebb662",
    "semantic_title": "ssat: a symmetric semantic-aware transformer network for makeup transfer and removal",
    "citation_count": 12,
    "authors": [
      "Zhaoyang Sun",
      "Yaxiong Chen",
      "Shengwu Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20132": {
    "title": "Adversarial Bone Length Attack on Action Recognition",
    "volume": "main",
    "abstract": "Skeleton-based action recognition models have recently been shown to be vulnerable to adversarial attacks. Compared to adversarial attacks on images, perturbations to skeletons are typically bounded to a lower dimension of approximately 100 per frame. This lower-dimensional setting makes it more difficult to generate imperceptible perturbations. Existing attacks resolve this by exploiting the temporal structure of the skeleton motion so that the perturbation dimension increases to thousands. In this paper, we show that adversarial attacks can be performed on skeleton-based action recognition models, even in a significantly low-dimensional setting without any temporal manipulation. Specifically, we restrict the perturbations to the lengths of the skeleton's bones, which allows an adversary to manipulate only approximately 30 effective dimensions. We conducted experiments on the NTU RGB+D and HDM05 datasets and demonstrate that the proposed attack successfully deceived models with sometimes greater than 90% success rate by small perturbations. Furthermore, we discovered an interesting phenomenon: in our low-dimensional setting, the adversarial training with the bone length attack shares a similar property with data augmentation, and it not only improves the adversarial robustness but also improves the classification accuracy on the original data. This is an interesting counterexample of the trade-off between adversarial robustness and clean accuracy, which has been widely observed in studies on adversarial training in the high-dimensional regime",
    "checked": true,
    "id": "98156c730d2a77bd4bec55c7196714578f764631",
    "semantic_title": "adversarial bone length attack on action recognition",
    "citation_count": 8,
    "authors": [
      "Nariki Tanaka",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20133": {
    "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
    "volume": "main",
    "abstract": "Transformers have sprung up in the field of computer vision. In this work, we explore whether the core self-attention module in Transformer is the key to achieving excellent performance in image recognition. To this end, we build an attention-free network called sMLPNet based on the existing MLP-based vision models. Specifically, we replace the MLP module in the token-mixing step with a novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along the axial directions and the parameters are shared among rows or columns. By sparse connection and weight sharing, sMLP module significantly reduces the number of model parameters and computational complexity, avoiding the common over-fitting problem that plagues the performance of MLP-like models. When only trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1 accuracy with only 24M parameters, which is much better than most CNNs and vision Transformers under the same model size constraint. When scaling up to 66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the state-of-the-art Swin Transformer. The success of sMLPNet suggests that the self-attention mechanism is not necessarily a silver bullet in computer vision. The code and models are publicly available at https://github.com/microsoft/SPACH",
    "checked": true,
    "id": "485c08025157973bb52a935c6aa3bee74f990c01",
    "semantic_title": "sparse mlp for image recognition: is self-attention really necessary?",
    "citation_count": 52,
    "authors": [
      "Chuanxin Tang",
      "Yucheng Zhao",
      "Guangting Wang",
      "Chong Luo",
      "Wenxuan Xie",
      "Wenjun Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20134": {
    "title": "Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective",
    "volume": "main",
    "abstract": "We revisit Semantic Scene Completion (SSC), a useful task to predict the semantic and occupancy representation of 3D scenes, in this paper. A number of methods for this task are always based on voxelized scene representations. Although voxel representations keep local structures of the scene, these methods suffer from heavy computation redundancy due to the existence of visible empty voxels when the network goes deeper. To address this dilemma, we propose our novel point-voxel aggregation network for this task. We first transfer the voxelized scenes to point clouds by removing these visible empty voxels and adopt a deep point stream to capture semantic information from the scene efficiently. Meanwhile, a light-weight voxel stream containing only two 3D convolution layers preserves local structures of the voxelized scenes. Furthermore, we design an anisotropic voxel aggregation operator to fuse the structure details from the voxel stream into the point stream, and a semantic-aware propagation module to enhance the up-sampling process in the point stream by semantic labels. We demonstrate that our model surpasses state-of-the-arts on two benchmarks by a large margin, with only the depth images as input",
    "checked": true,
    "id": "3a7a20ab726c58c853bac19c5ececac5f731d0c7",
    "semantic_title": "not all voxels are equal: semantic scene completion from the point-voxel perspective",
    "citation_count": 15,
    "authors": [
      "Jiaxiang Tang",
      "Xiaokang Chen",
      "Jingbo Wang",
      "Gang Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20135": {
    "title": "Transfer Learning for Color Constancy via Statistic Perspective",
    "volume": "main",
    "abstract": "Color Constancy aims to correct image color casts caused by scene illumination. Recently, although the deep learning approaches have remarkably improved on single-camera data, these models still suffer from the seriously insufficient data problem, resulting in shallow model capacity and degradation in multi-camera settings. In this paper, to alleviate this problem, we present a Transfer Learning Color Constancy (TLCC) method that leverages cross-camera RAW data and massive unlabeled sRGB data to support training. Specifically, TLCC consists of the Statistic Estimation Scheme (SE-Scheme) and Color-Guided Adaption Branch (CGA-Branch). SE-Scheme builds a statistic perspective to map the camera-related illumination labels into camera-agnostic form and produce pseudo labels for sRGB data, which greatly expands data for joint training. Then, CGA-Branch further promotes efficient transfer learning from sRGB to RAW data by extracting color information to regularize the backbone's features adaptively. Experimental results show the TLCC has overcome the data limitation and model degradation, outperforming the state-of-the-art performance on popular benchmarks. Moreover, the experiments also prove the TLCC is capable of learning new scenes information from sRGB data to improve accuracy on the RAW images with similar scenes",
    "checked": true,
    "id": "309efb8d1c4a471980b34f466f1e41b6fec6b44c",
    "semantic_title": "transfer learning for color constancy via statistic perspective",
    "citation_count": 4,
    "authors": [
      "Yuxiang Tang",
      "Xuejing Kang",
      "Chunxiao Li",
      "Zhaowen Lin",
      "Anlong Ming"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20136": {
    "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods",
    "checked": true,
    "id": "ff00791b780b10336cc02ee366446d16e1c5e17b",
    "semantic_title": "tvt: three-way vision transformer through multi-modal hypersphere learning for zero-shot sketch-based image retrieval",
    "citation_count": 15,
    "authors": [
      "Jialin Tian",
      "Xing Xu",
      "Fumin Shen",
      "Yang Yang",
      "Heng Tao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20137": {
    "title": "GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference",
    "volume": "main",
    "abstract": "Semi-supervised learning is a challenging problem which aims to construct a model by learning from limited labeled examples. Numerous methods for this task focus on utilizing the predictions of unlabeled instances consistency alone to regularize networks. However, treating labeled and unlabeled data separately often leads to the discarding of mass prior knowledge learned from the labeled examples. In this paper, we propose a novel method for semi-supervised semantic segmentation named GuidedMix-Net, by leveraging labeled information to guide the learning of unlabeled instances. Specifically, GuidedMix-Net employs three operations: 1) interpolation of similar labeled-unlabeled image pairs; 2) transfer of mutual information; 3) generalization of pseudo masks. It enables segmentation models can learning the higher-quality pseudo masks of unlabeled data by transfer the knowledge from labeled samples to unlabeled data. Along with supervised learning for labeled data, the prediction of unlabeled data is jointly learned with the generated pseudo masks from the mixed data. Extensive experiments on PASCAL VOC 2012, and Cityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves competitive segmentation accuracy and significantly improves the mIoU over 7$\\%$ compared to previous approaches",
    "checked": true,
    "id": "e2ad9bf3eff9adbf99c44d60c11bd3f98f7df4a6",
    "semantic_title": "guidedmix-net: semi-supervised semantic segmentation by using labeled images as reference",
    "citation_count": 10,
    "authors": [
      "Peng Tu",
      "Yawen Huang",
      "Feng Zheng",
      "Zhenyu He",
      "Liujuan Cao",
      "Ling Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20138": {
    "title": "MTLDesc: Looking Wider to Describe Better",
    "volume": "main",
    "abstract": "Limited by the locality of convolutional neural networks, most existing local features description methods only learn local descriptors with local information and lack awareness of global and surrounding spatial context. In this work, we focus on making local descriptors ``look wider to describe better'' by learning local Descriptors with More Than Local information (MTLDesc). Specifically, we resort to context augmentation and spatial attention mechanism to make the descriptors obtain non-local awareness. First, Adaptive Global Context Augmented Module and Diverse Local Context Augmented Module are proposed to construct robust local descriptors with context information from global to local. Second, we propose the Consistent Attention Weighted Triplet Loss to leverage spatial attention awareness in both optimization and matching of local descriptors. Third, Local Features Detection with Feature Pyramid is proposed to obtain more stable and accurate keypoints localization. With the above innovations, the performance of the proposed MTLDesc significantly surpasses the current state-of-the-art local descriptors on HPatches, Aachen Day-Night localization and InLoc indoor localization benchmarks. Our code is available at https://github.com/vignywang/MTLDesc",
    "checked": true,
    "id": "f7eb683d25d4c69a9a2ccd734347e0cdbd44a6c1",
    "semantic_title": "mtldesc: looking wider to describe better",
    "citation_count": 7,
    "authors": [
      "Changwei Wang",
      "Rongtao Xu",
      "Yuyang Zhang",
      "Shibiao Xu",
      "Weiliang Meng",
      "Bin Fan",
      "Xiaopeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20139": {
    "title": "Active Boundary Loss for Semantic Segmentation",
    "volume": "main",
    "abstract": "This paper proposes a novel active boundary loss for semantic segmentation. It can progressively encourage the alignment between predicted boundaries and ground-truth boundaries during end-to-end training, which is not explicitly enforced in commonly used cross-entropy loss. Based on the predicted boundaries detected from the segmentation results using current network parameters, we formulate the boundary alignment problem as a differentiable direction vector prediction problem to guide the movement of predicted boundaries in each iteration. Our loss is model-agnostic and can be plugged in to the training of segmentation networks to improve the boundary details. Experimental results show that training with the active boundary loss can effectively improve the boundary F-score and mean Intersection-over-Union on challenging image and video object segmentation datasets",
    "checked": true,
    "id": "c35ae7f62bd662eaf09834d962366202225d10dd",
    "semantic_title": "active boundary loss for semantic segmentation",
    "citation_count": 27,
    "authors": [
      "Chi Wang",
      "Yunke Zhang",
      "Miaomiao Cui",
      "Peiran Ren",
      "Yin Yang",
      "Xuansong Xie",
      "Xian-Sheng Hua",
      "Hujun Bao",
      "Weiwei Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20140": {
    "title": "Online-Updated High-Order Collaborative Networks for Single Image Deraining",
    "volume": "main",
    "abstract": "Single image deraining is an important and challenging task for some downstream artificial intelligence applications such as video surveillance and self-driving systems. Most of the existing deep-learning-based methods constrain the network to generate derained images but few of them explore features from intermediate layers, different levels, and different modules which are beneficial for rain streaks removal. In this paper, we propose a high-order collaborative network with multi-scale compact constraints and a bidirectional scale-content similarity mining module to exploit features from deep networks externally and internally for rain streaks removal. Externally, we design a deraining framework with three sub-networks trained in a collaborative manner, where the bottom network transmits intermediate features to the middle network which also receives shallower rainy features from the top network and sends back features to the bottom network. Internally, we enforce multi-scale compact constraints on the intermediate layers of deep networks to learn useful features via a Laplacian pyramid. Further, we develop a bidirectional scale-content similarity mining module to explore features at different scales in a down-to-up and up-to-down manner. To improve the model performance on real-world images, we propose an online-update learning approach, which uses real-world rainy images to fine-tune the network and update the deraining results in a self-supervised manner. Extensive experiments demonstrate that our proposed method performs favorably against eleven state-of-the-art methods on five public synthetic datasets and one real-world dataset",
    "checked": true,
    "id": "d6cb9791bf35ffe822fdaca940304f42e9acfb21",
    "semantic_title": "online-updated high-order collaborative networks for single image deraining",
    "citation_count": 4,
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Xiao-Ming Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20141": {
    "title": "FCA: Learning a 3D Full-Coverage Vehicle Camouflage for Multi-View Physical Adversarial Attack",
    "volume": "main",
    "abstract": "Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors",
    "checked": true,
    "id": "695685be105a0b9bf8b5cd50d80a6dff78204081",
    "semantic_title": "fca: learning a 3d full-coverage vehicle camouflage for multi-view physical adversarial attack",
    "citation_count": 30,
    "authors": [
      "Donghua Wang",
      "Tingsong Jiang",
      "Jialiang Sun",
      "Weien Zhou",
      "Zhiqiang Gong",
      "Xiaoya Zhang",
      "Wen Yao",
      "Xiaoqian Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20142": {
    "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "volume": "main",
    "abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH",
    "checked": true,
    "id": "b52844a746dafd8a5051cef49abbbda64a312605",
    "semantic_title": "when shift operation meets vision transformer: an extremely simple alternative to attention mechanism",
    "citation_count": 24,
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20143": {
    "title": "Self-Supervised Representation Learning Framework for Remote Physiological Measurement Using Spatiotemporal Augmentation Loss",
    "volume": "main",
    "abstract": "Recent advances in supervised deep learning methods are enabling remote measurements of photoplethysmography-based physiological signals using facial videos. The performance of these supervised methods, however, are dependent on the availability of large labelled data. Contrastive learning as a self-supervised method has recently achieved state-of-the-art performances in learning representative data features by maximising mutual information between different augmented views. However, existing data augmentation techniques for contrastive learning are not designed to learn physiological signals from videos and often fail when there are complicated noise and subtle and periodic colour/shape variations between video frames. To address these problems, we present a novel self-supervised spatiotemporal learning framework for remote physiological signal representation learning, where there is a lack of labelled training data. Firstly, we propose a landmark-based spatial augmentation that splits the face into several informative parts based on the Shafer's dichromatic reﬂection model to characterise subtle skin colour fluctuations. We also formulate a sparsity-based temporal augmentation exploiting Nyquist–Shannon sampling theorem to effectively capture periodic temporal changes by modelling physiological signal features. Furthermore, we introduce a constrained spatiotemporal loss which generates pseudo-labels for augmented video clips. It is used to regulate the training process and handle complicated noise. We evaluated our framework on 3 public datasets and demonstrated superior performances than other self-supervised methods and achieved competitive accuracy compared to the state-of-the-art supervised methods. Code is available at https://github.com/Dylan-H-Wang/SLF-RPM",
    "checked": true,
    "id": "2c8b82f018d64ec9b6035007e09ef9fe61a8c016",
    "semantic_title": "self-supervised representation learning framework for remote physiological measurement using spatiotemporal augmentation loss",
    "citation_count": 15,
    "authors": [
      "Hao Wang",
      "Euijoon Ahn",
      "Jinman Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20144": {
    "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer",
    "volume": "main",
    "abstract": "Most recent semantic segmentation methods adopt a U-Net framework with an encoder-decoder architecture. It is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context: 1) Not each skip connection setting is effective due to the issue of incompatible feature sets of encoder and decoder stage, even some skip connection negatively influence the segmentation performance; 2) The original U-Net is worse than the one without any skip connection on some datasets. Based on our findings, we propose a new segmentation framework, named UCTransNet (with a proposed CTrans module in U-Net), from the channel perspective with attention mechanism. Specifically, the CTrans (Channel Transformer) module is an alternate of the U-Net skip connections, which consists of a sub-module to conduct the multi-scale Channel Cross fusion with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention (named CCA) to guide the fused multi-scale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity. Hence, the proposed connection consisting of the CCT and CCA is able to replace the original skip connection to solve the semantic gaps for an accurate automatic medical image segmentation. The experimental results suggest that our UCTransNet produces more precise segmentation performance and achieves consistent improvements over the state-of-the-art for semantic segmentation across different datasets and conventional architectures involving transformer or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet",
    "checked": true,
    "id": "7623a054b85bed188ebd608915ee7a15fc947f7f",
    "semantic_title": "uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer",
    "citation_count": 197,
    "authors": [
      "Haonan Wang",
      "Peng Cao",
      "Jiaqi Wang",
      "Osmar R. Zaiane"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20145": {
    "title": "Renovate Yourself: Calibrating Feature Representation of Misclassified Pixels for Semantic Segmentation",
    "volume": "main",
    "abstract": "Existing image semantic segmentation methods favor learning consistent representations by extracting long-range contextual features with the attention, multi-scale, or graph aggregation strategies. These methods usually treat the misclassified and correctly classified pixels equally, hence misleading the optimization process and causing inconsistent intra-class pixel feature representations in the embedding space during learning. In this paper, we propose the auxiliary representation calibration head (RCH), which consists of the image decoupling, prototype clustering, error calibration modules and a metric loss function, to calibrate these error-prone feature representations for better intra-class consistency and segmentation performance. RCH could be incorporated into the hidden layers, trained together with the segmentation networks, and decoupled in the inference stage without additional parameters. Experimental results show that our method could significantly boost the performance of current segmentation methods on multiple datasets (e.g., we outperform the original HRNet and OCRNet by 1.1% and 0.9% mIoU on the Cityscapes test set). Codes are available at https://github.com/VipaiLab/RCH",
    "checked": true,
    "id": "9c8bd5a0adfd57941d454c4fd02d92a17aa8693c",
    "semantic_title": "renovate yourself: calibrating feature representation of misclassified pixels for semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Hualiang Wang",
      "Huanpeng Chu",
      "Siming FU",
      "Zuozhu Liu",
      "Haoji Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20146": {
    "title": "Separated Contrastive Learning for Organ-at-Risk and Gross-Tumor-Volume Segmentation with Limited Annotation",
    "volume": "main",
    "abstract": "Automatic delineation of organ-at-risk (OAR) and gross-tumor-volume (GTV) is of great significance for radiotherapy planning. However, it is a challenging task to learn powerful representations for accurate delineation under limited pixel (voxel)-wise annotations. Contrastive learning at pixel-level can alleviate the dependency on annotations by learning dense representations from unlabeled data. Recent studies in this direction design various contrastive losses on the feature maps, to yield discriminative features for each pixel in the map. However, pixels in the same map inevitably share semantics to be closer than they actually are, which may affect the discrimination of pixels in the same map and lead to the unfair comparison to pixels in other maps. To address these issues, we propose a separated region-level contrastive learning scheme, namely SepaReg, the core of which is to separate each image into regions and encode each region separately. Specifically, SepaReg comprises two components: a structure-aware image separation (SIS) module and an intra- and inter-organ distillation (IID) module. The SIS is proposed to operate on the image set to rebuild a region set under the guidance of structural information. The inter-organ representation will be learned from this set via typical contrastive losses cross regions. On the other hand, the IID is proposed to tackle the quantity imbalance in the region set as tiny organs may produce fewer regions, by exploiting intra-organ representations. We conducted extensive experiments to evaluate the proposed model on a public dataset and two private datasets. The experimental results demonstrate the effectiveness of the proposed model, consistently achieving better performance than state-of-the-art approaches. Code is available at https://github.com/jcwang123/Separate_CL",
    "checked": true,
    "id": "a086fc7ee0f3f6f8befd257e94a4ebf156262d64",
    "semantic_title": "separated contrastive learning for organ-at-risk and gross-tumor-volume segmentation with limited annotation",
    "citation_count": 5,
    "authors": [
      "Jiacheng Wang",
      "Xiaomeng Li",
      "Yiming Han",
      "Jing Qin",
      "Liansheng Wang",
      "Zhou Qichao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20147": {
    "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval",
    "volume": "main",
    "abstract": "The high efficiency in computation and storage makes hashing (including binary hashing and quantization) a common strategy in large-scale retrieval systems. To alleviate the reliance on expensive annotations, unsupervised deep hashing becomes an important research problem. This paper provides a novel solution to unsupervised deep quantization, namely Contrastive Quantization with Code Memory (MeCoQ). Different from existing reconstruction-based strategies, we learn unsupervised binary descriptors by contrastive learning, which can better capture discriminative visual semantics. Besides, we uncover that codeword diversity regularization is critical to prevent contrastive learning-based quantization from model degeneration. Moreover, we introduce a novel quantization code memory module that boosts contrastive learning with lower feature drift than conventional feature memories. Extensive experiments on benchmark datasets show that MeCoQ outperforms state-of-the-art methods. Code and configurations are publicly released",
    "checked": true,
    "id": "d96ad7741c25001b9f3ec91b2d095d97b9629584",
    "semantic_title": "contrastive quantization with code memory for unsupervised image retrieval",
    "citation_count": 12,
    "authors": [
      "Jinpeng Wang",
      "Ziyun Zeng",
      "Bin Chen",
      "Tao Dai",
      "Shu-Tao Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20148": {
    "title": "Learning Temporally and Semantically Consistent Unpaired Video-to-Video Translation through Pseudo-Supervision from Synthetic Optical Flow",
    "volume": "main",
    "abstract": "Unpaired video-to-video translation aims to translate videos between a source and a target domain without the need of paired training data, making it more feasible for real applications. Unfortunately, the translated videos generally suffer from temporal and semantic inconsistency. To address this, many existing works adopt spatiotemporal consistency constraints incorporating temporal information based on motion estimation. However, the inaccuracies in the estimation of motion deteriorate the quality of the guidance towards spatiotemporal consistency, which leads to unstable translation. In this work, we propose a novel paradigm that regularizes the spatiotemporal consistency by synthesizing motions in input videos with the generated optical flow instead of estimating them. Therefore, the synthetic motion can be applied in the regularization paradigm to keep motions consistent across domains without the risk of errors in motion estimation. Thereafter, we utilize our unsupervised recycle and unsupervised spatial loss, guided by the pseudo-supervision provided by the synthetic optical flow, to accurately enforce spatiotemporal consistency in both domains. Experiments show that our method is versatile in various scenarios and achieves state-of-the-art performance in generating temporally and semantically consistent videos. Code is available at: https://github.com/wangkaihong/Unsup_Recycle_GAN/",
    "checked": true,
    "id": "a1cac098cfc6339236e65e717fc425fb8763e360",
    "semantic_title": "learning temporally and semantically consistent unpaired video-to-video translation through pseudo-supervision from synthetic optical flow",
    "citation_count": 4,
    "authors": [
      "Kaihong Wang",
      "Kumar Akash",
      "Teruhisa Misu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20149": {
    "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "Semantic segmentation is an important task for scene understanding in self-driving cars and robotics, which aims to assign dense labels for all pixels in the image. Existing work typically improves semantic segmentation performance by exploring different network architectures on a target dataset. Little attention has been paid to build a unified system by simultaneously learning from multiple datasets due to the inherent distribution shift across different datasets. In this paper, we propose a simple, flexible, and general method for semantic segmentation, termed Cross-Dataset Collaborative Learning (CDCL). Our goal is to train a unified model for improving the performance in each dataset by leveraging information from all the datasets. Specifically, we first introduce a family of Dataset-Aware Blocks (DAB) as the fundamental computing units of the network, which help capture homogeneous convolutional representations and heterogeneous statistics across different datasets. Second, we present a Dataset Alternation Training (DAT) mechanism to facilitate the collaborative optimization procedure. We conduct extensive evaluations on diverse semantic segmentation datasets for autonomous driving. Experiments demonstrate that our method consistently achieves notable improvements over prior single-dataset and cross-dataset training methods without introducing extra FLOPs. Particularly, with the same architecture of PSPNet (ResNet-18), our method outperforms the single-dataset baseline by 5.65\\%, 6.57\\%, and 5.79\\% mIoU on the validation sets of Cityscapes, BDD100K, CamVid, respectively. We also apply CDCL for point cloud 3D semantic segmentation and achieve improved performance, which further validates the superiority and generality of our method. Code and models will be released",
    "checked": false,
    "id": "032d1cf4bbca9aabd36e9d379697c706dbe8ab5f",
    "semantic_title": "cross-dataset collaborative learning for semantic segmentation",
    "citation_count": 14,
    "authors": [
      "Li Wang",
      "Dong Li",
      "Han Liu",
      "JinZhang Peng",
      "Lu Tian",
      "Yi Shan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20150": {
    "title": "Scaled ReLU Matters for Training Vision Transformers",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) have been an alternative design paradigm to convolutional neural networks (CNNs). However, the training of ViTs is much harder than CNNs, as it is sensitive to the training parameters, such as learning rate, optimizer and warmup epoch. The reasons for training difficulty are empirically analysed in the paper Early Convolutions Help Transformers See Better, and the authors conjecture that the issue lies with the patchify-stem of ViT models. In this paper, we further investigate this problem and extend the above conclusion: only early convolutions do not help for stable training, but the scaled ReLU operation in the convolutional stem (conv-stem) matters. We verify, both theoretically and empirically, that scaled ReLU in conv-stem not only improves training stabilization, but also increases the diversity of patch tokens, thus boosting peak performance with a large margin via adding few parameters and flops. In addition, extensive experiments are conducted to demonstrate that previous ViTs are far from being well trained, further showing that ViTs have great potential to be a better substitute of CNNs",
    "checked": true,
    "id": "a9c352cce882f31e7f28dbe96794e10b089c6623",
    "semantic_title": "scaled relu matters for training vision transformers",
    "citation_count": 22,
    "authors": [
      "Pichao Wang",
      "Xue Wang",
      "Hao Luo",
      "Jingkai Zhou",
      "Zhipeng Zhou",
      "Fan Wang",
      "Hao Li",
      "Rong Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20151": {
    "title": "CQA-Face: Contrastive Quality-Aware Attentions for Face Recognition",
    "volume": "main",
    "abstract": "Few existing face recognition (FR) models take local representations into account. Although some works achieved this by extracting features on cropped parts around face landmarks, landmark detection may be inaccurate or even fail in some extreme cases. Recently, without relying on landmarks, attention-based networks can focus on useful parts automatically. However, there are two issues: 1) It is noticed that these approaches focus on few facial parts, while missing other potentially discriminative regions. This can cause performance drops when emphasized facial parts are invisible under heavy occlusions (e.g. face masks) or large pose variations; 2) Different facial parts may appear at various quality caused by occlusion, blur, or illumination changes. In this paper, we propose contrastive quality-aware attentions, called CQA-Face, to address these two issues. First, a Contrastive Attention Learning (CAL) module is proposed, pushing models to explore comprehensive facial parts. Consequently, more useful parts can help identification if some facial parts are invisible. Second, a Quality-Aware Network (QAN) is developed to emphasize important regions and suppress noisy parts in a global scope. Thus, our CQA-Face model is developed by integrating the CAL with QAN, which extracts diverse quality-aware local representations. It outperforms the state-of-the-art methods on several benchmarks, demonstrating its effectiveness and usefulness",
    "checked": true,
    "id": "c26a938b35652948e35e8e08517b555c87dde173",
    "semantic_title": "cqa-face: contrastive quality-aware attentions for face recognition",
    "citation_count": 3,
    "authors": [
      "Qiangchang Wang",
      "Guodong Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20152": {
    "title": "Category-Specific Nuance Exploration Network for Fine-Grained Object Retrieval",
    "volume": "main",
    "abstract": "Employing additional prior knowledge to model local features as a final fine-grained object representation has become a trend for fine-grained object retrieval (FGOR). A potential limitation of these methods is that they only focus on common parts across the dataset (e.g. head, body or even leg) by introducing additional prior knowledge, but the retrieval of a fine-grained object may rely on category-specific nuances that contribute to category prediction. To handle this limitation, we propose an end-to-end Category-specific Nuance Exploration Network (CNENet) that elaborately discovers category-specific nuances that contribute to category prediction, and semantically aligns these nuances grouped by subcategory without any additional prior knowledge, to directly emphasize the discrepancy among subcategories. Specifically, we design a Nuance Modelling Module that adaptively predicts a group of category-specific response (CARE) maps via implicitly digging into category-specific nuances, specifying the locations and scales for category-specific nuances. Upon this, two nuance regularizations are proposed: 1) semantic discrete loss that forces each CARE map to attend to different spatial regions to capture diverse nuances; 2) semantic alignment loss that constructs a consistent semantic correspondence for each CARE map of the same order with the same subcategory via guaranteeing each instance and its transformed counterpart to be spatially aligned. Moreover, we propose a Nuance Expansion Module, which exploits context appearance information of discovered nuances and refines the prediction of current nuance by its similar neighbors, leading to further improvement on nuance consistency and completeness. Extensive experiments validate that our CNENet consistently yields the best performance under the same settings against most competitive approaches on CUB Birds, Stanford Cars, and FGVC Aircraft datasets",
    "checked": true,
    "id": "701507ba60b6f73943da688724618233bfd309c1",
    "semantic_title": "category-specific nuance exploration network for fine-grained object retrieval",
    "citation_count": 3,
    "authors": [
      "Shijie Wang",
      "Zhihui Wang",
      "Haojie Li",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20153": {
    "title": "Detail-Preserving Transformer for Light Field Image Super-resolution",
    "volume": "main",
    "abstract": "Recently, numerous algorithms have been developed to tackle the problem of light field super-resolution (LFSR), i.e., super-resolving low-resolution light fields to gain high-resolution views. Despite delivering encouraging results, these approaches are all convolution-based, and are naturally weak in global relation modeling of sub-aperture images necessarily to characterize the inherent structure of light fields. In this paper, we put forth a novel formulation built upon Transformers, by treating LFSR as a sequence-to-sequence reconstruction task. In particular, our model regards sub-aperture images of each vertical or horizontal angular view as a sequence, and establishes long-range geometric dependencies within each sequence via a spatial-angular locally-enhanced self-attention layer, which maintains the locality of each sub-aperture image as well. Additionally, to better recover image details, we propose a detail-preserving Transformer (termed as DPT), by leveraging gradient maps of light field to guide the sequence learning. DPT consists of two branches, with each associated with a Transformer for learning from an original or gradient image sequence. The two branches are finally fused to obtain comprehensive feature representations for reconstruction. Evaluations are conducted on a number of light field datasets, including real-world scenes and synthetic data. The proposed method achieves superior performance comparing with other state-of-the-art schemes. Our code is publicly available at: https://github.com/BITszwang/DPT",
    "checked": true,
    "id": "cdd9036cfa2487c665033539978a1ba90f4f21a8",
    "semantic_title": "detail-preserving transformer for light field image super-resolution",
    "citation_count": 46,
    "authors": [
      "Shunzhou Wang",
      "Tianfei Zhou",
      "Yao Lu",
      "Huijun Di"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20154": {
    "title": "One-Shot Talking Face Generation from Single-Speaker Audio-Visual Correlation Learning",
    "volume": "main",
    "abstract": "Audio-driven one-shot talking face generation methods are usually trained on video resources of various persons. However, their created videos often suffer unnatural mouth shapes and asynchronous lips because those methods struggle to learn a consistent speech style from different speakers. We observe that it would be much easier to learn a consistent speech style from a specific speaker, which leads to authentic mouth movements. Hence, we propose a novel one-shot talking face generation framework by exploring consistent correlations between audio and visual motions from a specific speaker and then transferring audio-driven motion fields to a reference image. Specifically, we develop an Audio-Visual Correlation Transformer (AVCT) that aims to infer talking motions represented by keypoint based dense motion fields from an input audio. In particular, considering audio may come from different identities in deployment, we incorporate phonemes to represent audio signals. In this manner, our AVCT can inherently generalize to audio spoken by other identities. Moreover, as face keypoints are used to represent speakers, AVCT is agnostic against appearances of the training speaker, and thus allows us to manipulate face images of different identities readily. Considering different face shapes lead to different motions, a motion field transfer module is exploited to reduce the audio-driven dense motion field gap between the training identity and the one-shot reference. Once we obtained the dense motion field of the reference image, we employ an image renderer to generate its talking face videos from an audio clip. Thanks to our learned consistent speaking style, our method generates authentic mouth shapes and vivid movements. Extensive experiments demonstrate that our synthesized videos outperform the state-of-the-art in terms of visual quality and lip-sync",
    "checked": true,
    "id": "a5ec595ac6a70a50d9cb29702728ed14454d0515",
    "semantic_title": "one-shot talking face generation from single-speaker audio-visual correlation learning",
    "citation_count": 39,
    "authors": [
      "Suzhen Wang",
      "Lincheng Li",
      "Yu Ding",
      "Xin Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20155": {
    "title": "Pose-Guided Feature Disentangling for Occluded Person Re-identification Based on Transformer",
    "volume": "main",
    "abstract": "Occluded person re-identification is a challenging task as human body parts could be occluded by some obstacles (e.g. trees, cars, and pedestrians) in certain scenes. Some existing pose-guided methods solve this problem by aligning body parts according to graph matching, but these graph-based methods are not intuitive and complicated. Therefore, we propose a transformer-based Pose-guided Feature Disentangling (PFD) method by utilizing pose information to clearly disentangle semantic components (e.g. human body or joint parts) and selectively match non-occluded parts correspondingly. First, Vision Transformer (ViT) is used to extract the patch features with its strong capability. Second, to preliminarily disentangle the pose information from patch information, the matching and distributing mechanism is leveraged in Pose-guided Feature Aggregation (PFA) module. Third, a set of learnable semantic views are introduced in transformer decoder to implicitly enhance the disentangled body part features. However, those semantic views are not guaranteed to be related to the body without additional supervision. Therefore, Pose-View Matching (PVM) module is proposed to explicitly match visible body parts and automatically separate occlusion features. Fourth, to better prevent the interference of occlusions, we design a Pose-guided Push Loss to emphasize the features of visible body parts. Extensive experiments over five challenging datasets for two tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is superior promising, which performs favorably against state-of-the-art methods. Code is available at https://github.com/WangTaoAs/PFD_Net",
    "checked": true,
    "id": "d5960b9a7794bf6a69f1835b974a0a129ecffa65",
    "semantic_title": "pose-guided feature disentangling for occluded person re-identification based on transformer",
    "citation_count": 43,
    "authors": [
      "Tao Wang",
      "Hong Liu",
      "Pinhao Song",
      "Tianyu Guo",
      "Wei Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20156": {
    "title": "FFNet: Frequency Fusion Network for Semantic Scene Completion",
    "volume": "main",
    "abstract": "Semantic scene completion (SSC) requires the estimation of the 3D geometric occupancies of objects in the scene, along with the object categories. Currently, many methods employ RGB-D images to capture the geometric and semantic information of objects. These methods use simple but popular spatial- and channel-wise operations, which fuse the information of RGB and depth data. Yet, they ignore the large discrepancy of RGB-D data and the uncertainty measurements of depth data. To solve this problem, we propose the Frequency Fusion Network (FFNet), a novel method for boosting semantic scene completion by better utilizing RGB-D data. FFNet explicitly correlates the RGB-D data in the frequency domain, different from the features directly extracted by the convolution operation. Then, the network uses the correlated information to guide the feature learning from the RG- B and depth images, respectively. Moreover, FFNet accounts for the properties of different frequency components of RGB- D features. It has a learnable elliptical mask to decompose the features learned from the RGB and depth images, attending to various frequencies to facilitate the correlation process of RGB-D data. We evaluate FFNet intensively on the public SSC benchmarks, where FFNet surpasses the state-of- the-art methods. The code package of FFNet is available at https://github.com/alanWXZ/FFNet",
    "checked": true,
    "id": "0921e3a65f5bd687618fa801d8517f09f33646a4",
    "semantic_title": "ffnet: frequency fusion network for semantic scene completion",
    "citation_count": 7,
    "authors": [
      "Xuzhi Wang",
      "Di Lin",
      "Liang Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20157": {
    "title": "Privacy-Preserving Face Recognition in the Frequency Domain",
    "volume": "main",
    "abstract": "Some applications may require performing face recognition (FR) on third-party servers, which could be accessed by attackers with malicious intents to compromise the privacy of users' face information. This paper advocates a practical privacy-preserving FR scheme without key management realized in the frequency domain. The new scheme first collects the components of the same frequency from different blocks of a face image to form component channels. Only part of the channels are retained and fed into the analysis network that performs an interpretable privacy-accuracy trade-off analysis to identify channels important for face image visualization but not crucial for maintaining high FR accuracy. For this purpose, the loss function of the analysis network consists of the empirical FR error loss and a face visualization penalty term, and the network is trained in an end-to-end manner. We find that with the developed analysis network, more than 94% of the image energy can be dropped while the face recognition accuracy stays almost undegraded. In order to further protect the remaining frequency components, we propose a fast masking method. Effectiveness of the new scheme in removing the visual information of face images while maintaining their distinguishability is validated over several large face datasets. Results show that the proposed scheme achieves a recognition performance and inference time comparable to ArcFace operating on original face images directly",
    "checked": true,
    "id": "a41979f071e49ed20547840ce9971bdf7ef7a4c5",
    "semantic_title": "privacy-preserving face recognition in the frequency domain",
    "citation_count": 10,
    "authors": [
      "Yinggui Wang",
      "Jian Liu",
      "Man Luo",
      "Le Yang",
      "Li Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20158": {
    "title": "Anchor DETR: Query Design for Transformer-Based Detector",
    "volume": "main",
    "abstract": "In this paper, we propose a novel query design for the transformer-based object detection. In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode. In other words, each object query will not focus on a specific region. To solve these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. So each object query focuses on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difficulty: ``one region, multiple objects''. In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10x fewer training epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-research/AnchorDETR",
    "checked": true,
    "id": "b4ce19f3b3819accb160acffabffa849f18f4758",
    "semantic_title": "anchor detr: query design for transformer-based detector",
    "citation_count": 152,
    "authors": [
      "Yingming Wang",
      "Xiangyu Zhang",
      "Tong Yang",
      "Jian Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20159": {
    "title": "Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration",
    "volume": "main",
    "abstract": "Emerging high-quality face restoration (FR) methods often utilize pre-trained GAN models (i.e., StyleGAN2) as GAN Prior. However, these methods usually struggle to balance realness and fidelity when facing various degradation levels. Besides, there is still a noticeable visual quality gap compared with pre-trained GAN models. In this paper, we propose a novel GAN Prior based degradation-aware feature interpolation network, dubbed Panini-Net, for FR tasks by explicitly learning the abstract representations to distinguish various degradations. Specifically, an unsupervised degradation representation learning (UDRL) strategy is first developed to extract degradation representations (DR) of the input degraded images. Then, a degradation-aware feature interpolation (DAFI) module is proposed to dynamically fuse the two types of informative features (i.e., features from input images and features from GAN Prior) with flexible adaption to various degradations based on DR. Ablation studies reveal the working mechanism of DAFI and its potential for editable FR. Extensive experiments demonstrate that our Panini-Net achieves state-of-the-art performance for multi-degradation face restoration and face super-resolution. The source code is available at https://github.com/jianzhangcs/panini",
    "checked": true,
    "id": "c3136e616db4fce0549aef443a04e931bf787c2a",
    "semantic_title": "panini-net: gan prior based degradation-aware feature interpolation for face restoration",
    "citation_count": 10,
    "authors": [
      "Yinhuai Wang",
      "Yujie Hu",
      "Jian Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20160": {
    "title": "End-to-End Transformer Based Model for Image Captioning",
    "volume": "main",
    "abstract": "CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt SwinTransformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2% (single model) and 141.0% (ensemble of 4 models) CIDEr scores on 'Karpathy' offline test split and 136.0% (c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained models and source code will be released",
    "checked": true,
    "id": "23903b4c42fdbea0b7b35e3157b48d8dfd18e1a5",
    "semantic_title": "end-to-end transformer based model for image captioning",
    "citation_count": 26,
    "authors": [
      "Yiyu Wang",
      "Jungang Xu",
      "Yingfei Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20161": {
    "title": "Learning to Detect 3D Facial Landmarks via Heatmap Regression with Graph Convolutional Network",
    "volume": "main",
    "abstract": "3D facial landmark detection is extensively used in many research fields such as face registration, facial shape analysis, and face recognition. Most existing methods involve traditional features and 3D face models for the detection of landmarks, and their performances are limited by the hand-crafted intermediate process. In this paper, we propose a novel 3D facial landmark detection method, which directly locates the coordinates of landmarks from 3D point cloud with a well-customized graph convolutional network. The graph convolutional network learns geometric features adaptively for 3D facial landmark detection with the assistance of constructed 3D heatmaps, which are Gaussian functions of distances to each landmark on a 3D face. On this basis, we further develop a local surface unfolding and registration module to predict 3D landmarks from the heatmaps. The proposed method forms the first baseline of deep point cloud learning method for 3D facial landmark detection. We demonstrate experimentally that the proposed method exceeds the existing approaches by a clear margin on BU-3DFE and FRGC datasets for landmark localization accuracy and stability, and also achieves high-precision results on a recent large-scale dataset",
    "checked": true,
    "id": "deb5d104de7cfb8de6d1ded9c7149a705f805300",
    "semantic_title": "learning to detect 3d facial landmarks via heatmap regression with graph convolutional network",
    "citation_count": 3,
    "authors": [
      "Yuan Wang",
      "Min Cao",
      "Zhenfeng Fan",
      "Silong Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20162": {
    "title": "Low-Light Image Enhancement with Normalizing Flow",
    "volume": "main",
    "abstract": "To enhance low-light images to normally-exposed ones is highly ill-posed, namely that the mapping relationship between them is one-to-many. Previous works based on the pixel-wise reconstruction losses and deterministic processes fail to capture the complex conditional distribution of normally exposed images, which results in improper brightness, residual noise, and artifacts. In this paper, we investigate to model this one-to-many relationship via a proposed normalizing flow model. An invertible network that takes the low-light images/features as the condition and learns to map the distribution of normally exposed images into a Gaussian distribution. In this way, the conditional distribution of the normally exposed images can be well modeled, and the enhancement process, i.e., the other inference direction of the invertible network, is equivalent to being constrained by a loss function that better describes the manifold structure of natural images during the training. The experimental results on the existing benchmark datasets show our method achieves better quantitative and qualitative results, obtaining better-exposed illumination, less noise and artifact, and richer colors",
    "checked": true,
    "id": "4abede791129ff4eb42b91c9b1091c27f432114d",
    "semantic_title": "low-light image enhancement with normalizing flow",
    "citation_count": 115,
    "authors": [
      "Yufei Wang",
      "Renjie Wan",
      "Wenhan Yang",
      "Haoliang Li",
      "Lap-Pui Chau",
      "Alex Kot"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20163": {
    "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding",
    "volume": "main",
    "abstract": "Temporal grounding aims to localize a video moment which is semantically aligned with a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with the research focus on designing complicated prediction heads or fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Mutual Matching Network (MMN), to directly model the similarity between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs in a mutual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal mutual matching to maximize their mutual information. Experiments show that our MMN achieves highly competitive performance compared with the state-of-the-art methods on four video grounding benchmarks. Based on MMN, we present a winner solution for the HC-STVG challenge of the 3rd PIC workshop. This suggests that metric learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space. Code is available at https://github.com/MCG-NJU/MMN",
    "checked": true,
    "id": "3ca89413394c6a236c27b026c026c30c8c4eb7b6",
    "semantic_title": "negative sample matters: a renaissance of metric learning for temporal grounding",
    "citation_count": 38,
    "authors": [
      "Zhenzhi Wang",
      "Limin Wang",
      "Tao Wu",
      "Tianhao Li",
      "Gangshan Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20164": {
    "title": "Texture Reformer: Towards Fast and Universal Interactive Texture Transfer",
    "volume": "main",
    "abstract": "In this paper, we present the texture reformer, a fast and universal neural-based framework for interactive texture transfer with user-specified guidance. The challenges lie in three aspects: 1) the diversity of tasks, 2) the simplicity of guidance maps, and 3) the execution efficiency. To address these challenges, our key idea is to use a novel feed-forward multi-view and multi-stage synthesis procedure consisting of I) a global view structure alignment stage, II) a local view texture refinement stage, and III) a holistic effect enhancement stage to synthesize high-quality results with coherent structures and fine texture details in a coarse-to-fine fashion. In addition, we also introduce a novel learning-free view-specific texture reformation (VSTR) operation with a new semantic map guidance strategy to achieve more accurate semantic-guided and structure-preserved texture transfer. The experimental results on a variety of application scenarios demonstrate the effectiveness and superiority of our framework. And compared with the state-of-the-art interactive texture transfer algorithms, it not only achieves higher quality results but, more remarkably, also is 2-5 orders of magnitude faster",
    "checked": true,
    "id": "24f0014e9a77cfa500fe23a1a5b0b88d4e3a3739",
    "semantic_title": "texture reformer: towards fast and universal interactive texture transfer",
    "citation_count": 8,
    "authors": [
      "Zhizhong Wang",
      "Lei Zhao",
      "Haibo Chen",
      "Ailin Li",
      "Zhiwen Zuo",
      "Wei Xing",
      "Dongming Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20165": {
    "title": "Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification",
    "volume": "main",
    "abstract": "Multi-modal person Re-ID introduces more complementary information to assist the traditional Re-ID task. Existing multi-modal methods ignore the importance of modality-specific information in the feature fusion stage. To this end, we propose a novel method to boost modality-specific representations for multi-modal person Re-ID: Interact, Embed, and EnlargE (IEEE). First, we propose a cross-modal interacting module to exchange useful information between different modalities in the feature extraction phase. Second, we propose a relation-based embedding module to enhance the richness of feature descriptors by embedding the global feature into the fine-grained local information. Finally, we propose multi-modal margin loss to force the network to learn modality-specific information for each modality by enlarging the intra-class discrepancy. Superior performance on multi-modal Re-ID dataset RGBNT201 and three constructed Re-ID datasets validate the effectiveness of the proposed method compared with the state-of-the-art approaches",
    "checked": true,
    "id": "6df11ddfb66ca545bc3c0613ed10ceb19d8ed8b6",
    "semantic_title": "interact, embed, and enlarge: boosting modality-specific representations for multi-modal person re-identification",
    "citation_count": 7,
    "authors": [
      "Zi Wang",
      "Chenglong Li",
      "Aihua Zheng",
      "Ran He",
      "Jin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20166": {
    "title": "Can Semantic Labels Assist Self-Supervised Visual Representation Learning?",
    "volume": "main",
    "abstract": "Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community",
    "checked": true,
    "id": "87f17b5d24e1d8398d004fc6919264db3ef3efe4",
    "semantic_title": "can semantic labels assist self-supervised visual representation learning?",
    "citation_count": 18,
    "authors": [
      "Longhui Wei",
      "Lingxi Xie",
      "Jianzhong He",
      "Xiaopeng Zhang",
      "Qi Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20167": {
    "title": "Rethinking the Two-Stage Framework for Grounded Situation Recognition",
    "volume": "main",
    "abstract": "Grounded Situation Recognition (GSR), i.e., recognizing the salient activity (or verb) category in an image (e.g.,buying) and detecting all corresponding semantic roles (e.g.,agent and goods), is an essential step towards \"human-like\" event understanding. Since each verb is associated with a specific set of semantic roles, all existing GSR methods resort to a two-stage framework: predicting the verb in the first stage and detecting the semantic roles in the second stage. However, there are obvious drawbacks in both stages: 1) The widely-used cross-entropy (XE) loss for object recognition is insufficient in verb classification due to the large intra-class variation and high inter-class similarity among daily activities. 2) All semantic roles are detected in an autoregressive manner, which fails to model the complex semantic relations between different roles. To this end, we propose a novel SituFormerfor GSR which consists of a Coarse-to-Fine Verb Model (CFVM) and a Transformer-based Noun Model (TNM). CFVM is a two-step verb prediction model: a coarse-grained model trained with XE loss first proposes a set of verb candidates, and then a fine-grained model trained with triplet loss re-ranks these candidates with enhanced verb features (not only separable but also discriminative). TNM is a transformer-based semantic role detection model, which detects all roles parallelly. Owing to the global relation modeling ability and flexibility of the transformer decoder, TNM can fully explore the statistical dependency of the roles. Extensive validations on the challenging SWiG benchmark show that SituFormer achieves a new state-of-the-art performance with significant gains under various metrics. Code is available at https://github.com/kellyiss/SituFormer",
    "checked": true,
    "id": "3bd68745f52ce3270d7abc3212906cfcafce6641",
    "semantic_title": "rethinking the two-stage framework for grounded situation recognition",
    "citation_count": 13,
    "authors": [
      "Meng Wei",
      "Long Chen",
      "Wei Ji",
      "Xiaoyu Yue",
      "Tat-Seng Chua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20168": {
    "title": "Boosting the Transferability of Video Adversarial Examples via Temporal Translation",
    "volume": "main",
    "abstract": "Although deep-learning based video recognition models have achieved remarkable success, they are vulnerable to adversarial examples that are generated by adding human-imperceptible perturbations on clean video samples. As indicated in recent studies, adversarial examples are transferable, which makes it feasible for black-box attacks in real-world applications. Nevertheless, most existing adversarial attack methods have poor transferability when attacking other video models and transfer-based attacks on video models are still unexplored. To this end, we propose to boost the transferability of video adversarial examples for black-box attacks on video recognition models. Through extensive analysis, we discover that different video recognition models rely on different discriminative temporal patterns, leading to the poor transferability of video adversarial examples. This motivates us to introduce a temporal translation attack method, which optimizes the adversarial perturbations over a set of temporal translated video clips. By generating adversarial examples over translated videos, the resulting adversarial examples are less sensitive to temporal patterns existed in the white-box model being attacked and thus can be better transferred. Extensive experiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate that our method can significantly boost the transferability of video adversarial examples. For transfer-based attack against video recognition models, it achieves a 61.56% average attack success rate on the Kinetics-400 and 48.60% on the UCF-101",
    "checked": true,
    "id": "cea5fec383a7bda5e7be8f89c54e9aec26cf39c0",
    "semantic_title": "boosting the transferability of video adversarial examples via temporal translation",
    "citation_count": 11,
    "authors": [
      "Zhipeng Wei",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20169": {
    "title": "Towards Transferable Adversarial Attacks on Vision Transformers",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance",
    "checked": true,
    "id": "3c2622daa8a658d5c85ea9869cb460a70b0f878d",
    "semantic_title": "towards transferable adversarial attacks on vision transformers",
    "citation_count": 40,
    "authors": [
      "Zhipeng Wei",
      "Jingjing Chen",
      "Micah Goldblum",
      "Zuxuan Wu",
      "Tom Goldstein",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20170": {
    "title": "L-CoDe:Language-Based Colorization Using Color-Object Decoupled Conditions",
    "volume": "main",
    "abstract": "Colorizing a grayscale image is inherently an ill-posed problem with multi-modal uncertainty. Language-based colorization offers a natural way of interaction to reduce such uncertainty via a user-provided caption. However, the color-object coupling and mismatch issues make the mapping from word to color difficult. In this paper, we propose L-CoDe, a Language-based Colorization network using color-object Decoupled conditions. A predictor for object-color corresponding matrix (OCCM) and a novel attention transfer module (ATM) are introduced to solve the color-object coupling problem. To deal with color-object mismatch that results in incorrect color-object correspondence, we adopt a soft-gated injection module (SIM). We further present a new dataset containing annotated color-object pairs to provide supervisory signals for resolving the coupling problem. Experimental results show that our approach outperforms state-of-the-art methods conditioned on captions",
    "checked": false,
    "id": "b6d341d24393403708bcfb00bdd6eb8e124635f9",
    "semantic_title": "l-code: language-based colorization using color-object decoupled conditions",
    "citation_count": 14,
    "authors": [
      "Shuchen Weng",
      "Hao Wu",
      "Zheng Chang",
      "Jiajun Tang",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20171": {
    "title": "Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields",
    "volume": "main",
    "abstract": "Astronomical interferometry enables a collection of telescopes to achieve angular resolutions comparable to that of a single, much larger telescope. This is achieved by combining simultaneous observations from pairs of telescopes such that the signal is mathematically equivalent to sampling the Fourier domain of the object. However, reconstructing images from such sparse sampling is a challenging and ill-posed problem, with current methods requiring precise tuning of parameters and manual, iterative cleaning by experts. We present a novel deep learning approach in which the representation in the Fourier domain of an astronomical source is learned implicitly using a neural field representation. Data-driven priors can be added through a transformer encoder. Results on synthetically observed galaxies show that transformer-conditioned neural fields can successfully reconstruct astronomical observations even when the number of visibilities is very sparse",
    "checked": true,
    "id": "f7a0dc64f25581ca61a36261e4cbadb7e0d2bc3f",
    "semantic_title": "neural interferometry: image reconstruction from astronomical interferometers using transformer-conditioned neural fields",
    "citation_count": 3,
    "authors": [
      "Benjamin Wu",
      "Chao Liu",
      "Benjamin Eckart",
      "Jan Kautz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20172": {
    "title": "TDv2: A Novel Tree-Structured Decoder for Offline Mathematical Expression Recognition",
    "volume": "main",
    "abstract": "In recent years, tree decoders become more popular than LaTeX string decoders in the field of handwritten mathematical expression recognition (HMER) as they can capture the hierarchical tree structure of mathematical expressions. However previous tree decoders converted the tree structure labels into a fixed and ordered sequence, which could not make full use of the diversified expression of tree labels. In this study, we propose a novel tree decoder (TDv2) to fully utilize the tree structure labels. Compared with previous tree decoders, this new model does not require a fixed priority for different branches of a node during training and inference, which can effectively improve the model generalization capability. The input and output of the model make full use of the tree structure label, so that there is no need to find the parent node in the decoding process, which simplifies the decoding process and adds a prior information to help predict the node. We verified the effectiveness of each part of the model through comprehensive ablation experiments and attention visualization analysis. On the authoritative CROHME 14/16/19 datasets, our method achieves the state-of-the-art results",
    "checked": true,
    "id": "743696c8d4cc736a83adf2aac02049bed3ea8260",
    "semantic_title": "tdv2: a novel tree-structured decoder for offline mathematical expression recognition",
    "citation_count": 2,
    "authors": [
      "Changjie Wu",
      "Jun Du",
      "Yunqing Li",
      "Jianshu Zhang",
      "Chen Yang",
      "Bo Ren",
      "Yiqing Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20173": {
    "title": "Learning Token-Based Representation for Image Retrieval",
    "volume": "main",
    "abstract": "In image retrieval, deep local features learned in a data-driven manner have been demonstrated effective to improve retrieval performance. To realize efficient retrieval on large image database, some approaches quantize deep local features with a large codebook and match images with aggregated match kernel. However, the complexity of these approaches is non-trivial with large memory footprint, which limits their capability to jointly perform feature learning and aggregation. To generate compact global representations while maintaining regional matching capability, we propose a unified framework to jointly learn local feature representation and aggregation. In our framework, we first extract local features using CNNs. Then, we design a tokenizer module to aggregate them into a few visual tokens, each corresponding to a specific visual pattern. This helps to remove background noise, and capture more discriminative regions in the image. Next, a refinement block is introduced to enhance the visual tokens with self-attention and cross-attention. Finally, different visual tokens are concatenated to generate a compact global representation. The whole framework is trained end-to-end with image-level labels. Extensive experiments are conducted to evaluate our approach, which outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets",
    "checked": true,
    "id": "a6ded609a83b88ee2da5b00ad314d39be9843064",
    "semantic_title": "learning token-based representation for image retrieval",
    "citation_count": 9,
    "authors": [
      "Hui Wu",
      "Min Wang",
      "Wengang Zhou",
      "Yang Hu",
      "Houqiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20174": {
    "title": "Multi-Modal Answer Validation for Knowledge-Based VQA",
    "volume": "main",
    "abstract": "The problem of knowledge-based visual question answering involves answering questions that require external knowledge in addition to the content of the image. Such knowledge typically comes in various forms, including visual, textual, and commonsense knowledge. Using more knowledge sources increases the chance of retrieving more irrelevant or noisy facts, making it challenging to comprehend the facts and find the answer. To address this challenge, we propose Multi-modal Answer Validation using External knowledge (MAVEx), where the idea is to validate a set of promising answer candidates based on answer-specific knowledge retrieval. Instead of searching for the answer in a vast collection of often irrelevant facts as most existing approaches do, MAVEx aims to learn how to extract relevant knowledge from noisy sources, which knowledge source to trust for each answer candidate, and how to validate the candidate using that source. Our multi-modal setting is the first to leverage external visual knowledge (images searched using Google), in addition to textual knowledge in the form of Wikipedia sentences and ConceptNet concepts. Our experiments with OK-VQA, a challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new state-of-the-art results. Our code is available at https://github.com/jialinwu17/MAVEX",
    "checked": true,
    "id": "8dce342a435034fa0521b24b61393397df95c095",
    "semantic_title": "multi-modal answer validation for knowledge-based vqa",
    "citation_count": 60,
    "authors": [
      "Jialin Wu",
      "Jiasen Lu",
      "Ashish Sabharwal",
      "Roozbeh Mottaghi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20175": {
    "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation",
    "volume": "main",
    "abstract": "In object re-identification (ReID), the development of deep learning techniques often involves model updates and deployment. It is unbearable to re-embedding and re-index with the system suspended when deploying new models. Therefore, backward-compatible representation is proposed to enable ``new'' features to be compared with ``old'' features directly, which means that the database is active when there are both ``new'' and ``old'' features in it. Thus we can scroll-refresh the database or even do nothing on the database to update. The existing backward-compatible methods either require a strong overlap between old and new training data or simply conduct constraints at the instance level. Thus they are difficult in handling complicated cluster structures and are limited in eliminating the impact of outliers in old embeddings, resulting in a risk of damaging the discriminative capability of new features. In this work, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method. With no assumptions about the new training data, we estimate the sub-cluster structures of old embeddings. A new embedding is constrained with multiple old embeddings in both embedding space and discrimination space at the sub-class level. The effect of outliers diminished, as the multiple samples serve as ``mean teachers''. Besides, we propose a scheme to filter the old embeddings with low credibility, further improving the compatibility robustness. Our method ensures the compatibility without impairing the accuracy of the new model. It can even improve the new model's accuracy in most scenarios",
    "checked": true,
    "id": "633983322c62f7ce8f872f060523e5131c98aa78",
    "semantic_title": "neighborhood consensus contrastive learning for backward-compatible representation",
    "citation_count": 4,
    "authors": [
      "Shengsen Wu",
      "Liang Chen",
      "Yihang Lou",
      "Yan Bai",
      "Tao Bai",
      "Minghua Deng",
      "Ling-Yu Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20176": {
    "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "volume": "main",
    "abstract": "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224x224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT",
    "checked": true,
    "id": "2a4024163826151303aa0bbb18320b8a67167794",
    "semantic_title": "pale transformer: a general vision transformer backbone with pale-shaped attention",
    "citation_count": 31,
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20177": {
    "title": "Style Mixing and Patchwise Prototypical Matching for One-Shot Unsupervised Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of one-shot unsupervised domain adaptation (OSUDA) for semantic segmentation where the segmentors only see one unlabeled target image during training. In this case, traditional unsupervised domain adaptation models usually fail since they cannot adapt to the target domain with over-fitting to one (or few) target samples. To address this problem, existing OSUDA methods usually integrate a style-transfer module to perform domain randomization based on the unlabeled target sample, with which multiple domains around the target sample can be explored during training. However, such a style-transfer module relies on an additional set of images as style reference for pre-training and also increases the memory demand for domain adaptation. Here we propose a new OSUDA method that can effectively relieve such computational burden. Specifically, we integrate several style-mixing layers into the segmentor which play the role of style-transfer module to stylize the source images without introducing any learned parameters. Moreover, we propose a patchwise prototypical matching (PPM) method to weighted consider the importance of source pixels during the supervised training to relieve the negative adaptation. Experimental results show that our method achieves new state-of-the-art performance on two commonly used benchmarks for domain adaptive semantic segmentation under the one-shot setting and is more efficient than all comparison approaches",
    "checked": true,
    "id": "bd50790300adfc95af09fff4fac316fe8e685748",
    "semantic_title": "style mixing and patchwise prototypical matching for one-shot unsupervised domain adaptive semantic segmentation",
    "citation_count": 13,
    "authors": [
      "Xinyi Wu",
      "Zhenyao Wu",
      "Yuhang Lu",
      "Lili Ju",
      "Song Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20178": {
    "title": "Multi-Centroid Representation Network for Domain Adaptive Person Re-ID",
    "volume": "main",
    "abstract": "Recently, many approaches tackle the Unsupervised Domain Adaptive person re-identification (UDA re-ID) problem through pseudo-label-based contrastive learning. During training, a uni-centroid representation is obtained by simply averaging all the instance features from a cluster with the same pseudo label. However, a cluster may contain images with different identities (label noises) due to the imperfect clustering results, which makes the uni-centroid representation inappropriate. In this paper, we present a novel Multi-Centroid Memory (MCM) to adaptively capture different identity information within the cluster. MCM can effectively alleviate the issue of label noises by selecting proper positive/negative centroids for the query image. Moreover, we further propose two strategies to improve the contrastive learning process. First, we present a Domain-Specific Contrastive Learning (DSCL) mechanism to fully explore intra-domain information by comparing samples only from the same domain. Second, we propose Second-Order Nearest Interpolation (SONI) to obtain abundant and informative negative samples. We integrate MCM, DSCL, and SONI into a unified framework named Multi-Centroid Representation Network (MCRN). Extensive experiments demonstrate the superiority of MCRN over state-of-the-art approaches on multiple UDA re-ID tasks and fully unsupervised re-ID tasks",
    "checked": true,
    "id": "f62bf2ed53e413b874d0f714e367df1e6c68b8ad",
    "semantic_title": "multi-centroid representation network for domain adaptive person re-id",
    "citation_count": 14,
    "authors": [
      "Yuhang Wu",
      "Tengteng Huang",
      "Haotian Yao",
      "Chi Zhang",
      "Yuanjie Shao",
      "Chuchu Han",
      "Changxin Gao",
      "Nong Sang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20179": {
    "title": "Efficient Non-local Contrastive Attention for Image Super-resolution",
    "volume": "main",
    "abstract": "Non-Local Attention (NLA) brings significant improvement for Single Image Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural images. However, NLA gives noisy information large weights and consumes quadratic computation resources with respect to the input size, limiting its performance and application. In this paper, we propose a novel Efficient Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling and leverage more relevant non-local features. Specifically, ENLCA consists of two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA adopts the kernel method to approximate exponential function and obtains linear computation complexity. For Sparse Aggregation, we multiply inputs by an amplification factor to focus on informative features, yet the variance of approximation increases exponentially. Therefore, contrastive learning is applied to further separate relevant and irrelevant features. To demonstrate the effectiveness of ENLCA, we build an architecture called Efficient Non-Local Contrastive Network (ENLCN) by adding a few of our modules in a simple backbone. Extensive experimental results show that ENLCN reaches superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations",
    "checked": true,
    "id": "856bf400db06a5732d891ad480c65a1eec539595",
    "semantic_title": "efficient non-local contrastive attention for image super-resolution",
    "citation_count": 28,
    "authors": [
      "Bin Xia",
      "Yucheng Hang",
      "Yapeng Tian",
      "Wenming Yang",
      "Qingmin Liao",
      "Jie Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20180": {
    "title": "Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-Based Super-resolution",
    "volume": "main",
    "abstract": "Reference-based super-resolution (RefSR) has made significant progress in producing realistic textures using an external reference (Ref) image. However, existing RefSR methods obtain high-quality correspondence matchings consuming quadratic computation resources with respect to the input size, limiting its application. Moreover, these approaches usually suffer from scale misalignments between the low-resolution (LR) image and Ref image. In this paper, we propose an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch) and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching efficiency, we design a novel Embedded PatchMacth scheme with random samples propagation, which involves end-to-end training with asymptotic linear computational cost to the input size. To further reduce computational cost and speed up convergence, we apply the coarse-to-fine strategy on Embedded PatchMacth constituting CFE-PatchMatch. To fully leverage reference information across multiple scales and enhance robustness to scale misalignment, we develop the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation. The Dynamic Aggregation corrects minor scale misalignment by dynamically aggregating features, and the Multi-Scale Aggregation brings robustness to large scale misalignment by fusing multi-scale information. Experimental results show that the proposed AMSA achieves superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations",
    "checked": true,
    "id": "d32ca9dc4fa729a0022ef08546d25b9f16cbd836",
    "semantic_title": "coarse-to-fine embedded patchmatch and multi-scale dynamic aggregation for reference-based super-resolution",
    "citation_count": 7,
    "authors": [
      "Bin Xia",
      "Yapeng Tian",
      "Yucheng Hang",
      "Wenming Yang",
      "Qingmin Liao",
      "Jie Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20181": {
    "title": "Cross-Domain Collaborative Normalization via Structural Knowledge",
    "volume": "main",
    "abstract": "Batch Normalization (BN) as an important component assists Deep Neural Networks in achieving promising performance for extensive learning tasks by scaling distribution of feature representations within mini-batches. However, the application of BN suffers from performance degradation under the scenario of Unsupervised Domain Adaptation (UDA), since the estimated statistics fail to concurrently describe two different domains. In this paper, we develop a novel normalization technique, named Collaborative Normalization (CoN), for eliminating domain discrepancy and accelerating the model training of neural networks for UDA. Unlike typical strategies only exploiting domain-specific statistics during normalization, our CoN excavates cross-domain knowledge and simultaneously scales features from various domains by mimicking the merits of collaborative representation. Our CoN can be easily plugged into popular neural network backbones for cross-domain learning. On the one hand, theoretical analysis guarantees that models with CoN promote discriminability of feature representations and accelerate convergence rate; on the other hand, empirical study verifies that replacing BN with CoN in popular network backbones effectively improves classification accuracy in most learning tasks across three cross-domain visual benchmarks",
    "checked": true,
    "id": "6aa6f15b6bbd0dd3433a88a35fd7b918e6b8431a",
    "semantic_title": "cross-domain collaborative normalization via structural knowledge",
    "citation_count": 3,
    "authors": [
      "Haifeng Xia",
      "Zhengming Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20182": {
    "title": "ReMoNet: Recurrent Multi-Output Network for Efficient Video Denoising",
    "volume": "main",
    "abstract": "While deep neural network-based video denoising methods have achieved promising results, it is still hard to deploy them on mobile devices due to their high computational cost and memory demands. This paper aims to develop a lightweight deep video denoising method that is friendly to resource-constrained mobile devices. Inspired by the facts that 1) consecutive video frames usually contain redundant temporal coherency, and 2) neural networks are usually over-parameterized, we propose a multi-input multi-output (MIMO) paradigm to process consecutive video frames within one-forward-pass. The basic idea is concretized to a novel architecture termed Recurrent Multi-output Network (ReMoNet), which consists of recurrent temporal fusion and temporal aggregation blocks and is further reinforced by similarity-based mutual distillation. We conduct extensive experiments on NVIDIA GPU and Qualcomm Snapdragon 888 mobile platform with Gaussian noise and simulated Image-Signal-Processor (ISP) noise. The experimental results show that ReMoNet is both effective and efficient on video denoising. Moreover, we show that ReMoNet is more robust under higher noise level scenarios",
    "checked": true,
    "id": "7c7597d95c6c1ea657a43e6edd41aa52b1e24f23",
    "semantic_title": "remonet: recurrent multi-output network for efficient video denoising",
    "citation_count": 6,
    "authors": [
      "Liuyu Xiang",
      "Jundong Zhou",
      "Jirui Liu",
      "Zerun Wang",
      "Haidong Huang",
      "Jie Hu",
      "Jungong Han",
      "Yuchen Guo",
      "Guiguang Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20183": {
    "title": "Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation",
    "volume": "main",
    "abstract": "Knowledge transfer from synthetic to real data has been widely studied to mitigate data annotation constraints in various computer vision tasks such as semantic segmentation. However, the study focused on 2D images and its counterpart in 3D point clouds segmentation lags far behind due to the lack of large-scale synthetic datasets and effective transfer methods. We address this issue by collecting SynLiDAR, a large-scale synthetic LiDAR dataset that contains point-wise annotated point clouds with accurate geometric shapes and comprehensive semantic classes. SynLiDAR was collected from multiple virtual environments with rich scenes and layouts which consists of over 19 billion points of 32 semantic classes. In addition, we design PCT, a novel point cloud translator that effectively mitigates the gap between synthetic and real point clouds. Specifically, we decompose the synthetic-to-real gap into an appearance component and a sparsity component and handle them separately which improves the point cloud translation greatly. We conducted extensive experiments over three transfer learning setups including data augmentation, semi-supervised domain adaptation and unsupervised domain adaptation. Extensive experiments show that SynLiDAR provides a high-quality data source for studying 3D transfer and the proposed PCT achieves superior point cloud translation consistently across the three setups. The dataset is available at https://github.com/xiaoaoran/SynLiDAR",
    "checked": true,
    "id": "2372e7b7dcab84495d9798d698ca6cfcf65959c0",
    "semantic_title": "transfer learning from synthetic to real lidar point cloud for semantic segmentation",
    "citation_count": 34,
    "authors": [
      "Aoran Xiao",
      "Jiaxing Huang",
      "Dayan Guan",
      "Fangneng Zhan",
      "Shijian Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20184": {
    "title": "Video as Conditional Graph Hierarchy for Multi-Granular Question Answering",
    "volume": "main",
    "abstract": "Video question answering requires the models to understand and reason about both the complex video and language data to correctly derive the answers. Existing efforts have been focused on designing sophisticated cross-modal interactions to fuse the information from two modalities, while encoding the video and question holistically as frame and word sequences. Despite their success, these methods are essentially revolving around the sequential nature of video- and question-contents, providing little insight to the problem of question-answering and lacking interpretability as well. In this work, we argue that while video is presented in frame sequence, the visual elements (e.g., objects, actions, activities and events) are not sequential but rather hierarchical in semantic space. To align with the multi-granular essence of linguistic concepts in language queries, we propose to model video as a conditional graph hierarchy which weaves together visual facts of different granularity in a level-wise manner, with the guidance of corresponding textual cues. Despite the simplicity, our extensive experiments demonstrate the superiority of such conditional hierarchical graph architecture, with clear performance improvements over prior methods and also better generalization across different type of questions. Further analyses also demonstrate the model's reliability as it shows meaningful visual-textual evidences for the predicted answers",
    "checked": true,
    "id": "10ab178bb5251f82c1ba3520135cf8471a2fa30c",
    "semantic_title": "video as conditional graph hierarchy for multi-granular question answering",
    "citation_count": 43,
    "authors": [
      "Junbin Xiao",
      "Angela Yao",
      "Zhiyuan Liu",
      "Yicong Li",
      "Wei Ji",
      "Tat-Seng Chua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20185": {
    "title": "AdaptivePose: Human Parts as Adaptive Points",
    "volume": "main",
    "abstract": "Multi-person pose estimation methods generally follow top-down and bottom-up paradigms, both of which can be considered as two-stage approaches thus leading to the high computation cost and low efficiency. Towards a compact and efficient pipeline for multi-person pose estimation task, in this paper, we propose to represent the human parts as points and present a novel body representation, which leverages an adaptive point set including the human center and seven human-part related points to represent the human instance in a more fine-grained manner. The novel representation is more capable of capturing the various pose deformation and adaptively factorizes the long-range center-to-joint displacement thus delivers a single-stage differentiable network to more precisely regress multi-person pose, termed as AdaptivePose. For inference, our proposed network eliminates the grouping as well as refinements and only needs a single-step disentangling process to form multi-person pose. Without any bells and whistles, we achieve the best speed-accuracy trade-offs of 67.4% AP / 29.4 fps with DLA-34 and 71.3% AP / 9.1 fps with HRNet-W48 on COCO test-dev dataset",
    "checked": true,
    "id": "a47f5faa7f23ecf536a4f421ffb0b5087d0ab78c",
    "semantic_title": "adaptivepose: human parts as adaptive points",
    "citation_count": 8,
    "authors": [
      "Yabo Xiao",
      "Xiao Juan Wang",
      "Dongdong Yu",
      "Guoli Wang",
      "Qian Zhang",
      "Mingshu HE"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20186": {
    "title": "Learning Quality-Aware Representation for Multi-Person Pose Regression",
    "volume": "main",
    "abstract": "Off-the-shelf single-stage multi-person pose regression methods generally leverage the instance score (i.e., confidence of the instance localization) to indicate the pose quality for selecting the pose candidates. We consider that there are two gaps involved in existing paradigm: 1) The instance score is not well interrelated with the pose regression quality. 2) The instance feature representation, which is used for predicting the instance score, does not explicitly encode the structural pose information to predict the reasonable score that represents pose regression quality. To address the aforementioned issues, we propose to learn the pose regression quality-aware representation. Concretely, for the first gap, instead of using the previous instance confidence label (e.g., discrete {1,0} or Gaussian representation) to denote the position and confidence for person instance, we firstly introduce the Consistent Instance Representation (CIR) that unifies the pose regression quality score of instance and the confidence of background into a pixel-wise score map to calibrates the inconsistency between instance score and pose regression quality. To fill the second gap, we further present the Query Encoding Module (QEM) including the Keypoint Query Encoding (KQE) to encode the positional and semantic information for each keypoint and the Pose Query Encoding (PQE) which explicitly encodes the predicted structural pose information to better fit the Consistent Instance Representation (CIR). By using the proposed components, we significantly alleviate the above gaps. Our method outperforms previous single-stage regression-based even bottom-up methods and achieves the state-of-the-art result of 71.7 AP on MS COCO test-dev set",
    "checked": true,
    "id": "0eafca483b74b5a8b0255d73f1f1b1e660ea44a6",
    "semantic_title": "learning quality-aware representation for multi-person pose regression",
    "citation_count": 7,
    "authors": [
      "Yabo Xiao",
      "Dongdong Yu",
      "Xiao Juan Wang",
      "Lei Jin",
      "Guoli Wang",
      "Qian Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20187": {
    "title": "Attribute-Based Progressive Fusion Network for RGBT Tracking",
    "volume": "main",
    "abstract": "RGBT tracking usually suffers from various challenge factors, such as fast motion, scale variation, illumination variation, thermal crossover and occlusion, to name a few. Existing works often study fusion models to solve all challenges simultaneously, and it requires fusion models complex enough and training data large enough, which are usually difficult to be constructed in real-world scenarios. In this work, we disentangle the fusion process via the challenge attributes, and thus propose a novel Attribute-based Progressive Fusion Network (APFNet) to increase the fusion capacity with a small number of parameters while reducing the dependence on large-scale training data. In particular, we design five attribute-specific fusion branches to integrate RGB and thermal features under the challenges of thermal crossover, illumination variation, scale variation, occlusion and fast motion respectively. By disentangling the fusion process, we can use a small number of parameters for each branch to achieve robust fusion of different modalities and train each branch using the small training subset with the corresponding attribute annotation. Then, to adaptive fuse features of all branches, we design an aggregation fusion module based on SKNet. Finally, we also design an enhancement fusion transformer to strengthen the aggregated feature and modality-specific features. Experimental results on benchmark datasets demonstrate the effectiveness of our APFNet against other state-of-the-art methods",
    "checked": true,
    "id": "96b0d333ea77ca85f74015f21f8a89380b092e5a",
    "semantic_title": "attribute-based progressive fusion network for rgbt tracking",
    "citation_count": 32,
    "authors": [
      "Yun Xiao",
      "MengMeng Yang",
      "Chenglong Li",
      "Lei Liu",
      "Jin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20188": {
    "title": "Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function",
    "volume": "main",
    "abstract": "Recovering detailed facial geometry from a set of calibrated multi-view images is valuable for its wide range of applications. Traditional multi-view stereo (MVS) methods adopt an optimization-based scheme to regularize the matching cost. Recently, learning-based methods integrate all these into an end-to-end neural network and show superiority of efficiency. In this paper, we propose a novel architecture to recover extremely detailed 3D faces within dozens of seconds. Unlike previous learning-based methods that regularize the cost volume via 3D CNN, we propose to learn an implicit function for regressing the matching cost. By fitting a 3D morphable model from multi-view images, the features of multiple images are extracted and aggregated in the mesh-attached UV space, which makes the implicit function more effective in recovering detailed facial shape. Our method outperforms SOTA learning-based MVS in accuracy by a large margin on the FaceScape dataset. The code and data are released in https://github.com/zhuhao-nju/mvfr",
    "checked": true,
    "id": "68ea434244ce953bb10c600767994326b48a8d42",
    "semantic_title": "detailed facial geometry recovery from multi-view images by learning an implicit function",
    "citation_count": 9,
    "authors": [
      "Yunze Xiao",
      "Hao Zhu",
      "Haotian Yang",
      "Zhengyu Diao",
      "Xiangju Lu",
      "Xun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20189": {
    "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration",
    "volume": "main",
    "abstract": "Data association is important in the point cloud registration. In this work, we propose to solve the partial-to-partial registration from a new perspective, by introducing multi-level feature interactions between the source and the reference clouds at the feature extraction stage, such that the registration can be realized without the attentions or explicit mask estimation for the overlapping detection as adopted previously. Specifically, we present FINet, a feature interactionbased structure with the capability to enable and strengthen the information associating between the inputs at multiple stages. To achieve this, we first split the features into two components, one for rotation and one for translation, based on the fact that they belong to different solution spaces, yielding a dual branches structure. Second, we insert several interaction modules at the feature extractor for the data association. Third, we propose a transformation sensitivity loss to obtain rotation-attentive and translation-attentive features. Experiments demonstrate that our method performs higher precision and robustness compared to the state-of-the-art traditional and learning-based methods. Code is available at https://github.com/megvii-research/FINet",
    "checked": true,
    "id": "6cfd3a46078a7cf3a29235f0d67da1cfe69414ee",
    "semantic_title": "finet: dual branches feature interaction for partial-to-partial point cloud registration",
    "citation_count": 15,
    "authors": [
      "Hao Xu",
      "Nianjin Ye",
      "Guanghui Liu",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20190": {
    "title": "Rendering-Aware HDR Environment Map Prediction from a Single Image",
    "volume": "main",
    "abstract": "High dynamic range (HDR) illumination estimation from a single low dynamic range (LDR) image is a significant task in computer vision, graphics, and augmented reality. We present a two-stage deep learning-based method to predict an HDR environment map from a single narrow field-of-view LDR image. We first learn a hybrid parametric representation that sufficiently covers high- and low-frequency illumination components in the environment. Taking the estimated illuminations as guidance, we build a generative adversarial network to synthesize an HDR environment map that enables realistic rendering effects. We specifically consider the rendering effect by supervising the networks using rendering losses in both stages, on the predicted environment map as well as the hybrid illumination representation. Quantitative and qualitative experiments demonstrate that our approach achieves lower relighting errors for virtual object insertion and is preferred by users compared to state-of-the-art methods",
    "checked": true,
    "id": "f35d9f98880165d0595fdca39732e47bda2516c6",
    "semantic_title": "rendering-aware hdr environment map prediction from a single image",
    "citation_count": 4,
    "authors": [
      "Jun-Peng Xu",
      "Chenyu Zuo",
      "Fang-Lue Zhang",
      "Miao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20191": {
    "title": "Topology-Aware Convolutional Neural Network for Efficient Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "In the context of skeleton-based action recognition, graph convolutional networks (GCNs) have been rapidly developed, whereas convolutional neural networks (CNNs) have received less attention. One reason is that CNNs are considered poor in modeling the irregular skeleton topology. To alleviate this limitation, we propose a pure CNN architecture named Topology-aware CNN (Ta-CNN) in this paper. In particular, we develop a novel cross-channel feature augmentation module, which is a combo of map-attend-group-map operations. By applying the module to the coordinate level and the joint level subsequently, the topology feature is effectively enhanced. Notably, we theoretically prove that graph convolution is a special case of normal convolution when the joint dimension is treated as channels. This confirms that the topology modeling power of GCNs can also be implemented by using a CNN. Moreover, we creatively design a SkeletonMix strategy which mixes two persons in a unique manner and further boosts the performance. Extensive experiments are conducted on four widely used datasets, i.e. N-UCLA, SBU, NTU RGB+D and NTU RGB+D 120 to verify the effectiveness of Ta-CNN. We surpass existing CNN-based methods significantly. Compared with leading GCN-based methods, we achieve comparable performance with much less complexity in terms of the required GFLOPs and parameters",
    "checked": true,
    "id": "5ef1bff0a108cf3436a001e41399d9c669efc119",
    "semantic_title": "topology-aware convolutional neural network for efficient skeleton-based action recognition",
    "citation_count": 42,
    "authors": [
      "Kailin Xu",
      "Fanfan Ye",
      "Qiaoyong Zhong",
      "Di Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20192": {
    "title": "Transcoded Video Restoration by Temporal Spatial Auxiliary Network",
    "volume": "main",
    "abstract": "In most video platforms, such as Youtube, Kwai, and TikTok, the played videos usually have undergone multiple video encodings such as hardware encoding by recording devices, software encoding by video editing apps, and single/multiple video transcoding by video application servers. Previous works in compressed video restoration typically assume the compression artifacts are caused by one-time encoding. Thus, the derived solution usually does not work very well in practice. In this paper, we propose a new method, temporal spatial auxiliary network (TSAN), for transcoded video restoration. Our method considers the unique traits between video encoding and transcoding, and we consider the initial shallow encoded videos as the intermediate labels to assist the network to conduct self-supervised attention training. In addition, we employ adjacent multi-frame information and propose the temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. The experimental results demonstrate that the performance of the proposed method is superior to that of the previous techniques. The code is available at https://github.com/icecherylXuli/TSAN",
    "checked": true,
    "id": "dadb4e13c6a59cf19657fa703377dc9a0d28359b",
    "semantic_title": "transcoded video restoration by temporal spatial auxiliary network",
    "citation_count": 1,
    "authors": [
      "Li Xu",
      "Gang He",
      "Jinjia Zhou",
      "Jie Lei",
      "Weiying Xie",
      "Yunsong Li",
      "Yu-Wing Tai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20193": {
    "title": "DIRL: Domain-Invariant Representation Learning for Generalizable Semantic Segmentation",
    "volume": "main",
    "abstract": "Model generalization to the unseen scenes is crucial to real-world applications, such as autonomous driving, which requires robust vision systems. To enhance the model generalization, domain generalization through learning the domain-invariant representation has been widely studied. However, most existing works learn the shared feature space within multi-source domains but ignore the characteristic of the feature itself (e.g., the feature sensitivity to the domain-specific style). Therefore, we propose the Domain-invariant Representation Learning (DIRL) for domain generalization which utilizes the feature sensitivity as the feature prior to guide the enhancement of the model generalization capability. The guidance reflects in two folds: 1) Feature re-calibration that introduces the Prior Guided Attention Module (PGAM) to emphasize the insensitive features and suppress the sensitive features. 2): Feature whiting that proposes the Guided Feature Whiting (GFW) to remove the feature correlations which are sensitive to the domain-specific style. We construct the domain-invariant representation which suppresses the effect of the domain-specific style on the quality and correlation of the features. As a result, our method is simple yet effective, and can enhance the robustness of various backbone networks with little computational cost. Extensive experiments over multiple domains generalizable segmentation tasks show the superiority of our approach to other methods",
    "checked": true,
    "id": "fb76d171599e542d6b52102dd3bcaf14992233fc",
    "semantic_title": "dirl: domain-invariant representation learning for generalizable semantic segmentation",
    "citation_count": 18,
    "authors": [
      "Qi Xu",
      "Liang Yao",
      "Zhengkai Jiang",
      "Guannan Jiang",
      "Wenqing Chu",
      "Wenhui Han",
      "Wei Zhang",
      "Chengjie Wang",
      "Ying Tai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20194": {
    "title": "Behind the Curtain: Learning Occluded Shapes for 3D Object Detection",
    "volume": "main",
    "abstract": "Advances in LiDAR sensors provide rich 3D data that supports 3D scene understanding. However, due to occlusion and signal miss, LiDAR point clouds are in practice 2.5D as they cover only partial underlying shapes, which poses a fundamental challenge to 3D perception. To tackle the challenge, we present a novel LiDAR-based 3D object detection model, dubbed Behind the Curtain Detector (BtcDet), which learns the object shape priors and estimates the complete object shapes that are partially occluded (curtained) in point clouds. BtcDet first identifies the regions that are affected by occlusion and signal miss. In these regions, our model predicts the probability of occupancy that indicates if a region contains object shapes and integrates this probability map with detection features and generates high-quality 3D proposals. Finally, the occupancy estimation is integrated into the proposal refinement module to generate accurate bounding boxes. Extensive experiments on the KITTI Dataset and the Waymo Open Dataset demonstrate the effectiveness of BtcDet. Particularly for the 3D detection of both cars and cyclists on the KITTI benchmark, BtcDet surpasses all of the published state-of-the-art methods by remarkable margins. Code is released",
    "checked": true,
    "id": "2bff1086d28fe704d259d2fb400a1bc64624e85f",
    "semantic_title": "behind the curtain: learning occluded shapes for 3d object detection",
    "citation_count": 61,
    "authors": [
      "Qiangeng Xu",
      "Yiqi Zhong",
      "Ulrich Neumann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20195": {
    "title": "Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval",
    "volume": "main",
    "abstract": "Sketch-based 3D shape retrieval is a challenging task due to the large domain discrepancy between sketches and 3D shapes. Since existing methods are trained and evaluated on the same categories, they cannot effectively recognize the categories that have not been used during training. In this paper, we propose a novel domain disentangled generative adversarial network (DD-GAN) for zero-shot sketch-based 3D retrieval, which can retrieve the unseen categories that are not accessed during training. Specifically, we first generate domain-invariant features and domain-specific features by disentangling the learned features of sketches and 3D shapes, where the domain-invariant features are used to align with the corresponding word embeddings. Then, we develop a generative adversarial network that combines the domain-specific features of the seen categories with the aligned domain-invariant features to synthesize samples, where the synthesized samples of the unseen categories are generated by using the corresponding word embeddings. Finally, we use the synthesized samples of the unseen categories combined with the real samples of the seen categories to train the network for retrieval, so that the unseen categories can be recognized. In order to reduce the domain shift problem, we utilize unlabeled unseen samples to enhance the discrimination ability of the discriminator. With the discriminator distinguishing the generated samples from the unlabeled unseen samples, the generator can generate more realistic unseen samples. Extensive experiments on the SHREC'13 and SHREC'14 datasets show that our method significantly improves the retrieval performance of the unseen categories",
    "checked": true,
    "id": "030557706326cbd43a97968da64521416ad58081",
    "semantic_title": "domain disentangled generative adversarial network for zero-shot sketch-based 3d shape retrieval",
    "citation_count": 7,
    "authors": [
      "Rui Xu",
      "Zongyan Han",
      "Le Hui",
      "Jianjun Qian",
      "Jin Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20196": {
    "title": "Dual Attention Networks for Few-Shot Fine-Grained Recognition",
    "volume": "main",
    "abstract": "The task of few-shot fine-grained recognition is to classify images belonging to subordinate categories merely depending on few examples. Due to the fine-grained nature, it is desirable to capture subtle but discriminative part-level patterns from limited training data, which makes it a challenging problem. In this paper, to generate fine-grained tailored representations for few-shot recognition, we propose a Dual Attention Network (Dual Att-Net) consisting of two dual branches of both hard- and soft-attentions. Specifically, by producing attention guidance from deep activations of input images, our hard-attention is realized by keeping a few useful deep descriptors and forming them as a bag of multi-instance learning. Since these deep descriptors could correspond to objects' parts, the advantage of modeling as a multi-instance bag is able to exploit inherent correlation of these fine-grained parts. On the other side, a soft attended activation representation can be obtained by applying attention guidance upon original activations, which brings comprehensive attention information as the counterpart of hard-attention. After that, both outputs of dual branches are aggregated as a holistic image embedding w.r.t. input images. By performing meta-learning, we can learn a powerful image embedding in such a metric space to generalize to novel classes. Experiments on three popular fine-grained benchmark datasets show that our Dual Att-Net obviously outperforms other existing state-of-the-art methods",
    "checked": true,
    "id": "58e8f34b4161112564f68799e8ea62ccc847f94d",
    "semantic_title": "dual attention networks for few-shot fine-grained recognition",
    "citation_count": 1,
    "authors": [
      "Shu-Lin Xu",
      "Faen Zhang",
      "Xiu-Shen Wei",
      "Jianhua Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20197": {
    "title": "Sparse Cross-Scale Attention Network for Efficient LiDAR Panoptic Segmentation",
    "volume": "main",
    "abstract": "Two major challenges of 3D LiDAR Panoptic Segmentation (PS) are that point clouds of an object are surface-aggregated and thus hard to model the long-range dependency especially for large instances, and that objects are too close to separate each other. Recent literature addresses these problems by time-consuming grouping processes such as dual-clustering, mean-shift offsets and etc., or by bird-eye-view (BEV) dense centroid representation that downplays geometry. However, the long-range geometry relationship has not been sufficiently modeled by local feature learning from the above methods. To this end, we present SCAN, a novel sparse cross-scale attention network to first align multi-scale sparse features with global voxel-encoded attention to capture the long-range relationship of instance context, which is able to boost the regression accuracy of the over-segmented large objects. For the surface-aggregated points, SCAN adopts a novel sparse class-agnostic representation of instance centroids, which can not only maintain the sparsity of aligned features to solve the under-segmentation on small objects, but also reduce the computation amount of the network through sparse convolution. Our method outperforms previous methods by a large margin in the SemanticKITTI dataset for the challenging 3D PS task, achieving 1st place with a real-time inference speed",
    "checked": true,
    "id": "13a3eaf87de07f0e7b1f0ccf2536bf9ee6072505",
    "semantic_title": "sparse cross-scale attention network for efficient lidar panoptic segmentation",
    "citation_count": 14,
    "authors": [
      "Shuangjie Xu",
      "Rui Wan",
      "Maosheng Ye",
      "Xiaoyi Zou",
      "Tongyi Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20198": {
    "title": "Towards Fully Sparse Training: Information Restoration with Spatial Similarity",
    "volume": "main",
    "abstract": "The 2:4 structured sparsity pattern released by NVIDIA Ampere architecture, requiring four consecutive values containing at least two zeros, enables doubling math throughput for matrix multiplications. Recent works mainly focus on inference speedup via 2:4 sparsity while training acceleration has been largely overwhelmed where backpropagation consumes around 70% of the training time. However, unlike inference, training speedup with structured pruning is nontrivial due to the need to maintain the fidelity of gradients and reduce the additional overhead of performing 2:4 sparsity online. For the first time, this article proposes fully sparse training (FST) where `fully' indicates that ALL matrix multiplications in forward/backward propagation are structurally pruned while maintaining accuracy. To this end, we begin with saliency analysis, investigating the sensitivity of different sparse objects to structured pruning. Based on the observation of spatial similarity among activations, we propose pruning activations with fixed 2:4 masks. Moreover, an Information Restoration block is proposed to retrieve the lost information, which can be implemented by efficient gradient-shift operation. Evaluation of accuracy and efficiency shows that we can achieve 2× training acceleration with negligible accuracy degradation on challenging large-scale classification and detection tasks",
    "checked": true,
    "id": "722e0640c064c4068c46a02fdf0ddf3037740ac3",
    "semantic_title": "towards fully sparse training: information restoration with spatial similarity",
    "citation_count": 2,
    "authors": [
      "Weixiang Xu",
      "Xiangyu He",
      "Ke Cheng",
      "Peisong Wang",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20199": {
    "title": "Hierarchical Image Generation via Transformer-Based Sequential Patch Selection",
    "volume": "main",
    "abstract": "To synthesize images with preferred objects and interactions, a controllable way is to generate the image from a scene graph and a large pool of object crops, where the spatial arrangements of the objects in the image are defined by the scene graph while their appearances are determined by the retrieved crops from the pool. In this paper, we propose a novel framework with such a semi-parametric generation strategy. First, to encourage the retrieval of mutually compatible crops, we design a sequential selection strategy where the crop selection for each object is determined by the contents and locations of all object crops that have been chosen previously. Such process is implemented via a transformer trained with contrastive losses. Second, to generate the final image, our hierarchical generation strategy leverages hierarchical gated convolutions which are employed to synthesize areas not covered by any image crops, and a patch guided spatially adaptive normalization module which is proposed to guarantee the final generated images complying with the crop appearance and the scene graph. Evaluated on the challenging Visual Genome and COCO-Stuff dataset, our experimental results demonstrate the superiority of our proposed method over existing state-of-the-art methods",
    "checked": true,
    "id": "72b8902d84526d7d712144a4a011f8d4948ce718",
    "semantic_title": "hierarchical image generation via transformer-based sequential patch selection",
    "citation_count": 5,
    "authors": [
      "Xiaogang Xu",
      "Ning Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20200": {
    "title": "Reliable Propagation-Correction Modulation for Video Object Segmentation",
    "volume": "main",
    "abstract": "Error propagation is a general but crucial problem in online semi-supervised video object segmentation. We aim to suppress error propagation through a correction mechanism with high reliability. The key insight is to disentangle the correction from the conventional mask propagation process with reliable cues. We introduce two modulators, propagation and correction modulators, to separately perform channel-wise recalibration on the target frame embeddings according to local temporal correlations and reliable references respectively. Specifically, we assemble the modulators with a cascaded propagation-correction scheme. This avoids overriding the effects of the reliable correction modulator by the propagation modulator. Although the reference frame with the ground truth label provides reliable cues, it could be very different from the target frame and introduce uncertain or incomplete correlations. We augment the reference cues by supplementing reliable feature patches to a maintained pool, thus offering more comprehensive and expressive object representations to the modulators. In addition, a reliability filter is designed to retrieve reliable patches and pass them in subsequent frames. Our model achieves state-of-the-art performance on YouTube-VOS18, YouTube-VOS19 and DAVIS17-Val/Test benchmarks. Extensive experiments demonstrate that the correction mechanism provides considerable performance gain by fully utilizing reliable guidance",
    "checked": true,
    "id": "a6c29950e1f164a6fcd222e0938d465e2f67c2ba",
    "semantic_title": "reliable propagation-correction modulation for video object segmentation",
    "citation_count": 28,
    "authors": [
      "Xiaohao Xu",
      "Jinglu Wang",
      "Xiao Li",
      "Yan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20201": {
    "title": "Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation",
    "volume": "main",
    "abstract": "This paper proposes a novel two-stage hypergraph-based framework, dubbed ADaptive Hypergraph Neural Network (AD-HNN) to estimate multiple human poses from a single image, with a keypoint localization network and an Adaptive-Pose Hypergraph Neural Network (AP-HNN) added onto the former network. For providing better guided representations of AP-HNN, we employ a Semantic Interaction Convolution (SIC) module within the initial localization network to acquire more explicit predictions. Build upon this, we design a novel adaptive hypergraph to represent a human body for capturing high-order semantic relations among different joints. Notably, it can adaptively adjust the relations between joints and seek the most reasonable structure for the variable poses to benefit the keypoint localization. These two stages are combined to be trained in an end-to-end fashion. Unlike traditional Graph Convolutional Networks (GCNs) that are based on a fixed tree structure, AP-HNN can deal with ambiguity in human pose estimation. Experimental results demonstrate that the AD-HNN achieves state-of-the-art performance both on the MS-COCO, MPII and CrowdPose datasets",
    "checked": true,
    "id": "693febb90a32545ee15e4170d2edcdc86bd4463c",
    "semantic_title": "adaptive hypergraph neural network for multi-person pose estimation",
    "citation_count": 3,
    "authors": [
      "Xixia Xu",
      "Qi Zou",
      "Xue Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20202": {
    "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency",
    "checked": true,
    "id": "d045133e6e022684329ff944d67f91888be1bc3b",
    "semantic_title": "evo-vit: slow-fast token evolution for dynamic vision transformer",
    "citation_count": 81,
    "authors": [
      "Yifan Xu",
      "Zhijie Zhang",
      "Mengdan Zhang",
      "Kekai Sheng",
      "Ke Li",
      "Weiming Dong",
      "Liqing Zhang",
      "Changsheng Xu",
      "Xing Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20203": {
    "title": "MobileFaceSwap: A Lightweight Framework for Video Face Swapping",
    "volume": "main",
    "abstract": "Advanced face swapping methods have achieved appealing results. However, most of these methods have many parameters and computations, which makes it challenging to apply them in real-time applications or deploy them on edge devices like mobile phones. In this work, we propose a lightweight Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by dynamically adjusting the model parameters according to the identity information. In particular, we design an efficient Identity Injection Module (IIM) by introducing two dynamic neural network techniques, including the weights prediction and weights modulation. Once the IDN is updated, it can be applied to swap faces given any target image or video. The presented IDN contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it capable for real-time video face swapping on mobile phones. In addition, we introduce a knowledge distillation-based method for stable training, and a loss reweighting module is employed to obtain better synthesized results. Finally, our method achieves comparable results with the teacher models and other state-of-the-art methods",
    "checked": true,
    "id": "5dc56bbcc75984ca8d147b1a24ed1a4e3265188a",
    "semantic_title": "mobilefaceswap: a lightweight framework for video face swapping",
    "citation_count": 18,
    "authors": [
      "Zhiliang Xu",
      "Zhibin Hong",
      "Changxing Ding",
      "Zhen Zhu",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20204": {
    "title": "Clinical-BERT: Vision-Language Pre-training for Radiograph Diagnosis and Reports Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a vision-language pre-training model, Clinical-BERT, for the medical domain, and devise three domain-specific tasks: Clinical Diagnosis (CD), Masked MeSH Modeling (MMM), Image-MeSH Matching (IMM), together with one general pre-training task: Masked Language Modeling (MLM), to pre-train the model. The CD task helps the model to learn medical domain knowledge by predicting disease from radiographs. Medical Subject Headings (MeSH) words are important semantic components in radiograph reports, and the MMM task helps the model focus on the prediction of MeSH words. The IMM task helps the model learn the alignment of MeSH words with radiographs by matching scores obtained by a two-level sparse attention: region sparse attention and word sparse attention. Region sparse attention generates corresponding visual features for each word, and word sparse attention enhances the contribution of images-MeSH matching to the matching scores. To the best of our knowledge, this is the first attempt to learn domain knowledge during pre-training for the medical domain. We evaluate the pre-training model on Radiograph Diagnosis and Reports Generation tasks across four challenging datasets: MIMIC-CXR, IU X-Ray, COV-CTR, and NIH, and achieve state-of-the-art results for all the tasks, which demonstrates the effectiveness of our pre-training model",
    "checked": true,
    "id": "e015e07c76ff85f624b78112fd58937221f5ea0c",
    "semantic_title": "clinical-bert: vision-language pre-training for radiograph diagnosis and reports generation",
    "citation_count": 22,
    "authors": [
      "Bin Yan",
      "Mingtao Pei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20205": {
    "title": "Inferring Prototypes for Multi-Label Few-Shot Image Classification with Word Vector Guided Attention",
    "volume": "main",
    "abstract": "Multi-label few-shot image classification (ML-FSIC) is the task of assigning descriptive labels to previously unseen images, based on a small number of training examples. A key feature of the multi-label setting is that images often have multiple labels, which typically refer to different regions of the image. When estimating prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data makes this highly challenging. As a solution, in this paper we propose to use word embeddings as a form of prior knowledge about the meaning of the labels. In particular, visual prototypes are obtained by aggregating the local feature maps of the support images, using an attention mechanism that relies on the label embeddings. As an important advantage, our model can infer prototypes for unseen labels without the need for fine-tuning any model parameters, which demonstrates its strong generalization abilities. Experiments on COCO and PASCAL VOC furthermore show that our model substantially improves the current state-of-the-art",
    "checked": true,
    "id": "6db761cdb20a6c84687a6cbe97d9cf5a0505b37b",
    "semantic_title": "inferring prototypes for multi-label few-shot image classification with word vector guided attention",
    "citation_count": 7,
    "authors": [
      "Kun Yan",
      "Chenbin Zhang",
      "Jun Hou",
      "Ping Wang",
      "Zied Bouraoui",
      "Shoaib Jameel",
      "Steven Schockaert"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20206": {
    "title": "Unsupervised Domain Adaptive Salient Object Detection through Uncertainty-Aware Pseudo-Label Learning",
    "volume": "main",
    "abstract": "Recent advances in deep learning significantly boost the performance of salient object detection (SOD) at the expense of labeling larger-scale per-pixel annotations. To relieve the burden of labor-intensive labeling, deep unsupervised SOD methods have been proposed to exploit noisy labels generated by handcrafted saliency methods. However, it is still difficult to learn accurate saliency details from rough noisy labels. In this paper, we propose to learn saliency from synthetic but clean labels, which naturally has higher pixel-labeling quality without the effort of manual annotations. Specifically, we first construct a novel synthetic SOD dataset by a simple copy-paste strategy. Considering the large appearance differences between the synthetic and real-world scenarios, directly training with synthetic data will lead to performance degradation on real-world scenarios. To mitigate this problem, we propose a novel unsupervised domain adaptive SOD method to adapt between these two domains by uncertainty-aware self-training. Experimental results show that our proposed method outperforms the existing state-of-the-art deep unsupervised SOD methods on several benchmark datasets, and is even comparable to fully-supervised ones",
    "checked": true,
    "id": "19193cafa7a9864ee89810ec573fd9d3bf66cf2f",
    "semantic_title": "unsupervised domain adaptive salient object detection through uncertainty-aware pseudo-label learning",
    "citation_count": 10,
    "authors": [
      "Pengxiang Yan",
      "Ziyi Wu",
      "Mengmeng Liu",
      "Kun Zeng",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20207": {
    "title": "Transmission-Guided Bayesian Generative Model for Smoke Segmentation",
    "volume": "main",
    "abstract": "Smoke segmentation is essential to precisely localize wildﬁre so that it can be extinguished in an early phase. Although deep neural networks have achieved promising results on image segmentation tasks, they are prone to be overconﬁdent for smoke segmentation due to its non-rigid shape and transparent appearance. This is caused by both knowledge level uncertainty due to limited training data for accurate smoke segmentation and labeling level uncertainty representing the difﬁculty in labeling ground-truth. To effectively model the two types of uncertainty, we introduce a Bayesian generative model to simultaneously estimate the posterior distribution of model parameters and its predictions. Further, smoke images suffer from low contrast and ambiguity, inspired by physics-based image dehazing methods, we design a transmission-guided local coherence loss to guide the network to learn pair-wise relationships based on pixel distance and the transmission feature. To promote the development of this ﬁeld, we also contribute a high-quality smoke segmentation dataset, SMOKE5K, consisting of 1,400 real and 4,000 synthetic images with pixel-wise annotation. Experimental results on benchmark testing datasets illustrate that our model achieves both accurate predictions and reliable uncertainty maps representing model ignorance about its prediction. Our code and dataset are publicly available at: https://github.com/redlessme/Transmission-BVM",
    "checked": true,
    "id": "b395119ed7fcb3f952faef16817bfcd4e041a383",
    "semantic_title": "transmission-guided bayesian generative model for smoke segmentation",
    "citation_count": 4,
    "authors": [
      "Siyuan Yan",
      "Jing Zhang",
      "Nick Barnes"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20208": {
    "title": "Cross-Species 3D Face Morphing via Alignment-Aware Controller",
    "volume": "main",
    "abstract": "We address cross-species 3D face morphing (i.e., 3D face morphing from human to animal), a novel problem with promising applications in social media and movie industry. It remains challenging how to preserve target structural information and source ﬁne-grained facial details simultaneously. To this end, we propose an Alignment-aware 3D Face Morphing (AFM) framework, which builds semantic-adaptive correspondence between source and target faces across species, via an alignment-aware controller mesh (Explicit Controller, EC) with explicit source/target mesh binding. Based on EC, we introduce Controller-Based Mapping (CBM), which builds semantic consistency between source and target faces according to the semantic importance of different face regions. Additionally, an inference-stage coarse-to-ﬁne strategy is exploited to produce ﬁne-grained meshes with rich facial details from rough meshes. Extensive experimental results in multiple people and animals demonstrate that our method produces high-quality deformation results",
    "checked": true,
    "id": "2c2333c6f5982b34ccbfba616f1249e02345fc08",
    "semantic_title": "cross-species 3d face morphing via alignment-aware controller",
    "citation_count": 0,
    "authors": [
      "Xirui Yan",
      "Zhenbo Yu",
      "Bingbing Ni",
      "Hang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20209": {
    "title": "Exploring Visual Context for Weakly Supervised Person Search",
    "volume": "main",
    "abstract": "Person search has recently emerged as a challenging task that jointly addresses pedestrian detection and person re-identification. Existing approaches follow a fully supervised setting where both bounding box and identity annotations are available. However, annotating identities is labor-intensive, limiting the practicability and scalability of current frameworks. This paper inventively considers weakly supervised person search with only bounding box annotations. We propose to address this novel task by investigating three levels of context clues (i.e., detection, memory and scene) in unconstrained natural images. The first two are employed to promote local and global discriminative capabilities, while the latter enhances clustering accuracy. Despite its simple design, our CGPS boosts the baseline model by 8.8% in mAP on CUHK-SYSU. Surprisingly, it even achieves comparable performance with several supervised person search models. Our code is available at https://github. com/ljpadam/CGPS",
    "checked": true,
    "id": "0779d8b3cb32eeb0a2d4021fd325f6267647dd1e",
    "semantic_title": "exploring visual context for weakly supervised person search",
    "citation_count": 13,
    "authors": [
      "Yichao Yan",
      "Jinpeng Li",
      "Shengcai Liao",
      "Jie Qin",
      "Bingbing Ni",
      "Ke Lu",
      "Xiaokang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20210": {
    "title": "Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation",
    "volume": "main",
    "abstract": "As a key characteristic in audio-visual speech recognition (AVSR), relating linguistic information observed across visual and audio data has been a challenge, benefiting not only audio/visual speech recognition (ASR/VSR) but also for manipulating data within/across modalities. In this paper, we present a feature disentanglement-based framework for jointly addressing the above tasks. By advancing cross-modal mutual learning strategies, our model is able to convert visual or audio-based linguistic features into modality-agnostic representations. Such derived linguistic representations not only allow one to perform ASR, VSR, and AVSR, but also to manipulate audio and visual data output based on the desirable subject identity and linguistic content information. We perform extensive experiments on different recognition and synthesis tasks to show that our model performs favorably against state-of-the-art approaches on each individual task, while ours is a unified solution that is able to jointly tackle the aforementioned audio-visual learning tasks",
    "checked": true,
    "id": "261e9f21292a44dbf065385aa06996e9811d0ba2",
    "semantic_title": "cross-modal mutual learning for audio-visual speech recognition and manipulation",
    "citation_count": 8,
    "authors": [
      "Chih-Chun Yang",
      "Wan-Cyuan Fan",
      "Cheng-Fu Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20211": {
    "title": "Mutual Contrastive Learning for Visual Representation Learning",
    "volume": "main",
    "abstract": "We present a collaborative learning method called Mutual Contrastive Learning (MCL) for general visual representation learning. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks. A crucial component of MCL is Interactive Contrastive Learning (ICL). Compared with vanilla contrastive learning, ICL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations for visual recognition tasks. We emphasize that the resulting MCL is conceptually simple yet empirically powerful. It is a generic framework that can be applied to both supervised and self-supervised representation learning. Experimental results on image classification and transfer learning to object detection show that MCL can lead to consistent performance gains, demonstrating that MCL can guide the network to generate better feature representations. Code is available at https://github.com/winycg/MCL",
    "checked": true,
    "id": "edce12538963c689deb7504bb3b0664184ccfc20",
    "semantic_title": "mutual contrastive learning for visual representation learning",
    "citation_count": 32,
    "authors": [
      "Chuanguang Yang",
      "Zhulin An",
      "Linhang Cai",
      "Yongjun Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20212": {
    "title": "Temporal Action Proposal Generation with Background Constraint",
    "volume": "main",
    "abstract": "Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task",
    "checked": true,
    "id": "274f1f618ae4e96ba476a455da6b82f7bd4bf6a9",
    "semantic_title": "temporal action proposal generation with background constraint",
    "citation_count": 17,
    "authors": [
      "Haosen Yang",
      "Wenhao Wu",
      "Lining Wang",
      "Sheng Jin",
      "Boyang Xia",
      "Hongxun Yao",
      "Hujie Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20213": {
    "title": "Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we propose a new task of cross-modal federated human activity recognition (CMF-HAR), which is conducive to promote the large-scale use of the HAR model on more local devices. To address the new task, we propose a feature-disentangled activity recognition network (FDARN), which has five important modules of altruistic encoder, egocentric encoder, shared activity classifier, private activity classifier and modality discriminator. The altruistic encoder aims to collaboratively embed local instances on different clients into a modality-agnostic feature subspace. The egocentric encoder aims to produce modality-specific features that cannot be shared across clients with different modalities. The modality discriminator is used to adversarially guide the parameter learning of the altruistic and egocentric encoders. Through decentralized optimization with a spherical modality discriminative loss, our model can not only generalize well across different clients by leveraging the modality-agnostic features but also capture the modality-specific discriminative characteristics of each client. Extensive experiment results on four datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "8b8622df31976bcb26d42b3b14e18612192d72dc",
    "semantic_title": "cross-modal federated human activity recognition via modality-agnostic and modality-specific representation learning",
    "citation_count": 3,
    "authors": [
      "Xiaoshan Yang",
      "Baochen Xiong",
      "Yi Huang",
      "Changsheng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20214": {
    "title": "Polygon-to-Polygon Distance Loss for Rotated Object Detection",
    "volume": "main",
    "abstract": "There are two key issues that limit further improvements in the performance of existing rotational detectors: 1) Periodic sudden change of the parameters in the rotating bounding box (RBBox) definition causes a numerical discontinuity in the loss (such as smoothL1 loss). 2) There is a gap of optimization asynchrony between the loss in the RBBox regression and evaluation metrics. In this paper, we define a new distance formulation between two convex polygons describing the overlapping degree and non-overlapping degree. Based on this smooth distance, we propose a loss called Polygon-to-Polygon distance loss (P2P Loss). The distance is derived from the area sum of triangles specified by the vertexes of one polygon and the edges of the other. Therefore, the P2P Loss is continuous, differentiable, and inherently free from any RBBox definition. Our P2P Loss is not only consistent with the detection metrics but also able to measure how far, as well as how similar, a RBBox is from another one even when they are completely non-overlapping. These features allow the RetinaNet using the P2P Loss to achieve 79.15% mAP on the DOTA dataset, which is quite competitive compared with many state-of-the-art rotated object detectors",
    "checked": true,
    "id": "8e7e8845dbd0f38070cd9b61396912367a859997",
    "semantic_title": "polygon-to-polygon distance loss for rotated object detection",
    "citation_count": 4,
    "authors": [
      "Yang Yang",
      "Jifeng Chen",
      "Xiaopin Zhong",
      "Yuanlong Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20215": {
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "volume": "main",
    "abstract": "Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance",
    "checked": true,
    "id": "2672777d25562c9df6fc13b653181db62d39bece",
    "semantic_title": "an empirical study of gpt-3 for few-shot knowledge-based vqa",
    "citation_count": 165,
    "authors": [
      "Zhengyuan Yang",
      "Zhe Gan",
      "Jianfeng Wang",
      "Xiaowei Hu",
      "Yumao Lu",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20216": {
    "title": "ACGNet: Action Complement Graph Network for Weakly-Supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach",
    "checked": true,
    "id": "fc911c620b95a29fc9f687d8f1330f8a566ed873",
    "semantic_title": "acgnet: action complement graph network for weakly-supervised temporal action localization",
    "citation_count": 23,
    "authors": [
      "Zichen Yang",
      "Jie Qin",
      "Di Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20217": {
    "title": "Enhancing Pseudo Label Quality for Semi-supervised Domain-Generalized Medical Image Segmentation",
    "volume": "main",
    "abstract": "Generalizing the medical image segmentation algorithms to unseen domains is an important research topic for computer-aided diagnosis and surgery. Most existing methods require a fully labeled dataset in each source domain. Although some researchers developed a semi-supervised domain generalized method, it still requires the domain labels. This paper presents a novel confidence-aware cross pseudo supervision algorithm for semi-supervised domain generalized medical image segmentation. The main goal is to enhance the pseudo label quality for unlabeled images from unknown distributions. To achieve it, we perform the Fourier transformation to learn low-level statistic information across domains and augment the images to incorporate cross-domain information. With these augmentations as perturbations, we feed the input to a confidence-aware cross pseudo supervision network to measure the variance of pseudo labels and regularize the network to learn with more confident pseudo labels. Our method sets new records on public datasets, i.e., M&Ms and SCGM. Notably, without using domain labels, our method surpasses the prior art that even uses domain labels by 11.67% on Dice on M&Ms dataset with 2% labeled data. Code is available at https://github.com/XMed-Lab/EPL SemiDG",
    "checked": false,
    "id": "8bdd1fe75f77dcc84159b3a2accf5c91781886eb",
    "semantic_title": "enhancing pseudo label quality for semi-superviseddomain-generalized medical image segmentation",
    "citation_count": 30,
    "authors": [
      "Huifeng Yao",
      "Xiaowei Hu",
      "Xiaomeng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20218": {
    "title": "Image Difference Captioning with Pre-training and Contrastive Learning",
    "volume": "main",
    "abstract": "The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at https://github.com/yaolinli/IDC",
    "checked": true,
    "id": "4993b87d3e26983c37bf1d05b07c8179d81ad196",
    "semantic_title": "image difference captioning with pre-training and contrastive learning",
    "citation_count": 9,
    "authors": [
      "Linli Yao",
      "Weiying Wang",
      "Qin Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20219": {
    "title": "Safe Distillation Box",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has recently emerged as a powerful strategy to transfer knowledge from a pre-trained teacher model to a lightweight student, and has demonstrated its unprecedented success over a wide spectrum of applications. In spite of the encouraging results, the KD process \\emph{per se} poses a potential threat to network ownership protection, since the knowledge contained in network can be effortlessly distilled and hence exposed to a malicious user. In this paper, we propose a novel framework, termed as Safe Distillation Box~(SDB), that allows us to wrap a pre-trained model in a virtual box for intellectual property protection. Specifically, SDB preserves the inference capability of the wrapped model to all users, but precludes KD from unauthorized users. For authorized users, on the other hand, SDB carries out a knowledge augmentation scheme to strengthen the KD performances and the results of the student model. In other words, all users may employ a model in SDB for inference, but only authorized users get access to KD from the model. The proposed SDB imposes no constraints over the model architecture, and may readily serve as a plug-and-play solution to protect the ownership of a pre-trained network. Experiments across various datasets and architectures demonstrate that, with SDB, the performance of an unauthorized KD drops significantly while that of an authorized gets enhanced, demonstrating the effectiveness of SDB",
    "checked": true,
    "id": "2b76643e28f288d03f0dd5cbb01acf6c9c1c6969",
    "semantic_title": "safe distillation box",
    "citation_count": 9,
    "authors": [
      "Jingwen Ye",
      "Yining Mao",
      "Jie Song",
      "Xinchao Wang",
      "Cheng Jin",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20220": {
    "title": "Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections",
    "volume": "main",
    "abstract": "Graph matching aims to establish correspondences between vertices of graphs such that both the node and edge attributes agree. Various learning-based methods were recently proposed for finding correspondences between image key points based on deep graph matching formulations. While these approaches mainly focus on learning node and edge attributes, they completely ignore the 3D geometry of the underlying 3D objects depicted in the 2D images. We fill this gap by proposing a trainable framework that takes advantage of graph neural networks for learning a deformable 3D geometry model from inhomogeneous image collections, i.e. a set of images that depict different instances of objects from the same category. Experimentally we demonstrate that our method outperforms recent learning-based approaches for graph matching considering both accuracy and cycle-consistency error, while we in addition obtain the underlying 3D geometry of the objects depicted in the 2D images",
    "checked": true,
    "id": "f02d30dc073c46636d837f26431d143e49d372fa",
    "semantic_title": "joint deep multi-graph matching and 3d geometry learning from inhomogeneous 2d image collections",
    "citation_count": 5,
    "authors": [
      "Zhenzhang Ye",
      "Tarun Yenamandra",
      "Florian Bernard",
      "Daniel Cremers"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20221": {
    "title": "Content-Variant Reference Image Quality Assessment via Knowledge Distillation",
    "volume": "main",
    "abstract": "Generally, humans are more skilled at perceiving differences between high-quality (HQ) and low-quality (LQ) images than directly judging the quality of a single LQ image. This situation also applies to image quality assessment (IQA). Although recent no-reference (NR-IQA) methods have made great progress to predict image quality free from the reference image, they still have the potential to achieve better performance since HQ image information is not fully exploited. In contrast, full-reference (FR-IQA) methods tend to provide more reliable quality evaluation, but its practicability is affected by the requirement for pixel-level aligned reference images. To address this, we firstly propose the content-variant reference method via knowledge distillation (CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to introduce various prior distributions of high-quality images. The comparisons of distribution differences between HQ and LQ images can help our model better assess the image quality. Further, the knowledge distillation transfers more HQ-LQ distribution difference information from the FR-teacher to the NAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the local-global combined information, while achieving faster inference speed, our model directly processes multiple image patches from the input with the MLP-mixer. Cross-dataset experiments verify that our model can outperform all NAR/NR-IQA SOTAs, even reach comparable performance than FR-IQA methods on some occasions. Since the content-variant and non-aligned reference HQ images are easy to obtain, our model can support more IQA applications with its robustness to content variations. Our code is available: https://github.com/guanghaoyin/CVRKD-IQA",
    "checked": true,
    "id": "437fb14ee99e647501955e7be38384290d0cf3aa",
    "semantic_title": "content-variant reference image quality assessment via knowledge distillation",
    "citation_count": 13,
    "authors": [
      "Guanghao Yin",
      "Wei Wang",
      "Zehuan Yuan",
      "Chuchu Han",
      "Wei Ji",
      "Shouqian Sun",
      "Changhu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20222": {
    "title": "Width & Depth Pruning for Vision Transformers",
    "volume": "main",
    "abstract": "Transformer models have demonstrated their promising potential and achieved excellent performance on a series of computer vision tasks. However, the huge computational cost of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to ﬁnd and remove the unimportant units of vision transformers. Despite achieving remarkable results, these methods take one dimension of network width into consideration and ignore network depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width & Depth Pruning (WDPruning) framework that reduces both width and depth dimensions simultaneously. Speciﬁcally, for width pruning, a set of learnable pruning-related parameters is used to adaptively adjust the width of transformer. For depth pruning, we introduce several shallow classiﬁers by using the intermediate information of the transformer blocks, which allows images to be classiﬁed by shallow classiﬁers instead of the deeper classiﬁers. In the inference period, all of the blocks after shallow classiﬁers can be dropped so they don't bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the proposed method can signiﬁcantly reduce the computational costs of mainstream vision transformers such as DeiT and Swin Transformer with a minor accuracy drop. In particular, on ILSVRC-12, we achieve over 22% pruning ratio of FLOPs by compressing DeiT-Base, even with an increase of 0.14% Top-1 accuracy",
    "checked": true,
    "id": "d451901a6a12c61179289cac7a4588a86c234112",
    "semantic_title": "width & depth pruning for vision transformers",
    "citation_count": 29,
    "authors": [
      "Fang Yu",
      "Kun Huang",
      "Meng Wang",
      "Yuan Cheng",
      "Wei Chu",
      "Li Cui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20223": {
    "title": "Anisotropic Fourier Features for Neural Image-Based Rendering and Relighting",
    "volume": "main",
    "abstract": "Recent neural rendering techniques have greatly benefited image-based modeling and relighting tasks. They provide a continuous, compact, and parallelable representation by modeling the plenoptic function as multilayer perceptrons (MLPs). However, vanilla MLPs suffer from spectral biases on multidimensional datasets. Recent rescues based on isotropic Fourier features mapping mitigate the problem but still fall short of handling heterogeneity across different dimensions, causing imbalanced regression and visual artifacts such as excessive blurs. We present an anisotropic random Fourier features (RFF) mapping scheme to tackle spectral biases. We first analyze the influence of bandwidth from a different perspective: we show that the optimal bandwidth exhibits strong correlations with the frequency spectrum of the training data across various dimensions. We then introduce an anisotropic feature mapping scheme with multiple bandwidths to model the multidimensional signal characteristics. We further propose an efficient bandwidth searching scheme through iterative golden-section search that can significantly reduce the training overload from polynomial time to logarithm. Our anisotropic scheme directly applies to neural surface light-field rendering and image-based relighting. Comprehensive experiments show that our scheme can more faithfully model lighting conditions and object features as well as preserve fine texture details and smooth view transitions even when angular and spatial samples are highly imbalanced",
    "checked": true,
    "id": "b3301a3b46bb4e9eda155bef82a54ab05865e7c5",
    "semantic_title": "anisotropic fourier features for neural image-based rendering and relighting",
    "citation_count": 5,
    "authors": [
      "Huangjie Yu",
      "Anpei Chen",
      "Xin Chen",
      "Lan Xu",
      "Ziyu Shao",
      "Jingyi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20224": {
    "title": "Self-Labeling Framework for Novel Category Discovery over Domains",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation (UDA) has been highly successful in transferring knowledge acquired from a label-rich source domain to a label-scarce target domain. Open-set domain adaptation (open-set DA) and universal domain adaptation (UniDA) have been proposed as solutions to the problem concerning the presence of additional novel categories in the target domain. Existing open-set DA and UniDA approaches treat all novel categories as one unified unknown class and attempt to detect this unknown class during the training process. However, the features of the novel categories learned by these methods are not discriminative. This limits the applicability of UDA in the further classification of these novel categories into their original categories, rather than assigning them to a single unified class. In this paper, we propose a self-labeling framework to cluster all target samples, including those in the ''unknown'' categories. We train the network to learn the representations of target samples via self-supervised learning (SSL) and to identify the seen and unseen (novel) target-sample categories simultaneously by maximizing the mutual information between labels and input data. We evaluated our approach under different DA settings and concluded that our method generally outperformed existing ones by a wide margin",
    "checked": true,
    "id": "5641cf2602d0bd119d67bff71e5ea8e193d5e377",
    "semantic_title": "self-labeling framework for novel category discovery over domains",
    "citation_count": 10,
    "authors": [
      "Qing Yu",
      "Daiki Ikami",
      "Go Irie",
      "Kiyoharu Aizawa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20225": {
    "title": "Efficient Compact Bilinear Pooling via Kronecker Product",
    "volume": "main",
    "abstract": "Bilinear pooling has achieved excellent performance in fine-grained recognition tasks. Nevertheless, high-dimensional bilinear features suffer from over-fitting and inefficiency. To alleviate these issues, compact bilinear pooling (CBP) methods were developed to generate low-dimensional features. Although the low-dimensional features from existing CBP methods enable high efficiency in subsequent classification, CBP methods themselves are inefficient. Thus, the inefficiency issue of the bilinear pooling is still unsolved. In this work, we propose an efficient compact bilinear pooling method to solve the inefficiency problem inherited in bilinear pooling thoroughly. It decomposes the huge-scale projection matrix into a two-level Kronecker product of several small-scale matrices. By exploiting the ``vec trick'' and the tensor modal product, we can obtain the compact bilinear feature through the decomposed projection matrices in a speedy manner. Systematic experiments on four public benchmarks using two backbones demonstrate the efficiency and effectiveness of the proposed method in fine-grained recognition",
    "checked": true,
    "id": "8b402d7fa5a3827beb12d92a9ce52202b9d21251",
    "semantic_title": "efficient compact bilinear pooling via kronecker product",
    "citation_count": 5,
    "authors": [
      "Tan Yu",
      "Yunfeng Cai",
      "Ping Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20226": {
    "title": "Hybrid Graph Neural Networks for Few-Shot Learning",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have been used to tackle the few-shot learning (FSL) problem and shown great potentials under the transductive setting. However under the inductive setting, existing GNN based methods are less competitive. This is because they use an instance GNN as a label propagation/classification module, which is jointly meta-learned with a feature embedding network. This design is problematic because the classifier needs to adapt quickly to new tasks while the embedding does not. To overcome this problem, in this paper we propose a novel hybrid GNN (HGNN) model consisting of two GNNs, an instance GNN and a prototype GNN. Instead of label propagation, they act as feature embedding adaptation modules for quick adaptation of the meta-learned feature embedding to new tasks. Importantly they are designed to deal with a fundamental yet often neglected challenge in FSL, that is, with only a handful of shots per class, any few-shot classifier would be sensitive to badly sampled shots which are either outliers or can cause inter-class distribution overlapping. Extensive experiments show that our HGNN obtains new state-of-the-art on three FSL benchmarks. The code and models are available at https://github.com/TianyuanYu/HGNN",
    "checked": true,
    "id": "5b14efe841ad41bbe5cb825179e45acb30e7469d",
    "semantic_title": "hybrid graph neural networks for few-shot learning",
    "citation_count": 20,
    "authors": [
      "Tianyuan Yu",
      "Sen He",
      "Yi-Zhe Song",
      "Tao Xiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20227": {
    "title": "SOIT: Segmenting Objects with Instance-Aware Transformers",
    "volume": "main",
    "abstract": "This paper presents an end-to-end instance segmentation framework, termed SOIT, that Segments Objects with Instance-aware Transformers. Inspired by DETR, our method views instance segmentation as a direct set prediction problem and effectively removes the need for many hand-crafted components like RoI cropping, one-to-many label assignment, and non-maximum suppression (NMS). In SOIT, multiple queries are learned to directly reason a set of object embeddings of semantic category, bounding-box location, and pixel-wise mask in parallel under the global image context. The class and bounding-box can be easily embedded by a fixed-length vector. The pixel-wise mask, especially, is embedded by a group of parameters to construct a lightweight instance-aware transformer. Afterward, a full-resolution mask is produced by the instance-aware transformer without involving any RoI-based operation. Overall, SOIT introduces a simple single-stage instance segmentation framework that is both RoI- and NMS-free. Experimental results on the MS COCO dataset demonstrate that SOIT outperforms state-of-the-art instance segmentation approaches significantly. Moreover, the joint learning of multiple tasks in a unified query embedding can also substantially improve the detection performance. Code is available at https://github.com/yuxiaodongHRI/SOIT",
    "checked": true,
    "id": "12d0826a9cf2646d75cae9ad569a602dd7684c39",
    "semantic_title": "soit: segmenting objects with instance-aware transformers",
    "citation_count": 16,
    "authors": [
      "Xiaodong Yu",
      "Dahu Shi",
      "Xing Wei",
      "Ye Ren",
      "Tingqun Ye",
      "Wenming Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20228": {
    "title": "MSML: Enhancing Occlusion-Robustness by Multi-Scale Segmentation-Based Mask Learning for Face Recognition",
    "volume": "main",
    "abstract": "In unconstrained scenarios, face recognition remains challenging, particularly when faces are occluded. Existing methods generalize poorly due to the distribution distortion induced by unpredictable occlusions. To tackle this problem, we propose a hierarchical segmentation-based mask learning strategy for face recognition, enhancing occlusion-robustness by integrating segmentation representations of occlusion into face recognition in the latent space. We present a novel multi-scale segmentation-based mask learning (MSML) network, which consists of a face recognition branch (FRB), an occlusion segmentation branch (OSB), and hierarchical elaborate feature masking (FM) operators. With the guidance of hierarchical segmentation representations of occlusion learned by the OSB, the FM operators can generate multi-scale latent masks to eliminate mistaken responses introduced by occlusions and purify the contaminated facial features at multiple layers. In this way, the proposed MSML network can effectively identify and remove the occlusions from feature representations at multiple levels and aggregate features from visible facial areas. Experiments on face verification and recognition under synthetic or realistic occlusions demonstrate the effectiveness of our method compared to state-of-the-art methods",
    "checked": true,
    "id": "68e0d68882ec258ab4c090ec07fa8acd67dfc006",
    "semantic_title": "msml: enhancing occlusion-robustness by multi-scale segmentation-based mask learning for face recognition",
    "citation_count": 3,
    "authors": [
      "Ge Yuan",
      "Huicheng Zheng",
      "Jiayu Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20229": {
    "title": "Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics",
    "volume": "main",
    "abstract": "Human-Object Interaction (HOI) detection is an essential task to understand human-centric images from a fine-grained perspective. Although end-to-end HOI detection models thrive, their paradigm of parallel human/object detection and verb class prediction loses two-stage methods' merit: object-guided hierarchy. The object in one HOI triplet gives direct clues to the verb to be predicted. In this paper, we aim to boost end-to-end models with object-guided statistical priors. Specifically, We propose to utilize a Verb Semantic Model (VSM) and use semantic aggregation to profit from this object-guided hierarchy. Similarity KL (SKL) loss is proposed to optimize VSM to align with the HOI dataset's priors. To overcome the static semantic embedding problem, we propose to generate cross-modality-aware visual and semantic features by Cross-Modal Calibration (CMC). The above modules combined composes Object-guided Cross-modal Calibration Network (OCN). Experiments conducted on two popular HOI detection benchmarks demonstrate the significance of incorporating the statistical prior knowledge and produce state-of-the-art performances. More detailed analysis indicates proposed modules serve as a stronger verb predictor and a more superior method of utilizing prior knowledge. The codes are available at https://github.com/JacobYuan7/OCN-HOI-Benchmark",
    "checked": true,
    "id": "a4355807a9e44313b8e2a2947a67631feeef6376",
    "semantic_title": "detecting human-object interactions with object-guided cross-modal calibrated semantics",
    "citation_count": 15,
    "authors": [
      "Hangjie Yuan",
      "Mang Wang",
      "Dong Ni",
      "Liangpeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20230": {
    "title": "Task-Level Self-Supervision for Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "Learning with limited labeled data is a long-standing problem. Among various solutions, episodic training progres-sively classifies a series of few-shot tasks and thereby is as-sumed to be beneficial for improving the model's generalization ability. However, recent studies show that it is eveninferior to the baseline model when facing domain shift between base and novel classes. To tackle this problem, we pro-pose a domain-independent task-level self-supervised (TL-SS) method for cross-domain few-shot learning.TL-SS strategy promotes the general idea of label-based instance-levelsupervision to task-level self-supervision by augmenting mul-tiple views of tasks. Two regularizations on task consistencyand correlation metric are introduced to remarkably stabi-lize the training process and endow the generalization ability into the prediction model. We also propose a high-order associated encoder (HAE) being adaptive to various tasks.By utilizing 3D convolution module, HAE is able to generate proper parameters and enables the encoder to flexibly toany unseen tasks. Two modules complement each other andshow great promotion against state-of-the-art methods experimentally. Finally, we design a generalized task-agnostic test,where our intriguing findings highlight the need to re-think the generalization ability of existing few-shot approaches",
    "checked": true,
    "id": "e22db1c9eb5ce1ab4226e26309878126c6fe3d1f",
    "semantic_title": "task-level self-supervision for cross-domain few-shot learning",
    "citation_count": 6,
    "authors": [
      "Wang Yuan",
      "Zhizhong Zhang",
      "Cong Wang",
      "Haichuan Song",
      "Yuan Xie",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20231": {
    "title": "Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-Supervised Learning",
    "volume": "main",
    "abstract": "Due to difficulties in acquiring ground truth depth of equirectangular (360) images, the quality and quantity of equirectangular depth data today is insufficient to represent the various scenes in the world. Therefore, 360 depth estimation studies, which relied solely on supervised learning, are destined to produce unsatisfactory results. Although self-supervised learning methods focusing on equirectangular images (EIs) are introduced, they often have incorrect or non-unique solutions, causing unstable performance. In this paper, we propose 360 monocular depth estimation methods which improve on the areas that limited previous studies. First, we introduce a self-supervised 360 depth learning method that only utilizes gravity-aligned videos, which has the potential to eliminate the needs for depth data during the training procedure. Second, we propose a joint learning scheme realized by combining supervised and self-supervised learning. The weakness of each learning is compensated, thus leading to more accurate depth estimation. Third, we propose a non-local fusion block, which can further retain the global information encoded by vision transformer when reconstructing the depths. With the proposed methods, we successfully apply the transformer to 360 depth estimations, to the best of our knowledge, which has not been tried before. On several benchmarks, our approach achieves significant improvements over previous works and establishes a state of the art",
    "checked": true,
    "id": "d4641a900b44990fb604b91fe08290a0d8c04027",
    "semantic_title": "improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning",
    "citation_count": 18,
    "authors": [
      "Ilwi Yun",
      "Hyuk-Jae Lee",
      "Chae Eun Rhee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20232": {
    "title": "Homography Decomposition Networks for Planar Object Tracking",
    "volume": "main",
    "abstract": "Planar object tracking plays an important role in AI applications, such as robotics, visual servoing, and visual SLAM. Although the previous planar trackers work well in most scenarios, it is still a challenging task due to the rapid motion and large transformation between two consecutive frames. The essential reason behind this problem is that the condition number of such a non-linear system changes unstably when the searching range of the homography parameter space becomes larger. To this end, we propose a novel Homography Decomposition Networks~(HDN) approach that drastically reduces and stabilizes the condition number by decomposing the homography transformation into two groups. Specifically, a similarity transformation estimator is designed to predict the first group robustly by a deep convolution equivariant network. By taking advantage of the scale and rotation estimation with high confidence, a residual transformation is estimated by a simple regression model. Furthermore, the proposed end-to-end network is trained in a semi-supervised fashion. Extensive experiments show that our proposed approach outperforms the state-of-the-art planar tracking methods at a large margin on the challenging POT, UCSB and POIC datasets. Codes and models are available at https://github.com/zhanxinrui/HDN",
    "checked": true,
    "id": "ebf7a1f10bfb7112487976f414efb28af6e6a08c",
    "semantic_title": "homography decomposition networks for planar object tracking",
    "citation_count": 7,
    "authors": [
      "Xinrui Zhan",
      "Yueran Liu",
      "Jianke Zhu",
      "Yang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20233": {
    "title": "Patch Diffusion: A General Module for Face Manipulation Detection",
    "volume": "main",
    "abstract": "Detection of manipulated face images has attracted a lot of interest recently. Various schemes have been proposed to tackle this challenging problem, where the patch-based approaches are shown to be promising. However, the existing patch-based approaches tend to treat different patches equally, which do not fully exploit the patch discrepancy for effective feature learning. In this paper, we propose a Patch Diffusion (PD) module which can be integrated into the existing face manipulation detection networks to boost the performance. The PD consists of Discrepancy Patch Feature Learning (DPFL) and Attention-Aware Message Passing (AMP). The DPFL effectively learns the patch features by a newly designed Pairwise Patch Loss (PPLoss), which takes both the patch importance and correlations into consideration. The AMP diffuses the patches through attention-aware message passing in a graph network, where the attentions are explicitly computed based on the patch features learnt in DPFL. We integrate our PD module into four recent face manipulation detection networks, and carry out the experiments on four popular datasets. The results demonstrate that our PD module is able to boost the performance of the existing networks for face manipulation detection",
    "checked": true,
    "id": "487e79c5016c4ad8c4db777ee60da8ee9a832b58",
    "semantic_title": "patch diffusion: a general module for face manipulation detection",
    "citation_count": 11,
    "authors": [
      "Baogen Zhang",
      "Sheng Li",
      "Guorui Feng",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20234": {
    "title": "Semi-supervised Object Detection with Adaptive Class-Rebalancing Self-Training",
    "volume": "main",
    "abstract": "While self-training achieves state-of-the-art results in semi-supervised object detection (SSOD), it severely suffers from foreground-background and foreground-foreground imbalances in SSOD. In this paper, we propose an Adaptive Class-Rebalancing Self-Training (ACRST) with a novel memory module called CropBank to alleviate these imbalances and generate unbiased pseudo-labels. Besides, we observe that both self-training and data-rebalancing procedures suffer from noisy pseudo-labels in SSOD. Therefore, we contribute a simple yet effective two-stage pseudo-label filtering scheme to obtain accurate supervision. Our method achieves competitive performance on MS-COCO and VOC benchmarks. When using only 1% labeled data of MS-COCO, our method achieves 17.02 mAP improvement over the supervised method and 5.32 mAP gains compared with state-of-the-arts",
    "checked": true,
    "id": "86d6fb1f39a45712591d72ed2eac39c1b3574087",
    "semantic_title": "semi-supervised object detection with adaptive class-rebalancing self-training",
    "citation_count": 28,
    "authors": [
      "Fangyuan Zhang",
      "Tianxiang Pan",
      "Bin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20235": {
    "title": "Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching",
    "volume": "main",
    "abstract": "Image-text matching bridges vision and language, which is a crucial task in the field of multi-modal intelligence. The key challenge lies in how to measure image-text relevance accurately as matching evidence. Most existing works aggregate the local semantic similarities of matched region-word pairs as the overall relevance, and they typically assume that the matched pairs are equally reliable. However, although a region-word pair is locally matched across modalities, it may be inconsistent/unreliable from the global perspective of image-text, resulting in inaccurate relevance measurement. In this paper, we propose a novel Cross-Modal Confidence-Aware Network to infer the matching confidence that indicates the reliability of matched region-word pairs, which is combined with the local semantic similarities to refine the relevance measurement. Specifically, we first calculate the matching confidence via the relevance between the semantic of image regions and the complete described semantic in the image, with the text as a bridge. Further, to richly express the region semantics, we extend the region to its visual context in the image. Then, local semantic similarities are weighted with the inferred confidence to filter out unreliable matched pairs in aggregating. Comprehensive experiments show that our method achieves state-of-the-art performance on benchmarks Flickr30K and MSCOCO",
    "checked": true,
    "id": "5b430df8b79b609503ebb0cb2fea346194fbc8fa",
    "semantic_title": "show your faith: cross-modal confidence-aware network for image-text matching",
    "citation_count": 16,
    "authors": [
      "Huatian Zhang",
      "Zhendong Mao",
      "Kun Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20236": {
    "title": "SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-resolution",
    "volume": "main",
    "abstract": "In the practical application of restoring low-resolution gray-scale images, we generally need to run three separate processes of image colorization, super-resolution, and dows-sampling operation for the target device. However, this pipeline is redundant and inefficient for the independent processes, and some inner features could have been shared. Therefore, we present an efficient paradigm to perform Simultaneously Image Colorization and Super-resolution (SCS) and propose an end-to-end SCSNet to achieve this goal. The proposed method consists of two parts: colorization branch for learning color information that employs the proposed plug-and-play Pyramid Valve Cross Attention (PVCAttn) module to aggregate feature maps between source and reference images; and super-resolution branch for integrating color and texture information to predict target images, which uses the designed Continuous Pixel Mapping (CPM) module to predict high-resolution images at continuous magnification. Furthermore, our SCSNet supports both automatic and referential modes that is more flexible for practical application. Abundant experiments demonstrate the superiority of our method for generating authentic images over state-of-the-art methods, e.g., averagely decreasing FID by 1.8 and 5.1 compared with current best scores for automatic and referential modes, respectively, while owning fewer parameters (more than x2) and faster running speed (more than x3)",
    "checked": true,
    "id": "a9a4246fda4484f0b79dad0c2506e3843074e9de",
    "semantic_title": "scsnet: an efficient paradigm for learning simultaneously image colorization and super-resolution",
    "citation_count": 8,
    "authors": [
      "Jiangning Zhang",
      "Chao Xu",
      "Jian Li",
      "Yue Han",
      "Yabiao Wang",
      "Ying Tai",
      "Yong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20237": {
    "title": "Energy-Based Generative Cooperative Saliency Prediction",
    "volume": "main",
    "abstract": "Conventional saliency prediction models typically learn a deterministic mapping from an image to its saliency map, and thus fail to explain the subjective nature of human attention. In this paper, to model the uncertainty of visual saliency, we study the saliency prediction problem from the perspective of generative models by learning a conditional probability distribution over the saliency map given an input image, and treating the saliency prediction as a sampling process from the learned distribution. Specifically, we propose a generative cooperative saliency prediction framework, where a conditional latent variable model~(LVM) and a conditional energy-based model~(EBM) are jointly trained to predict salient objects in a cooperative manner. The LVM serves as a fast but coarse predictor to efficiently produce an initial saliency map, which is then refined by the iterative Langevin revision of the EBM that serves as a slow but fine predictor. Such a coarse-to-fine cooperative saliency prediction strategy offers the best of both worlds. Moreover, we propose a ``cooperative learning while recovering\" strategy and apply it to weakly supervised saliency prediction, where saliency annotations of training images are partially observed. Lastly, we find that the learned energy function in the EBM can serve as a refinement module that can refine the results of other pre-trained saliency prediction models. Experimental results show that our model can produce a set of diverse and plausible saliency maps of an image, and obtain state-of-the-art performance in both fully supervised and weakly supervised saliency prediction tasks",
    "checked": true,
    "id": "ec2bbf8628197d01dec5fb0ca33cc621074b3f43",
    "semantic_title": "energy-based generative cooperative saliency prediction",
    "citation_count": 7,
    "authors": [
      "Jing Zhang",
      "Jianwen Xie",
      "Zilong Zheng",
      "Nick Barnes"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20238": {
    "title": "Attention-Based Transformation from Latent Features to Point Clouds",
    "volume": "main",
    "abstract": "In point cloud generation and completion, previous methods for transforming latent features to point clouds are generally based on fully connected layers (FC-based) or folding operations (Folding-based). However, point clouds generated by FC-based methods are usually troubled by outliers and rough surfaces. For folding-based methods, their data flow is large, convergence speed is slow, and they are also hard to handle the generation of non-smooth surfaces. In this work, we propose AXform, an attention-based method to transform latent features to point clouds. AXform first generates points in an interim space, using a fully connected layer. These interim points are then aggregated to generate the target point cloud. AXform takes both parameter sharing and data flow into account, which makes it has fewer outliers, fewer network parameters, and a faster convergence speed. The points generated by AXform do not have the strong 2-manifold constraint, which improves the generation of non-smooth surfaces. When AXform is expanded to multiple branches for local generations, the centripetal constraint makes it has properties of self-clustering and space consistency, which further enables unsupervised semantic segmentation. We also adopt this scheme and design AXformNet for point cloud completion. Considerable experiments on different datasets show that our methods achieve state-of-the-art results",
    "checked": true,
    "id": "880e2cc7715b277766c26a8cf07fc7c112ae4270",
    "semantic_title": "attention-based transformation from latent features to point clouds",
    "citation_count": 5,
    "authors": [
      "Kaiyi Zhang",
      "Ximing Yang",
      "Yuan Wu",
      "Cheng Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20239": {
    "title": "Suppressing Static Visual Cues via Normalizing Flows for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "Despite the great progress in video understanding made by deep convolutional neural networks, feature representation learned by existing methods may be biased to static visual cues. To address this issue, we propose a novel method to suppress static visual cues (SSVC) based on probabilistic analysis for self-supervised video representation learning. In our method, video frames are first encoded to obtain latent variables under standard normal distribution via normalizing flows. By modelling static factors in a video as a random variable, the conditional distribution of each latent variable becomes shifted and scaled normal. Then, the less-varying latent variables along time are selected as static cues and suppressed to generate motion-preserved videos. Finally, positive pairs are constructed by motion-preserved videos for contrastive learning to alleviate the problem of representation bias to static cues. The less-biased video representation can be better generalized to various downstream tasks. Extensive experiments on publicly available benchmarks demonstrate that the proposed method outperforms the state of the art when only single RGB modality is used for pre-training",
    "checked": true,
    "id": "9cc6e0ddd457f5a40c0f5e2534c0dd463ec3d8be",
    "semantic_title": "suppressing static visual cues via normalizing flows for self-supervised video representation learning",
    "citation_count": 7,
    "authors": [
      "Manlin Zhang",
      "Jinpeng Wang",
      "Andy J. Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20240": {
    "title": "LGD: Label-Guided Self-Distillation for Object Detection",
    "volume": "main",
    "abstract": "In this paper, we propose the first self-distillation framework for general object detection, termed LGD (Label-Guided self-Distillation). Previous studies rely on a strong pretrained teacher to provide instructive knowledge that could be unavailable in real-world scenarios. Instead, we generate an instructive knowledge by inter-and-intra relation modeling among objects, requiring only student representations and regular labels. Concretely, our framework involves sparse label-appearance encoding, inter-object relation adaptation and intra-object knowledge mapping to obtain the instructive knowledge. They jointly form an implicit teacher at training phase, dynamically dependent on labels and evolving student representations. Modules in LGD are trained end-to-end with student detector and are discarded in inference. Experimentally, LGD obtains decent results on various detectors, datasets, and extensive tasks like instance segmentation. For example in MS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale training from 36.2% to 39.0% mAP (+ 2.8%). It boosts much stronger detectors like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training from 46.1% to 47.9% (+ 1.8%). Compared with a classical teacher-based method FGFI, LGD not only performs better without requiring pretrained teacher but also reduces 51% training cost beyond inherent student learning",
    "checked": true,
    "id": "2d572286f816c03bbfb733e4d5402cd2caf78fef",
    "semantic_title": "lgd: label-guided self-distillation for object detection",
    "citation_count": 17,
    "authors": [
      "Peizhen Zhang",
      "Zijian Kang",
      "Tong Yang",
      "Xiangyu Zhang",
      "Nanning Zheng",
      "Jian Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20241": {
    "title": "Uncertainty Modeling with Second-Order Transformer for Group Re-identification",
    "volume": "main",
    "abstract": "Group re-identification (G-ReID) focuses on associating the group images containing the same persons under different cameras. The key challenge of G-ReID is that all the cases of the intra-group member and layout variations are hard to exhaust. To this end, we propose a novel uncertainty modeling, which treats each image as a distribution depending on the current member and layout, then digs out potential group features by random samplings. Based on potential and original group features, uncertainty modeling can learn better decision boundaries, which is implemented by two modules, member variation module (MVM) and layout variation module (LVM). Furthermore, we propose a novel second-order transformer framework (SOT), which is inspired by the fact that the position modeling in the transformer is coped with the G-ReID task. SOT is composed of the intra-member module and inter-member module. Specifically, the intra-member module extracts the first-order token for each member, and then the inter-member module learns a second-order token as a group feature by the above first-order tokens, which can be regarded as the token of tokens. A large number of experiments have been conducted on three available datasets, including CSG, DukeGroup and RoadGroup. The experimental results show that the proposed SOT outperforms all previous state-of-the-art methods",
    "checked": true,
    "id": "a6b028f1e09bb93b6e6e891d7e9a9776476c4604",
    "semantic_title": "uncertainty modeling with second-order transformer for group re-identification",
    "citation_count": 4,
    "authors": [
      "Quan Zhang",
      "Jian-Huang Lai",
      "Zhanxiang Feng",
      "Xiaohua Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20242": {
    "title": "Deep Spatial Adaptive Network for Real Image Demosaicing",
    "volume": "main",
    "abstract": "Demosaicing is the crucial step in the image processing pipeline and is a highly ill-posed inverse problem. Recently, various deep learning based demosaicing methods have achieved promising performance, but they often design the same nonlinear mapping function for different spatial location and are not well consider the difference of mosaic pattern for each color. In this paper, we propose a deep spatial adaptive network (SANet) for real image demosaicing, which can adaptively learn the nonlinear mapping function for different locations. The weights of spatial adaptive convolution layer are generated by the pattern information in the receptive filed. Besides, we collect a paired real demosaicing dataset to train and evaluate the deep network, which can make the learned demosaicing network more practical in the real world. The experimental results show that our SANet outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality in both noiseless and noisy cases",
    "checked": true,
    "id": "cdd86f086bfb7063eec8154fa69d823a2fc15454",
    "semantic_title": "deep spatial adaptive network for real image demosaicing",
    "citation_count": 4,
    "authors": [
      "Tao Zhang",
      "Ying Fu",
      "Cheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20243": {
    "title": "MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning",
    "volume": "main",
    "abstract": "Text-based image captioning (TextCap) requires simultaneous comprehension of visual content and reading the text of images to generate a natural language description. Although a task can teach machines to understand the complex human environment further given that text is omnipresent in our daily surroundings, it poses additional challenges in normal captioning. A text-based image intuitively contains abundant and complex multimodal relational content, that is, image details can be described diversely from multiview rather than a single caption. Certainly, we can introduce additional paired training data to show the diversity of images' descriptions, this process is labor-intensive and time-consuming for TextCap pair annotations with extra texts. Based on the insight mentioned above, we investigate how to generate diverse captions that focus on different image parts using an unpaired training paradigm. We propose the Multimodal relAtional Graph adversarIal InferenCe (MAGIC) framework for diverse and unpaired TextCap. This framework can adaptively construct multiple multimodal relational graphs of images and model complex relationships among graphs to represent descriptive diversity. Moreover, a cascaded generative adversarial network is developed from modeled graphs to infer the unpaired caption generation in image–sentence feature alignment and linguistic coherence levels. We validate the effectiveness of MAGIC in generating diverse captions from different relational information items of an image. Experimental results show that MAGIC can generate very promising outcomes without using any image–caption training pairs",
    "checked": true,
    "id": "1e91743216e8b9a0a7a2908634e7412084d3fc0f",
    "semantic_title": "magic: multimodal relational graph adversarial inference for diverse and unpaired text-based image captioning",
    "citation_count": 22,
    "authors": [
      "Wenqiao Zhang",
      "Haochen Shi",
      "Jiannan Guo",
      "Shengyu Zhang",
      "Qingpeng Cai",
      "Juncheng Li",
      "Sihui Luo",
      "Yueting Zhuang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20244": {
    "title": "Class Guided Channel Weighting Network for Fine-Grained Semantic Segmentation",
    "volume": "main",
    "abstract": "Deep learning has achieved promising performance on semantic segmentation, but few works focus on semantic segmentation at the fine-grained level. Fine-grained semantic segmentation requires recognizing and distinguishing hundreds of sub-categories. Due to the high similarity of different sub-categories and large variations in poses, scales, rotations, and color of the same sub-category in the fine-grained image set, the performance of traditional semantic segmentation methods will decline sharply. To alleviate these dilemmas, a new approach, named Class Guided Channel Weighting Network (CGCWNet), is developed in this paper to enable fine-grained semantic segmentation. For the large intra-class variations, we propose a Class Guided Weighting (CGW) module, which learns the image-level fine-grained category probabilities by exploiting second-order feature statistics, and use them as global information to guide semantic segmentation. For the high similarity between different sub-categories, we specially build a Channel Relationship Attention (CRA) module to amplify the distinction of features. Furthermore, a Detail Enhanced Guided Filter (DEGF) module is proposed to refine the boundaries of object masks by using an edge contour cue extracted from the enhanced original image. Experimental results on PASCAL VOC 2012 and six fine-grained image sets show that our proposed CGCWNet has achieved state-of-the-art results",
    "checked": true,
    "id": "76c0f4ffbd4cac432a35fcfb0e879f331b814329",
    "semantic_title": "class guided channel weighting network for fine-grained semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Wanqing Zhao",
      "Hangzai Luo",
      "Jinye Peng",
      "Jianping Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20245": {
    "title": "Context-Based Contrastive Learning for Scene Text Recognition",
    "volume": "main",
    "abstract": "Pursuing accurate and robust recognizers has been a long-lasting goal for scene text recognition (STR) researchers. Recently, attention-based methods have demonstrated their effectiveness and achieved impressive results on public benchmarks. The attention mechanism enables models to recognize scene text with severe visual distortions by leveraging contextual information. However, recent studies revealed that the implicit over-reliance of context leads to catastrophic out-of-vocabulary performance. On the contrary to the superior accuracy of the seen text, models are prone to misrecognize unseen text even with good image quality. We propose a novel framework, Context-based contrastive learning (ConCLR), to alleviate this issue. Our proposed method first generates characters with different contexts via simple image concatenation operations and then optimizes contrastive loss on their embeddings. By pulling together clusters of identical characters within various contexts and pushing apart clusters of different characters in embedding space, ConCLR suppresses the side-effect of overfitting to specific contexts and learns a more robust representation. Experiments show that ConCLR significantly improves out-of-vocabulary generalization and achieves state-of-the-art performance on public benchmarks together with attention-based recognizers",
    "checked": true,
    "id": "e3bb2f3d9296badad67daf7a00cf4fa3ada0f814",
    "semantic_title": "context-based contrastive learning for scene text recognition",
    "citation_count": 28,
    "authors": [
      "Xinyun Zhang",
      "Binwu Zhu",
      "Xufeng Yao",
      "Qi Sun",
      "Ruiyu Li",
      "Bei Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20246": {
    "title": "Learning Network Architecture for Open-Set Recognition",
    "volume": "main",
    "abstract": "Given the incomplete knowledge of classes that exist in the world, Open-set Recognition (OSR) enables networks to identify and reject the unseen classes after training. This problem of breaking the common closed-set assumption is far from being solved. Recent studies focus on designing new losses, neural network encoding structures, and calibration methods to optimize a feature space for OSR relevant tasks. In this work, we make the first attempt to tackle OSR by searching the architecture of a Neural Network (NN) under the open-set assumption. In contrast to the prior arts, we develop a mechanism to both search the architecture of the network and train a network suitable for tackling OSR. Inspired by the compact abating probability (CAP) model, which is theoretically proven to reduce the open space risk, we regularize the searching space by VAE contrastive learning. To discover a more robust structure for OSR, we propose Pseudo Auxiliary Searching (PAS), in which we split a pretended set of know-unknown classes from the original training set in the searching phase, hence enabling the super-net to explore an effective architecture that can handle unseen classes in advance. We demonstrate the benefits of this learning pipeline on 5 OSR datasets, including MNIST, SVHN, CIFAR10, CIFARAdd10, and CIFARAdd50, where our approach outperforms prior state-of-the-art networks designed by humans. To spark research in this field, our code is available at https://github.com/zxl101/NAS OSR",
    "checked": true,
    "id": "f3a8cf15e493d0636b633d41ea0c408c19bbefdb",
    "semantic_title": "learning network architecture for open-set recognition",
    "citation_count": 1,
    "authors": [
      "Xuelin Zhang",
      "Xuelian Cheng",
      "Donghao Zhang",
      "Paul Bonnington",
      "Zongyuan Ge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20247": {
    "title": "An Adversarial Framework for Generating Unseen Images by Activation Maximization",
    "volume": "main",
    "abstract": "Activation maximization (AM) refers to the task of generating input examples that maximize the activation of a target class of a classifier, which can be used for class-conditional image generation and model interpretation. A popular class of AM method, GAN-based AM, introduces a GAN pre-trained on a large image set, and performs AM over its input random seed or style embeddings, so that the generated images are natural and adversarial attacks are prevented. Most of these methods would require the image set to contain some images of the target class to be visualized. Otherwise they tend to generate other seen class images that most maximizes the target class activation. In this paper, we aim to tackle the case where information about the target class is completely removed from the image set. This would ensure that the generated images truly reflect the target class information residing in the classifier, not the target class information in the image set, which contributes to a more faithful interpretation technique. To this end, we propose PROBEGAN, a GAN-based AM algorithm capable of generating image classes unseen in the image set. Rather than using a pre-trained GAN, PROBEGAN trains a new GAN with AM explicitly included in its training objective. PROBEGAN consists of a class-conditional generator, a seen-class discriminator, and an all-class unconditional discriminator. It can be shown that such a framework can generate images with the features of the unseen target class, while retaining the naturalness as depicted in the image set. Experiments have shown that PROBEGAN can generate unseen-class images with much higher quality than the baselines. We also explore using PROBEGAN as a model interpretation tool. Our code is at https://github.com/csmiler/ProbeGAN/",
    "checked": true,
    "id": "93dd8a502df263a8eef2444676be0dafd08dd542",
    "semantic_title": "an adversarial framework for generating unseen images by activation maximization",
    "citation_count": 0,
    "authors": [
      "Yang Zhang",
      "Wang Zhou",
      "Gaoyuan Zhang",
      "David Cox",
      "Shiyu Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20248": {
    "title": "Contrastive Spatio-Temporal Pretext Learning for Self-Supervised Video Representation",
    "volume": "main",
    "abstract": "Spatio-temporal representation learning is critical for video self-supervised representation. Recent approaches mainly use contrastive learning and pretext tasks. However, these approaches learn representation by discriminating sampled instances via feature similarity in the latent space while ignoring the intermediate state of the learned representations, which limits the overall performance. In this work, taking into account the degree of similarity of sampled instances as the intermediate state, we propose a novel pretext task - spatio-temporal overlap rate (STOR) prediction. It stems from the observation that humans are capable of discriminating the overlap rates of videos in space and time. This task encourages the model to discriminate the STOR of two generated samples to learn the representations. Moreover, we employ a joint optimization combining pretext tasks with contrastive learning to further enhance the spatio-temporal representation learning. We also study the mutual influence of each component in the proposed scheme. Extensive experiments demonstrate that our proposed STOR task can favor both contrastive learning and pretext tasks and the joint optimization scheme can significantly improve the spatio-temporal representation in video understanding. The code is available at https://github.com/Katou2/CSTP",
    "checked": true,
    "id": "9bdc9f9bb8b91a962d6661ccbf422fa07dcc66f8",
    "semantic_title": "contrastive spatio-temporal pretext learning for self-supervised video representation",
    "citation_count": 8,
    "authors": [
      "Yujia Zhang",
      "Lai-Man Po",
      "Xuyuan Xu",
      "Mengyang Liu",
      "Yexin Wang",
      "Weifeng Ou",
      "Yuzhi Zhao",
      "Wing-Yin Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20249": {
    "title": "Pose-Invariant Face Recognition via Adaptive Angular Distillation",
    "volume": "main",
    "abstract": "Pose-invariant face recognition is a practically useful but challenging task. This paper introduces a novel method to learn pose-invariant feature representation without normalizing profile faces to frontal ones or learning disentangled features. We first design a novel strategy to learn pose-invariant feature embeddings by distilling the angular knowledge of frontal faces extracted by teacher network to student network, which enables the handling of faces with large pose variations. In this way, the features of faces across variant poses can cluster compactly for the same person to create a pose-invariant face representation. Secondly, we propose a Pose-Adaptive Angular Distillation loss to mitigate the negative effect of uneven distribution of face poses in the training dataset to pay more attention to the samples with large pose variations. Extensive experiments on two challenging benchmarks (IJB-A and CFP-FP) show that our approach consistently outperforms the existing methods",
    "checked": true,
    "id": "f30f33b0b13b624360a619151d1f4e2b55de14ad",
    "semantic_title": "pose-invariant face recognition via adaptive angular distillation",
    "citation_count": 0,
    "authors": [
      "Zhenduo Zhang",
      "Yongru Chen",
      "Wenming Yang",
      "Guijin Wang",
      "Qingmin Liao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20250": {
    "title": "End-to-End Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration",
    "volume": "main",
    "abstract": "Even though considerable progress has been made in deep learning-based 3D point cloud processing, how to obtain accurate correspondences for robust registration remains a major challenge because existing hard assignment methods cannot deal with outliers naturally. Alternatively, the soft matching-based methods have been proposed to learn the matching probability rather than hard assignment. However, in this paper, we prove that these methods have an inherent ambiguity causing many deceptive correspondences. To address the above challenges, we propose to learn a partial permutation matching matrix, which does not assign corresponding points to outliers, and implements hard assignment to prevent ambiguity. However, this proposal poses two new problems, i.e. existing hard assignment algorithms can only solve a full rank permutation matrix rather than a partial permutation matrix, and this desired matrix is defined in the discrete space, which is non-differentiable. In response, we design a dedicated soft-to-hard (S2H) matching procedure within the registration pipeline consisting of two steps: solving the soft matching matrix (S-step) and projecting this soft matrix to the partial permutation matrix (H-step). Specifically, we augment the profit matrix before the hard assignment to solve an augmented permutation matrix, which is cropped to achieve the final partial permutation matrix. Moreover, to guarantee end-to-end learning, we supervise the learned partial permutation matrix but propagate the gradient to the soft matrix instead. Our S2H matching procedure can be easily integrated with existing registration frameworks, which has been verified in representative frameworks including DCP, RPMNet, and DGR. Extensive experiments have validated our method, which creates a new state-of-the-art performance",
    "checked": true,
    "id": "8e6c07e24e91bf1dbce40f9c0dd6a1642bed46db",
    "semantic_title": "end-to-end learning the partial permutation matrix for robust 3d point cloud registration",
    "citation_count": 5,
    "authors": [
      "Zhiyuan Zhang",
      "Jiadai Sun",
      "Yuchao Dai",
      "Dingfu Zhou",
      "Xibin Song",
      "Mingyi He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20251": {
    "title": "PetsGAN: Rethinking Priors for Single Image Generation",
    "volume": "main",
    "abstract": "Single image generation (SIG), described as generating diverse samples that have the same visual content as the given natural image, is first introduced by SinGAN, which builds a pyramid of GANs to progressively learn the internal patch distribution of the single image. It shows excellent performance in a wide range of image manipulation tasks. However, SinGAN has some limitations. Firstly, due to lack of semantic information, SinGAN cannot handle the object images well as it does on the scene and texture images. Secondly, the independent progressive training scheme is time-consuming and easy to cause artifacts accumulation. To tackle these problems, in this paper, we dig into the single image generation problem and improve SinGAN by fully-utilization of internal and external priors. The main contributions of this paper include: 1) We interpret single image generation from the perspective of the general generative task, that is, to learn a diverse distribution from the Dirac distribution composed of a single image. In order to solve this non-trivial problem, we construct a regularized latent variable model to formulate SIG. To the best of our knowledge, it is the first time to give a clear formulation and optimization goal of SIG, and all the existing methods for SIG can be regarded as special cases of this model. 2) We design a novel Prior-based end-to-end training GAN (PetsGAN), which is infused with internal prior and external prior to overcome the problems of SinGAN. For one thing, we employ the pre-trained GAN model to inject external prior for image generation, which can alleviate the problem of lack of semantic information and generate natural, reasonable and diverse samples, even for the object image. For another, we fully-utilize the internal prior by a differential Patch Matching module and an effective reconstruction network to generate consistent and realistic texture. 3) We construct abundant of qualitative and quantitative experiments on three datasets. The experimental results show our method surpasses other methods on both generated image quality, diversity, and training speed. Moreover, we apply our method to other image manipulation tasks (e.g., style transfer, harmonization) and the results further prove the effectiveness and efficiency of our method",
    "checked": true,
    "id": "a1caeccf0902cf7cc5532f01ac3b331757ad9ca9",
    "semantic_title": "petsgan: rethinking priors for single image generation",
    "citation_count": 11,
    "authors": [
      "Zicheng Zhang",
      "Yinglu Liu",
      "Congying Han",
      "Hailin Shi",
      "Tiande Guo",
      "Bowen Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20252": {
    "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
    "volume": "main",
    "abstract": "Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8 times faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer",
    "checked": true,
    "id": "f80775a79d42a1ddfc0df808ea760c57af4949d0",
    "semantic_title": "nested hierarchical transformer: towards accurate, data-efficient and interpretable visual understanding",
    "citation_count": 90,
    "authors": [
      "Zizhao Zhang",
      "Han Zhang",
      "Long Zhao",
      "Ting Chen",
      "Sercan Ö. Arik",
      "Tomas Pfister"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20253": {
    "title": "OA-FSUI2IT: A Novel Few-Shot Cross Domain Object Detection Framework with Object-Aware Few-Shot Unsupervised Image-to-Image Translation",
    "volume": "main",
    "abstract": "Unsupervised image-to-image (UI2I) translation methods aim to learn a mapping between different visual domains with well-preserved content and consistent structure. It has been proven that the generated images are quite useful for enhancing the performance of computer vision tasks like object detection in a different domain with distribution discrepancies. Current methods require large amounts of images in both source and target domains for successful translation. However, data collection and annotations in many scenarios are infeasible or even impossible. In this paper, we propose an Object-Aware Few-Shot UI2I Translation (OA-FSUI2IT) framework to address the few-shot cross domain (FSCD) object detection task with limited unlabeled images in the target domain. To this end, we first introduce a discriminator augmentation (DA) module into the OA-FSUI2IT framework for successful few-shot UI2I translation. Then, we present a patch pyramid contrastive learning (PPCL) strategy to further improve the quality of the generated images. Last, we propose a self-supervised content-consistency (SSCC) loss to enforce the content-consistency in the translation. We implement extensive experiments to demonstrate the effectiveness of our OA-FSUI2IT framework for FSCD object detection and achieve state-of-the-art performance on the benchmarks of Normal-to-Foggy, Day-to-Night, and Cross-scene adaptation. The source code of our proposed method is also available at https://github.com/emdata-ailab/FSCD-Det",
    "checked": true,
    "id": "a6686dbb3e5d53ba9e116dd83cbf8ae82566baa7",
    "semantic_title": "oa-fsui2it: a novel few-shot cross domain object detection framework with object-aware few-shot unsupervised image-to-image translation",
    "citation_count": 2,
    "authors": [
      "Lifan Zhao",
      "Yunlong Meng",
      "Lin Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20254": {
    "title": "Static-Dynamic Co-teaching for Class-Incremental 3D Object Detection",
    "volume": "main",
    "abstract": "Deep learning-based approaches have shown remarkable performance in the 3D object detection task. However, they suffer from a catastrophic performance drop on the originally trained classes when incrementally learning new classes without revisiting the old data. This \"catastrophic forgetting\" phenomenon impedes the deployment of 3D object detection approaches in real-world scenarios, where continuous learning systems are needed. In this paper, we study the unexplored yet important class-incremental 3D object detection problem and present the first solution - SDCoT, a novel static-dynamic co-teaching method. Our SDCoT alleviates the catastrophic forgetting of old classes via a static teacher, which provides pseudo annotations for old classes in the new samples and regularizes the current model by extracting previous knowledge with a distillation loss. At the same time, SDCoT consistently learns the underlying knowledge from new data via a dynamic teacher. We conduct extensive experiments on two benchmark datasets and demonstrate the superior performance of our SDCoT over baseline approaches in several incremental learning scenarios. Our code is available at https://github.com/Na-Z/SDCoT",
    "checked": true,
    "id": "99f44f204344125f337575c30152f8eeafa4a490",
    "semantic_title": "static-dynamic co-teaching for class-incremental 3d object detection",
    "citation_count": 6,
    "authors": [
      "Na Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20255": {
    "title": "Local Surface Descriptor for Geometry and Feature Preserved Mesh Denoising",
    "volume": "main",
    "abstract": "3D meshes are widely employed to represent geometry structure of 3D shapes. Due to limitation of scanning sensor precision and other issues, meshes are inevitably affected by noise, which hampers the subsequent applications. Convolultional neural networks (CNNs) achieve great success in image processing tasks, including 2D image denoising, and have been proven to own the capacity of modeling complex features at different scales, which is also particularly useful for mesh denoising. However, due to the nature of irregular structure, CNNs-based denosing strategies cannot be trivially applied for meshes. To circumvent this limitation, in the paper, we propose the local surface descriptor (LSD), which is able to transform the local deformable surface around a face into 2D grid representation and thus facilitates the deployment of CNNs to generate denoised face normals. To verify the superiority of LSD, we directly feed LSD into the classical Resnet without any complicated network design. The extensive experimental results show that, compared to the state-of-the-arts, our method achieves encouraging performance with respect to both objective and subjective evaluations",
    "checked": true,
    "id": "793cf5c359893bd65c26ef8004cad781bff92649",
    "semantic_title": "local surface descriptor for geometry and feature preserved mesh denoising",
    "citation_count": 1,
    "authors": [
      "Wenbo Zhao",
      "Xianming Liu",
      "Junjun Jiang",
      "Debin Zhao",
      "Ge Li",
      "Xiangyang Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20256": {
    "title": "Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features with Attribute Augmentation",
    "volume": "main",
    "abstract": "The recent advance in deep generative models outlines a promising perspective in the realm of Zero-Shot Learning (ZSL). Most generative ZSL methods use category semantic attributes plus a Gaussian noise to generate visual features. After generating unseen samples, this family of approaches effectively transforms the ZSL problem into a supervised classification scheme. However, the existing models use a single semantic attribute, which contains the complete attribute information of the category. The generated data also carry the complete attribute information, but in reality, visual samples usually have limited attributes. Therefore, the generated data from attribute could have incomplete semantics. Based on this fact, we propose a novel framework to boost ZSL by synthesizing diverse features. This method uses augmented semantic attributes to train the generative model, so as to simulate the real distribution of visual features. We evaluate the proposed model on four benchmark datasets, observing significant performance improvement against the state-of-the-art",
    "checked": true,
    "id": "6485c7873c503ab05de2297145b63443a750491d",
    "semantic_title": "boosting generative zero-shot learning by synthesizing diverse features with attribute augmentation",
    "citation_count": 12,
    "authors": [
      "Xiaojie Zhao",
      "Yuming Shen",
      "Shidong Wang",
      "Haofeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20257": {
    "title": "Self-Supervised Pretraining for RGB-D Salient Object Detection",
    "volume": "main",
    "abstract": "Existing CNNs-Based RGB-D salient object detection (SOD) networks are all required to be pretrained on the ImageNet to learn the hierarchy features which helps provide a good initialization. However, the collection and annotation of large-scale datasets are time-consuming and expensive. In this paper, we utilize self-supervised representation learning (SSL) to design two pretext tasks: the cross-modal auto-encoder and the depth-contour estimation. Our pretext tasks require only a few and unlabeled RGB-D datasets to perform pretraining, which makes the network capture rich semantic contexts and reduce the gap between two modalities, thereby providing an effective initialization for the downstream task. In addition, for the inherent problem of cross-modal fusion in RGB-D SOD, we propose a consistency-difference aggregation (CDA) module that splits a single feature fusion into multi-path fusion to achieve an adequate perception of consistent and differential information. The CDA module is general and suitable for cross-modal and cross-level feature fusion. Extensive experiments on six benchmark datasets show that our self-supervised pretrained model performs favorably against most state-of-the-art methods pretrained on ImageNet. The source code will be publicly available at https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD",
    "checked": true,
    "id": "94cc6f1218a45234ad1530aa48bcc289e8723eed",
    "semantic_title": "self-supervised pretraining for rgb-d salient object detection",
    "citation_count": 21,
    "authors": [
      "Xiaoqi Zhao",
      "Youwei Pang",
      "Lihe Zhang",
      "Huchuan Lu",
      "Xiang Ruan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20258": {
    "title": "Adaptive Logit Adjustment Loss for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "Data in the real world tends to exhibit a long-tailed label distribution, which poses great challenges for the training of neural networks in visual recognition. Existing methods tackle this problem mainly from the perspective of data quantity, i.e., the number of samples in each class. To be specific, they pay more attention to tail classes, like applying larger adjustments to the logit. However, in the training process, the quantity and difficulty of data are two intertwined and equally crucial problems. For some tail classes, the features of their instances are distinct and discriminative, which can also bring satisfactory accuracy; for some head classes, although with sufficient samples, the high semantic similarity with other classes and lack of discriminative features will bring bad accuracy. Based on these observations, we propose Adaptive Logit Adjustment Loss (ALA Loss) to apply an adaptive adjusting term to the logit. The adaptive adjusting term is composed of two complementary factors: 1) quantity factor, which pays more attention to tail classes, and 2) difficulty factor, which adaptively pays more attention to hard instances in the training process. The difficulty factor can alleviate the over-optimization on tail yet easy instances and under-optimization on head yet hard instances. The synergy of the two factors can not only advance the performance on tail classes even further, but also promote the accuracy on head classes. Unlike previous logit adjusting methods that only concerned about data quantity, ALA Loss tackles the long-tailed problem from a more comprehensive, fine-grained and adaptive perspective. Extensive experimental results show that our method achieves the state-of-the-art performance on challenging recognition benchmarks, including ImageNet-LT, iNaturalist 2018, and Places-LT",
    "checked": true,
    "id": "7fe905f0fbfa17d6aa554ef425a7a06ea9468836",
    "semantic_title": "adaptive logit adjustment loss for long-tailed visual recognition",
    "citation_count": 21,
    "authors": [
      "Yan Zhao",
      "Weicong Chen",
      "Xu Tan",
      "Kai Huang",
      "Jihong Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20259": {
    "title": "CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-Based Autonomous Urban Driving",
    "volume": "main",
    "abstract": "Vision-based autonomous urban driving in dense traffic is quite challenging due to the complicated urban environment and the dynamics of the driving behaviors. Widely-applied methods either heavily rely on hand-crafted rules or learn from limited human experience, which makes them hard to generalize to rare but critical scenarios. In this paper, we present a novel CAscade Deep REinforcement learning framework, CADRE, to achieve model-free vision-based autonomous urban driving. In CADRE, to derive representative latent features from raw observations, we first offline train a Co-attention Perception Module (CoPM) that leverages the co-attention mechanism to learn the inter-relationships between the visual and control information from a pre-collected driving dataset. Cascaded by the frozen CoPM, we then present an efficient distributed proximal policy optimization framework to online learn the driving policy under the guidance of particularly designed reward functions. We perform a comprehensive empirical study with the CARLA NoCrash benchmark as well as specific obstacle avoidance scenarios in autonomous urban driving tasks. The experimental results well justify the effectiveness of CADRE and its superiority over the state-of-the-art by a wide margin",
    "checked": true,
    "id": "651f256bf9f8a12672cfd29e8fd228a6ebe085d4",
    "semantic_title": "cadre: a cascade deep reinforcement learning framework for vision-based autonomous urban driving",
    "citation_count": 6,
    "authors": [
      "Yinuo Zhao",
      "Kun Wu",
      "Zhiyuan Xu",
      "Zhengping Che",
      "Qi Lu",
      "Jian Tang",
      "Chi Harold Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20260": {
    "title": "Learning from the Tangram to Solve Mini Visual Tasks",
    "volume": "main",
    "abstract": "Current pre-training methods in computer vision focus on natural images in the daily-life context. However, abstract diagrams such as icons and symbols are common and important in the real world. We are inspired by Tangram, a game that requires replicating an abstract pattern from seven dissected shapes. By recording human experience in solving tangram puzzles, we present the Tangram dataset and show that a pre-trained neural model on the Tangram helps solve some mini visual tasks based on low-resolution vision. Extensive experiments demonstrate that our proposed method generates intelligent solutions for aesthetic tasks such as folding clothes and evaluating room layouts. The pre-trained feature extractor can facilitate the convergence of few-shot learning tasks on human handwriting and improve the accuracy in identifying icons by their contours. The Tangram dataset is available at https://github.com/yizhouzhao/Tangram",
    "checked": true,
    "id": "9aeddad39085e3a5874fbe3436ce276b9451c0e6",
    "semantic_title": "learning from the tangram to solve mini visual tasks",
    "citation_count": 2,
    "authors": [
      "Yizhou Zhao",
      "Liang Qiu",
      "Pan Lu",
      "Feng Shi",
      "Tian Han",
      "Song-Chun Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20261": {
    "title": "Handling Slice Permutations Variability in Tensor Recovery",
    "volume": "main",
    "abstract": "This work studies the influence of slice permutations on tensor recovery, which is derived from a reasonable assumption about algorithm, i.e. changing data order should not affect the effectiveness of the algorithm. However, as we will discussed in this paper, this assumption is not satisfied by tensor recovery under some cases. We call this interesting problem as Slice Permutations Variability (SPV) in tensor recovery. In this paper, we discuss SPV of several key tensor recovery problems theoretically and experimentally. The obtained results show that there is a huge gap between results by tensor recovery using tensor with different slices sequences. To overcome SPV in tensor recovery, we develop a novel tensor recovery algorithm by Minimum Hamiltonian Circle for SPV (TRSPV) which exploits a low dimensional subspace structures within data tensor more exactly. To the best of our knowledge, this is the first work to discuss and effectively solve the SPV problem in tensor recovery. The experimental results demonstrate the effectiveness of the proposed algorithm in eliminating SPV in tensor recovery",
    "checked": true,
    "id": "f78abfd45c6d570ef65ec4e0356780db0551fca0",
    "semantic_title": "handling slice permutations variability in tensor recovery",
    "citation_count": 1,
    "authors": [
      "Jingjing Zheng",
      "Xiaoqin Zhang",
      "Wenzhe Wang",
      "Xianta Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20262": {
    "title": "Boosting Contrastive Learning with Relation Knowledge Distillation",
    "volume": "main",
    "abstract": "While self-supervised representation learning (SSL) has proved to be effective in the large model, there is still a huge gap between the SSL and supervised method in the lightweight model when following the same solution. We delve into this problem and find that the lightweight model is prone to collapse in semantic space when simply performing instance-wise contrast. To address this issue, we propose a relation-wise contrastive paradigm with Relation Knowledge Distillation (ReKD). We introduce a heterogeneous teacher to explicitly mine the semantic information and transferring a novel relation knowledge to the student (lightweight model). The theoretical analysis supports our main concern about instance-wise contrast and verify the effectiveness of our relation-wise contrastive learning. Extensive experimental results also demonstrate that our method achieves significant improvements on multiple lightweight models. Particularly, the linear evaluation on AlexNet obviously improves the current state-of-art from 44.7% to 50.1% , which is the first work to get close to the supervised (50.5%). Code will be made available",
    "checked": true,
    "id": "e7b9857d8314809ccaef058f430178e9932c3d12",
    "semantic_title": "boosting contrastive learning with relation knowledge distillation",
    "citation_count": 5,
    "authors": [
      "Kai Zheng",
      "Yuanjiang Wang",
      "Ye Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20263": {
    "title": "Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining",
    "volume": "main",
    "abstract": "Video moment localization aims at localizing the video segments which are most related to the given free-form natural language query. The weakly supervised setting, where only video level description is available during training, is getting more and more attention due to its lower annotation cost. Prior weakly supervised methods mainly use sliding windows to generate temporal proposals, which are independent of video content and low quality, and train the model to distinguish matched video-query pairs and unmatched ones collected from different videos, while neglecting what the model needs is to distinguish the unaligned segments within the video. In this work, we propose a novel weakly supervised solution by introducing Contrastive Negative sample Mining (CNM). Specifically, we use a learnable Gaussian mask to generate positive samples, highlighting the video frames most related to the query, and consider other frames of the video and the whole video as easy and hard negative samples respectively. We then train our network with the Intra-Video Contrastive loss to make our positive and negative samples more discriminative. Our method has two advantages: (1) Our proposal generation process with a learnable Gaussian mask is more efficient and makes our positive sample higher quality. (2) The more difficult intra-video negative samples enable our model to distinguish highly confusing scenes. Experiments on two datasets show the effectiveness of our method. Code can be found at https://github.com/minghangz/cnm",
    "checked": true,
    "id": "93324dfb8b02f43edb4122d08ccc6b90d3a6b577",
    "semantic_title": "weakly supervised video moment localization with contrastive negative sample mining",
    "citation_count": 21,
    "authors": [
      "Minghang Zheng",
      "Yanjie Huang",
      "Qingchao Chen",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20264": {
    "title": "Dual Decoupling Training for Semi-supervised Object Detection with Noise-Bypass Head",
    "volume": "main",
    "abstract": "Pseudo bounding boxes from the self-training paradigm are inevitably noisy for semi-supervised object detection. To cope with that, a dual decoupling training framework is proposed in the present study, i.e. clean and noisy data decoupling, and classification and localization task decoupling. In the first decoupling, two-level thresholds are used to categorize pseudo boxes into three groups, i.e. clean backgrounds, noisy foregrounds and clean foregrounds. With a specially designed noise-bypass head focusing on noisy data, backbone networks can extract coarse but diverse information; and meanwhile, an original head learns from clean samples for more precise predictions. In the second decoupling, we take advantage of the two-head structure for better evaluation of localization quality, thus the category label and location of a pseudo box can remain independent of each other during training. The approach of two-level thresholds is also applied to group pseudo boxes into three sections of different location accuracy. We outperform existing works by a large margin on VOC datasets, reaching 54.8 mAP(+1.8), and even up to 55.9 mAP(+1.5) by leveraging MS-COCO train2017 as extra unlabeled data. On MS-COCO benchmark, our method also achieves about 1.0 mAP improvements averaging across protocols compared with the prior state-of-the-art",
    "checked": true,
    "id": "818ff32b6e04788226b07384a06eb348b845b55c",
    "semantic_title": "dual decoupling training for semi-supervised object detection with noise-bypass head",
    "citation_count": 7,
    "authors": [
      "Shida Zheng",
      "Chenshu Chen",
      "Xiaowei Cai",
      "Tingqun Ye",
      "Wenming Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20265": {
    "title": "SCALoss: Side and Corner Aligned Loss for Bounding Box Regression",
    "volume": "main",
    "abstract": "Bounding box regression is an important component in object detection. Recent work achieves promising performance by optimizing the Intersection over Union (IoU). However, IoU-based loss has the gradient vanish problem in the case of low overlapping bounding boxes, and the model could easily ignore these simple cases. In this paper, we propose Side Overlap (SO) loss by maximizing the side overlap of two bounding boxes, which puts more penalty for low overlapping bounding box cases. Besides, to speed up the convergence, the Corner Distance (CD) is added into the objective function. Combining the Side Overlap and Corner Distance, we get a new regression objective function, Side and Corner Align Loss (SCALoss). The SCALoss is well-correlated with IoU loss, which also benefits the evaluation metric but produces more penalty for low-overlapping cases. It can serve as a comprehensive similarity measure, leading to better localization performance and faster convergence speed. Experiments on COCO, PASCAL VOC, and LVIS benchmarks show that SCALoss can bring consistent improvement and outperform ln loss and IoU based loss with popular object detectors such as YOLOV3, SSD, Faster-RCNN. Code is available at: https://github.com/Turoad/SCALoss",
    "checked": true,
    "id": "04614b2a9c50902ffa2523b1c67aed8c54a65a8d",
    "semantic_title": "scaloss: side and corner aligned loss for bounding box regression",
    "citation_count": 4,
    "authors": [
      "Tu Zheng",
      "Shuai Zhao",
      "Yang Liu",
      "Zili Liu",
      "Deng Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20266": {
    "title": "SepFusion: Finding Optimal Fusion Structures for Visual Sound Separation",
    "volume": "main",
    "abstract": "Multiple modalities can provide rich semantic information; and exploiting such information will normally lead to better performance compared with the single-modality counterpart. However, it is not easy to devise an effective cross-modal fusion structure due to the variations of feature dimensions and semantics, especially when the inputs even come from different sensors, as in the field of audio-visual learning. In this work, we propose SepFusion, a novel framework that can smoothly produce optimal fusion structures for visual-sound separation. The framework is composed of two components, namely the model generator and the evaluator. To construct the generator, we devise a lightweight architecture space that can adapt to different input modalities. In this way, we can easily obtain audio-visual fusion structures according to our demands. For the evaluator, we adopt the idea of neural architecture search to select superior networks effectively. This automatic process can significantly save human efforts while achieving competitive performances. Moreover, since our SepFusion provides a series of strong models, we can utilize the model family for broader applications, such as further promoting performance via model assembly, or providing suitable architectures for the separation of certain instrument classes. These potential applications further enhance the competitiveness of our approach",
    "checked": true,
    "id": "71c85b49eda8cd51adac408fd2271e3a3e67215c",
    "semantic_title": "sepfusion: finding optimal fusion structures for visual sound separation",
    "citation_count": 3,
    "authors": [
      "Dongzhan Zhou",
      "Xinchi Zhou",
      "Di Hu",
      "Hang Zhou",
      "Lei Bai",
      "Ziwei Liu",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20267": {
    "title": "Pan-Sharpening with Customized Transformer and Invertible Neural Network",
    "volume": "main",
    "abstract": "In remote sensing imaging systems, pan-sharpening is an important technique to obtain high-resolution multispectral images from a high-resolution panchromatic image and its corresponding low-resolution multispectral image. Owing to the powerful learning capability of convolution neural network (CNN), CNN-based methods have dominated this field. However, due to the limitation of the convolution operator, long-range spatial features are often not accurately obtained, thus limiting the overall performance. To this end, we propose a novel and effective method by exploiting a customized transformer architecture and information-lossless invertible neural module for long-range dependencies modeling and effective feature fusion in this paper. Specifically, the customized transformer formulates the PAN and MS features as queries and keys to encourage joint feature learning across two modalities while the designed invertible neural module enables effective feature fusion to generate the expected pan-sharpened results. To the best of our knowledge, this is the first attempt to introduce transformer and invertible neural network into pan-sharpening field. Extensive experiments over different kinds of satellite datasets demonstrate that our method outperforms state-of-the-art algorithms both visually and quantitatively with fewer parameters and flops. Further, the ablation experiments also prove the effectiveness of the proposed customized long-range transformer and effective invertible neural feature fusion module for pan-sharpening",
    "checked": true,
    "id": "5c4cdc1375a0a18e04dfe28862a3a1fe46ae5162",
    "semantic_title": "pan-sharpening with customized transformer and invertible neural network",
    "citation_count": 15,
    "authors": [
      "Man Zhou",
      "Jie Huang",
      "Yanchi Fang",
      "Xueyang Fu",
      "Aiping Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20268": {
    "title": "Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation",
    "volume": "main",
    "abstract": "In recent years, optical flow methods develop rapidly, achieving unprecedented high performance. Most of the methods only consider single-modal optical flow under the well-known brightness-constancy assumption. However, in many application systems, images of different modalities need to be aligned, which demands to estimate cross-modal flow between the cross-modal image pairs. A lot of cross-modal matching methods are designed for some specific cross-modal scenarios. We argue that the prior knowledge of the advanced optical flow models can be transferred to the cross-modal flow estimation, which may be a simple but unified solution for diverse cross-modal matching tasks. To verify our hypothesis, we design a self-supervised framework to promote the single-modal optical flow networks for diverse corss-modal flow estimation. Moreover, we add a Cross-Modal-Adapter block as a plugin to the state-of-the-art optical flow model RAFT for better performance in cross-modal scenarios. Our proposed Modality Promotion Framework and Cross-Modal Adapter have multiple advantages compared to the existing methods. The experiments demonstrate that our method is effective on multiple datasets of different cross-modal scenarios",
    "checked": true,
    "id": "52cfe9a77ebbd621e1e8f0f09d9637aab98aa865",
    "semantic_title": "promoting single-modal optical flow network for diverse cross-modal flow estimation",
    "citation_count": 3,
    "authors": [
      "Shili Zhou",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20269": {
    "title": "Edge-Aware Guidance Fusion Network for RGB–Thermal Scene Parsing",
    "volume": "main",
    "abstract": "RGB–thermal scene parsing has recently attracted increasing research interest in the field of computer vision. However, most existing methods fail to perform good boundary extraction for prediction maps and cannot fully use high-level features. In addition, these methods simply fuse the features from RGB and thermal modalities but are unable to obtain comprehensive fused features. To address these problems, we propose an edge-aware guidance fusion network (EGFNet) for RGB–thermal scene parsing. First, we introduce a prior edge map generated using the RGB and thermal images to capture detailed information in the prediction map and then embed the prior edge information in the feature maps. To effectively fuse the RGB and thermal information, we propose a multimodal fusion module that guarantees adequate cross-modal fusion. Considering the importance of high-level semantic information, we propose a global information module and a semantic information module to extract rich semantic information from the high-level features. For decoding, we use simple elementwise addition for cascaded feature fusion. Finally, to improve the parsing accuracy, we apply multitask deep supervision to the semantic and boundary maps. Extensive experiments were performed on benchmark datasets to demonstrate the effectiveness of the proposed EGFNet and its superior performance compared with state-of-the-art methods. The code and results can be found at https://github.com/ShaohuaDong2021/EGFNet",
    "checked": false,
    "id": "6ea00d90331b4f58574656d931241d681c347465",
    "semantic_title": "edge-aware guidance fusion network for rgb thermal scene parsing",
    "citation_count": 31,
    "authors": [
      "Wujie Zhou",
      "Shaohua Dong",
      "Caie Xu",
      "Yaguan Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20270": {
    "title": "TiGAN: Text-Based Interactive Image Generation and Manipulation",
    "volume": "main",
    "abstract": "Using natural-language feedback to guide image generation and manipulation can greatly lower the required efforts and skills. This topic has received increased attention in recent years through refinement of Generative Adversarial Networks (GANs); however, most existing works are limited to single-round interaction, which is not reflective of real world interactive image editing workflows. Furthermore, previous works dealing with multi-round scenarios are limited to predefined feedback sequences, which is also impractical. In this paper, we propose a novel framework for Text-based Interactive image generation and manipulation (TiGAN) that responds to users' natural-language feedback. TiGAN utilizes the powerful pre-trained CLIP model to understand users' natural-language feedback and exploits contrastive learning for a better text-to-image mapping. To maintain the image consistency during interactions, TiGAN generates intermediate feature vectors aligned with the feedback and selectively feeds these vectors to our proposed generative model. Empirical results on several datasets show that TiGAN improves both interaction efficiency and image quality while better avoids undesirable image manipulation during interactions",
    "checked": true,
    "id": "839dc73c1adae268144d9cfb9d70985b2001304f",
    "semantic_title": "tigan: text-based interactive image generation and manipulation",
    "citation_count": 5,
    "authors": [
      "Yufan Zhou",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Chris Tensmeyer",
      "Tong Yu",
      "Changyou Chen",
      "Jinhui Xu",
      "Tong Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20271": {
    "title": "Cross-Domain Empirical Risk Minimization for Unbiased Long-Tailed Classification",
    "volume": "main",
    "abstract": "We address the overlooked unbiasedness in existing long-tailed classification methods: we find that their overall improvement is mostly attributed to the biased preference of \"tail\" over \"head\", as the test distribution is assumed to be balanced; however, when the test is as imbalanced as the long-tailed training data---let the test respect Zipf's law of nature---the \"tail\" bias is no longer beneficial overall because it hurts the \"head\" majorities. In this paper, we propose Cross-Domain Empirical Risk Minimization (xERM) for training an unbiased test-agnostic model to achieve strong performances on both test distributions, which empirically demonstrates that xERM fundamentally improves the classification by learning better feature representation rather than the \"head vs. tail\" game. Based on causality, we further theoretically explain why xERM achieves unbiasedness: the bias caused by the domain selection is removed by adjusting the empirical risks on the imbalanced domain and the balanced but unseen domain",
    "checked": true,
    "id": "cadc9f7c86cee6eccb50a1b7696ac18c3be189d9",
    "semantic_title": "cross-domain empirical risk minimization for unbiased long-tailed classification",
    "citation_count": 19,
    "authors": [
      "Beier Zhu",
      "Yulei Niu",
      "Xian-Sheng Hua",
      "Hanwang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20272": {
    "title": "Deep Recurrent Neural Network with Multi-Scale Bi-directional Propagation for Video Deblurring",
    "volume": "main",
    "abstract": "The success of the state-of-the-art video deblurring methods stems mainly from implicit or explicit estimation of alignment among the adjacent frames for latent video restoration. However, due to the influence of the blur effect, estimating the alignment information from the blurry adjacent frames is not a trivial task. Inaccurate estimations will interfere the following frame restoration. Instead of estimating alignment information, we propose a simple and effective deep Recurrent Neural Network with Multi-scale Bi-directional Propagation (RNN-MBP) to effectively propagate and gather the information from unaligned neighboring frames for better video deblurring. Specifically, we build a Multi-scale Bi-directional Propagation (MBP) module with two U-Net RNN cells which can directly exploit the inter-frame information from unaligned neighboring hidden states by integrating them in different scales. Moreover, to better evaluate the proposed algorithm and existing state-of-the-art methods on real-world blurry scenes, we also create a Real-World Blurry Video Dataset (RBVD) by a well-designed Digital Video Acquisition System (DVAS) and use it as the training and evaluation dataset. Extensive experimental results demonstrate that the proposed RBVD dataset effectively improve the performance of existing algorithms on real-world blurry videos, and the proposed algorithm performs favorably against the state-of-the-art methods on three typical benchmarks. The code is available at https://github.com/XJTU-CVLAB-LOWLEVEL/RNN-MBP",
    "checked": true,
    "id": "26a3f9072e995a9dcc39b0bcb2c6ce080d99b9ef",
    "semantic_title": "deep recurrent neural network with multi-scale bi-directional propagation for video deblurring",
    "citation_count": 12,
    "authors": [
      "Chao Zhu",
      "Hang Dong",
      "Jinshan Pan",
      "Boyang Liang",
      "Yuhao Huang",
      "Lean Fu",
      "Fei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20273": {
    "title": "I Can Find You! Boundary-Guided Separated Attention Network for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Can you find me? By simulating how humans to discover the so-called 'perfectly'-camouflaged object, we present a novel boundary-guided separated attention network (call BSA-Net). Beyond the existing camouflaged object detection (COD) wisdom, BSA-Net utilizes two-stream separated attention modules to highlight the separator (or say the camouflaged object's boundary) between an image's background and foreground: the reverse attention stream helps erase the camouflaged object's interior to focus on the background, while the normal attention stream recovers the interior and thus pay more attention to the foreground; and both streams are followed by a boundary guider module and combined to strengthen the understanding of boundary. The core design of such separated attention is motivated by the COD procedure of humans: find the subtle difference between the foreground and background to delineate the boundary of a camouflaged object, then the boundary can help further enhance the COD accuracy. We validate on three benchmark datasets that the proposed BSA-Net is very beneficial to detect camouflaged objects with the blurred boundaries and similar colors/patterns with their backgrounds. Extensive results exhibit very clear COD improvements on our BSA-Net over sixteen SOTAs",
    "checked": true,
    "id": "4bcc7717f99b1a3bc798be7417686e140fd6a654",
    "semantic_title": "i can find you! boundary-guided separated attention network for camouflaged object detection",
    "citation_count": 35,
    "authors": [
      "Hongwei Zhu",
      "Peng Li",
      "Haoran Xie",
      "Xuefeng Yan",
      "Dong Liang",
      "Dapeng Chen",
      "Mingqiang Wei",
      "Jing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20274": {
    "title": "MoCaNet: Motion Retargeting In-the-Wild via Canonicalization Networks",
    "volume": "main",
    "abstract": "We present a novel framework that brings the 3D motion retargeting task from controlled environments to in-the-wild scenarios. In particular, our method is capable of retargeting body motion from a character in a 2D monocular video to a 3D character without using any motion capture system or 3D reconstruction procedure. It is designed to leverage massive online videos for unsupervised training, needless of 3D annotations or motion-body pairing information. The proposed method is built upon two novel canonicalization operations, structure canonicalization and view canonicalization. Trained with the canonicalization operations and the derived regularizations, our method learns to factorize a skeleton sequence into three independent semantic subspaces, i.e., motion, structure, and view angle. The disentangled representation enables motion retargeting from 2D to 3D with high precision. Our method achieves superior performance on motion transfer benchmarks with large body variations and challenging actions. Notably, the canonicalized skeleton sequence could serve as a disentangled and interpretable representation of human motion that benefits action analysis and motion retrieval",
    "checked": true,
    "id": "3e2b595476ed9da5a96c068fe250edfa804c2cb6",
    "semantic_title": "mocanet: motion retargeting in-the-wild via canonicalization networks",
    "citation_count": 9,
    "authors": [
      "Wentao Zhu",
      "Zhuoqian Yang",
      "Ziang Di",
      "Wayne Wu",
      "Yizhou Wang",
      "Chen Change Loy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20275": {
    "title": "Robust Depth Completion with Uncertainty-Driven Loss Functions",
    "volume": "main",
    "abstract": "Recovering a dense depth image from sparse LiDAR scans is a challenging task. Despite the popularity of color-guided methods for sparse-to-dense depth completion, they treated pixels equally during optimization, ignoring the uneven distribution characteristics in the sparse depth map and the accumulated outliers in the synthesized ground truth. In this work, we introduce uncertainty-driven loss functions to improve the robustness of depth completion and handle the uncertainty in depth completion. Specifically, we propose an explicit uncertainty formulation for robust depth completion with Jeffrey's prior. A parametric uncertain-driven loss is introduced and translated to new loss functions that are robust to noisy or missing data. Meanwhile, we propose a multiscale joint prediction model that can simultaneously predict depth and uncertainty maps. The estimated uncertainty map is also used to perform adaptive prediction on the pixels with high uncertainty, leading to a residual map for refining the completion results. Our method has been tested on KITTI Depth Completion Benchmark and achieved the state-of-the-art robustness performance in terms of MAE, IMAE, and IRMSE metrics",
    "checked": true,
    "id": "5d06ec1b2d24699cf1475663af6af974cc87d31a",
    "semantic_title": "robust depth completion with uncertainty-driven loss functions",
    "citation_count": 21,
    "authors": [
      "Yufan Zhu",
      "Weisheng Dong",
      "Leida Li",
      "Jinjian Wu",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20276": {
    "title": "Efficient Model-Driven Network for Shadow Removal",
    "volume": "main",
    "abstract": "Deep Convolutional Neural Networks (CNNs) based methods have achieved significant breakthroughs in the task of single image shadow removal. However, the performance of these methods remains limited for several reasons. First, the existing shadow illumination model ignores the spatially variant property of the shadow images, hindering their further performance. Second, most deep CNNs based methods directly estimate the shadow free results from the input shadow images like a black box, thus losing the desired interpretability. To address these issues, we first propose a new shadow illumination model for the shadow removal task. This new shadow illumination model ensures the identity mapping among unshaded regions, and adaptively performs fine grained spatial mapping between shadow regions and their references. Then, based on the shadow illumination model, we reformulate the shadow removal task as a variational optimization problem. To effectively solve the variational problem, we design an iterative algorithm and unfold it into a deep network, naturally increasing the interpretability of the deep model. Experiments show that our method could achieve SOTA performance with less than half parameters, one-fifth of floating-point of operations (FLOPs), and over seventeen times faster than SOTA method (DHAN)",
    "checked": true,
    "id": "06e4aca19539b79cd87b60279bea7fab1743f41f",
    "semantic_title": "efficient model-driven network for shadow removal",
    "citation_count": 15,
    "authors": [
      "Yurui Zhu",
      "Zeyu Xiao",
      "Yanchi Fang",
      "Xueyang Fu",
      "Zhiwei Xiong",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20277": {
    "title": "Learning Disentangled Classification and Localization Representations for Temporal Action Localization",
    "volume": "main",
    "abstract": "A common approach to Temporal Action Localization (TAL) is to generate action proposals and then perform action classification and localization on them. For each proposal, existing methods universally use a shared proposal-level representation for both tasks. However, our analysis indicates that this shared representation focuses on the most discriminative frames for classification, e.g., ``take-offs\" rather than ``run-ups\" in distinguishing ``high jump\" and ``long jump\", while frames most relevant to localization, such as the start and end frames of an action, are largely ignored. In other words, such a shared representation can not simultaneously handle both classification and localization tasks well, and it makes precise TAL difficult. To address this challenge, this paper disentangles the shared representation into classification and localization representations. The disentangled classification representation focuses on the most discriminative frames, and the disentangled localization representation focuses on the action phase as well as the action start and end. Our model could be divided into two sub-networks, i.e., the disentanglement network and the context-based aggregation network. The disentanglement network is an autoencoder to learn orthogonal hidden variables of classification and localization. The context-based aggregation network aggregates the classification and localization representations by modeling local and global contexts. We evaluate our proposed method on two popular benchmarks for TAL, which outperforms all state-of-the-art methods",
    "checked": true,
    "id": "dd1e216fe9cf4aeda25c85772c4de2967c69f94f",
    "semantic_title": "learning disentangled classification and localization representations for temporal action localization",
    "citation_count": 6,
    "authors": [
      "Zixin Zhu",
      "Le Wang",
      "Wei Tang",
      "Ziyi Liu",
      "Nanning Zheng",
      "Gang Hua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20278": {
    "title": "ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation",
    "volume": "main",
    "abstract": "Depth estimation is a crucial step for 3D reconstruction with panorama images in recent years. Panorama images maintain the complete spatial information but introduce distortion with equirectangular projection. In this paper, we propose an ACDNet based on the adaptively combined dilated convolution to predict the dense depth map for a monocular panoramic image. Specifically, we combine the convolution kernels with different dilations to extend the receptive field in the equirectangular projection. Meanwhile, we introduce an adaptive channel-wise fusion module to summarize the feature maps and get diverse attention areas in the receptive field along the channels. Due to the utilization of channel-wise attention in constructing the adaptive channel-wise fusion module, the network can capture and leverage the cross-channel contextual information efficiently. Finally, we conduct depth estimation experiments on three datasets (both virtual and real-world) and the experimental results demonstrate that our proposed ACDNet substantially outperforms the current state-of-the-art (SOTA) methods. Our codes and model parameters are accessed in https://github.com/zcq15/ACDNet",
    "checked": true,
    "id": "077b76322c2953d4fb3c53e139c3685800cab54d",
    "semantic_title": "acdnet: adaptively combined dilated convolution for monocular panorama depth estimation",
    "citation_count": 24,
    "authors": [
      "Chuanqing Zhuang",
      "Zhengda Lu",
      "Yiqun Wang",
      "Jun Xiao",
      "Ying Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20279": {
    "title": "Making Adversarial Examples More Transferable and Indistinguishable",
    "volume": "main",
    "abstract": "Fast gradient sign attack series are popular methods that are used to generate adversarial examples. However, most of the approaches based on fast gradient sign attack series cannot balance the indistinguishability and transferability due to the limitations of the basic sign structure. To address this problem, we propose a method, called Adam Iterative Fast Gradient Tanh Method (AI-FGTM), to generate indistinguishable adversarial examples with high transferability. Besides, smaller kernels and dynamic step size are also applied to generate adversarial examples for further increasing the attack success rates. Extensive experiments on an ImageNet-compatible dataset show that our method generates more indistinguishable adversarial examples and achieves higher attack success rates without extra running time and resource. Our best transfer-based attack NI-TI-DI-AITM can fool six classic defense models with an average success rate of 89.3% and three advanced defense models with an average success rate of 82.7%, which are higher than the state-of-the-art gradient-based attacks. Additionally, our method can also reduce nearly 20% mean perturbation. We expect that our method will serve as a new baseline for generating adversarial examples with better transferability and indistinguishability",
    "checked": true,
    "id": "9aed5318e0b4c26b4b6e242002f3aae567ded3fb",
    "semantic_title": "making adversarial examples more transferable and indistinguishable",
    "citation_count": 13,
    "authors": [
      "Junhua Zou",
      "Yexin Duan",
      "Boyu Li",
      "Wu Zhang",
      "Yu Pan",
      "Zhisong Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20280": {
    "title": "Undercover Boolean Matrix Factorization with MaxSAT",
    "volume": "main",
    "abstract": "The k-undercover Boolean matrix factorization problem aims to approximate a m×n Boolean matrix X as the Boolean product of an m×k and a k×n matrices A◦B such that X is a cover of A◦B, i.e., no representation error is allowed on the 0's entries of the matrix X. To infer an optimal and \"block-optimal\" k-undercover, we propose two exact methods based on MaxSAT encodings. From a theoretical standpoint, we prove that our method of inferring \"block-optimal\" k-undercover is a (1 - 1/e) ≈ 0.632 approximation for the optimal k-undercover problem. From a practical standpoint, experimental results indicate that our \"block-optimal\" k-undercover algorithm outperforms the state-of-the-art even when compared with algorithms for the more general k-undercover Boolean Matrix Factorization problem for which only minimizing reconstruction error is required",
    "checked": true,
    "id": "ed7380a21f70a36c013f0fcd42f32c0a4f270b49",
    "semantic_title": "undercover boolean matrix factorization with maxsat",
    "citation_count": 0,
    "authors": [
      "Florent Avellaneda",
      "Roger Villemaire"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20281": {
    "title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach",
    "volume": "main",
    "abstract": "Reinforcement learning is widely used in applications where one needs to perform sequential decisions while interacting with the environment. The problem becomes more challenging when the decision requirement includes satisfying some safety constraints. The problem is mathematically formulated as constrained Markov decision process (CMDP). In the literature, various algorithms are available to solve CMDP problems in a model-free manner to achieve epsilon-optimal cumulative reward with epsilon feasible policies. An epsilon-feasible policy implies that it suffers from constraint violation. An important question here is whether we can achieve epsilon-optimal cumulative reward with zero constraint violations or not. To achieve that, we advocate the use of a randomized primal-dual approach to solve the CMDP problems and propose a conservative stochastic primal-dual algorithm (CSPDA) which is shown to exhibit O(1/epsilon^2) sample complexity to achieve epsilon-optimal cumulative reward with zero constraint violations. In the prior works, the best available sample complexity for the epsilon-optimal policy with zero constraint violation is O(1/epsilon^5). Hence, the proposed algorithm provides a significant improvement compared to the state of the art",
    "checked": true,
    "id": "6a0d4cc369093089b7c2384bb9045f28fe78a42f",
    "semantic_title": "achieving zero constraint violation for constrained reinforcement learning via primal-dual approach",
    "citation_count": 31,
    "authors": [
      "Qinbo Bai",
      "Amrit Singh Bedi",
      "Mridul Agarwal",
      "Alec Koppel",
      "Vaneet Aggarwal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20282": {
    "title": "GEQCA: Generic Qualitative Constraint Acquisition",
    "volume": "main",
    "abstract": "Many planning, scheduling or multi-dimensional packing problems involve the design of subtle logical combinations of temporal or spatial constraints. On the one hand, the precise modelling of these constraints, which are formulated in various relation algebras, entails a number of possible logical combinations and requires expertise in constraint-based modelling. On the other hand, active constraint acquisition (CA) has been used successfully to support non-experienced users in learning conjunctive constraint networks through the generation of a sequence of queries. In this paper, we propose GEACQ, which stands for Generic Qualitative Constraint Acquisition, an active CA method that learns qualitative constraints via the concept of qualitative queries. GEACQ combines qualitative queries with time-bounded path consistency (PC) and background knowledge propagation to acquire the qualitative constraints of any scheduling or packing problem. We prove soundness, completeness and termination of GEACQ by exploiting the jointly exhaustive and pairwise disjoint property of qualitative calculus and we give an experimental evaluation that shows (i) the efficiency of our approach in learning temporal constraints and, (ii) the use of GEACQ on real scheduling instances",
    "checked": true,
    "id": "6e934e00a612fd9d6deae355aa7d7cfc18cabf3c",
    "semantic_title": "geqca: generic qualitative constraint acquisition",
    "citation_count": 2,
    "authors": [
      "Mohamed-Bachir Belaid",
      "Nassim Belmecheri",
      "Arnaud Gotlieb",
      "Nadjib Lazaar",
      "Helge Spieker"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20283": {
    "title": "Certified Symmetry and Dominance Breaking for Combinatorial Optimisation",
    "volume": "main",
    "abstract": "Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification method for optimisation problems in which symmetry and dominance breaking are easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes XOR and cardinality reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems",
    "checked": true,
    "id": "2315a711e1eb8c5b0f1c785f847fee6cd6ae31aa",
    "semantic_title": "certified symmetry and dominance breaking for combinatorial optimisation",
    "citation_count": 13,
    "authors": [
      "Bart Bogaerts",
      "Stephan Gocht",
      "Ciaran McCreesh",
      "Jakob Nordström"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20284": {
    "title": "The Perils of Learning Before Optimizing",
    "volume": "main",
    "abstract": "Formulating real-world optimization problems often begins with making predictions from historical data (e.g., an optimizer that aims to recommend fast routes relies upon travel-time predictions). Typically, learning the prediction model used to generate the optimization problem and solving that problem are performed in two separate stages. Recent work has showed how such prediction models can be learned end-to-end by differentiating through the optimization task. Such methods often yield empirical improvements, which are typically attributed to end-to-end making better error tradeoffs than the standard loss function used in a two-stage solution. We refine this explanation and more precisely characterize when end-to-end can improve performance. When prediction targets are stochastic, a two-stage solution must make an a priori choice about which statistics of the target distribution to model---we consider expectations over prediction targets---while an end-to-end solution can make this choice adaptively. We show that the performance gap between a two-stage and end-to-end approach is closely related to the \\emph{price of correlation} concept in stochastic optimization and show the implications of some existing POC results for the predict-then-optimize problem. We then consider a novel and particularly practical setting, where multiple prediction targets are combined to obtain each of the objective function's coefficients. We give explicit constructions where (1) two-stage performs unboundedly worse than end-to-end; and (2) two-stage is optimal. We use simulations to experimentally quantify performance gaps and identify a wide range of real-world applications from the literature whose objective functions rely on multiple prediction targets, suggesting that end-to-end learning could yield significant improvements",
    "checked": true,
    "id": "381df0bd337f75cf5352be6c90103ef6ea4db9a7",
    "semantic_title": "the perils of learning before optimizing",
    "citation_count": 6,
    "authors": [
      "Chris Cameron",
      "Jason Hartford",
      "Taylor Lundy",
      "Kevin Leyton-Brown"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20285": {
    "title": "A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback",
    "volume": "main",
    "abstract": "In a wide variety of applications including online advertising, contractual hiring, and wireless scheduling, the controller is constrained by a stringent budget constraint on the available resources, which are consumed in a random amount by each action, and a stochastic feasibility constraint that may impose important operational limitations on decision-making. In this work, we consider a general model to address such problems, where each action returns a random reward, cost, and penalty from an unknown joint distribution, and the decision-maker aims to maximize the total reward under a budget constraint B on the total cost and a stochastic constraint on the time-average penalty. We propose a novel low-complexity algorithm based on Lyapunov optimization methodology, named LyOn, and prove that for K arms it achieves square root of KBlog(B) regret and zero constraint-violation when B is sufficiently large. The low computational cost and sharp performance bounds of LyOn suggest that Lyapunov-based algorithm design methodology can be effective in solving constrained bandit optimization problems",
    "checked": true,
    "id": "6dc958f2311526919827b6705aeed62fe930b75b",
    "semantic_title": "a lyapunov-based methodology for constrained optimization with bandit feedback",
    "citation_count": 4,
    "authors": [
      "Semih Cayci",
      "Yilin Zheng",
      "Atilla Eryilmaz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20286": {
    "title": "Resolving Inconsistencies in Simple Temporal Problems: A Parameterized Approach",
    "volume": "main",
    "abstract": "The simple temporal problem (STP) is one of the most influential reasoning formalisms for representing temporal information in AI. We study the problem of resolving inconsistency of data encoded in the STP. We prove that the problem of identifying a maximally large consistent subset of data is NP-hard. In practical instances, it is reasonable to assume that the amount of erroneous data is small. We therefore parameterize by the number of constraints that need to be removed to achieve consistency. Using tools from parameterized complexity we design fixed-parameter tractable algorithms for two large fragments of the STP. Our main algorithmic results employ reductions to the Directed Subset Feedback Arc Set problem and iterative compression combined with an efficient algorithm for the Edge Multicut problem. We complement our algorithmic results with hardness results that rule out fixed-parameter tractable algorithms for all remaining non-trivial fragments of the STP (under standard complexity-theoretic assumptions). Together, our results give a full classification of the classical and parameterized complexity of the problem",
    "checked": true,
    "id": "c642c0f4e1026f27f54fc997463c6bffc68a3f17",
    "semantic_title": "resolving inconsistencies in simple temporal problems: a parameterized approach",
    "citation_count": 1,
    "authors": [
      "Konrad K. Dabrowski",
      "Peter Jonsson",
      "Sebastian Ordyniak",
      "George Osipov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20287": {
    "title": "Efficient Riemannian Meta-Optimization by Implicit Differentiation",
    "volume": "main",
    "abstract": "To solve optimization problems with nonlinear constrains, the recently developed Riemannian meta-optimization methods show promise, which train neural networks as an optimizer to perform optimization on Riemannian manifolds. A key challenge is the heavy computational and memory burdens, because computing the meta-gradient with respect to the optimizer involves a series of time-consuming derivatives, and stores large computation graphs in memory. In this paper, we propose an efficient Riemannian meta-optimization method that decouples the complex computation scheme from the meta-gradient. We derive Riemannian implicit differentiation to compute the meta-gradient by establishing a link between Riemannian optimization and the implicit function theorem. As a result, the updating our optimizer is only related to the final two iterations, which in turn speeds up our method and reduces the memory footprint significantly. We theoretically study the computational load and memory footprint of our method for long optimization trajectories, and conduct an empirical study to demonstrate the benefits of the proposed method. Evaluations of three optimization problems on different Riemannian manifolds show that our method achieves state-of-the-art performance in terms of the convergence speed and the quality of optima",
    "checked": true,
    "id": "710a8acc80f9bcdd3fae6fb2e9a83898bc71718f",
    "semantic_title": "efficient riemannian meta-optimization by implicit differentiation",
    "citation_count": 1,
    "authors": [
      "Xiaomeng Fan",
      "Yuwei Wu",
      "Zhi Gao",
      "Yunde Jia",
      "Mehrtash Harandi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20288": {
    "title": "Faster Algorithms for Weak Backdoors",
    "volume": "main",
    "abstract": "A weak backdoor, or simply a backdoor, for a Boolean SAT formula F into a class of SAT formulae C is a partial truth assignment T such that F[T] is in C and satisfiability is preserved. The problem of finding a backdoor from class C1 into class C2, or WB(C1,C2), can be stated as follows: Given a formula F in C1, and a natural number k, determine whether there exists a backdoor for F into C2 assigning at most k variables. The class 0-Val contains all Boolean formulae with at least one negative literal in each clause. We design a new algorithm for WB(3CNF, 0-Val) by reducing it to a local search variant of 3-SAT. We show that our algorithm runs in time O*(2.562^k), improving on the previous state-of-the-art of O*(2.85^k). Here, the O* notation is a variant of the big-O notation that allows to omit polynomial factors in the input size. Next, we look at WB(3CNF, Null), where Null is the class consisting of the empty formula. This problem was known to have a trivial running time upper bound of O*(6^k) and can easily be solved in O*(3^k) time. We use a reduction to Conflict-Free-d-Hitting-Set to prove an upper bound of O*(2.2738^k), and also prove a lower bound of 2^o(k) assuming the Exponential Time Hypothesis. Finally, Horn is the class of formulae with at most one positive literal per clause. We improve the previous O*(4.54^k) running time for WB(3CNF, Horn) problem to O*(4.17^k), by exploiting the structure of the SAT instance to give a novel proof of the non-existence of the slowest cases after a slight restructuring of the branching priorities",
    "checked": true,
    "id": "5397eaffddcdc177ff0e0e2eeb04702b4a1bd29d",
    "semantic_title": "faster algorithms for weak backdoors",
    "citation_count": 0,
    "authors": [
      "Serge Gaspers",
      "Andrew Kaploun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20289": {
    "title": "A Divide and Conquer Algorithm for Predict+Optimize with Non-convex Problems",
    "volume": "main",
    "abstract": "The predict+optimize problem combines machine learning and combinatorial optimization by predicting the problem coefficients first and then using these coefficients to solve the optimization problem. While this problem can be solved in two separate stages, recent research shows end to end models can achieve better results. This requires differentiating through a discrete combinatorial function. Models that use differentiable surrogates are prone to approximation errors, while existing exact models are limited to dynamic programming, or they do not generalize well with scarce data. In this work we propose a novel divide and conquer algorithm based on transition points to reason over exact optimization problems and predict the coefficients using the optimization loss. Moreover, our model is not limited to dynamic programming problems. We also introduce a greedy version, which achieves similar results with less computation. In comparison with other predict+optimize frameworks, we show our method outperforms existing exact frameworks and can reason over hard combinatorial problems better than surrogate methods",
    "checked": true,
    "id": "eb3ee0e0934965ec2113032d9ef00ff6e39766ac",
    "semantic_title": "a divide and conquer algorithm for predict+optimize with non-convex problems",
    "citation_count": 5,
    "authors": [
      "Ali Ugur Guler",
      "Emir Demirović",
      "Jeffrey Chan",
      "James Bailey",
      "Christopher Leckie",
      "Peter J. Stuckey"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20290": {
    "title": "Computing Diverse Shortest Paths Efficiently: A Theoretical and Experimental Study",
    "volume": "main",
    "abstract": "Finding diverse solutions in combinatorial problems recently has received considerable attention (Baste et al. 2020; Fomin et al. 2020; Hanaka et al. 2021). In this paper we study the following type of problems: given an integer k, the problem asks for k solutions such that the sum of pairwise (weighted) Hamming distances between these solutions is maximized. Such solutions are called diverse solutions. We present a polynomial-time algorithm for finding diverse shortest st-paths in weighted directed graphs. Moreover, we study the diverse version of other classical combinatorial problems such as diverse weighted matroid bases, diverse weighted arborescences, and diverse bipartite matchings. We show that these problems can be solved in polynomial time as well. To evaluate the practical performance of our algorithm for finding diverse shortest st-paths, we conduct a computational experiment with synthetic and real-world instances. The experiment shows that our algorithm successfully computes diverse solutions within reasonable computational time",
    "checked": true,
    "id": "86b0d4e2f14cf4e03e3fa9cec6ea1c26b0bb5460",
    "semantic_title": "computing diverse shortest paths efficiently: a theoretical and experimental study",
    "citation_count": 12,
    "authors": [
      "Tesshu Hanaka",
      "Yasuaki Kobayashi",
      "Kazuhiro Kurita",
      "See Woo Lee",
      "Yota Otachi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20291": {
    "title": "Optimizing Binary Decision Diagrams with MaxSAT for Classification",
    "volume": "main",
    "abstract": "The growing interest in explainable artificial intelligence(XAI) for critical decision making motivates the need for interpretable machine learning (ML) models. In fact, due to their structure (especially with small sizes), these models are inherently understandable by humans. Recently, several exact methods for computing such models are proposed to overcome weaknesses of traditional heuristic methods by providing more compact models or better prediction quality. Despite their compressed representation of Boolean functions, Binary decision diagrams (BDDs) did not gain enough interest as other interpretable ML models. In this paper, we first propose SAT-based models for learning optimal BDDs (in terms of the number of features) that classify all input examples. Then, we lift the encoding to a MaxSAT model to learn optimal BDDs in limited depths, that maximize the number of examples correctly classified. Finally, we tackle the fragmentation problem by introducing a method to merge compatible subtrees for the BDDs found via the MaxSAT model. Our empirical study shows clear benefits of the proposed approach in terms of prediction quality and interpretability (i.e., lighter size) compared to the state-of-the-art approaches",
    "checked": true,
    "id": "5d7d469eba0032dd287ca3ccc0319f61a18b4dc8",
    "semantic_title": "optimizing binary decision diagrams with maxsat for classification",
    "citation_count": 4,
    "authors": [
      "Hao Hu",
      "Marie-José Huguet",
      "Mohamed Siala"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20292": {
    "title": "Using MaxSAT for Efficient Explanations of Tree Ensembles",
    "volume": "main",
    "abstract": "Tree ensembles (TEs) denote a prevalent machine learning model that do not offer guarantees of interpretability, that represent a challenge from the perspective of explainable artificial intelligence. Besides model agnostic approaches, recent work proposed to explain TEs with formally-defined explanations, which are computed with oracles for propositional satisfiability (SAT) and satisfiability modulo theories. The computation of explanations for TEs involves linear constraints to express the prediction. In practice, this deteriorates scalability of the underlying reasoners. Motivated by the inherent propositional nature of TEs, this paper proposes to circumvent the need for linear constraints and instead employ an optimization engine for pure propositional logic to efficiently handle the prediction. Concretely, the paper proposes to use a MaxSAT solver and exploit the objective function to determine a winning class. This is achieved by devising a propositional encoding for computing explanations of TEs. Furthermore, the paper proposes additional heuristics to improve the underlying MaxSAT solving procedure. Experimental results obtained on a wide range of publicly available datasets demonstrate that the proposed MaxSAT-based approach is either on par or outperforms the existing reasoning-based explainers, thus representing a robust and efficient alternative for computing formal explanations for TEs",
    "checked": true,
    "id": "b3a4cc32f10648cf84c2a2725fb9a8f504edfa86",
    "semantic_title": "using maxsat for efficient explanations of tree ensembles",
    "citation_count": 26,
    "authors": [
      "Alexey Ignatiev",
      "Yacine Izza",
      "Peter J. Stuckey",
      "Joao Marques-Silva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20293": {
    "title": "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework",
    "volume": "main",
    "abstract": "In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ``small\" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently",
    "checked": true,
    "id": "61b821bf8fcd96901cbd87486dd2474dc19da4c3",
    "semantic_title": "finding backdoors to integer programs: a monte carlo tree search framework",
    "citation_count": 8,
    "authors": [
      "Elias B. Khalil",
      "Pashootan Vaezipoor",
      "Bistra Dilkina"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20294": {
    "title": "Learning to Search in Local Branching",
    "volume": "main",
    "abstract": "Finding high-quality solutions to mixed-integer linear programming problems (MILPs) is of great importance for many practical applications. In this respect, the refinement heuristic local branching (LB) has been proposed to produce improving solutions and has been highly influential for the development of local search methods in MILP. The algorithm iteratively explores a sequence of solution neighborhoods defined by the so-called local branching constraint, namely, a linear inequality limiting the distance from a reference solution. For a LB algorithm, the choice of the neighborhood size is critical to performance. Although it was initialized by a conservative value in the original LB scheme, our new observation is that the \"best\" size is strongly dependent on the particular MILP instance. In this work, we investigate the relation between the size of the search neighborhood and the behavior of the underlying LB algorithm, and we devise a leaning-based framework for guiding the neighborhood search of the LB heuristic. The framework consists of a two-phase strategy. For the first phase, a scaled regression model is trained to predict the size of the LB neighborhood at the first iteration through a regression task. In the second phase, we leverage reinforcement learning and devise a reinforced neighborhood search strategy to dynamically adapt the size at the subsequent iterations. We computationally show that the neighborhood size can indeed be learned, leading to improved performances and that the overall algorithm generalizes well both with respect to the instance size and, remarkably, across instances",
    "checked": true,
    "id": "ddcb55152e5e73b978e4fd8ba1bf656551154bbd",
    "semantic_title": "learning to search in local branching",
    "citation_count": 15,
    "authors": [
      "Defeng Liu",
      "Matteo Fischetti",
      "Andrea Lodi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20295": {
    "title": "Analysis of Pure Literal Elimination Rule for Non-uniform Random (MAX) k-SAT Problem with an Arbitrary Degree Distribution",
    "volume": "main",
    "abstract": "MAX k-SAT is one of the archetypal NP-hard problems. Its variation called random MAX k-SAT problem was introduced in order to understand how hard it is to solve instances of the problem on average. The most common model to sample random instances is the uniform model, which has received a large amount of attention. However, the uniform model often fails to capture important structural properties we observe in the real-world instances. To address these limitations, a more general (in a certain sense) model has been proposed, the configuration model, which is able to produce instances with an arbitrary distribution of variables' degrees, and so can simulate biases in instances appearing in various applications. Our overall goal is to expand the theory built around the uniform model to the more general configuration model for a wide range of degree distributions. This includes locating satisfiability thresholds and analysing the performance of the standard heuristics applied to instances sampled from the configuration model. In this paper we analyse the performance of the pure literal elimination rule. We provide an equation that given an underlying degree distribution gives the number of clauses the pure literal elimination rule satisfies w.h.p. We also show how the distribution of variable degrees changes over time as the algorithm is being executed",
    "checked": true,
    "id": "240bcb949881b8991e4c98d70c555a6cbfa986a0",
    "semantic_title": "analysis of pure literal elimination rule for non-uniform random (max) k-sat problem with an arbitrary degree distribution",
    "citation_count": 0,
    "authors": [
      "Oleksii Omelchenko",
      "Andrei A. Bulatov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20296": {
    "title": "The SoftCumulative Constraint with Quadratic Penalty",
    "volume": "main",
    "abstract": "The Cumulative constraint greatly contributes to the success of constraint programming at solving scheduling problems. The SoftCumulative, a version of the Cumulative where overloading the resource incurs a penalty is, however, less studied. We introduce a checker and a filtering algorithm for the SoftCumulative, which are inspired by the powerful energetic reasoning rule for the Cumulative. Both algorithms can be used with classic linear penalty function, but also with a quadratic penalty function, where the penalty of overloading the resource increases quadratically with the amount of the overload. We show that these algorithms are more general than existing algorithms and vastly outperform a decomposition of the SoftCumulative in practice",
    "checked": true,
    "id": "e3056ee58a6bf71adc43faa7f8978f639a99b2bc",
    "semantic_title": "the softcumulative constraint with quadratic penalty",
    "citation_count": 0,
    "authors": [
      "Yanick Ouellet",
      "Claude-Guy Quimper"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20297": {
    "title": "Efficient Vertex-Oriented Polytopic Projection for Web-Scale Applications",
    "volume": "main",
    "abstract": "We consider applications involving a large set of instances of projecting points to polytopes. We develop an intuition guided by theoretical and empirical analysis to show that when these instances follow certain structures, a large majority of the projections lie on vertices of the polytopes. To do these projections efficiently we derive a vertex-oriented incremental algorithm to project a point onto any arbitrary polytope, as well as give specific algorithms to cater to simplex projection and polytopes where the unit box is cut by planes. Such settings are especially useful in web-scale applications such as optimal matching or allocation problems. Several such problems in internet marketplaces (e-commerce, ride-sharing, food delivery, professional services, advertising, etc.), can be formulated as Linear Programs (LP) with such polytope constraints that require a projection step in the overall optimization process. We show that in some of the very recent works, the polytopic projection is the most expensive step and our efficient projection algorithms help in gaining massive improvements in performance",
    "checked": true,
    "id": "3c895ba108ce257136ae244407951fa2b49b66c1",
    "semantic_title": "efficient vertex-oriented polytopic projection for web-scale applications",
    "citation_count": 2,
    "authors": [
      "Rohan Ramanath",
      "S. Sathiya Keerthi",
      "Yao Pan",
      "Konstantin Salomatin",
      "Kinjal Basu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20298": {
    "title": "A Variant of Concurrent Constraint Programming on GPU",
    "volume": "main",
    "abstract": "The number of cores on graphical computing units (GPUs) is reaching thousands nowadays, whereas the clock speed of processors stagnates. Unfortunately, constraint programming solvers do not take advantage yet of GPU parallelism. One reason is that constraint solvers were primarily designed within the mental frame of sequential computation. To solve this issue, we take a step back and contribute to a simple, intrinsically parallel, lock-free and formally correct programming language based on concurrent constraint programming. We then re-examine parallel constraint solving on GPUs within this formalism, and develop Turbo, a simple constraint solver entirely programmed on GPUs. Turbo validates the correctness of our approach and compares positively to a parallel CPU-based solver",
    "checked": true,
    "id": "ecada436b177dcf54aaada58372286921dd6d25d",
    "semantic_title": "a variant of concurrent constraint programming on gpu",
    "citation_count": 0,
    "authors": [
      "Pierre Talbot",
      "Frédéric G Pinel",
      "Pascal Bouvry"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20299": {
    "title": "Real-Time Driver-Request Assignment in Ridesourcing",
    "volume": "main",
    "abstract": "Online on-demand ridesourcing service has played a huge role in transforming urban transportation. A central function in most on-demand ridesourcing platforms is to dynamically assign drivers to rider requests that could balance the request waiting times and the driver pick-up distances. To deal with the online nature of this problem, existing literature either divides the time horizon into short windows and applies a static offline assignment algorithm within each window or assumes a fully online setting that makes decisions for each request immediately upon its arrival. In this paper, we propose a more realistic model for the driver-request assignment that bridges the above two settings together. Our model allows the requests to wait after their arrival but assumes that they may leave at any time following a quitting function. Under this model, we design an efficient algorithm for assigning available drivers to requests in real-time. Our algorithm is able to incorporate future estimated driver arrivals into consideration and make strategic waiting and matching decisions that could balance the waiting time and pick-up distance of the assignment. We prove that our algorithm is optimal ex-ante in the single-request setting, and demonstrate its effectiveness in the general multi-request setting through experiments on both synthetic and real-world datasets",
    "checked": true,
    "id": "321c703aea583da14814654fea1ef35859f89f04",
    "semantic_title": "real-time driver-request assignment in ridesourcing",
    "citation_count": 2,
    "authors": [
      "Hao Wang",
      "Xiaohui Bei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20300": {
    "title": "Encoding Multi-Valued Decision Diagram Constraints as Binary Constraint Trees",
    "volume": "main",
    "abstract": "Ordered Multi-valued Decision Diagram (MDD) is a compact representation used to model various constraints, such as regular constraints and table constraints. It can be particularly useful for representing ad-hoc problem specific constraints. Many algorithms have been proposed to enforce Generalized Arc Consistency (GAC) on MDD constraints. In this paper, we introduce a new compact representation called Binary Constraint Tree (BCT). We propose tree binary encodings to transform any MDD constraint into a BCT constraint. We also present a specialized algorithm enforcing GAC on the BCT constraint resulting from a MDD constraint. Experimental results on a large set of benchmarks show that the BCT GAC algorithm can significantly outperform state-of-the-art MDD as well as table GAC algorithms",
    "checked": true,
    "id": "6be25eac0e7faaa04fa44b793a99fe0f79171ba7",
    "semantic_title": "encoding multi-valued decision diagram constraints as binary constraint trees",
    "citation_count": 4,
    "authors": [
      "Ruiwei Wang",
      "Roland H.C. Yap"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20301": {
    "title": "Sample Average Approximation for Stochastic Optimization with Dependent Data: Performance Guarantees and Tractability",
    "volume": "main",
    "abstract": "Sample average approximation (SAA), a popular method for tractably solving stochastic optimization problems, enjoys strong asymptotic performance guarantees in settings with independent training samples. However, these guarantees are not known to hold generally with dependent samples, such as in online learning with time series data or distributed computing with Markovian training samples. In this paper, we show that SAA remains tractable when the distribution of unknown parameters is only observable through dependent instances and still enjoys asymptotic consistency and finite sample guarantees. Specifically, we provide a rigorous probability error analysis to derive 1 - beta confidence bounds for the out-of-sample performance of SAA estimators and show that these estimators are asymptotically consistent. We then, using monotone operator theory, study the performance of a class of stochastic first-order algorithms trained on a dependent source of data. We show that approximation error for these algorithms is bounded and concentrates around zero, and establish deviation bounds for iterates when the underlying stochastic process is phi-mixing. The algorithms presented can be used to handle numerically inconvenient loss functions such as the sum of a smooth and non-smooth function or of non-smooth functions with constraints. To illustrate the usefulness of our results, we present several stochastic versions of popular algorithms such as stochastic proximal gradient descent (S-PGD), stochastic relaxed Peaceman-Rachford splitting algorithms (S-rPRS), and numerical experiment",
    "checked": true,
    "id": "c51705bee088be57d61d689c0a34f12f8459a8f2",
    "semantic_title": "sample average approximation for stochastic optimization with dependent data: performance guarantees and tractability",
    "citation_count": 5,
    "authors": [
      "Yafei Wang",
      "Bo Pan",
      "Wei Tu",
      "Peng Liu",
      "Bei Jiang",
      "Chao Gao",
      "Wei Lu",
      "Shangling Jui",
      "Linglong Kong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20302": {
    "title": "A Provably-Efficient Model-Free Algorithm for Infinite-Horizon Average-Reward Constrained Markov Decision Processes",
    "volume": "main",
    "abstract": "This paper presents a model-free reinforcement learning (RL) algorithm for infinite-horizon average-reward Constrained Markov Decision Processes (CMDPs). Considering a learning horizon K, which is sufficiently large, the proposed algorithm achieves sublinear regret and zero constraint violation. The bounds depend on the number of states S, the number of actions A, and two constants which are independent of the learning horizon K",
    "checked": true,
    "id": "c16e98201765272ccb8dec49b392ee7a3734f1f7",
    "semantic_title": "a provably-efficient model-free algorithm for infinite-horizon average-reward constrained markov decision processes",
    "citation_count": 13,
    "authors": [
      "Honghao Wei",
      "Xin Liu",
      "Lei Ying"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20303": {
    "title": "TextHoaxer: Budgeted Hard-Label Adversarial Attacks on Text",
    "volume": "main",
    "abstract": "This paper focuses on a newly challenging setting in hard-label adversarial attacks on text data by taking the budget information into account. Although existing approaches can successfully generate adversarial examples in the hard-label setting, they follow an ideal assumption that the victim model does not restrict the number of queries. However, in real-world applications the query budget is usually tight or limited. Moreover, existing hard-label adversarial attack techniques use the genetic algorithm to optimize discrete text data by maintaining a number of adversarial candidates during optimization, which can lead to the problem of generating low-quality adversarial examples in the tight-budget setting. To solve this problem, in this paper, we propose a new method named TextHoaxer by formulating the budgeted hard-label adversarial attack task on text data as a gradient-based optimization problem of perturbation matrix in the continuous word embedding space. Compared with the genetic algorithm-based optimization, our solution only uses a single initialized adversarial example as the adversarial candidate for optimization, which significantly reduces the number of queries. The optimization is guided by a new objective function consisting of three terms, i.e., semantic similarity term, pair-wise perturbation constraint, and sparsity constraint. Semantic similarity term and pair-wise perturbation constraint can ensure the high semantic similarity of adversarial examples from both comprehensive text-level and individual word-level, while the sparsity constraint explicitly restricts the number of perturbed words, which is also helpful for enhancing the quality of generated text. We conduct extensive experiments on eight text datasets against three representative natural language models, and experimental results show that TextHoaxer can generate high-quality adversarial examples with higher semantic similarity and lower perturbation rate under the tight-budget setting",
    "checked": true,
    "id": "88fb3feacc9826cf8305776489c69197ab78d478",
    "semantic_title": "texthoaxer: budgeted hard-label adversarial attacks on text",
    "citation_count": 13,
    "authors": [
      "Muchao Ye",
      "Chenglin Miao",
      "Ting Wang",
      "Fenglong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20304": {
    "title": "Two Compacted Models for Efficient Model-Based Diagnosis",
    "volume": "main",
    "abstract": "Model-based diagnosis (MBD) with multiple observations is complicated and difficult to manage over. In this paper, we proposed two new diagnosis models, namely, the Compacted Model with Multiple Observations (CMMO) and the Dominated-based Compacted Model with Multiple Observations (D-CMMO), to solve the problem in which a considerable amount of time is needed when multiple observations are given and more than one fault is injected. Three ideas are presented in this paper. First, we propose to encode MBD with each observation as a subsystem and share as many system variables as possible to compress the size of encoded clauses. Second, we utilize the notion of gate dominance in the CMMO approach to compute Top-Level Diagnosis with Compacted Model (CM-TLD) to reduce the solution space. Finally, we explore the performance of our model using three fault models. Experimental results on the ISCAS-85 benchmarks show that CMMO and D-CMMO perform better than the state-of-the-art algorithms",
    "checked": true,
    "id": "726b6261e1523982b86727ed27c482c665d583cc",
    "semantic_title": "two compacted models for efficient model-based diagnosis",
    "citation_count": 1,
    "authors": [
      "Huisi Zhou",
      "Dantong Ouyang",
      "Xiangfu Zhao",
      "Liming Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20305": {
    "title": "Parameterized Approximation Algorithms for K-center Clustering and Variants",
    "volume": "main",
    "abstract": "k-center is one of the most popular clustering models. While it admits a simple 2-approximation in polynomial time in general metrics, the Euclidean version is NP-hard to approximate within a factor of 1.93, even in the plane, if one insists the dependence on k in the running time be polynomial. Without this restriction, a classic algorithm yields a 2^{O((klog k)/{epsilon})}dn-time (1+epsilon)-approximation for Euclidean k-center, where d is the dimension. In this work, we give a faster algorithm for small dimensions: roughly speaking an O^*(2^{O((1/epsilon)^{O(d)} k^{1-1/d} log k)})-time (1+epsilon)-approximation. In particular, the running time is roughly O^*(2^{O((1/epsilon)^{O(1)}sqrt{k}log k)}) in the plane. We complement our algorithmic result with a matching hardness lower bound. We also consider a well-studied generalization of k-center, called Non-uniform k-center (NUkC), where we allow different radii clusters. NUkC is NP-hard to approximate within any factor, even in the Euclidean case. We design a 2^{O(klog k)}n^2 time 3-approximation for NUkC, and a 2^{O((klog k)/epsilon)}dn time (1+\\epsilon)-approximation for Euclidean NUkC. The latter time bound matches the bound for k-center",
    "checked": true,
    "id": "6e518eaee58c9599b7367bc10c3364a48e529748",
    "semantic_title": "parameterized approximation algorithms for k-center clustering and variants",
    "citation_count": 2,
    "authors": [
      "Sayan Bandyapadhyay",
      "Zachary Friggstad",
      "Ramin Mousavi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20306": {
    "title": "How to Find a Good Explanation for Clustering?",
    "volume": "main",
    "abstract": "k-means and k-median clustering are powerful unsupervised machine learning techniques. However, due to complicated dependences on all the features, it is challenging to interpret the resulting cluster assignments. Moshkovitz, Dasgupta, Rashtchian, and Frost proposed an elegant model of explainable k-means and k-median clustering in ICML 2020. In this model, a decision tree with k leaves provides a straightforward characterization of the data set into clusters. We study two natural algorithmic questions about explainable clustering. (1) For a given clustering, how to find the ``best explanation'' by using a decision tree with k leaves? (2) For a given set of points, how to find a decision tree with k leaves minimizing the k-means/median objective of the resulting explainable clustering? To address the first question, we introduce a new model of explainable clustering. Our model, inspired by the notion of outliers in robust statistics, is the following. We are seeking a small number of points (outliers) whose removal makes the existing clustering well-explainable. For addressing the second question, we initiate the study of the model of Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous algorithmic analysis sheds some light on the influence of parameters like the input size, dimension of the data, the number of outliers, the number of clusters, and the approximation ratio, on the computational complexity of explainable clustering",
    "checked": true,
    "id": "e7906710360fe56a0bdc6a14313329d0c31c1b10",
    "semantic_title": "how to find a good explanation for clustering?",
    "citation_count": 10,
    "authors": [
      "Sayan Bandyapadhyay",
      "Fedor Fomin",
      "Petr A Golovach",
      "William Lochet",
      "Nidhi Purohit",
      "Kirill Simonov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20307": {
    "title": "Regularizing Graph Neural Networks via Consistency-Diversity Graph Augmentations",
    "volume": "main",
    "abstract": "Despite the remarkable performance of graph neural networks (GNNs) in semi-supervised learning, it is criticized for not making full use of unlabeled data and suffering from over-fitting. Recently, graph data augmentation, used to improve both accuracy and generalization of GNNs, has received considerable attentions. However, one fundamental question is how to evaluate the quality of graph augmentations in principle? In this paper, we propose two metrics, Consistency and Diversity, from the aspects of augmentation correctness and generalization. Moreover, we discover that existing augmentations fall into a dilemma between these two metrics. Can we find a graph augmentation satisfying both consistency and diversity? A well-informed answer can help us understand the mechanism behind graph augmentation and improve the performance of GNNs. To tackle this challenge, we analyze two representative semi-supervised learning algorithms: label propagation (LP) and consistency regularization (CR). We find that LP utilizes the prior knowledge of graphs to improve consistency and CR adopts variable augmentations to promote diversity. Based on this discovery, we treat neighbors as augmentations to capture the prior knowledge embodying homophily assumption, which promises a high consistency of augmentations. To further promote diversity, we randomly replace the immediate neighbors of each node with its remote neighbors. After that, a neighbor-constrained regularization is proposed to enforce the predictions of the augmented neighbors to be consistent with each other. Extensive experiments on five real-world graphs validate the superiority of our method in improving the accuracy and generalization of GNNs",
    "checked": true,
    "id": "c4e7967f2ae109957f415818e1148b594ae2bb31",
    "semantic_title": "regularizing graph neural networks via consistency-diversity graph augmentations",
    "citation_count": 15,
    "authors": [
      "Deyu Bo",
      "Binbin Hu",
      "Xiao Wang",
      "Zhiqiang Zhang",
      "Chuan Shi",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20308": {
    "title": "Two-Stage Octave Residual Network for End-to-End Image Compression",
    "volume": "main",
    "abstract": "Octave Convolution (OctConv) is a generic convolutional unit that has already achieved good performances in many computer vision tasks. Recent studies also have shown the potential of applying the OctConv in end-to-end image compression. However, considering the characteristic of image compression task, current works of OctConv may limit the performance of the image compression network due to the loss of spatial information caused by the sampling operations of inter-frequency communication. Besides, the correlation between multi-frequency latents produced by OctConv is not utilized in current architectures. In this paper, to address these problems, we propose a novel Two-stage Octave Residual (ToRes) block which strips the sampling operation from OctConv to strengthen the capability of preserving useful information. Moreover, to capture the redundancy between the multi-frequency latents, a context transfer module is designed. The results show that both ToRes block and the incorporation of context transfer module help to improve the Rate-Distortion performance, and the combination of these two strategies makes our model achieve the state-of-the-art performance and outperform the latest compression standard Versatile Video Coding (VVC) in terms of both PSNR and MS-SSIM",
    "checked": true,
    "id": "ecfab429b242e9ab7e30a492a55eaff7a39a06aa",
    "semantic_title": "two-stage octave residual network for end-to-end image compression",
    "citation_count": 9,
    "authors": [
      "Fangdong Chen",
      "Yumeng Xu",
      "Li Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20309": {
    "title": "DANets: Deep Abstract Networks for Tabular Data Classification and Regression",
    "volume": "main",
    "abstract": "Tabular data are ubiquitous in real world applications. Although many commonly-used neural components (e.g., convolution) and extensible neural networks (e.g., ResNet) have been developed by the machine learning community, few of them were effective for tabular data and few designs were adequately tailored for tabular data structures. In this paper, we propose a novel and flexible neural component for tabular data, called Abstract Layer (AbstLay), which learns to explicitly group correlative input features and generate higher-level features for semantics abstraction. Also, we design a structure re-parameterization method to compress the trained AbstLay, thus reducing the computational complexity by a clear margin in the reference phase. A special basic block is built using AbstLays, and we construct a family of Deep Abstract Networks (DANets) for tabular data classification and regression by stacking such blocks. In DANets, a special shortcut path is introduced to fetch information from raw tabular features, assisting feature interactions across different levels. Comprehensive experiments on seven real-world tabular datasets show that our AbstLay and DANets are effective for tabular data classification and regression, and the computational complexity is superior to competitive methods. Besides, we evaluate the performance gains of DANet as it goes deep, verifying the extendibility of our method. Our code is available at https://github.com/WhatAShot/DANet",
    "checked": true,
    "id": "329e65711f346b76cee445d76025a80c7bc62e5d",
    "semantic_title": "danets: deep abstract networks for tabular data classification and regression",
    "citation_count": 15,
    "authors": [
      "Jintai Chen",
      "Kuanlun Liao",
      "Yao Wan",
      "Danny Z. Chen",
      "Jian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20310": {
    "title": "Fuzzy Logic Based Logical Query Answering on Knowledge Graphs",
    "volume": "main",
    "abstract": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data",
    "checked": false,
    "id": "ee6da7e7c6785f9aa7c610884ae3294f39958d1a",
    "semantic_title": "fuzzy logic based logical query answering on knowledge graph",
    "citation_count": 31,
    "authors": [
      "Xuelu Chen",
      "Ziniu Hu",
      "Yizhou Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20311": {
    "title": "TAG: Learning Timed Automata from Logs",
    "volume": "main",
    "abstract": "Event logs are often one of the main sources of information to understand the behavior of a system. While numerous approaches have extracted partial information from event logs, in this work, we aim at inferring a global model of a system from its event logs. We consider real-time systems, which can be modeled with Timed Automata: our approach is thus a Timed Automata learner. There is a handful of related work, however, they might require a lot of parameters or produce Timed Automata that either are undeterministic or lack precision. In contrast, our proposed approach, called TAG, requires only one parameter and learns a deterministic Timed Automaton having a good tradeoff between accuracy and complexity of the automata. This allows getting an interpretable and accurate global model of the real-time system considered. Our experiments compare our approach to the related work and demonstrate its merits",
    "checked": true,
    "id": "9d284ee6ae05064ecbb5cb0fa5950d5546dc82d2",
    "semantic_title": "tag: learning timed automata from logs",
    "citation_count": 2,
    "authors": [
      "Lénaïg Cornanguer",
      "Christine Largouët",
      "Laurence Rozé",
      "Alexandre Termier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20312": {
    "title": "Differentially Describing Groups of Graphs",
    "volume": "main",
    "abstract": "How does neural connectivity in autistic children differ from neural connectivity in healthy children or autistic youths? What patterns in global trade networks are shared across classes of goods, and how do these patterns change over time? Answering questions like these requires us to differentially describe groups of graphs: Given a set of graphs and a partition of these graphs into groups, discover what graphs in one group have in common, how they systematically differ from graphs in other groups, and how multiple groups of graphs are related. We refer to this task as graph group analysis, which seeks to describe similarities and differences between graph groups by means of statistically significant subgraphs. To perform graph group analysis, we introduce Gragra, which uses maximum entropy modeling to identify a non-redundant set of subgraphs with statistically significant associations to one or more graph groups. Through an extensive set of experiments on a wide range of synthetic and real-world graph groups, we confirm that Gragra works well in practice",
    "checked": true,
    "id": "4a9f46c63200541963d7bd2e62648d8d2566a493",
    "semantic_title": "differentially describing groups of graphs",
    "citation_count": 1,
    "authors": [
      "Corinna Coupette",
      "Sebastian Dalleiger",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20313": {
    "title": "Molecular Contrastive Learning with Chemical Element Knowledge Graph",
    "volume": "main",
    "abstract": "Molecular representation learning contributes to multiple downstream tasks such as molecular property prediction and drug design. To properly represent molecules, graph contrastive learning is a promising paradigm as it utilizes self-supervision signals and has no requirements for human annotations. However, prior works fail to incorporate fundamental domain knowledge into graph semantics and thus ignore the correlations between atoms that have common attributes but are not directly connected by bonds. To address these issues, we construct a Chemical Element Knowledge Graph (KG) to summarize microscopic associations between elements and propose a novel Knowledge-enhanced Contrastive Learning (KCL) framework for molecular representation learning. KCL framework consists of three modules. The first module, knowledge-guided graph augmentation, augments the original molecular graph based on the Chemical Element KG. The second module, knowledge-aware graph representation, extracts molecular representations with a common graph encoder for the original molecular graph and a Knowledge-aware Message Passing Neural Network (KMPNN) to encode complex information in the augmented molecular graph. The final module is a contrastive objective, where we maximize agreement between these two views of molecular graphs. Extensive experiments demonstrated that KCL obtained superior performances against state-of-the-art baselines on eight molecular datasets. Visualization experiments properly interpret what KCL has learned from atoms and attributes in the augmented molecular graphs",
    "checked": true,
    "id": "67d131c852c394bb748759a1b661671aff378aba",
    "semantic_title": "molecular contrastive learning with chemical element knowledge graph",
    "citation_count": 37,
    "authors": [
      "Yin Fang",
      "Qiang Zhang",
      "Haihong Yang",
      "Xiang Zhuang",
      "Shumin Deng",
      "Wen Zhang",
      "Ming Qin",
      "Zhuo Chen",
      "Xiaohui Fan",
      "Huajun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20314": {
    "title": "Heterogeneity-Aware Twitter Bot Detection with Relational Graph Transformers",
    "volume": "main",
    "abstract": "Twitter bot detection has become an important and challenging task to combat misinformation and protect the integrity of the online discourse. State-of-the-art approaches generally leverage the topological structure of the Twittersphere, while they neglect the heterogeneity of relations and influence among users. In this paper, we propose a novel bot detection framework to alleviate this problem, which leverages the topological structure of user-formed heterogeneous graphs and models varying influence intensity between users. Specifically, we construct a heterogeneous information network with users as nodes and diversified relations as edges. We then propose relational graph transformers to model heterogeneous influence between users and learn node representations. Finally, we use semantic attention networks to aggregate messages across users and relations and conduct heterogeneity-aware Twitter bot detection. Extensive experiments demonstrate that our proposal outperforms state-of-the-art methods on a comprehensive Twitter bot detection benchmark. Additional studies also bear out the effectiveness of our proposed relational graph transformers, semantic attention networks and the graph-based approach in general",
    "checked": true,
    "id": "69db89e6debb78e22f78a4443415e84ca735aea3",
    "semantic_title": "heterogeneity-aware twitter bot detection with relational graph transformers",
    "citation_count": 32,
    "authors": [
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Rui Li",
      "Minnan Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20315": {
    "title": "Subspace Differential Privacy",
    "volume": "main",
    "abstract": "Many data applications have certain invariant constraints due to practical needs. Data curators who employ differential privacy need to respect such constraints on the sanitized data product as a primary utility requirement. Invariants challenge the formulation, implementation, and interpretation of privacy guarantees. We propose subspace differential privacy, to honestly characterize the dependence of the sanitized output on confidential aspects of the data. We discuss two design frameworks that convert well-known differentially private mechanisms, such as the Gaussian and the Laplace mechanisms, to subspace differentially private ones that respect the invariants specified by the curator. For linear queries, we discuss the design of near-optimal mechanisms that minimize the mean squared error. Subspace differentially private mechanisms rid the need for post-processing due to invariants, preserve transparency and statistical intelligibility of the output, and can be suitable for distributed implementation. We showcase the proposed mechanisms on the 2020 Census Disclosure Avoidance demonstration data, and a spatio-temporal dataset of mobile access point connections on a large university campus",
    "checked": true,
    "id": "342c8ef336bcdf2c12fe6438857d7a8933a72027",
    "semantic_title": "subspace differential privacy",
    "citation_count": 9,
    "authors": [
      "Jie Gao",
      "Ruobin Gong",
      "Fang-Yi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20316": {
    "title": "Orthogonal Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have received tremendous attention due to their superiority in learning node representations. These models rely on message passing and feature transformation functions to encode the structural and feature information from neighbors. However, stacking more convolutional layers significantly decreases the performance of GNNs. Most recent studies attribute this limitation to the over-smoothing issue, where node embeddings converge to indistinguishable vectors. Through a number of experimental observations, we argue that the main factor degrading the performance is the unstable forward normalization and backward gradient resulted from the improper design of the feature transformation, especially for shallow GNNs where the over-smoothing has not happened. Therefore, we propose a novel orthogonal feature transformation, named Ortho-GConv, which could generally augment the existing GNN backbones to stabilize the model training and improve the model's generalization performance. Specifically, we maintain the orthogonality of the feature transformation comprehensively from three perspectives, namely hybrid weight initialization, orthogonal transformation, and orthogonal regularization. By equipping the existing GNNs (e.g. GCN, JKNet, GCNII) with Ortho-GConv, we demonstrate the generality of the orthogonal feature transformation to enable stable training, and show its effectiveness for node and graph classification tasks",
    "checked": true,
    "id": "04319217b303156f5aa1c58b13de33d49fe8a3a8",
    "semantic_title": "orthogonal graph neural networks",
    "citation_count": 13,
    "authors": [
      "Kai Guo",
      "Kaixiong Zhou",
      "Xia Hu",
      "Yu Li",
      "Yi Chang",
      "Xin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20317": {
    "title": "Learning Temporal Point Processes for Efficient Retrieval of Continuous Time Event Sequences",
    "volume": "main",
    "abstract": "Recent developments in predictive modeling using marked temporal point processes (MTPPs) have enabled an accurate characterization of several real-world applications involving continuous-time event sequences (CTESs). However, the retrieval problem of such sequences remains largely unaddressed in literature. To tackle this, we propose NEUROSEQRET which learns to retrieve and rank a relevant set of continuous-time event sequences for a given query sequence, from a large corpus of sequences. More specifically, NEUROSEQRET first applies a trainable unwarping function on the query sequence, which makes it comparable with corpus sequences, especially when a relevant query-corpus pair has individually different attributes. Next, it feeds the unwarped query sequence and the corpus sequence into MTPP guided neural relevance models. We develop two variants of the relevance model which offer a tradeoff between accuracy and efficiency. We also propose an optimization framework to learn binary sequence embeddings from the relevance scores, suitable for the locality-sensitive hashing leading to a significant speedup in returning top-K results for a given query sequence. Our experiments with several datasets show the significant accuracy boost of NEUROSEQRET beyond several baselines, as well as the efficacy of our hashing mechanism",
    "checked": true,
    "id": "c80d7bfd847c9e30cf7e6e780d100d595a47f895",
    "semantic_title": "learning temporal point processes for efficient retrieval of continuous time event sequences",
    "citation_count": 7,
    "authors": [
      "Vinayak Gupta",
      "Srikanta Bedathur",
      "Abir De"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20318": {
    "title": "GNN-Retro: Retrosynthetic Planning with Graph Neural Networks",
    "volume": "main",
    "abstract": "Retrosynthetic planning plays an important role in the field of organic chemistry, which could generate a synthetic route for the target product. The synthetic route is a series of reactions which are started from the available molecules. The most challenging problem in the generation of the synthetic route is the large search space of the candidate reactions. Estimating the cost of candidate reactions has been proved effectively to prune the search space, which could achieve a higher accuracy with the same search iteration. And the estimation of one reaction is comprised of the estimations of all its reactants. So, how to estimate the cost of these reactants will directly influence the quality of results. To get a better performance, we propose a new framework, named GNN-Retro, for retrosynthetic planning problem by combining graph neural networks(GNN) and the latest search algorithm. The structure of GNN in our framework could incorporate the information of neighboring molecules, which will improve the estimation accuracy of our framework. The experiments on the USPTO dataset show that our framework could outperform the state-of-the-art methods with a large margin under the same settings",
    "checked": true,
    "id": "fb9b736ecad8607ee0240e83ad225c4ac90d638c",
    "semantic_title": "gnn-retro: retrosynthetic planning with graph neural networks",
    "citation_count": 15,
    "authors": [
      "Peng Han",
      "Peilin Zhao",
      "Chan Lu",
      "Junzhou Huang",
      "Jiaxiang Wu",
      "Shuo Shang",
      "Bin Yao",
      "Xiangliang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20319": {
    "title": "Block Modeling-Guided Graph Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Graph Convolutional Network (GCN) has shown remarkable potential of exploring graph representation. However, the GCN aggregating mechanism fails to generalize to networks with heterophily where most nodes have neighbors from different classes, which commonly exists in real-world networks. In order to make the propagation and aggregation mechanism of GCN suitable for both homophily and heterophily (or even their mixture), we introduce block modelling into the framework of GCN so that it can realize \"block-guided classified aggregation\", and automatically learn the corresponding aggregation rules for neighbors of different classes. By incorporating block modelling into the aggregation process, GCN is able to automatically aggregate information from homophilic and heterophilic neighbors discriminately according to their homophily degree. We compared our algorithm with state-of-art methods which deal with the heterophily problem. Empirical results demonstrate the superiority of our new approach over existing methods in heterophilic datasets while maintaining a competitive performance in homophilic datasets",
    "checked": true,
    "id": "98b74858831ae4218d91de91f980893eb13ce870",
    "semantic_title": "block modeling-guided graph convolutional neural networks",
    "citation_count": 28,
    "authors": [
      "Dongxiao He",
      "Chundong Liang",
      "Huixin Liu",
      "Mingxiang Wen",
      "Pengfei Jiao",
      "Zhiyong Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20320": {
    "title": "CATN: Cross Attentive Tree-Aware Network for Multivariate Time Series Forecasting",
    "volume": "main",
    "abstract": "Modeling complex hierarchical and grouped feature interaction in the multivariate time series data is indispensable to comprehend the data dynamics and predicting the future condition. The implicit feature interaction and high-dimensional data make multivariate forecasting very challenging. Many existing works did not put more emphasis on exploring explicit correlation among multiple time series data, and complicated models are designed to capture long- and short-range pattern with the aid of attention mechanism. In this work, we think that pre-defined graph or general learning method is difficult due to their irregular structure. Hence, we present CATN, an end-to-end model of Cross Attentive Tree-aware Network to jointly capture the inter-series correlation and intra-series temporal pattern. We first construct a tree structure to learn hierarchical and grouped correlation and design an embedding approach that can pass dynamic message to generalize implicit but interpretable cross features among multiple time series. Next in temporal aspect, we propose a multi-level dependency learning mechanism including global&local learning and cross attention mechanism, which can combine long-range dependencies, short-range dependencies as well as cross dependencies at different time steps. The extensive experiments on different datasets from real world show the effectiveness and robustness of the method we proposed when compared with existing state-of-the-art methods",
    "checked": true,
    "id": "c5b23e07f786a7a8616f19d3ed2bd2aeb0152d0d",
    "semantic_title": "catn: cross attentive tree-aware network for multivariate time series forecasting",
    "citation_count": 9,
    "authors": [
      "Hui He",
      "Qi Zhang",
      "Simeng Bai",
      "Kun Yi",
      "Zhendong Niu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20321": {
    "title": "FPAdaMetric: False-Positive-Aware Adaptive Metric Learning for Session-Based Recommendation",
    "volume": "main",
    "abstract": "Modern recommendation systems are mostly based on implicit feedback data which can be quite noisy due to false positives (FPs) caused by many reasons, such as misclicks or quick curiosity. Numerous recommendation algorithms based on collaborative filtering have leveraged post-click user behavior (e.g., skip) to identify false positives. They effectively involved these false positives in the model supervision as negative-like signals. Yet, false positives had not been considered in existing session-based recommendation systems (SBRs) although they provide just as deleterious effects. To resolve false positives in SBRs, we first introduce FP-Metric model which reformulates the objective of the session-based recommendation with FP constraints into metric learning regularization. In addition, we propose FP-AdaMetric that enhances the metric-learning regularization terms with an adaptive module that elaborately calculates the impact of FPs inside sequential patterns. We verify that FP-AdaMetric improves several session-based recommendation models' performances in terms of Hit Rate (HR), MRR, and NDCG on datasets from different domains including music, movie, and game. Furthermore, we show that the adaptive module plays a much more crucial role in FP-AdaMetric model than in other baselines",
    "checked": true,
    "id": "448f82280ccb085bc7cb2338faa2fdcfc3787676",
    "semantic_title": "fpadametric: false-positive-aware adaptive metric learning for session-based recommendation",
    "citation_count": 1,
    "authors": [
      "Jongwon Jeong",
      "Jeong Choi",
      "Hyunsouk Cho",
      "Sehee Chung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20322": {
    "title": "STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction",
    "volume": "main",
    "abstract": "High-performance traffic flow prediction model designing, a core technology of Intelligent Transportation System, is a long-standing but still challenging task for industrial and academic communities. The lack of integration between physical principles and data-driven models is an important reason for limiting the development of this field. In the literature, physics-based methods can usually provide a clear interpretation of the dynamic process of traffic flow systems but are with limited accuracy, while data-driven methods, especially deep learning with black-box structures, can achieve improved performance but can not be fully trusted due to lack of a reasonable physical basis. To bridge the gap between purely data-driven and physics-driven approaches, we propose a physics-guided deep learning model named Spatio-Temporal Differential Equation Network (STDEN), which casts the physical mechanism of traffic flow dynamics into a deep neural network framework. Specifically, we assume the traffic flow on road networks is driven by a latent potential energy field (like water flows are driven by the gravity field), and model the spatio-temporal dynamic process of the potential energy field as a differential equation network. STDEN absorbs both the performance advantage of data-driven models and the interpretability of physics-based models, so is named a physics-guided prediction model. Experiments on three real-world traffic datasets in Beijing show that our model outperforms state-of-the-art baselines by a significant margin. A case study further verifies that STDEN can capture the mechanism of urban traffic and generate accurate predictions with physical meaning. The proposed framework of differential equation network modeling may also cast light on other similar applications",
    "checked": true,
    "id": "f999e08a588b7822c25ad7d34b3a120515ddc3cc",
    "semantic_title": "stden: towards physics-guided neural networks for traffic flow prediction",
    "citation_count": 29,
    "authors": [
      "Jiahao Ji",
      "Jingyuan Wang",
      "Zhe Jiang",
      "Jiawei Jiang",
      "Hu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20323": {
    "title": "Naming the Most Anomalous Cluster in Hilbert Space for Structures with Attribute Information",
    "volume": "main",
    "abstract": "We consider datasets consisting of arbitrarily structured entities (e.g., molecules, sequences, graphs, etc) whose similarity can be assessed with a reproducing ker- nel (or a family thereof). These entities are assumed to additionally have a set of named attributes (e.g.: number_of_atoms, stock_price, etc). These attributes can be used to classify the structured entities in discrete sets (e.g., ‘number_of_atoms < 3', ‘stock_price ≤ 100', etc) and can effectively serve as Boolean predicates. Our goal is to use this side-information to provide explain- able kernel-based clustering. To this end, we propose a method which is able to find among all possible entity subsets that can be described as a conjunction of the available predicates either a) the optimal cluster within the Reproducing Kernel Hilbert Space, or b) the most anomalous subset within the same space. Our method works employs combinatorial optimisation via an adaptation of the Maximum-Mean-Discrepancy measure that captures the above intuition. Finally, we propose a criterion to select the optimal one out of a family of kernels in a way that preserves the available side-information. We provide several real world datasets that demonstrate the usefulness of our proposed method",
    "checked": true,
    "id": "ac9a3efc13c10c5c20db9c61e93159d2582dfe9c",
    "semantic_title": "naming the most anomalous cluster in hilbert space for structures with attribute information",
    "citation_count": 0,
    "authors": [
      "Janis Kalofolias",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20324": {
    "title": "Meta-Learning for Online Update of Recommender Systems",
    "volume": "main",
    "abstract": "Online recommender systems should be always aligned with users' current interest to accurately suggest items that each user would like. Since user interest usually evolves over time, the update strategy should be flexible to quickly catch users' current interest from continuously generated new user-item interactions. Existing update strategies focus either on the importance of each user-item interaction or the learning rate for each recommender parameter, but such one-directional flexibility is insufficient to adapt to varying relationships between interactions and parameters. In this paper, we propose MeLON, a meta-learning based novel online recommender update strategy that supports two-directional flexibility. It is featured with an adaptive learning rate for each parameter-interaction pair for inducing a recommender to quickly learn users' up-to-date interest. The procedure of MeLON is optimized following a meta-learning approach: it learns how a recommender learns to generate the optimal learning rates for future updates. Specifically, MeLON first enriches the meaning of each interaction based on previous interactions and identifies the role of each parameter for the interaction; and then combines these two pieces of information to generate an adaptive learning rate. Theoretical analysis and extensive evaluation on three real-world online recommender datasets validate the effectiveness of MeLON",
    "checked": true,
    "id": "d3a88b229ee1eba9a350b75ef58cf41c4d1b1479",
    "semantic_title": "meta-learning for online update of recommender systems",
    "citation_count": 9,
    "authors": [
      "Minseok Kim",
      "Hwanjun Song",
      "Yooju Shin",
      "Dongmin Park",
      "Kijung Shin",
      "Jae-Gil Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20325": {
    "title": "The Triangle-Densest-K-Subgraph Problem: Hardness, Lovász Extension, and Application to Document Summarization",
    "volume": "main",
    "abstract": "We introduce the triangle-densest-K-subgraph problem (TDKS) for undirected graphs: given a size parameter K, compute a subset of K vertices that maximizes the number of induced triangles. The problem corresponds to the simplest generalization of the edge based densest-K-subgraph problem (DKS) to the case of higher-order network motifs. We prove that TDKS is NP-hard and is not amenable to efficient approximation, in the worst-case. By judiciously exploiting the structure of the problem, we propose a relaxation algorithm for the purpose of obtaining high-quality, sub-optimal solutions. Our approach utilizes the fact that the cost function of TDKS is submodular to construct a convex relaxation for the problem based on the Lovász extension for submodular functions. We demonstrate that our approaches attain state-of-the-art performance on real-world graphs and can offer substantially improved exploration of the optimal density-size curve compared to sophisticated approximation baselines for DKS. We use document summarization to showcase why TDKS is a useful generalization of DKS",
    "checked": true,
    "id": "b2e45527e56d781b57b5db1f1c0fc364f378d090",
    "semantic_title": "the triangle-densest-k-subgraph problem: hardness, lovász extension, and application to document summarization",
    "citation_count": 2,
    "authors": [
      "Aritra Konar",
      "Nicholas D. Sidiropoulos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20326": {
    "title": "Obtaining Calibrated Probabilities with Personalized Ranking Models",
    "volume": "main",
    "abstract": "For personalized ranking models, the well-calibrated probability of an item being preferred by a user has great practical value. While existing work shows promising results in image classification, probability calibration has not been much explored for personalized ranking. In this paper, we aim to estimate the calibrated probability of how likely a user will prefer an item. We investigate various parametric distributions and propose two parametric calibration methods, namely Gaussian calibration and Gamma calibration. Each proposed method can be seen as a post-processing function that maps the ranking scores of pre-trained models to well-calibrated preference probabilities, without affecting the recommendation performance. We also design the unbiased empirical risk minimization framework that guides the calibration methods to learning of true preference probability from the biased user-item interaction dataset. Extensive evaluations with various personalized ranking models on real-world datasets show that both the proposed calibration methods and the unbiased empirical risk minimization significantly improve the calibration performance",
    "checked": true,
    "id": "2eb715a97a91ec9c183bf0e94d98a796a55c3d25",
    "semantic_title": "obtaining calibrated probabilities with personalized ranking models",
    "citation_count": 6,
    "authors": [
      "Wonbin Kweon",
      "SeongKu Kang",
      "Hwanjo Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20327": {
    "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation",
    "volume": "main",
    "abstract": "In many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as the concept drift in the literature. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work. In this paper, we propose a novel method DDG-DA, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data. We conduct experiments on three real-world tasks (forecasting on stock price trend, electricity load and solar irradiance) and obtained significant improvement on multiple widely-used models",
    "checked": true,
    "id": "5363ca2113e74a294dbeaf2f309c4fd828a0babd",
    "semantic_title": "ddg-da: data distribution generation for predictable concept drift adaptation",
    "citation_count": 10,
    "authors": [
      "Wendi Li",
      "Xiao Yang",
      "Weiqing Liu",
      "Yingce Xia",
      "Jiang Bian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20328": {
    "title": "Unsupervised Anomaly Detection by Robust Density Estimation",
    "volume": "main",
    "abstract": "Density estimation is a widely used method to perform unsupervised anomaly detection. By learning the density function, data points with relatively low densities are classified as anomalies. Unfortunately, the presence of anomalies in training data may significantly impact the density estimation process, thereby imposing significant challenges to the use of more sophisticated density estimation methods such as those based on deep neural networks. In this work, we propose RobustRealNVP, a deep density estimation framework that enhances the robustness of flow-based density estimation methods, enabling their application to unsupervised anomaly detection. RobustRealNVP differs from existing flow-based models from two perspectives. First, RobustRealNVP discards data points with low estimated densities during optimization to prevent them from corrupting the density estimation process. Furthermore, it imposes Lipschitz regularization to the flow-based model to enforce smoothness in the estimated density function. We demonstrate the robustness of our algorithm against anomalies in training data from both theoretical and empirical perspectives. The results show that our algorithm achieves competitive results as compared to state-of-the-art unsupervised anomaly detection methods",
    "checked": true,
    "id": "5aa877991da1b9d826d1600fc428c1cc704b8ca6",
    "semantic_title": "unsupervised anomaly detection by robust density estimation",
    "citation_count": 8,
    "authors": [
      "Boyang Liu",
      "Pang-Ning Tan",
      "Jiayu Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20329": {
    "title": "From One to All: Learning to Match Heterogeneous and Partially Overlapped Graphs",
    "volume": "main",
    "abstract": "Recent years have witnessed a flurry of research activity in graph matching, which aims at finding the correspondence of nodes across two graphs and lies at the heart of many artificial intelligence applications. However, matching heterogeneous graphs with partial overlap remains a challenging problem in real-world applications. This paper proposes the first practical learning-to-match method to meet this challenge. The proposed unsupervised method adopts a novel partial optimal transport paradigm to learn a transport plan and node embeddings simultaneously. In a from-one-to-all manner, the entire learning procedure is decomposed into a series of easy-to-solve sub-procedures, each of which only handles the alignment of a single type of nodes. A mechanism for searching the transport mass is also proposed. Experimental results demonstrate that the proposed method outperforms state-of-the-art graph matching methods",
    "checked": true,
    "id": "2379c49e346e3c5486c1aa4b52770110f5d4f144",
    "semantic_title": "from one to all: learning to match heterogeneous and partially overlapped graphs",
    "citation_count": 0,
    "authors": [
      "Weijie Liu",
      "Hui Qian",
      "Chao Zhang",
      "Jiahao Xie",
      "Zebang Shen",
      "Nenggan Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20330": {
    "title": "TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs",
    "volume": "main",
    "abstract": "Conventional static knowledge graphs model entities in relational data as nodes, connected by edges of specific relation types. However, information and knowledge evolve continuously, and temporal dynamics emerge, which are expected to influence future situations. In temporal knowledge graphs, time information is integrated into the graph by equipping each edge with a timestamp or a time range. Embedding-based methods have been introduced for link prediction on temporal knowledge graphs, but they mostly lack explainability and comprehensible reasoning chains. Particularly, they are usually not designed to deal with link forecasting -- event prediction involving future timestamps. We address the task of link forecasting on temporal knowledge graphs and introduce TLogic, an explainable framework that is based on temporal logical rules extracted via temporal random walks. We compare TLogic with state-of-the-art baselines on three benchmark datasets and show better overall performance while our method also provides explanations that preserve time consistency. Furthermore, in contrast to most state-of-the-art embedding-based methods, TLogic works well in the inductive setting where already learned rules are transferred to related datasets with a common vocabulary",
    "checked": true,
    "id": "e9da2ce19846c5dd2497e323ebbacd991fbe1c20",
    "semantic_title": "tlogic: temporal logical rules for explainable link forecasting on temporal knowledge graphs",
    "citation_count": 36,
    "authors": [
      "Yushan Liu",
      "Yunpu Ma",
      "Marcel Hildebrandt",
      "Mitchell Joblin",
      "Volker Tresp"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20331": {
    "title": "Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity",
    "volume": "main",
    "abstract": "Anomaly detection attempts to find examples in a dataset that do not conform to the expected behavior. Algorithms for this task assign an anomaly score to each example representing its degree of anomalousness. Setting a threshold on the anomaly scores enables converting these scores into a discrete prediction for each example. Setting an appropriate threshold is challenging in practice since anomaly detection is often treated as an unsupervised problem. A common approach is to set the threshold based on the dataset's contamination factor, i.e., the proportion of anomalous examples in the data. While the contamination factor may be known based on domain knowledge, it is often necessary to estimate it by labeling data. However, many anomaly detection problems involve monitoring multiple related, yet slightly different entities (e.g., a fleet of machines). Then, estimating the contamination factor for each dataset separately by labeling data would be extremely time-consuming. Therefore, this paper introduces a method for transferring the known contamination factor from one dataset (the source domain) to a related dataset where it is unknown (the target domain). Our approach does not require labeled target data and is based on modeling the shape of the distribution of the anomaly scores in both domains. We theoretically analyze how our method behaves when the (biased) target domain anomaly score distribution converges to its true one. Empirically, our method outperforms several baselines on real-world datasets",
    "checked": true,
    "id": "b68b9b84f3a42abceffb2bc792aaaef0728a8983",
    "semantic_title": "transferring the contamination factor between anomaly detection domains by shape similarity",
    "citation_count": 8,
    "authors": [
      "Lorenzo Perini",
      "Vincent Vercruyssen",
      "Jesse Davis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20332": {
    "title": "Unifying Knowledge Base Completion with PU Learning to Mitigate the Observation Bias",
    "volume": "main",
    "abstract": "Methods for Knowledge Base Completion (KBC) reason about a knowledge base (KB) in order to derive new facts that should be included in the KB. This is challenging for two reasons. First, KBs only contain positive examples. This complicates model evaluation which needs both positive and negative examples. Second, those facts that were selected to be included in the knowledge base, are most likely not an i.i.d. sample of the true facts, due to the way knowledge bases are constructed. In this paper, we focus on rule-based approaches, which traditionally address the first challenge by making assumptions that enable identifying negative examples, which in turn makes it possible to compute a rule's confidence or precision. However, they largely ignore the second challenge, which means that their estimates of a rule's confidence can be biased. This paper approaches rule-based KBC through the lens of PU-learning, which can cope with both challenges. We make three contributions.: (1) We provide a unifying view that formalizes the relationship between multiple existing confidences measures based on (i) what assumption they make about and (ii) how their accuracy depends on the selection mechanism. (2) We introduce two new confidence measures that can mitigate known biases by using propensity scores that quantify how likely a fact is to be included the KB. (3) We show through theoretical and empirical analysis that taking the bias into account improves the confidence estimates, even when the propensity scores are not known exactly",
    "checked": true,
    "id": "82d60aae09d5c06ef3156fd97d5ef311d8ef93bb",
    "semantic_title": "unifying knowledge base completion with pu learning to mitigate the observation bias",
    "citation_count": 1,
    "authors": [
      "Jonas Schouterden",
      "Jessa Bekker",
      "Jesse Davis",
      "Hendrik Blockeel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20333": {
    "title": "A Self-Supervised Mixed-Curvature Graph Neural Network",
    "volume": "main",
    "abstract": "Graph representation learning received increasing attentions in recent years. Most of the existing methods ignore the complexity of the graph structures and restrict graphs in a single constant-curvature representation space, which is only suitable to particular kinds of graph structure indeed. Additionally, these methods follow the supervised or semi-supervised learning paradigm, and thereby notably limit their deployment on the unlabeled graphs in real applications. To address these aforementioned limitations, we take the first attempt to study the self-supervised graph representation learning in the mixed-curvature spaces. In this paper, we present a novel Self-Supervised Mixed-Curvature Graph Neural Network (SelfMGNN). To capture the complex graph structures, we construct a mixed-curvature space via the Cartesian product of multiple Riemannian component spaces, and design hierarchical attention mechanisms for learning and fusing graph representations across these component spaces. To enable the self-supervised learning, we propose a novel dual contrastive approach. The constructed mixed-curvature space actually provides multiple Riemannian views for the contrastive learning. We introduce a Riemannian projector to reveal these views, and utilize a well-designed Riemannian discriminator for the single-view and cross-view contrastive learning within and across the Riemannian views. Finally, extensive experiments show that SelfMGNN captures the complex graph structures and outperforms state-of-the-art baselines",
    "checked": true,
    "id": "745e5ea4223575d460e3c7422b04d7c06ce01b3c",
    "semantic_title": "a self-supervised mixed-curvature graph neural network",
    "citation_count": 15,
    "authors": [
      "Li Sun",
      "Zhongbao Zhang",
      "Junda Ye",
      "Hao Peng",
      "Jiawei Zhang",
      "Sen Su",
      "Philip  S Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20334": {
    "title": "MS-HGAT: Memory-Enhanced Sequential Hypergraph Attention Network for Information Diffusion Prediction",
    "volume": "main",
    "abstract": "Predicting the diffusion cascades is a critical task to understand information spread on social networks. Previous methods usually focus on the order or structure of the infected users in a single cascade, thus ignoring the global dependencies of users and cascades, limiting the performance of prediction. Current strategies to introduce social networks only learn the social homogeneity among users, which is not enough to describe their interaction preferences, let alone the dynamic changes. To address the above issues, we propose a novel information diffusion prediction model named Memory-enhanced Sequential Hypergraph Attention Networks (MS-HGAT). Specifically, to introduce the global dependencies of users, we not only take advantages of their friendships, but also consider their interactions at the cascade level. Furthermore, to dynamically capture user' preferences, we divide the diffusion hypergraph into several sub graphs based on timestamps, develop Hypergraph Attention Networks to learn the sequential hypergraphs, and connect them with gated fusion strategy. In addition, a memory-enhanced embedding lookup module is proposed to capture the learned user representations into the cascade-specific embedding space, thus highlighting the feature interaction within the cascade. The experimental results over four realistic datasets demonstrate that MS-HGAT significantly outperforms the state-of-the-art diffusion prediction models in both Hits@K and MAP@k metrics",
    "checked": true,
    "id": "cd29ac0d85605fc9054911303bfef03ed3127d1a",
    "semantic_title": "ms-hgat: memory-enhanced sequential hypergraph attention network for information diffusion prediction",
    "citation_count": 12,
    "authors": [
      "Ling Sun",
      "Yuan Rao",
      "Xiangbo Zhang",
      "Yuqian Lan",
      "Shuanghe Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20335": {
    "title": "Graph Structure Learning with Variational Information Bottleneck",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real-world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL is the first attempt to advance the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of the proposed VIB-GSL",
    "checked": true,
    "id": "7fc37fbd2f808984cfc5c78410a993f89eb0ef49",
    "semantic_title": "graph structure learning with variational information bottleneck",
    "citation_count": 51,
    "authors": [
      "Qingyun Sun",
      "Jianxin Li",
      "Hao Peng",
      "Jia Wu",
      "Xingcheng Fu",
      "Cheng Ji",
      "Philip  S Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20336": {
    "title": "Heterogeneous Peer Effects in the Linear Threshold Model",
    "volume": "main",
    "abstract": "The Linear Threshold Model is a widely used model that describes how information diffuses through a social network. According to this model, an individual adopts an idea or product after the proportion of their neighbors who have adopted it reaches a certain threshold. Typical applications of the Linear Threshold Model assume that thresholds are either the same for all network nodes or randomly distributed, even though some people may be more susceptible to peer pressure than others. To address individual-level differences, we propose causal inference methods for estimating individual thresholds that can more accurately predict whether and when individuals will be affected by their peers. We introduce the concept of heterogeneous peer effects and develop a Structural Causal Model which corresponds to the Linear Threshold Model and supports heterogeneous peer effect identification and estimation. We develop two algorithms for individual threshold estimation, one based on causal trees and one based on causal meta-learners. Our experimental results on synthetic and real- world datasets show that our proposed models can better predict individual-level thresholds in the Linear Threshold Model and thus more precisely predict which nodes will get activated over time",
    "checked": true,
    "id": "4f45950443902323a28bd45e1072df6f51fb1db7",
    "semantic_title": "heterogeneous peer effects in the linear threshold model",
    "citation_count": 4,
    "authors": [
      "Christopher Tran",
      "Elena Zheleva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20337": {
    "title": "Exploring Relational Semantics for Inductive Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Knowledge graph completion (KGC) aims to infer missing information in incomplete knowledge graphs (KGs). Most previous works only consider the transductive scenario where entities are existing in KGs, which cannot work effectively for the inductive scenario containing emerging entities. Recently some graph neural network-based methods have been proposed for inductive KGC by aggregating neighborhood information to capture some uncertainty semantics from the neighboring auxiliary triples. But these methods ignore the more general relational semantics underlying all the known triples that can provide richer information to represent emerging entities so as to satisfy the inductive scenario. In this paper, we propose a novel model called CFAG, which utilizes two granularity levels of relational semantics in a coarse-grained aggregator (CG-AGG) and a fine-grained generative adversarial net (FG-GAN), for inductive KGC. The CG-AGG firstly generates entity representations with multiple semantics through a hypergraph neural network-based global aggregator and a graph neural network-based local aggregator, and the FG-GAN further enhances entity representations with specific semantics through conditional generative adversarial nets. Experimental results on benchmark datasets show that our model outperforms state-of-the-art models for inductive KGC",
    "checked": true,
    "id": "397491a080855339052ed61f8c60778225851c6d",
    "semantic_title": "exploring relational semantics for inductive knowledge graph completion",
    "citation_count": 6,
    "authors": [
      "Changjian Wang",
      "Xiaofei Zhou",
      "Shirui Pan",
      "Linhua Dong",
      "Zeliang Song",
      "Ying Sha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20338": {
    "title": "HAGEN: Homophily-Aware Graph Convolutional Recurrent Network for Crime Forecasting",
    "volume": "main",
    "abstract": "The goal of the crime forecasting problem is to predict different types of crimes for each geographical region (like a neighborhood or censor tract) in the near future. Since nearby regions usually have similar socioeconomic characteristics which indicate similar crime patterns, recent state-of-the-art solutions constructed a distance-based region graph and utilized Graph Neural Network (GNN) techniques for crime forecasting, because the GNN techniques could effectively exploit the latent relationships between neighboring region nodes in the graph if the edges reveal high dependency or correlation. However, this distance-based pre-defined graph can not fully capture crime correlation between regions that are far from each other but share similar crime patterns. Hence, to make a more accurate crime prediction, the main challenge is to learn a better graph that reveals the dependencies between regions in crime occurrences and meanwhile captures the temporal patterns from historical crime records. To address these challenges, we propose an end-to-end graph convolutional recurrent network called HAGEN with several novel designs for crime prediction. Specifically, our framework could jointly capture the crime correlation between regions and the temporal crime dynamics by combining an adaptive region graph learning module with the Diffusion Convolution Gated Recurrent Unit (DCGRU). Based on the homophily assumption of GNN (i.e., graph convolution works better where neighboring nodes share the same label), we propose a homophily-aware constraint to regularize the optimization of the region graph so that neighboring region nodes on the learned graph share similar crime patterns, thus fitting the mechanism of diffusion convolution. Empirical experiments and comprehensive analysis on two real-world datasets showcase the effectiveness of HAGEN",
    "checked": true,
    "id": "a08bbf1956eb4137371334b138c6304d61d69771",
    "semantic_title": "hagen: homophily-aware graph convolutional recurrent network for crime forecasting",
    "citation_count": 14,
    "authors": [
      "Chenyu Wang",
      "Zongyu Lin",
      "Xiaochen Yang",
      "Jiao Sun",
      "Mingxuan Yue",
      "Cyrus Shahabi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20339": {
    "title": "Calibrated Nonparametric Scan Statistics for Anomalous Pattern Detection in Graphs",
    "volume": "main",
    "abstract": "We propose a new approach, the calibrated nonparametric scan statistic (CNSS), for more accurate detection of anomalous patterns in large-scale, real-world graphs. Scan statistics identify connected subgraphs that are interesting or unexpected through maximization of a likelihood ratio statistic; in particular, nonparametric scan statistics (NPSSs) identify subgraphs with a higher than expected proportion of individually significant nodes. However, we show that recently proposed NPSS methods are miscalibrated, failing to account for the maximization of the statistic over the multiplicity of subgraphs. This results in both reduced detection power for subtle signals, and low precision of the detected subgraph even for stronger signals. Thus we develop a new statistical approach to recalibrate NPSSs, correctly adjusting for multiple hypothesis testing and taking the underlying graph structure into account. While the recalibration, based on randomization testing, is computationally expensive, we propose both an efficient (approximate) algorithm and new, closed-form lower bounds (on the expected maximum proportion of significant nodes for subgraphs of a given size, under the null hypothesis of no anomalous patterns). These advances, along with the integration of recent core-tree decomposition methods, enable CNSS to scale to large real-world graphs, with substantial improvement in the accuracy of detected subgraphs. Extensive experiments on both semi-synthetic and real-world datasets are demonstrated to validate the effectiveness of our proposed methods, in comparison with state-of-the-art counterparts",
    "checked": true,
    "id": "c32c10e3bf0ec2133ead4475c86b92e329d9c852",
    "semantic_title": "calibrated nonparametric scan statistics for anomalous pattern detection in graphs",
    "citation_count": 2,
    "authors": [
      "Chunpai Wang",
      "Daniel B. Neill",
      "Feng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20340": {
    "title": "Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily",
    "volume": "main",
    "abstract": "Graph Convolutional Networks (GCNs) have been widely applied in various fields due to their significant power on processing graph-structured data. Typical GCN and its variants work under a homophily assumption (i.e., nodes with same class are prone to connect to each other), while ignoring the heterophily which exists in many real-world networks (i.e., nodes with different classes tend to form edges). Existing methods deal with heterophily by mainly aggregating higher-order neighborhoods or combing the immediate representations, which leads to noise and irrelevant information in the result. But these methods did not change the propagation mechanism which works under homophily assumption (that is a fundamental part of GCNs). This makes it difficult to distinguish the representation of nodes from different classes. To address this problem, in this paper we design a novel propagation mechanism, which can automatically change the propagation and aggregation process according to homophily or heterophily between node pairs. To adaptively learn the propagation process, we introduce two measurements of homophily degree between node pairs, which is learned based on topological and attribute information, respectively. Then we incorporate the learnable homophily degree into the graph convolution framework, which is trained in an end-to-end schema, enabling it to go beyond the assumption of homophily. More importantly, we theoretically prove that our model can constrain the similarity of representations between nodes according to their homophily degree. Experiments on seven real-world datasets demonstrate that this new approach outperforms the state-of-the-art methods under heterophily or low homophily, and gains competitive performance under homophily",
    "checked": true,
    "id": "3c9bcaf2201373efb4a18f62208710932af8321a",
    "semantic_title": "powerful graph convolutional networks with adaptive propagation mechanism for homophily and heterophily",
    "citation_count": 5,
    "authors": [
      "Tao Wang",
      "Di Jin",
      "Rui Wang",
      "Dongxiao He",
      "Yuxiao Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20341": {
    "title": "ShuttleNet: Position-Aware Fusion of Rally Progress and Player Styles for Stroke Forecasting in Badminton",
    "volume": "main",
    "abstract": "The increasing demand for analyzing the insights in sports has stimulated a line of productive studies from a variety of perspectives, e.g., health state monitoring, outcome prediction. In this paper, we focus on objectively judging what and where to return strokes, which is still unexplored in turn-based sports. By formulating stroke forecasting as a sequence prediction task, existing works can tackle the problem but fail to model information based on the characteristics of badminton. To address these limitations, we propose a novel Position-aware Fusion of Rally Progress and Player Styles framework (ShuttleNet) that incorporates rally progress and information of the players by two modified encoder-decoder extractors. Moreover, we design a fusion network to integrate rally contexts and contexts of the players by conditioning on information dependency and different positions. Extensive experiments on the badminton dataset demonstrate that ShuttleNet significantly outperforms the state-of-the-art methods and also empirically validates the feasibility of each component in ShuttleNet. On top of that, we provide an analysis scenario for the stroke forecasting problem",
    "checked": true,
    "id": "9145c3426b54be4ab7013d20fc584be76d2e2d58",
    "semantic_title": "shuttlenet: position-aware fusion of rally progress and player styles for stroke forecasting in badminton",
    "citation_count": 21,
    "authors": [
      "Wei-Yao Wang",
      "Hong-Han Shuai",
      "Kai-Shiang Chang",
      "Wen-Chih Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20342": {
    "title": "Event-Aware Multimodal Mobility Nowcasting",
    "volume": "main",
    "abstract": "As a decisive part in the success of Mobility-as-a-Service (MaaS), spatio-temporal predictive modeling for crowd movements is a challenging task particularly considering scenarios where societal events drive mobility behavior deviated from the normality. While tremendous progress has been made to model high-level spatio-temporal regularities with deep learning, most, if not all of the existing methods are neither aware of the dynamic interactions among multiple transport modes nor adaptive to unprecedented volatility brought by potential societal events. In this paper, we are therefore motivated to improve the canonical spatio-temporal network (ST-Net) from two perspectives: (1) design a heterogeneous mobility information network (HMIN) to explicitly represent intermodality in multimodal mobility; (2) propose a memory-augmented dynamic filter generator (MDFG) to generate sequence-specific parameters in an on-the-fly fashion for various scenarios. The enhanced event-aware spatio-temporal network, namely EAST-Net, is evaluated on several real-world datasets with a wide variety and coverage of societal events. Both quantitative and qualitative experimental results verify the superiority of our approach compared with the state-of-the-art baselines. Code and data are published on https://github.com/underdoc-wang/EAST-Net",
    "checked": true,
    "id": "5f447bc454eb572374751576ee0932e5ecba7f8b",
    "semantic_title": "event-aware multimodal mobility nowcasting",
    "citation_count": 14,
    "authors": [
      "Zhaonan Wang",
      "Renhe Jiang",
      "Hao Xue",
      "Flora D. Salim",
      "Xuan Song",
      "Ryosuke Shibasaki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20343": {
    "title": "Discovering Interpretable Data-to-Sequence Generators",
    "volume": "main",
    "abstract": "We study the problem of predicting an event sequence given some meta data. In particular, we are interested in learning easily interpretable models that can accurately generate a sequence based on an attribute vector. To this end, we propose to learn a sparse event-flow graph over the training sequences, and statistically robust rules that use meta data to determine which paths to follow. We formalize the problem in terms of the Minimum Description Length (MDL) principle, by which we identify the best model as the one that compresses the data best. As the resulting optimization problem is NP-hard, we propose the efficient ConSequence algorithm to discover good event-flow graphs from data. Through an extensive set of experiments including a case study, we show that it ably discovers compact, interpretable and accurate models for the generation and prediction of event sequences from data, has a low sample complexity, and is particularly robust against noise",
    "checked": true,
    "id": "5470dc35d234a36c174a6e7ea99c45acc7390f4b",
    "semantic_title": "discovering interpretable data-to-sequence generators",
    "citation_count": 1,
    "authors": [
      "Boris Wiegand",
      "Dietrich Klakow",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20344": {
    "title": "DeepGPD: A Deep Learning Approach for Modeling Geospatio-Temporal Extreme Events",
    "volume": "main",
    "abstract": "Geospatio-temporal data are pervasive across numerous application domains.These rich datasets can be harnessed to predict extreme events such as disease outbreaks, flooding, crime spikes, etc. However, since the extreme events are rare, predicting them is a hard problem. Statistical methods based on extreme value theory provide a systematic way for modeling the distribution of extreme values. In particular, the generalized Pareto distribution (GPD) is useful for modeling the distribution of excess values above a certain threshold. However, applying such methods to large-scale geospatio-temporal data is a challenge due to the difficulty in capturing the complex spatial relationships between extreme events at multiple locations. This paper presents a deep learning framework for long-term prediction of the distribution of extreme values at different locations. We highlight its computational challenges and present a novel framework that combines convolutional neural networks with deep set and GPD. We demonstrate the effectiveness of our approach on a real-world dataset for modeling extreme climate events",
    "checked": true,
    "id": "92b818a39e0eae157c15e73f1422d19f08f6831a",
    "semantic_title": "deepgpd: a deep learning approach for modeling geospatio-temporal extreme events",
    "citation_count": 6,
    "authors": [
      "Tyler Wilson",
      "Pang-Ning Tan",
      "Lifeng Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20345": {
    "title": "SmartIdx: Reducing Communication Cost in Federated Learning by Exploiting the CNNs Structures",
    "volume": "main",
    "abstract": "Top-k sparsification method is popular and powerful forreducing the communication cost in Federated Learning(FL). However, according to our experimental observation, it spends most of the total communication cost on the index of the selected parameters (i.e., their position informa-tion), which is inefficient for FL training. To solve this problem, we propose a FL compression algorithm for convolution neural networks (CNNs), called SmartIdx, by extending the traditional Top-k largest variation selection strategy intothe convolution-kernel-based selection, to reduce the proportion of the index in the overall communication cost and thusachieve a high compression ratio. The basic idea of SmartIdx is to improve the 1:1 proportion relationship betweenthe value and index of the parameters to n:1, by regarding the convolution kernel as the basic selecting unit in parameter selection, which can potentially deliver more informationto the parameter server under the limited network traffic. Tothis end, a set of rules are designed for judging which kernel should be selected and the corresponding packaging strategies are also proposed for further improving the compressionratio. Experiments on mainstream CNNs and datasets show that our proposed SmartIdx performs 2.5×−69.2× higher compression ratio than the state-of-the-art FL compression algorithms without degrading model performance",
    "checked": true,
    "id": "6b089a9b211a41941d7a40af91e4b4ff1ffa31fe",
    "semantic_title": "smartidx: reducing communication cost in federated learning by exploiting the cnns structures",
    "citation_count": 4,
    "authors": [
      "Donglei Wu",
      "Xiangyu Zou",
      "Shuyu Zhang",
      "Haoyu Jin",
      "Wen Xia",
      "Binxing Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20346": {
    "title": "Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data",
    "volume": "main",
    "abstract": "With the vigorous development of multimedia equipments and applications, efficient retrieval of large-scale multi-modal data has become a trendy research topic. Thereinto, hashing has become a prevalent choice due to its retrieval efficiency and low storage cost. Although multi-modal hashing has drawn lots of attention in recent years, there still remain some problems. The first point is that existing methods are mainly designed in batch mode and not able to efficiently handle streaming multi-modal data. The second point is that all existing online multi-modal hashing methods fail to effectively handle unseen new classes which come continuously with streaming data chunks. In this paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS). We design novel semantic-enhanced representation for data, which could help handle the new coming classes, and thereby construct the enhanced semantic objective function. An efficient and effective discrete online optimization algorithm is further proposed for OASIS. Extensive experiments show that our method can exceed the state-of-the-art models. For good reproducibility and benefiting the community, our code and data are already publicly available",
    "checked": true,
    "id": "85a5de740b2c4496a0df6fee2c7206c9a5fef70c",
    "semantic_title": "online enhanced semantic hashing: towards effective and efficient retrieval for streaming multi-modal data",
    "citation_count": 2,
    "authors": [
      "Xiao-Ming Wu",
      "Xin Luo",
      "Yu-Wei Zhan",
      "Chen-Lu Ding",
      "Zhen-Duo Chen",
      "Xin-Shun Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20347": {
    "title": "CoCoS: Enhancing Semi-supervised Learning on Graphs with Unlabeled Data via Contrastive Context Sharing",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have recently become a popular framework for semi-supervised learning on graph-structured data. However, typical GNN models heavily rely on labeled data in the learning process, while ignoring or paying little attention to the data that are unlabeled but available. To make full use of available data, we propose a generic framework, Contrastive Context Sharing (CoCoS), to enhance the learning capacity of GNNs for semi-supervised tasks. By sharing the contextual information among nodes estimated to be in the same class, different nodes can be correlated even if they are unlabeled and remote from each other in the graph. Models can therefore learn different combinations of contextual patterns, which improves the robustness of node representations. Additionally, motivated by recent advances in self-supervised learning, we augment the context sharing strategy by integrating with contrastive learning, which naturally correlates intra-class and inter-class data. Such operations utilize all available data for training and effectively improve a model's learning capacity. CoCoS can be easily extended to a wide range of GNN-based models with little computational overheads. Extensive experiments show that CoCoS considerably enhances typical GNN models, especially when labeled data are sparse in a graph, and achieves state-of-the-art or competitive results in real-world public datasets. The code of CoCoS is available online",
    "checked": true,
    "id": "ff08f6c77d1e8014d2085244e5b5df4f124b89ab",
    "semantic_title": "cocos: enhancing semi-supervised learning on graphs with unlabeled data via contrastive context sharing",
    "citation_count": 2,
    "authors": [
      "Siyue Xie",
      "Da Sun Handason Tam",
      "Wing Cheong Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20348": {
    "title": "Ensemble Semi-supervised Entity Alignment via Cycle-Teaching",
    "volume": "main",
    "abstract": "Entity alignment is to find identical entities in different knowledge graphs. Although embedding-based entity alignment has recently achieved remarkable progress, training data insufficiency remains a critical challenge. Conventional semi-supervised methods also suffer from the incorrect entity alignment in newly proposed training data. To resolve these issues, we design an iterative cycle-teaching framework for semi-supervised entity alignment. The key idea is to train multiple entity alignment models (called aligners) simultaneously and let each aligner iteratively teach its successor the proposed new entity alignment. We propose a diversity-aware alignment selection method to choose reliable entity alignment for each aligner. We also design a conflict resolution mechanism to resolve the alignment conflict when combining the new alignment of an aligner and that from its teacher. Besides, considering the influence of cycle-teaching order, we elaborately design a strategy to arrange the optimal order that can maximize the overall performance of multiple aligners. The cycle-teaching process can break the limitations of each model's learning capability and reduce the noise in new training data, leading to improved performance. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed cycle-teaching framework, which significantly outperforms the state-of-the-art models when the training data is insufficient and the new entity alignment has much noise",
    "checked": true,
    "id": "390313403c892f396380946ff04ffae297d7ef4e",
    "semantic_title": "ensemble semi-supervised entity alignment via cycle-teaching",
    "citation_count": 6,
    "authors": [
      "Kexuan Xin",
      "Zequn Sun",
      "Wen Hua",
      "Bing Liu",
      "Wei Hu",
      "Jianfeng Qu",
      "Xiaofang Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20349": {
    "title": "Unsupervised Adversarially Robust Representation Learning on Graphs",
    "volume": "main",
    "abstract": "Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods",
    "checked": true,
    "id": "aeda9bbae0e2060bd98d597befaab708f01bb557",
    "semantic_title": "unsupervised adversarially robust representation learning on graphs",
    "citation_count": 17,
    "authors": [
      "Jiarong Xu",
      "Yang Yang",
      "Junru Chen",
      "Xin Jiang",
      "Chunping Wang",
      "Jiangang Lu",
      "Yizhou Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20350": {
    "title": "Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs",
    "volume": "main",
    "abstract": "Adversarial attacks on graphs have attracted considerable research interests. Existing works assume the attacker is either (partly) aware of the victim model, or able to send queries to it. These assumptions are, however, unrealistic. To bridge the gap between theoretical graph attacks and real-world scenarios, in this work, we propose a novel and more realistic setting: strict black-box graph attack, in which the attacker has no knowledge about the victim model at all and is not allowed to send any queries. To design such an attack strategy, we first propose a generic graph filter to unify different families of graph-based models. The strength of attacks can then be quantified by the change in the graph filter before and after attack. By maximizing this change, we are able to find an effective attack strategy, regardless of the underlying model. To solve this optimization problem, we also propose a relaxation technique and approximation theories to reduce the difficulty as well as the computational expense. Experiments demonstrate that, even with no exposure to the model, the Macro-F1 drops 6.4% in node classification and 29.5% in graph classification, which is a significant result compared with existent works",
    "checked": true,
    "id": "f9ebb29fb97651ebe90febff450379b2561fd0fd",
    "semantic_title": "blindfolded attackers still threatening: strict black-box adversarial attacks on graphs",
    "citation_count": 3,
    "authors": [
      "Jiarong Xu",
      "Yizhou Sun",
      "Xin Jiang",
      "Yanhao Wang",
      "Chunping Wang",
      "Jiangang Lu",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20351": {
    "title": "PolygonE: Modeling N-ary Relational Data as Gyro-Polygons in Hyperbolic Space",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "31fd80ce52587af64e3277e5a5cb74d95e420de1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shiyao Yan",
      "Zequn Zhang",
      "Xian Sun",
      "Guangluan Xu",
      "Shuchao Li",
      "Qing Liu",
      "Nayu Liu",
      "Shensi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20352": {
    "title": "Cross-Task Knowledge Distillation in Multi-Task Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6bc729a25797ae1c4e08b832daa708f699c6669d",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Chenxiao Yang",
      "Junwei Pan",
      "Xiaofeng Gao",
      "Tingyu Jiang",
      "Dapeng Liu",
      "Guihai Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20353": {
    "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1163c29ae6a26b1cd8d85280740410e99001a959",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Liang Yang",
      "Cheng Chen",
      "Weixun Li",
      "Bingxin Niu",
      "Junhua Gu",
      "Chuan Wang",
      "Dongxiao He",
      "Yuanfang Guo",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20354": {
    "title": "Multi-Scale Distillation from Multiple Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "74374b54ac2ce6464def966e74ef35ae1c9a9f49",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Chunhai Zhang",
      "Jie Liu",
      "Kai Dang",
      "Wenzheng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20355": {
    "title": "Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a896d05fcbdc55424a4b27f9b9138c02702db089",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Fuwei Zhang",
      "Zhao Zhang",
      "Xiang Ao",
      "Dehong Gao",
      "Fuzhen Zhuang",
      "Yi Wei",
      "Qing He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20356": {
    "title": "Anisotropic Additive Quantization for Fast Inner Product Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "23cff8ecfc5feb4208dbb90ff3fa7a6adb214e9c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jin Zhang",
      "Qi Liu",
      "Defu Lian",
      "Zheng Liu",
      "Le Wu",
      "Enhong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20357": {
    "title": "Robust Heterogeneous Graph Neural Networks against Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "07808da6b32d464f66aac31297f070c15499b92b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mengmei Zhang",
      "Xiao Wang",
      "Meiqi Zhu",
      "Chuan Shi",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20358": {
    "title": "Multi-Dimensional Prediction of Guild Health in Online Games: A Stability-Aware Multi-Task Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "af79b3e7ff74c9fbf5272cbefd1922f37bfa2e3b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chuang Zhao",
      "Hongke Zhao",
      "Runze Wu",
      "Qilin Deng",
      "Yu Ding",
      "Jianrong Tao",
      "Changjie Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20359": {
    "title": "Multi-View Intent Disentangle Graph Networks for Bundle Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "23944e3712669bfda7863567090bfcfebab944d1",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Sen Zhao",
      "Wei Wei",
      "Ding Zou",
      "Xianling Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20360": {
    "title": "Multi-Type Urban Crime Prediction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f50dbd3e6cebe0830ba2c95935137a83b3b8f3df",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xiangyu Zhao",
      "Wenqi Fan",
      "Hui Liu",
      "Jiliang Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20361": {
    "title": "Forecasting Asset Dependencies to Reduce Portfolio Risk",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "850ffdf395880e389a3a610df9802b70e4fe81b2",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoren Zhu",
      "Shih-Yang Liu",
      "Pengfei Zhao",
      "Yingying Chen",
      "Dik Lun Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20362": {
    "title": "Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "52bb8cb9cd9c130e18f6dcd6935b8b23d9b470d1",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jun Zhuang",
      "Mohammad Al Hasan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20363": {
    "title": "Can Machines Read Coding Manuals Yet? – A Benchmark for Building Better Language Models for Code Understanding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5fde5c44197473ad2ac0645f643e577188b316c4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ibrahim Abdelaziz",
      "Julian Dolby",
      "Jamie McCusker",
      "Kavitha Srinivas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20364": {
    "title": "No Task Left Behind: Multi-Task Learning of Knowledge Tracing and Option Tracing for Better Student Assessment",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "494b1a644fbd3c167f5e8946e573c999f334a20d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Suyeong An",
      "Junghoon Kim",
      "Minsam Kim",
      "Juneyoung Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20365": {
    "title": "Diaformer: Automatic Diagnosis via Symptoms Sequence Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "47d9133629a7dee1664483944547658e8cea83f2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Junying Chen",
      "Dongfang Li",
      "Qingcai Chen",
      "Wenxiu Zhou",
      "Xin Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20366": {
    "title": "Zero-Shot Audio Source Separation through Query-Based Learning from Weakly-Labeled Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9cba6d03d13e18c0100ad9e7858eaa0ec2d18ee1",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Ke Chen",
      "Xingjian Du",
      "Bilei Zhu",
      "Zejun Ma",
      "Taylor Berg-Kirkpatrick",
      "Shlomo Dubnov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20367": {
    "title": "DeepHardMark: Towards Watermarking Neural Network Hardware",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "67e1d3ff6268c487beb9d3382ccb58e02823945e",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Joseph Clements",
      "Yingjie Lao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20368": {
    "title": "A Unified Framework for Real Time Motion Completion",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9ebd72eab381d92e8f686f3a3f921b786f8c1112",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yinglin Duan",
      "Yue Lin",
      "Zhengxia Zou",
      "Yi Yuan",
      "Zhehui Qian",
      "Bohan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20369": {
    "title": "FactorVAE: A Probabilistic Dynamic Factor Model Based on Variational Autoencoder for Predicting Cross-Sectional Stock Returns",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b7fc2f1d9caf20e1e213f7032ac29e965b3ec063",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yitong Duan",
      "Lei Wang",
      "Qizhong Zhang",
      "Jian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20370": {
    "title": "AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "740a4524801dd40e69326664b45bb54edc3f4cde",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ammarah Farooq",
      "Muhammad Awais",
      "Josef Kittler",
      "Syed Safwan Khalid"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20371": {
    "title": "SCIR-Net: Structured Color Image Representation Based 3D Object Detection Network from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2e20fb355cf473d9fa666c6d66790abcd86b35ea",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Qingdong He",
      "Hao Zeng",
      "Yi Zeng",
      "Yijun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20372": {
    "title": "Learning and Dynamical Models for Sub-seasonal Climate Forecasting: Comparison and Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2386e292d492931b9223143f2e9352ae8c237740",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sijie He",
      "Xinyan Li",
      "Laurie Trenary",
      "Benjamin A Cash",
      "Timothy DelSole",
      "Arindam Banerjee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20373": {
    "title": "Solving PDE-Constrained Control Problems Using Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b22ba5a85a2eac31631eecc13991da49826e4496",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Rakhoon Hwang",
      "Jae Yong Lee",
      "Jin Young Shin",
      "Hyung Ju Hwang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20374": {
    "title": "Proxy Learning of Visual Concepts of Fine Art Paintings from Styles through Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4af251d39fb92684f15e9f993dee612a66c673c0",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diana Kim",
      "Ahmed Elgammal",
      "Marian Mazzone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20375": {
    "title": "SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9f86031e05d8c290ab468ef23b5e90ccea47e6f5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Konstantin Klemmer",
      "Tianlin Xu",
      "Beatrice Acciaio",
      "Daniel B. Neill"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20376": {
    "title": "Intra-Inter Subject Self-Supervised Learning for Multivariate Cardiac Signals",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bf10bec6c7127df551f5431fa83e4a2f853d2515",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Xiang Lan",
      "Dianwen Ng",
      "Shenda Hong",
      "Mengling Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20377": {
    "title": "GeomGCL: Geometric Graph Contrastive Learning for Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0a3764d605a317c5618a8cc4e83589d87bfff3b9",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Shuangli Li",
      "Jingbo Zhou",
      "Tong Xu",
      "Dejing Dou",
      "Hui Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20378": {
    "title": "OAM: An Option-Action Reinforcement Learning Framework for Universal Multi-Intersection Control",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "de4ef43663c8f43b9cca14333e66f3e4267a0fac",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Enming Liang",
      "Zicheng Su",
      "Chilin Fang",
      "Renxin Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20379": {
    "title": "End-to-End Line Drawing Vectorization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0e2cefeb0c3b2a012e1f6d4fab1a1e07e0f2a9a9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyuan Liu",
      "Chengze Li",
      "Xueting Liu",
      "Tien-Tsin Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20380": {
    "title": "Context-Aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1bf0d09ea0b71ecbd65b07f1744656209ad53a78",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chang Lu",
      "Tian Han",
      "Yue Ning"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20381": {
    "title": "Hyperverlet: A Symplectic Hypersolver for Hamiltonian Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d51d703711621e3ff03d4566589fdedd1d133347",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Frederik Baymler Mathiesen",
      "Bin Yang",
      "Jilin Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20382": {
    "title": "Learning Human Driving Behaviors with Sequential Causal Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e2927337d24855728b7a5371f42104a3f546a7a7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kangrui Ruan",
      "Xuan Di"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20383": {
    "title": "EMVLight: A Decentralized Reinforcement Learning Framework for Efficient Passage of Emergency Vehicles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "92f745a53f3856cc5d9db0787de73abf0a53b521",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Haoran Su",
      "Yaofeng Desmond Zhong",
      "Biswadip Dey",
      "Amit Chakraborty"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20384": {
    "title": "Constrained Prescriptive Trees via Column Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4787694a28aa60bce3062cc60dc37f5dc3b17d98",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shivaram Subramanian",
      "Wei Sun",
      "Youssef Drissi",
      "Markus Ettl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20385": {
    "title": "DDGCN: Dual Dynamic Graph Convolutional Networks for Rumor Detection on Social Media",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f62479dae183fa83801bdc417bcb5f1ad765c649",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Mengzhu Sun",
      "Xi Zhang",
      "Jiaqi Zheng",
      "Guixiang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20386": {
    "title": "Contact-Distil: Boosting Low Homologous Protein Contact Map Prediction by Self-Supervised Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8b64f44dc6804ce6a5cc2924fc204c2098f1022e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Qin Wang",
      "Jiayang Chen",
      "Yuzhe Zhou",
      "Yu Li",
      "Liangzhen Zheng",
      "Sheng Wang",
      "Zhen Li",
      "Shuguang Cui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20387": {
    "title": "EtinyNet: Extremely Tiny Network for TinyML",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ffc1fb1701aa12411534876d15ae969d1945c9af",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Kunran Xu",
      "Yishi Li",
      "Huawei Zhang",
      "Rui Lai",
      "Lin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20388": {
    "title": "RepBin: Constraint-Based Graph Representation Learning for Metagenomic Binning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c1ab18add1bbb01dd11287f84af1e79c08be61bd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Hansheng Xue",
      "Vijini Mallawaarachchi",
      "Yujia Zhang",
      "Vaibhav Rajan",
      "Yu Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20389": {
    "title": "NSGZero: Efficiently Learning Non-exploitable Policy in Large-Scale Network Security Games with Neural Monte Carlo Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "450ae859b451e6917344369c7f22fc0d901417ca",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Wanqi Xue",
      "Bo An",
      "Chai Kiat Yeo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20390": {
    "title": "RID-Noise: Towards Robust Inverse Design under Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5e9a46f0d5062f725d2368ee20985647bf27cf81",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jia-Qi Yang",
      "Ke-Bin Fan",
      "Hao Ma",
      "De-Chuan Zhan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20391": {
    "title": "Deepfake Network Architecture Attribution",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "16317ac128c0583c926382a2dd143738c7a99209",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Tianyun Yang",
      "Ziyao Huang",
      "Juan Cao",
      "Lei Li",
      "Xirong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20392": {
    "title": "ZINB-Based Graph Embedding Autoencoder for Single-Cell RNA-Seq Interpretations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "aff7c738fd9a7b57eb9c1b011977cdbc7deb1018",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zhuohan Yu",
      "Yifu Lu",
      "Yunhe Wang",
      "Fan Tang",
      "Ka-Chun Wong",
      "Xiangtao Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20393": {
    "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4e90ab5310055d1cc9a951a476ed241f97fe76b0",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Xianyuan Zhan",
      "Haoran Xu",
      "Yue Zhang",
      "Xiangyu Zhu",
      "Honglei Yin",
      "Yu Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20394": {
    "title": "AlphaHoldem: High-Performance Artificial Intelligence for Heads-Up No-Limit Poker via End-to-End Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4fcf18bda55414c5f31cc4be560bae92e8e4b7e9",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Enmin Zhao",
      "Renye Yan",
      "Jinqiu Li",
      "Kai Li",
      "Junliang Xing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20395": {
    "title": "Hierarchical Multi-Supervision Multi-Interaction Graph Attention Network for Multi-Camera Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3e8e31d1ecabfde69f471fd140c51a308b60df22",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Guoliang Zhao",
      "Yuxun Zhou",
      "Zhanbo Xu",
      "Yadong Zhou",
      "Jiang Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20396": {
    "title": "6DCNN with Roto-Translational Convolution Filters for Volumetric Data Processing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f6825d3ffbd1b63446b41d6fb3bf802bc45444f8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Dmitrii Zhemchuzhnikov",
      "Ilia Igashov",
      "Sergei Grudinin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20397": {
    "title": "Deeply Tensor Compressed Transformers for End-to-End Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "42b0f689a4209428e0f7963eadd377196bda3741",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Peining Zhen",
      "Ziyang Gao",
      "Tianshu Hou",
      "Yuan Cheng",
      "Hai-Bao Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20398": {
    "title": "Dynamic Manifold Learning for Land Deformation Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9b753cf07d751947dd600dc01abb4e75bdad8826",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Zhou",
      "Rongfan Li",
      "Qiang Gao",
      "Goce Trajcevski",
      "Kunpeng Zhang",
      "Ting Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20399": {
    "title": "Fully Adaptive Framework: Neural Computerized Adaptive Testing for Online Education",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b5498c96af1373438ed562aae2ffc951b5dc2cdb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yan Zhuang",
      "Qi Liu",
      "Zhenya Huang",
      "Zhi Li",
      "Shuanghong Shen",
      "Haiping Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20400": {
    "title": "An Algorithmic Introduction to Savings Circles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a01181f29d874ea8a9aa2521725a0d4782d562d1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rediet Abebe",
      "Adam Eck",
      "Christian Ikeokwu",
      "Sam Taggart"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20401": {
    "title": "Locally Fair Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b4c1571ca03d3142cd2e97fbdc412634b9d415f3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pankaj K. Agarwal",
      "Shao-Heng Ko",
      "Kamesh Munagala",
      "Erin Taylor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20402": {
    "title": "Maximizing Nash Social Welfare in 2-Value Instances",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8f188f43117119766e5d756f191b6353fda27fa6",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Hannaneh Akrami",
      "Bhaskar Ray Chaudhury",
      "Martin Hoefer",
      "Kurt Mehlhorn",
      "Marco Schmalhofer",
      "Golnoosh Shahkarami",
      "Giovanna Varricchio",
      "Quentin Vermande",
      "Ernest van Wijland"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20403": {
    "title": "Truth-Tracking via Approval Voting: Size Matters",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "62ea8a00be1ddbdb242c1bab90828a8d14e5aeb4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tahar Allouche",
      "Jérôme Lang",
      "Florian Yger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20404": {
    "title": "Dimensionality and Coordination in Voting: The Distortion of STV",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4b64e105076aec8ce706f4a107871a23201e9e95",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Anagnostides",
      "Dimitris Fotakis",
      "Panagiotis Patsilinakos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20405": {
    "title": "Fair and Truthful Giveaway Lotteries",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f4d4d3ccea5b6230bbb7f4b7eca5f187c4e8bf61",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tal Arbiv",
      "Yonatan Aumann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20406": {
    "title": "Universal and Tight Online Algorithms for Generalized-Mean Welfare",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "96533400058669ee2a3d1f238e903843c392af99",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Siddharth Barman",
      "Arindam Khan",
      "Arnab Maiti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20407": {
    "title": "Truthful and Fair Mechanisms for Matroid-Rank Valuations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e080fbf23fa228a993d3613a5351afb79e97a0e9",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Siddharth Barman",
      "Paritosh Verma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20408": {
    "title": "Truthful Cake Sharing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d0c9bfb0e60eeb824b2f52265c155632032611ff",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xiaohui Bei",
      "Xinhang Lu",
      "Warut Suksompong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20409": {
    "title": "The Secretary Problem with Competing Employers on Random Edge Arrivals",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34856f1bb8186a8192e55924eba237bf30cd8a3f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Bei",
      "Shengyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20410": {
    "title": "Almost Full EFX Exists for Four Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1402f3d87e7dba38984bc69afd603d3077b142eb",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Ben Berger",
      "Avi Cohen",
      "Michal Feldman",
      "Amos Fiat"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20411": {
    "title": "Sequential Blocked Matching",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5fb6c2fcc0a634c2e23c8ec5735318cac63b1d11",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Nicholas Bishop",
      "Hau Chan",
      "Debmalya Mandal",
      "Long Tran-Thanh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20412": {
    "title": "Combating Collusion Rings Is Hard but Possible",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2a6f7cd518b6a7d00264fdc19aedf5df08dda7b6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Niclas Boehmer",
      "Robert Bredereck",
      "André Nichterlein"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20413": {
    "title": "Theory of and Experiments on Minimally Invasive Stability Preservation in Changing Two-Sided Matching Markets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4baa4c3453b4167ff880910f863af9296c529468",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Niclas Boehmer",
      "Klaus Heeger",
      "Rolf Niedermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20414": {
    "title": "A Calculus for Computing Structured Justifications for Election Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "260881cb452d8091493eb50ec9d8ce6f594a0f13",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Arthur Boixel",
      "Ulle Endriss",
      "Ronald de Haan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20415": {
    "title": "Single-Agent Dynamics in Additively Separable Hedonic Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1a631c38e3d0281154edd4fe254ed2003125b618",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Felix Brandt",
      "Martin Bullinger",
      "Leo Tappe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20416": {
    "title": "On Improving Resource Allocations by Sharing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "940c39f73f66c03223b73d1b79c7a3af552efb21",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Robert Bredereck",
      "Andrzej Kaczmarczyk",
      "Junjie Luo",
      "Rolf Niedermeier",
      "Florian Sachse"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20417": {
    "title": "Liquid Democracy with Ranked Delegations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7455d85f4c68afa9db3c13ed78044f1d61899c2e",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Markus Brill",
      "Théo Delemazure",
      "Anne-Marie George",
      "Martin Lackner",
      "Ulrike Schmidt-Kraepelin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20418": {
    "title": "Individual Representation in Approval-Based Committee Voting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "37c26b3593a971a2b2f66b518e59e4e082dd85bd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Markus Brill",
      "Jonas Israel",
      "Evi Micha",
      "Jannik Peters"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20419": {
    "title": "The Metric Distortion of Multiwinner Voting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7f8e0efd23e1f9647891b1155bfc650f86016776",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ioannis Caragiannis",
      "Nisarg Shah",
      "Alexandros A. Voudouris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20420": {
    "title": "A Little Charity Guarantees Fair Connected Graph Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "beb8019bdae9832c48a6ba573a35355af7190dbc",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ioannis Caragiannis",
      "Evi Micha",
      "Nisarg Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20421": {
    "title": "Truthful Aggregation of Budget Proposals with Proportionality Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e8d7394d94213c6206937a8d73fa61516295105f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ioannis Caragiannis",
      "George Christodoulou",
      "Nicos Protopapas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20422": {
    "title": "The Complexity of Learning Approval-Based Multiwinner Voting Rules",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6b956e8dc77d61475d2e253a8d3c3cd009a52eab",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ioannis Caragiannis",
      "Karl Fehrs"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20423": {
    "title": "Efficiency of Ad Auctions with Price Displaying",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e72015c2b022642baac6621e9159f2d31da55f42",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Matteo Castiglioni",
      "Diodato Ferraioli",
      "Nicola Gatti",
      "Alberto Marchesi",
      "Giulia Romano"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20424": {
    "title": "Signaling in Posted Price Auctions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2fa809a640bd2a6fbc578f61fdae68611752dfcf",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Matteo Castiglioni",
      "Giulia Romano",
      "Alberto Marchesi",
      "Nicola Gatti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20425": {
    "title": "Weighted Fairness Notions for Indivisible Items Revisited",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "41817cbb07cebe5491b45a841b62ba48ebcabb49",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Mithun Chakraborty",
      "Erel Segal-Halevi",
      "Warut Suksompong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20426": {
    "title": "Pizza Sharing Is PPA-Hard",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "31fb5fecc62a1742a3ecde265d525586a0f7efa6",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Argyrios Deligkas",
      "John Fearnley",
      "Themistoklis Melissourgos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20427": {
    "title": "Heterogeneous Facility Location with Limited Resources",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "050c545bc6d607337a3289cbcff8064ce1eb94b5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Argyrios Deligkas",
      "Aris Filos-Ratsikas",
      "Alexandros A. Voudouris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20428": {
    "title": "Complexity of Deliberative Coalition Formation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2fcbe843222aaf33a4a5a58288126706d88df1a3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Edith Elkind",
      "Abheek Ghosh",
      "Paul Goldberg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20429": {
    "title": "The Price of Justified Representation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ca137e945b6272126abdf9cde4572611afe66158",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Edith Elkind",
      "Piotr Faliszewski",
      "Ayumi Igarashi",
      "Pasin Manurangsi",
      "Ulrike Schmidt-Kraepelin",
      "Warut Suksompong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20430": {
    "title": "The Complexity of Subelection Isomorphism Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "58500f5e4545f2b7cb0cc1dc12d68fbeb7769de3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Faliszewski",
      "Krzysztof Sornat",
      "Stanisław Szufa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20431": {
    "title": "Fast Payoff Matrix Sparsification Techniques for Structured Extensive-Form Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "caf1a70dccb75f948dd9dd7b9e10e590eb77c626",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Gabriele Farina",
      "Tuomas Sandholm"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20432": {
    "title": "Two-Price Equilibrium",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68852389cfc1b82a9368cace7b8df00974cdc60a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Feldman",
      "Galia Shabtai",
      "Aner Wolfenfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20433": {
    "title": "Algorithmic Bayesian Persuasion with Combinatorial Actions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ff10f22a4474441a90783cf823c1363e902fcf24",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kaito Fujii",
      "Shinsaku Sakaue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20434": {
    "title": "Bayesian Persuasion in Sequential Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "635f3cf9e820a0bf2841d72dbbfc0336c18d2a38",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jiarui Gan",
      "Rupak Majumdar",
      "Goran Radanovic",
      "Adish Singla"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20435": {
    "title": "Hedonic Diversity Games: A Complexity Picture with More than Two Colors",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dffcf6529ee9ae61aa985fb6302c954e03ac4049",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Robert Ganian",
      "Thekla Hamm",
      "Dušan Knop",
      "Šimon Schierreich",
      "Ondřej Suchý"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20436": {
    "title": "Fair and Efficient Allocations of Chores under Bivalued Preferences",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "509c56f46aae8b5f03cf5dc0740a0da5250c4623",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Jugal Garg",
      "Aniket Murhekar",
      "John Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20437": {
    "title": "Secretary Matching with Vertex Arrivals and No Rejections",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "559fe6a8ac1b95d3f2a625a0214f6d2b33f20bde",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohak Goyal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20438": {
    "title": "Machine-Learned Prediction Equilibrium for Dynamic Traffic Assignment",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "afe0d12eb3f458c1eabdbe5b35dd9b1739f787c6",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Graf",
      "Tobias Harks",
      "Kostas Kollias",
      "Michael Markl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20439": {
    "title": "Multi-Leader Congestion Games with an Adversary",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e47374a278296b0a810a528b1c8fe17f6706acaa",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Harks",
      "Mona Henle",
      "Max Klimm",
      "Jannik Matuschke",
      "Anja Schedel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20440": {
    "title": "Approval-Based Committee Voting under Incomplete Information",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1a8cf6f3c1820007ee680e0f8b986cd8e5252225",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Aviram Imber",
      "Jonas Israel",
      "Markus Brill",
      "Benny Kimelfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20441": {
    "title": "Reforming an Envy-Free Matching",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "28abe52458b9ab0e34f61211df39cb15336df071",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Takehiro Ito",
      "Yuni Iwamasa",
      "Naonori Kakimura",
      "Naoyuki Kamiyama",
      "Yusuke Kobayashi",
      "Yuta Nozaki",
      "Yoshio Okamoto",
      "Kenta Ozeki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20442": {
    "title": "The Complexity of Proportionality Degree in Committee Elections",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "80d3c20d23954d6435092a7c8d1c6a1f605e3ff3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Łukasz Janeczko",
      "Piotr Faliszewski"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20443": {
    "title": "Worst-Case Voting When the Stakes Are High",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d4d25594b362dfbf0067481555b97033056125fb",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anson Kahng",
      "Gregory Kehne"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20444": {
    "title": "PageRank for Edges: Axiomatic Characterization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "aa4deb9f5224216fb676f39cde865a9ec4296088",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia Kucharczuk",
      "Tomasz Wąs",
      "Oskar Skibski"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20445": {
    "title": "Safe Subgame Resolving for Extensive Form Correlated Equilibrium",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e74d0a1ba493420145f641f438d303952c78a978",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Kai Ling",
      "Fei Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20446": {
    "title": "The Semi-random Likelihood of Doctrinal Paradoxes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ac092a72c6114e6644391dca599bfdc6df67300e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ao Liu",
      "Lirong Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20447": {
    "title": "Is There a Strongest Die in a Set of Dice with the Same Mean Pips?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "390e7ba09536aa17bc45fb2da3a303bd9fc85d13",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Lu",
      "Shuji Kijima"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20448": {
    "title": "Choices Are Not Independent: Stackelberg Security Games with Nested Quantal Response Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "61d3fbbe059702572fd35f7d7f9d55d60f61ce45",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Tien Mai",
      "Arunesh Sinha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20449": {
    "title": "Strictly Proper Contract Functions Can Be Arbitrage-Free",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6f46b9848550f5dcbdc4586a36890eb2e7002c7d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Neyman",
      "Tim Roughgarden"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20450": {
    "title": "Characterization of Incentive Compatibility of an Ex-ante Constrained Player",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "33fa09e968713a02576aecda17419834324c08ca",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bonan Ni",
      "Pingzhong Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20451": {
    "title": "Online Elicitation of Necessarily Optimal Matchings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f74351db7ce813bfb4e5ec92071818861b5a81d9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jannik Peters"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20452": {
    "title": "Generalized Dynamic Cognitive Hierarchy Models for Strategic Driving Behavior",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f4f47619f20a3bd4c612638e5a1febd660b86c00",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Atrisha Sarkar",
      "Kate Larson",
      "Krzysztof Czarnecki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20453": {
    "title": "Improved Maximin Guarantees for Subadditive and Fractionally Subadditive Fair Allocation Problem",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c3da09454a5f1d4e22610ef60544460bd6c91815",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Masoud Seddighin",
      "Saeed Seddighin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20454": {
    "title": "Proportional Public Decisions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "534161f1c043b4d7fbc0fb1385ed08a7f93b21b5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Piotr Skowron",
      "Adrian Górecki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20455": {
    "title": "Online Task Assignment Problems with Reusable Resources",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1bd19bdee67e6729b52408ac7aef41caf3ab08c4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hanna Sumita",
      "Shinji Ito",
      "Kei Takemura",
      "Daisuke Hatano",
      "Takuro Fukunaga",
      "Naonori Kakimura",
      "Ken-ichi Kawarabayashi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20456": {
    "title": "Iterative Calculus of Voting under Plurality",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4b2692b96cc90a0afa966a264d3c15b52c68cb18",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabricio Vasselai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20457": {
    "title": "Coordinating Followers to Reach Better Equilibria: End-to-End Gradient Descent for Stackelberg Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "759de41d8a03e8bfaf8bd8a2a9580ac6df96adeb",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Kai Wang",
      "Lily Xu",
      "Andrew Perrault",
      "Michael K. Reiter",
      "Milind Tambe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20458": {
    "title": "Multi-Unit Auction in Social Networks with Budgets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8a1d7a359d66b89fad2cab07b9ae040d186033ab",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mingyu Xiao",
      "Yuchao Song",
      "Bakh Khoussainov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20459": {
    "title": "The Strange Role of Information Asymmetry in Auctions—Does More Accurate Value Estimation Benefit a Bidder?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d538d5c5968d0f33223581e7daf2155659b20192",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Haifeng Xu",
      "Ruggiero  Cavallo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20460": {
    "title": "AutoCFR: Learning to Design Counterfactual Regret Minimization Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0ef355aff469bfeb96481aeeae914995d4a3b68f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hang Xu",
      "Kai Li",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20461": {
    "title": "Team Correlated Equilibria in Zero-Sum Extensive-Form Games via Tree Decompositions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "12cd4c73a649edadae04c4c747d13aa5e2a00233",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Brian Hu Zhang",
      "Tuomas Sandholm"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20462": {
    "title": "Planning with Participation Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4a2b87426c3c41c4a714374187b5b9f9dfb12d2d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hanrui Zhang",
      "Yu Cheng",
      "Vincent Conitzer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20463": {
    "title": "I Don't Think So\": Summarizing Policy Disagreements for Agent Comparison",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "41c721aec8c3be4334f433b6359ab8075fbbd189",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yotam Amitai",
      "Ofra Amir"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20464": {
    "title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "59cbd70b9bcdb4a15915c11b47bfcd93319d82c6",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Siddhant Arora",
      "Danish Pruthi",
      "Norman Sadeh",
      "William W. Cohen",
      "Zachary C. Lipton",
      "Graham Neubig"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20465": {
    "title": "Role of Human-AI Interaction in Selective Prediction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "76ffc00d9602ca557df0eaa9d9da0609d839e402",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Elizabeth Bondi",
      "Raphael Koster",
      "Hannah Sheahan",
      "Martin Chadwick",
      "Yoram Bachrach",
      "Taylan Cemgil",
      "Ulrich Paquet",
      "Krishnamurthy Dvijotham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20466": {
    "title": "How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Antonio Moreno Casares",
      "Bao Sheng Loe",
      "John Burden",
      "Sean hEigeartaigh",
      "José Hernández-Orallo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20467": {
    "title": "Adversarial Learning from Crowds",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e51575ce4261f82cfdefe1d1d03103d5ea78c0bf",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pengpeng Chen",
      "Hailong Sun",
      "Yongqiang Yang",
      "Zhijun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20468": {
    "title": "FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "337b96b55034e6ead100d26e547c942489ff2e93",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Ana Lucic",
      "Harrie Oosterhuis",
      "Hinda Haned",
      "Maarten de Rijke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20469": {
    "title": "Teaching Humans When to Defer to a Classifier via Exemplars",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "950414032f17dec4e024deef512402121f66cabc",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Hussein Mozannar",
      "Arvind Satyanarayan",
      "David Sontag"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20470": {
    "title": "Deceptive Decision-Making under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "08903e4d1b32df3b224893b589ad37296c597e39",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yagiz Savas",
      "Christos K. Verginis",
      "Ufuk Topcu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20471": {
    "title": "On Optimizing Interventions in Shared Autonomy",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "67ccaeddfbea6b9d46870e59f96a2ae8e595a3ef",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weihao Tan",
      "David Koleczek",
      "Siddhant Pradhan",
      "Nicholas Perello",
      "Vivek Chettiar",
      "Vishal Rohra",
      "Aaslesha Rajaram",
      "Soundararajan Srinivasan",
      "H M Sajjad Hossain",
      "Yash Chandak"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20472": {
    "title": "Open Vocabulary Electroencephalography-to-Text Decoding and Zero-Shot Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f8f5a3408683595c29e47f95c7fcd398d95064d4",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zhenhailong Wang",
      "Heng Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20473": {
    "title": "DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "373548edd6bcb8071460b93e74fa58220a14f4f1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xianglin Yang",
      "Yun Lin",
      "Ruofan Liu",
      "Zhenfeng He",
      "Chao Wang",
      "Jin Song Dong",
      "Hong Mei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20474": {
    "title": "When Facial Expression Recognition Meets Few-Shot Learning: A Joint and Alternate Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9a9a2dfc310dae1901f3c2b4dd64148a960e96e4",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xinyi Zou",
      "Yan Yan",
      "Jing-Hao Xue",
      "Si Chen",
      "Hanzi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20475": {
    "title": "Discovering State and Action Abstractions for Generalized Task and Motion Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5185759e89a8e52ee1184cbebba5917371673790",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Aidan Curtis",
      "Tom Silver",
      "Joshua B. Tenenbaum",
      "Tomás Lozano-Pérez",
      "Leslie Kaelbling"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20476": {
    "title": "Recurrent Neural Network Controllers Synthesis with Stability Guarantees for Partially Observed Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c14bfaaef13394ea48fbb19880395cb4acaf850c",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Fangda Gu",
      "He Yin",
      "Laurent   El Ghaoui",
      "Murat Arcak",
      "Peter Seiler",
      "Ming Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20477": {
    "title": "Random Mapping Method for Large-Scale Terrain Modeling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9bfb50723df26dfe59766c6ea32cf414827b7820",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Liu",
      "Decai Li",
      "Yuqing He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20478": {
    "title": "Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cf9c24df550fafc9cb439b02e61208eb71d5e933",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yecheng Jason Ma",
      "Andrew Shen",
      "Osbert Bastani",
      "Jayaraman Dinesh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20479": {
    "title": "CTIN: Robust Contextual Transformer Network for Inertial Navigation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bc528bfc34952401d698ea0b6d82205974a0084c",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Bingbing Rao",
      "Ehsan Kazemi",
      "Yifan Ding",
      "Devu M Shila",
      "Frank M Tucker",
      "Liqiang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20480": {
    "title": "Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "357256947116c1ca4a9c2c3cd19670ccd396e867",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tianqi Tang",
      "Heming Du",
      "Xin Yu",
      "Yi Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20481": {
    "title": "Robust Adversarial Reinforcement Learning with Dissipation Inequation Constraint",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d6ebaa2daebcb509250f633badc3fb154ef53f72",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Peng Zhai",
      "Jie Luo",
      "Zhiyan Dong",
      "Lihua Zhang",
      "Shunli Wang",
      "Dingkang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20482": {
    "title": "Sim2Real Object-Centric Keypoint Detection and Description",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "06b13fa2b4f18603b24174a2a1542e9d43e69948",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Chengliang Zhong",
      "Chao Yang",
      "Fuchun Sun",
      "Jinshan Qi",
      "Xiaodong Mu",
      "Huaping Liu",
      "Wenbing Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20483": {
    "title": "Incomplete Argumentation Frameworks: Properties and Complexity",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "eccbfb775274d6f60307cce1ca2606dada2d2a93",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Gianvincenzo Alfano",
      "Sergio Greco",
      "Francesco Parisi",
      "Irina Trubitsyna"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20484": {
    "title": "Trading Complexity for Sparsity in Random Forest Explanations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "022bedf7c1f314f787d1009d4b63506c2614f022",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Gilles Audemard",
      "Steve Bellart",
      "Louènas Bounia",
      "Frédéric Koriche",
      "Jean-Marie Lagniez",
      "Pierre Marquis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20485": {
    "title": "From Actions to Programs as Abstract Actual Causes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8e11742352f88f084d2ade6e8e603e969fd16fa3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bita Banihashemi",
      "Shakil M. Khan",
      "Mikhail Soutchanski"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20486": {
    "title": "Equivalence in Argumentation Frameworks with a Claim-Centric View – Classical Results with Novel Ingredients",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "794d108c43d49e4b2b3d23675daa91cf8ad2c743",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ringo Baumann",
      "Anna Rapberger",
      "Markus Ulbricht"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20487": {
    "title": "Finite Entailment of Local Queries in the Z Family of Description Logics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "86222101f84ac87020fed242eb8cb20e82cd2d5a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bartosz Bednarczyk",
      "Emanuel Kieroński"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20488": {
    "title": "The Price of Selfishness: Conjunctive Query Entailment for ALCSelf Is 2EXPTIME-Hard",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "233ef23c221030d259becd49fa19743aafff282c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Bartosz Bednarczyk",
      "Sebastian Rudolph"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20489": {
    "title": "Expressivity of Planning with Horn Description Logic Ontologies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0f714a1e5f43b7a8a54c3569a3fa67e42c3b678c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Stefan Borgwardt",
      "Jörg Hoffmann",
      "Alisa Kovtunova",
      "Markus Krötzsch",
      "Bernhard Nebel",
      "Marcel Steinmetz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20490": {
    "title": "ER: Equivariance Regularizer for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "172f842cf4d68a949b2b47ba52903dd0766ba87f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zongsheng Cao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Qingming Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20491": {
    "title": "Geometry Interaction Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3f6861639551b33da7276b6e27a9933c467e9d3b",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Zongsheng Cao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20492": {
    "title": "Multi-Relational Graph Representation Learning with Bayesian Gaussian Process Network",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68d0fc739612be741460af3bcf9e609bfda113e8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Guanzheng Chen",
      "Jinyuan Fang",
      "Zaiqiao Meng",
      "Qiang Zhang",
      "Shangsong Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20493": {
    "title": "ASP-Based Declarative Process Mining",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0e4ddca559f375c73448f92df1c20c7f454c1a7d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Francesco Chiariello",
      "Fabrizio Maria Maggi",
      "Fabio Patrizi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20494": {
    "title": "On Testing for Discrimination Using Causal Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "49ea99ae1df26a4dfa8d1c04f41033917abc9159",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hana Chockler",
      "Joseph Y. Halpern"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20495": {
    "title": "Monotone Abstractions in Ontology-Based Data Management",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cd455b96f21860f3727ee3ebd3acbfd09c67708e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Gianluca Cima",
      "Marco Console",
      "Maurizio Lenzerini",
      "Antonella Poggi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20496": {
    "title": "Lower Bounds on Intermediate Results in Bottom-Up Knowledge Compilation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ce4d61a7e1ad9b82e292aa6c01a1fe1539b60b62",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Alexis de Colnet",
      "Stefan Mengel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20497": {
    "title": "Enforcement Heuristics for Argumentation with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "53105ee348452c7f96429bbeabba823d31221a4e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Dennis Craandijk",
      "Floris Bex"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20498": {
    "title": "On the Computation of Necessary and Sufficient Explanations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "318147ab02c5afa7c0c49a119330dde1f38b03ce",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Adnan Darwiche",
      "Chunxi Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20499": {
    "title": "Machine Learning for Utility Prediction in Argument-Based Computational Persuasion",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "415c64bcf7cc8fc7e7ea6aed2467f9e52dbbd6e2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ivan Donadello",
      "Anthony Hunter",
      "Stefano Teso",
      "Mauro Dragoni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20500": {
    "title": "On the Complexity of Inductively Learning Guarded Clauses",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9a0b936b1b8514fa4bd966923602bd24ef1801fe",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Draghici",
      "Georg Gottlob",
      "Matthias Lanzinger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20501": {
    "title": "Tractable Abstract Argumentation via Backdoor-Treewidth",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "028ff96d72307b51e849d13285a140122d6383ba",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Wolfgang Dvořák",
      "Markus Hecher",
      "Matthias König",
      "André Schidler",
      "Stefan Szeider",
      "Stefan Woltran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20502": {
    "title": "Large-Neighbourhood Search for Optimisation in Answer-Set Solving",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "48b0e6717147c947930be605e559c8281ce2b0c6",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Thomas Eiter",
      "Tobias Geibinger",
      "Nelson Higuera Ruiz",
      "Nysret Musliu",
      "Johannes Oetsch",
      "Daria Stepanova"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20503": {
    "title": "Answering Queries with Negation over Existential Rules",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "03b92511cf9add7a56b40a88c81ee93574ab9248",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Stefan Ellmauthaler",
      "Markus Krötzsch",
      "Stephan Mennicke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20504": {
    "title": "Axiomatization of Aggregates in Answer Set Programming",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2f00b2a724ca7546cd248e8fca4991f9ae7ae2b7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jorge Fandinno",
      "Zachary Hansen",
      "Yuliya Lierler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20505": {
    "title": "Linear-Time Verification of Data-Aware Dynamic Systems with Arithmetic",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "95fb13af7accf2f19f37344ba3290b7a205f4aeb",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Paolo Felli",
      "Marco Montali",
      "Sarah Winkler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20506": {
    "title": "Rushing and Strolling among Answer Sets – Navigation Made Easy",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d65baf10a69841e91ed0952ae071bd0a69772f62",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Johannes Klaus Fichte",
      "Sarah Alice Gaggl",
      "Dominik Rusovac"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20507": {
    "title": "Sufficient Reasons for Classifier Decisions in the Presence of Domain Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ae0708938c2b0d3afe231ee03d65d6d4ab4e28e0",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Niku Gorji",
      "Sasha Rubin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20508": {
    "title": "Reasoning about Causal Models with Infinitely Many Variables",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f577f0ade31f5f702000d68c431b64cf09df252a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Joseph Y. Halpern",
      "Spencer Peters"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20509": {
    "title": "An Axiomatic Approach to Revising Preferences",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8319cc48fac7a5214c02eddd4892ed1de469053d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Adrian Haret",
      "Johannes Peter Wallner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20510": {
    "title": "BERTMap: A BERT-Based Ontology Alignment System",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3554aea851065fb2c8f59dd7b600b330d54c2da7",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Yuan He",
      "Jiaoyan Chen",
      "Denvar Antonyrajah",
      "Ian Horrocks"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20511": {
    "title": "Conditional Abstract Dialectical Frameworks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4a83f4b1cc5adc69a2dac61ab0aa40776ade9d0a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jesse Heyninck",
      "Matthias Thimm",
      "Gabriele Kern-Isberner",
      "Tjitze Rienstra",
      "Kenneth Skiba"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20512": {
    "title": "MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "62b1d761efc49a49852d1f13d79f7f94f7a47c71",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Nick Hoernle",
      "Rafael Michael Karampatsis",
      "Vaishak Belle",
      "Kobi Gal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20513": {
    "title": "Towards Explainable Action Recognition by Salient Qualitative Spatial Object Relation Chains",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34f7e68aa2c8cb05f7e86a9642fd4155b3f0a105",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Hua",
      "Dongxu Li",
      "Ruiqi Li",
      "Peng Zhang",
      "Jochen Renz",
      "Anthony Cohn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20514": {
    "title": "Tractable Explanations for d-DNNF Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d21caa6b41fd307480068ce36963373246c4a80d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Xuanxiang Huang",
      "Yacine Izza",
      "Alexey Ignatiev",
      "Martin Cooper",
      "Nicholas Asher",
      "Joao Marques-Silva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20515": {
    "title": "Understanding Enthymemes in Deductive Argumentation Using Semantic Distance Measures",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dc5832692a6871f781a7108e8eea982f73b2b15d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Anthony Hunter"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20516": {
    "title": "Inferring Lexicographically-Ordered Rewards from Preferences",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "303fac45d35975a1867ef47ad9d7cfecdc436d92",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Alihan Hüyük",
      "William R. Zame",
      "Mihaela van der Schaar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20517": {
    "title": "Towards Fine-Grained Reasoning for Fake News Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "21d54922f3f9441d9fb1d05925d33ebbc60e0b12",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yiqiao Jin",
      "Xiting Wang",
      "Ruichao Yang",
      "Yizhou Sun",
      "Wei Wang",
      "Hao Liao",
      "Xing Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20518": {
    "title": "ApproxASP – a Scalable Approximate Answer Set Counter",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3bc6e1be0f006096a2d1b3fad8b11ed788594da3",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mohimenul Kabir",
      "Flavio O Everardo",
      "Ankit K Shukla",
      "Markus Hecher",
      "Johannes Klaus Fichte",
      "Kuldeep S Meel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20519": {
    "title": "Unit Selection with Causal Diagram",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dcfde41983590f9a051e913a6db832c420e39807",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20520": {
    "title": "Bounds on Causal Effects and Application to High Dimensional Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3dfac124113ac1827dccfe06237784984642c969",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20521": {
    "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Ren Li",
      "Yanan Cao",
      "Qiannan Zhu",
      "Guanqun Bi",
      "Fang Fang",
      "Yi Liu",
      "Qian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20522": {
    "title": "Multi-View Graph Representation for Programming Language Processing: An Investigation into Algorithm Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "184e9beb79f57e79c77c4a54736cb37afb80318c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ting Long",
      "Yutong Xie",
      "Xianyu Chen",
      "Weinan Zhang",
      "Qinxiang Cao",
      "Yong Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20523": {
    "title": "Automated Synthesis of Generalized Invariant Strategies via Counterexample-Guided Strategy Refinement",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cb81f70e2ac876885ba5b13f50a3ec5a4f844f4d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailun Luo",
      "Yongmei Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20524": {
    "title": "Using Conditional Independence for Belief Revision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a35a47218c983b8483b3d31a1300a03d27e8bc23",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Matthew James Lynn",
      "James P. Delgrande",
      "Pavlos Peppas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20525": {
    "title": "Weighted Model Counting in FO2 with Cardinality Constraints and Counting Quantifiers: A Closed Form Formula",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0a0ae38f1a7e781d69b7d895d27e3732d2ffd685",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sagar Malhotra",
      "Luciano Serafini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20526": {
    "title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1f836ccdd422f35c1e2e470b1bcc7201b863cc63",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Costas Mavromatis",
      "Prasanna Lakkur Subramanyam",
      "Vassilis N. Ioannidis",
      "Adesoji Adeshina",
      "Phillip R Howard",
      "Tetiana Grinberg",
      "Nagib Hakim",
      "George Karypis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20527": {
    "title": "Compilation of Aggregates in ASP Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f8d7be1cdba89869c529bcb9b4722e9675dd2789",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Mazzotta",
      "Francesco Ricca",
      "Carmine Dodaro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20528": {
    "title": "Prevailing in the Dark: Information Walls in Strategic Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f2616446d0aee89fc7c63f23a77814fe80168e03",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pavel Naumov",
      "Wenxuan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20529": {
    "title": "Knowledge Compilation Meets Logical Separability",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "520cfc932a56e58f887312d9d35471e3c0b6326e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junming Qiu",
      "Wenqing Li",
      "Zhanhao Xiao",
      "Quanlong Guan",
      "Liangda Fang",
      "Zhao-Rong Lai",
      "Qian Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20530": {
    "title": "Propositional Encodings of Acyclicity and Reachability by Using Vertex Elimination",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "55afe3569a93ef3a94d44f22e52557d702f64189",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Masood Feyzbakhsh Rankooh",
      "Jussi Rintanen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20531": {
    "title": "Random vs. Best-First: Impact of Sampling Strategies on Decision Making in Model-Based Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ac265ede6acd4d859ef815b1329c1a3692213ce1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Patrick Rodler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20532": {
    "title": "On Paraconsistent Belief Revision in LP",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bd6abf366429229ea120c888773f13d0a6c0d7b6",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nicolas Schwind",
      "Sébastien Konieczny",
      "Ramón Pino Pérez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20533": {
    "title": "Weakly Supervised Neural Symbolic Learning for Cognitive Tasks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e0111007c19762f76ee4b69bb422cee51ffa8ca9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jidong Tian",
      "Yitian Li",
      "Wenqing Chen",
      "Liqiang Xiao",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20534": {
    "title": "First Order Rewritability in Ontology-Mediated Querying in Horn Description Logics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b6d0c7ad4b80eee8e8983f509464a77b922930b4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Toman",
      "Grant Weddell"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20535": {
    "title": "MeTeoR: Practical Reasoning in Datalog with Metric Temporal Operators",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "80f48e7c279cb0986de5ca513e4b7a6bede02f9f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Dingmin Wang",
      "Pan Hu",
      "Przemysław Andrzej Wałęga",
      "Bernardo Cuenca Grau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20536": {
    "title": "SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c11ee33dea3f83cd77dcbc14684ee305b7a7e184",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zhecan Wang",
      "Haoxuan You",
      "Liunian Harold Li",
      "Alireza Zareian",
      "Suji Park",
      "Yiqing Liang",
      "Kai-Wei Chang",
      "Shih-Fu Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20537": {
    "title": "Inductive Relation Prediction by BERT",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a54a72601f862517ce38984522a39e77cb9f4b3d",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Hanwen Zha",
      "Zhiyu Chen",
      "Xifeng Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20538": {
    "title": "Learning to Walk with Dual Agents for Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "28329a7f58f95f1b8127d93c3865b3c5edbbbca5",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Denghui Zhang",
      "Zixuan Yuan",
      "Hao Liu",
      "Xiaodong lin",
      "Hui Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20539": {
    "title": "Residual Similarity Based Conditional Independence Test and Its Application in Causal Discovery",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "13be7dfa6f53b2456ce83f736d5cc3d453814059",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hao Zhang",
      "Shuigeng Zhou",
      "Kun Zhang",
      "Jihong Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20540": {
    "title": "Characterizing the Program Expressive Power of Existential Rule Languages",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3c19a2e948b7862417cfa26b1b411fc597478640",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Zhang",
      "Guifei Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20541": {
    "title": "Context-Specific Representation Abstraction for Deep Option Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "48e841a2cf2a13152f507afd9cb4e83090d72039",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Marwa Abdulhai",
      "Dong-Ki Kim",
      "Matthew Riemer",
      "Miao Liu",
      "Gerald Tesauro",
      "Jonathan P. How"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20542": {
    "title": "FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "60f34e40a2ad9231a76b7e4a79c9d957729add58",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ola Ahmad",
      "Freddy Lecue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20543": {
    "title": "Distributed Learning with Strategic Users: A Repeated Game Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4a755968310e6628e14987afc3c9a1ac5fb4c8fb",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abdullah B Akbay",
      "Junshan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20544": {
    "title": "Private Rank Aggregation in Central and Local Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6e8b76095cb750fe44ab47e283964f0d8d70d356",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Daniel Alabi",
      "Badih Ghazi",
      "Ravi Kumar",
      "Pasin Manurangsi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20545": {
    "title": "Combating Adversaries with Anti-adversaries",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5cd20c68a8c8554b89f762e0a5eac2bda03059c4",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Motasem Alfarra",
      "Juan C. Perez",
      "Ali Thabet",
      "Adel Bibi",
      "Philip H.S. Torr",
      "Bernard Ghanem"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20546": {
    "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "69c936f8f3b5f7ce4ee9d80dacb5043d32154950",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Motasem Alfarra",
      "Adel Bibi",
      "Naeemullah Khan",
      "Philip H.S. Torr",
      "Bernard Ghanem"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20547": {
    "title": "Latent Time Neural Ordinary Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d7aeca7bdb9eeeddcf19e578bc445c8c7354d1af",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Srinivas Anumasa",
      "P. K. Srijith"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20548": {
    "title": "Beyond GNNs: An Efficient Architecture for Graph Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d64dfa7ae7f844755b362080f8b8982bb5fa66e3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjal Awasthi",
      "Abhimanyu Das",
      "Sreenivas Gollapudi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20549": {
    "title": "Programmatic Modeling and Generation of Real-Time Strategic Soccer Environments for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "54bb0ae305e0f99ff377e52d0497dbef6ab70050",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abdus Salam Azad",
      "Edward Kim",
      "Qiancheng Wu",
      "Kimin Lee",
      "Ion Stoica",
      "Pieter Abbeel",
      "Alberto Sangiovanni-Vincentelli",
      "Sanjit A. Seshia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20550": {
    "title": "Admissible Policy Teaching through Reward Design",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4bf4065016e6a9150c63fd5bc71330a634a6a1f9",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Kiarash Banihashem",
      "Adish Singla",
      "Jiarui Gan",
      "Goran Radanovic"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20551": {
    "title": "Entropy-Based Logic Explanations of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "829ac55e5f878d01f2bd62376b097e5da8ab8ee3",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Pietro Barbiero",
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Pietro Lió",
      "Marco Gori",
      "Stefano Melacci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20552": {
    "title": "Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2e1861457a87d185b14bdc7d194ed2ad4d36088a",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Taha Belkhouja",
      "Yan Yan",
      "Janardhan Rao Doppa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20553": {
    "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "03404882d623a00fb080ae0ecd611453279355d8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noa Ben-David",
      "Sivan Sabato"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20554": {
    "title": "Modeling Attrition in Recommender Systems with Departing Bandits",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3c37b9ec2ff1828877575acc600b73c3bcde138f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Omer Ben-Porat",
      "Lee Cohen",
      "Liu Leqi",
      "Zachary C. Lipton",
      "Yishay Mansour"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20555": {
    "title": "Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8657e637a20ff1775307adcde2ed10494661a908",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Sameer Bibikar",
      "Haris Vikalo",
      "Zhangyang Wang",
      "Xiaohan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20556": {
    "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f190deb9b5568fb50a8847712a29c9f2af4739e2",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kuluhan Binici",
      "Shivam Aggarwal",
      "Nam Trung Pham",
      "Karianto Leman",
      "Tulika Mitra"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20557": {
    "title": "ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1b2bdddfe81aa4fc6111d7cdcb6d7cf4a0493b35",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Koushik Biswas",
      "Sandeep Kumar",
      "Shilpak Banerjee",
      "Ashish Kumar Pandey"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20558": {
    "title": "Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f58d73b7321f0d8ab0072a9f437607bc217975a7",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Fanchen Bu",
      "Dong Eui Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20559": {
    "title": "Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f16dc2741d60f5f7fdf39cbeba970207f7f4ab2a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Param Budhraja",
      "Mayank Baranwal",
      "Kunal Garg",
      "Ashish Hota"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20560": {
    "title": "Shrub Ensembles for Online Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f46792d464661494552201107c3455c7e365ee98",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Buschjäger",
      "Sibylle Hess",
      "Katharina J. Morik"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20561": {
    "title": "NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f07bec152fc98259fcb3ebfd28403308272ffee3",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kirill Bykov",
      "Anna Hedström",
      "Shinichi Nakajima",
      "Marina M.-C. Höhne"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20562": {
    "title": "Leaping through Time with Gradient-Based Adaptation for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8527f41495fd2cd40fb80ba8efc8ba6b42a9ba88",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuttapong Chairatanakul",
      "Hoang NT",
      "Xin Liu",
      "Tsuyoshi Murata"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20563": {
    "title": "Active Sampling for Text Classification with Subinstance Level Queries",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4c12c9b87e96109a96ccd33a6c6c3271656e3ce1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shayok Chakraborty",
      "Ankita Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20564": {
    "title": "A Unifying Theory of Thompson Sampling for Continuous Risk-Averse Bandits",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "77a0fa0e2b81dcd82c0e64bf21b3e0486dcfdee6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Joel Q. L. Chang",
      "Vincent Y. F. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20565": {
    "title": "Locally Private k-Means Clustering with Constant Multiplicative Approximation and Near-Optimal Additive Error",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c51dc0fa5a7f9013932b5ba06d1a278ef3063d6f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Anamay Chaturvedi",
      "Matthew Jones",
      "Huy Lê Nguyễn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20566": {
    "title": "Safe Online Convex Optimization with Unknown Linear Safety Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0dd23ceed86694c73d7c658515bbaee1b33cc188",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Sapana Chaudhary",
      "Dileep Kalathil"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20567": {
    "title": "Deconvolutional Density Network: Modeling Free-Form Conditional Distributions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4b2e27bee22dd3081e943b28ed548fd9d58b3f7e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bing Chen",
      "Mazharul Islam",
      "Jisuo Gao",
      "Lin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20568": {
    "title": "Multiscale Generative Models: Improving Performance of a Generative Model Using Feedback from Other Dependent Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "39b8b8a54362c6ae4cc24a18d2fc4665bbcc7b85",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyu Chen",
      "Avinandan Bose",
      "Shih-Fen Cheng",
      "Arunesh Sinha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20569": {
    "title": "Simultaneously Learning Stochastic and Adversarial Bandits under the Position-Based Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "de16e94def5424ac99218b526db2d533c2947137",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Cheng Chen",
      "Canzhe Zhao",
      "Shuai Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20570": {
    "title": "Clustering Interval-Censored Time-Series for Disease Phenotyping",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8cafd6998befef448c1c0f52c353758c484fc66c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Irene Y. Chen",
      "Rahul  G. Krishnan",
      "David Sontag"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20571": {
    "title": "Efficient Robust Training via Backward Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "407b4eeae564f353fc7c7e922675c08bd9d8e779",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Jinghui Chen",
      "Yu Cheng",
      "Zhe Gan",
      "Quanquan Gu",
      "Jingjing Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20572": {
    "title": "An Online Learning Approach to Sequential User-Centric Selection Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3d33f61738b6399107769d3c25ab21a1dd17301a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpu Chen",
      "Hong Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20573": {
    "title": "Better Parameter-Free Stochastic Optimization with ODE Updates for Coin-Betting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0d38b4dea08eb971b17bb704d8057919b54bff2b",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Keyi Chen",
      "John Langford",
      "Francesco Orabona"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20574": {
    "title": "Mutual Nearest Neighbor Contrast and Hybrid Prototype Self-Training for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d0b91657fe0ad99301c75cfea1a37e0db3389990",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Liang Chen",
      "Qianjin Du",
      "Yihang Lou",
      "Jianzhong He",
      "Tao Bai",
      "Minghua Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20575": {
    "title": "Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b604c70173650ad0e72634a373eebdc663cb9c46",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Liang Chen",
      "Yihang Lou",
      "Jianzhong He",
      "Tao Bai",
      "Minghua Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20576": {
    "title": "Zero Stability Well Predicts Performance of Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8f5c879cc6cef35a6faa7f36920220935d84554f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Liangming Chen",
      "Long Jin",
      "Mingsheng Shang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20577": {
    "title": "Semi-supervised Learning with Multi-Head Co-Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ea974dd3ce54791ff261565f510aa055ee94952d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Mingcai Chen",
      "Yuntao Du",
      "Yi Zhang",
      "Shuwei Qian",
      "Chongjun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20578": {
    "title": "Instance Selection: A Bayesian Decision Theory Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b42cc0c8485001d508c2d8f83f51ac986105ae30",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Qingqiang Chen",
      "Fuyuan Cao",
      "Ying Xing",
      "Jiye Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20579": {
    "title": "Input-Specific Robustness Certification for Randomized Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "129d942368e1e75e6715a5c77d159d246ca110aa",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ruoxin Chen",
      "Jie Li",
      "Junchi Yan",
      "Ping Li",
      "Bin Sheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20580": {
    "title": "Multimodal Adversarially Learned Inference with Factorized Discriminators",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bc6e78be1b991c2666c70bd36d6e7986b7a7ba86",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Wenxue Chen",
      "Jianke Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20581": {
    "title": "Imbalance-Aware Uplift Modeling for Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7fa311c276a674539e60eb35cd635dcb7edbd13d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xuanying Chen",
      "Zhining Liu",
      "Li Yu",
      "Liuyi Yao",
      "Wenpeng Zhang",
      "Yi Dong",
      "Lihong Gu",
      "Xiaodong Zeng",
      "Yize Tan",
      "Jinjie Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20582": {
    "title": "KAM Theory Meets Statistical Learning Theory: Hamiltonian Neural Networks with Non-zero Training Loss",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "966a110bea593cf5bd566e4a1f250f0e53cd77ed",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yuhan Chen",
      "Takashi Matsubara",
      "Takaharu Yaguchi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20583": {
    "title": "BScNets: Block Simplicial Complex Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "01169e9b2cda158ceb3025e6f1ea54aa1b3273d9",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yuzhou Chen",
      "Yulia R. Gel",
      "H. Vincent Poor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20584": {
    "title": "ASM2TV: An Adaptive Semi-supervised Multi-Task Multi-View Learning Framework for Human Activity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "08c1e516852c15d6160ea0d3038139b3ade63707",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zekai Chen",
      "Xiao Zhang",
      "Xiuzhen Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20585": {
    "title": "Identification of Linear Latent Variable Model with Arbitrary Distribution",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8519c7101d4ced7d18ca7b3d0bb75022776cf61f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhengming Chen",
      "Feng Xie",
      "Jie Qiao",
      "Zhifeng Hao",
      "Kun Zhang",
      "Ruichu Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20586": {
    "title": "DPNAS: Neural Architecture Search for Deep Learning with Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6f00003b27b702b8e8dd6894ad4e7d60daf580fb",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Anda Cheng",
      "Jiaxing Wang",
      "Xi Sheryl Zhang",
      "Qiang Chen",
      "Peisong Wang",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20587": {
    "title": "Graph Neural Controlled Differential Equations for Traffic Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8d6a348c4b00da0c30f9f5c77619ea40e441652b",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Jeongwhan Choi",
      "Hwangyong Choi",
      "Jeehyun Hwang",
      "Noseong Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20588": {
    "title": "Differentially Private Regret Minimization in Episodic Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "57d006f1f2f82efa2099094fcc032eaff812225d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Sayak Ray Chowdhury",
      "Xingyu Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20589": {
    "title": "Learning by Competition of Self-Interested Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "61e2f4fcb09263456188b702cad65eeb99884713",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Stephen Chung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20590": {
    "title": "How to Distribute Data across Tasks for Meta-Learning?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "06983a454b3ea2926db94bb98f5493c8a0c024d6",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Alexandru Cioba",
      "Michael Bromberg",
      "Qian Wang",
      "Ritwik Niyogi",
      "Georgios Batzolis",
      "Jezabel Garcia",
      "Da-shan Shiu",
      "Alberto Bernacchia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20591": {
    "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9daeff2818838d9cdd227414812790ed3fa3d7d4",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Cody Coleman",
      "Edward Chou",
      "Julian Katz-Samuels",
      "Sean Culatana",
      "Peter Bailis",
      "Alexander C. Berg",
      "Robert Nowak",
      "Roshan Sumbaly",
      "Matei Zaharia",
      "I. Zeki Yalniz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20592": {
    "title": "Learning Influence Adoption in Heterogeneous Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "045e07e4addfc48d8c99c23a9f47f110185ce9b2",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Conitzer",
      "Debmalya Panigrahi",
      "Hanrui Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20593": {
    "title": "Graph-Wise Common Latent Factor Extraction for Unsupervised Graph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "83a2cfc3644db2d94a378c20aab59aaa21de47f9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Thilini Cooray",
      "Ngai-Man Cheung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20594": {
    "title": "Reinforcement Learning with Stochastic Reward Machines",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "44f925cad517464c61c0bd21f397cce556a5dbc2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jan Corazza",
      "Ivan Gavran",
      "Daniel Neider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20595": {
    "title": "Sparse-RS: A Versatile Framework for Query-Efficient Sparse Black-Box Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5b5a677bf371a4f357bb739bc954d376b41b0c9a",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Francesco Croce",
      "Maksym Andriushchenko",
      "Naman D. Singh",
      "Nicolas Flammarion",
      "Matthias Hein"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20596": {
    "title": "Learning Logic Programs Though Divide, Constrain, and Conquer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "856b876b4d7e918b6d18250fb849c8325e021435",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Andrew Cropper"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20597": {
    "title": "Implicit Gradient Alignment in Distributed and Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ef828c8b4fdd481d92fb3dfb6dbf2f548c7e0af7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yatin Dandi",
      "Luis Barba",
      "Martin Jaggi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20598": {
    "title": "How Good Are Low-Rank Approximations in Gaussian Process Regression?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5cca08cfa9e728febc7ff5e6360ab87a3bae48bb",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Constantinos Daskalakis",
      "Petros Dellaportas",
      "Aristeidis Panos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20599": {
    "title": "KOALA: A Kalman Optimization Algorithm with Loss Adaptivity",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0594fba89480e82864a4db4d11049e45427eb265",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aram Davtyan",
      "Sepehr Sameni",
      "Llukman Cerkezi",
      "Givi Meishvili",
      "Adam Bielski",
      "Paolo Favaro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20600": {
    "title": "First-Order Convex Fitting and Its Application to Economics and Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d66a040ec7cc1df07e63358cf51fb1dd9fd99765",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quinlan Dawkins",
      "Minbiao Han",
      "Haifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20601": {
    "title": "Gradient Temporal Difference with Momentum: Stability and Convergence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0175f4bd2399909e5aa93f03827808826aee583f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rohan Deb",
      "Shalabh Bhatnagar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20602": {
    "title": "Distillation of RL Policies with Formal Guarantees via Variational Abstraction of Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1b81c291ea9dbc7d2149419ae674e6111400db79",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Florent Delgrange",
      "Ann Nowé",
      "Guillermo A. Pérez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20603": {
    "title": "Reducing Flipping Errors in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "222f314912369fe7b660e343771710ae8d59882e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xiang Deng",
      "Yun Xiao",
      "Bo Long",
      "Zhongfei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20604": {
    "title": "Bayesian Optimization over Permutation Spaces",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "781be6c466ed327891b42dd9eba941205feb492b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aryan Deshwal",
      "Syrine Belakaria",
      "Janardhan Rao Doppa",
      "Dae Hyun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20605": {
    "title": "Meta Propagation Networks for Graph Few-shot Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f10a78ca84de4c255095ff44c5281c2e70f6385c",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Kaize Ding",
      "Jianling Wang",
      "James Caverlee",
      "Huan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20606": {
    "title": "Online Certification of Preference-Based Fairness for Personalized Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7ade79eb6e55c632e5e37fcac7e0cf19802fff7d",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Virginie Do",
      "Sam Corbett-Davies",
      "Jamal Atif",
      "Nicolas Usunier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20607": {
    "title": "Disentangled Spatiotemporal Graph Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "42cff9e8c3670d81ea78b944fa78e4b5160e4a3b",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yuanqi Du",
      "Xiaojie Guo",
      "Hengning Cao",
      "Yanfang Ye",
      "Liang Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20608": {
    "title": "Learning from the Dark: Boosting Graph Convolutional Neural Networks with Diverse Negative Samples",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b5a95b1044181b18af28dd684b98c97e4fe09140",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wei Duan",
      "Junyu Xuan",
      "Maoying Qiao",
      "Jie Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20609": {
    "title": "Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "90dea057c5c81b79ecf2da7cfe23872113cb7ff8",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Alina Ene",
      "Huy Lê Nguyễn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20610": {
    "title": "Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5d1f8960ee3b0a6e180776502e7d50543d06c4d9",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Sepideh Esmaeilpour",
      "Bing Liu",
      "Eric Robertson",
      "Lei Shu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20611": {
    "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "917b18b8dad23284c0a42f665f2ba1984fa360de",
    "semantic_title": "",
    "citation_count": 50,
    "authors": [
      "Utku Evci",
      "Yani Ioannou",
      "Cem Keskin",
      "Yann Dauphin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20612": {
    "title": "Dynamic Nonlinear Matrix Completion for Time-Varying Data Imputation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5e2fa23bb072787d1f7f277e6ea0a57cc123e294",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jicong Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20613": {
    "title": "Up to 100x Faster Data-Free Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e9e6921c2c4646198dda64dbbe8aca14e42c9d0f",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Gongfan Fang",
      "Kanya Mo",
      "Xinchao Wang",
      "Jie Song",
      "Shitao Bei",
      "Haofei Zhang",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20614": {
    "title": "Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "031a375b9dc796ec1e18c8f3c632f205c5ecabfb",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhiyu Fang",
      "Xiaobin Zhu",
      "Chun Yang",
      "Zheng Han",
      "Jingyan Qin",
      "Xu-Cheng Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20615": {
    "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5fb4947831352af6d6231a830a943f0f2069ee8b",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Aosong Feng",
      "Chenyu You",
      "Shiqiang Wang",
      "Leandros Tassiulas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20616": {
    "title": "Scaling Neural Program Synthesis with Distribution-Based Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bbc0c3b126971df6ed644287a1c4cb7b0eb3f763",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nathanaël Fijalkow",
      "Guillaume Lagarde",
      "Théo Matricon",
      "Kevin Ellis",
      "Pierre Ohlmann",
      "Akarsh Nayan Potta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20617": {
    "title": "Modification-Fair Cluster Editing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b7b23b82172a55921943e6c69cbd438a356f6e40",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Vincent Froese",
      "Leon Kellerhals",
      "Rolf Niedermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20618": {
    "title": "Reinforcement Learning Based Dynamic Model Combination for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3e72db97e20366bf52bf809b285b3f8746d2d90f",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Yuwei Fu",
      "Di Wu",
      "Benoit Boulet"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20619": {
    "title": "JFB: Jacobian-Free Backpropagation for Implicit Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9e4b3bce118e16a94f1cea5baf1d29ff27d46485",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Samy Wu Fung",
      "Howard Heaton",
      "Qiuwei Li",
      "Daniel Mckenzie",
      "Stanley Osher",
      "Wotao Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20620": {
    "title": "Smoothing Advantage Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d464b59ca7bcd9783254b2e6d55c2d91b5a981ab",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yaozhong Gan",
      "Zhe Zhang",
      "Xiaoyang Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20621": {
    "title": "Enhancing Counterfactual Classification Performance via Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "24757460dea113c60f3f1152f298d524f8308847",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijiang Gao",
      "Max Biggs",
      "Wei Sun",
      "Ligong Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20622": {
    "title": "Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6dfc71a75f74890bbf8680e9427cc61a45384544",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Gao",
      "Jianwen Xie",
      "Siyuan Huang",
      "Yufan Ren",
      "Song-Chun Zhu",
      "Ying Nian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20623": {
    "title": "Algorithmic Concept-Based Explainable Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d6f06d2a07fcb9429c0b3b7b97e71e77e2f8bd6b",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Dobrik Georgiev",
      "Pietro Barbiero",
      "Dmitry Kazhdan",
      "Petar Veličković",
      "Pietro Lió"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20624": {
    "title": "Recovering the Propensity Score from Biased Positive Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b795da59c019d383de77ae49ac31fd6a1ef44282",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Walter Gerych",
      "Thomas Hartvigsen",
      "Luke Buquicchio",
      "Emmanuel Agu",
      "Elke Rundensteiner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20625": {
    "title": "DiPS: Differentiable Policy for Sketching in Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b825116319d2a203b3e6673af8ef400bbe1c57b1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Ghosh",
      "Saayan Mitra",
      "Andrew Lan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20626": {
    "title": "Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7680ca2be41594e4e06883e6a997a08106bf52b1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pierre Gillot",
      "Pekka Parviainen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20627": {
    "title": "Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6a4523ed4dfe9c1849865d43a3913240ecc49df4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tieliang Gong",
      "Yuxin Dong",
      "Hong Chen",
      "Wei Feng",
      "Bo Dong",
      "Chen Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20628": {
    "title": "Partial Multi-Label Learning via Large Margin Nearest Neighbour Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ee87d9777a6932aba7d2aeb1083d9c6b0702a054",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xiuwen Gong",
      "Dong Yuan",
      "Wei Bao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20629": {
    "title": "LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a0aadb27b66c932dc1234cfd5ad6392bd87aa67d",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Adam Goodge",
      "Bryan Hooi",
      "See-Kiong Ng",
      "Wee Siong Ng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20630": {
    "title": "Semi-supervised Conditional Density Estimation with Wasserstein Laplacian Regularisation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "866c7447cc732025d6a2f70b61af9b5726635b58",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Graffeuille",
      "Yun Sing Koh",
      "Jörg Wicker",
      "Moritz K Lehmann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20631": {
    "title": "GoTube: Scalable Statistical Verification of Continuous-Depth Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "569107f1fffb2c1341c76618e3362a71ec347a96",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Sophie A. Gruenbacher",
      "Mathias Lechner",
      "Ramin Hasani",
      "Daniela Rus",
      "Thomas A. Henzinger",
      "Scott A. Smolka",
      "Radu Grosu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20632": {
    "title": "Balanced Self-Paced Learning for AUC Maximization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "797bbacf61d96c6af7918dcd99b7504606e821ca",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bin Gu",
      "Chenkang Zhang",
      "Huan Xiong",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20633": {
    "title": "Theoretical Guarantees of Fictitious Discount Algorithms for Episodic Reinforcement Learning and Global Convergence of Policy Gradient Methods",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "24fda3cbf8b776aea69ef4f2d5ef11f92d3d4011",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xin Guo",
      "Anran Hu",
      "Junzi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20634": {
    "title": "Adaptive Orthogonal Projection for Batch and Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6abfc9fc3716efc58843ac02aafb39e403426df2",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yiduo Guo",
      "Wenpeng Hu",
      "Dongyan Zhao",
      "Bing Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20635": {
    "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fd4970aa6c356d90cb1612896e4ed0870cc9f808",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yijie Guo",
      "Qiucheng Wu",
      "Honglak Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20636": {
    "title": "Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b21a4856a58ee3b13c16a91a03ff4f1c81c78df2",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Yuzhi Guo",
      "Jiaxiang Wu",
      "Hehuan Ma",
      "Junzhou Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20637": {
    "title": "Improved Gradient-Based Adversarial Attacks for Quantized Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "631e3969b292f77035ca8d95a0b74347a336b627",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Kartik Gupta",
      "Thalaiyasingam Ajanthan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20638": {
    "title": "TIGGER: Scalable Generative Modelling for Temporal Interaction Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "03492c9764cadf512726d922de7d337ed9ef54fb",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shubham Gupta",
      "Sahil Manchanda",
      "Srikanta Bedathur",
      "Sayan Ranu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20639": {
    "title": "A Generalized Bootstrap Target for Value-Learning, Efficiently Combining Value and Feature Predictions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "44b0ed8536919985c16179e39608996929dc9f75",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony GX-Chen",
      "Veronica Chelu",
      "Blake A. Richards",
      "Joelle Pineau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20640": {
    "title": "Oscillatory Fourier Neural Network: A Compact and Efficient Architecture for Sequential Processing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8408c4052f8f76126df5d66d68eb607c17508285",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bing Han",
      "Cheng Wang",
      "Kaushik Roy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20641": {
    "title": "End-to-End Probabilistic Label-Specific Feature Learning for Multi-Label Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dee1d892ed58cce60b07ce8c13760a7cce6808fc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yi Hang",
      "Min-Ling Zhang",
      "Yanghe Feng",
      "Xiaocheng Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20642": {
    "title": "Cross-Domain Few-Shot Graph Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a0ba53e5f7871840d754ed595202c8af548a6f00",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kaveh Hassani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20643": {
    "title": "SpreadGNN: Decentralized Multi-Task Federated Learning for Graph Neural Networks on Molecular Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "adcdc3a73ca193b77b0a238c695dfb63e2e38d88",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Chaoyang He",
      "Emir Ceyani",
      "Keshav Balasubramanian",
      "Murali Annavaram",
      "Salman Avestimehr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20644": {
    "title": "Not All Parameters Should Be Treated Equally: Deep Safe Semi-supervised Learning under Class Distribution Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b786f64e16475b6cb49cecea26e8d576e11666a5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Rundong He",
      "Zhongyi Han",
      "Yang Yang",
      "Yilong Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20645": {
    "title": "Wasserstein Unsupervised Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "65e36b8fc38819944528d232368d8669feb3b01a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shuncheng He",
      "Yuhang Jiang",
      "Hongchang Zhang",
      "Jianzhun Shao",
      "Xiangyang Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20646": {
    "title": "Multi-Mode Tensor Space Clustering Based on Low-Tensor-Rank Representation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c67e09cf55168a8c89ac9853d388e86b41fc700d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yicong He",
      "George K. Atia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20647": {
    "title": "Toward Physically Realizable Quantum Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c22d5ca4ba0abbdb9784fb96c408a8cfa96ecc93",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mohsen Heidari",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20648": {
    "title": "Reinforcement Learning of Causal Variables Using Mediation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "65bc0c923411934f35676db4df992c8c6c7675aa",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tue Herlau",
      "Rasmus Larsen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20649": {
    "title": "Anytime Guarantees under Heavy-Tailed Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c099956d8db099e4bb514005caed9f3cd00f714",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Matthew J. Holland"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20650": {
    "title": "Adversarial Examples Can Be Effective Data Augmentation for Unsupervised Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "566cac7a6c6b0f38c21bd839ccdd53135b797b7f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chia-Yi Hsu",
      "Pin-Yu Chen",
      "Songtao Lu",
      "Sijia Liu",
      "Chia-Mu Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20651": {
    "title": "Towards Automating Model Explanations with Certified Robustness Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6b29cb5f89926cfaa001a9239dec82bd865cb350",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mengdi Huai",
      "Jinduo Liu",
      "Chenglin Miao",
      "Liuyi Yao",
      "Aidong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20652": {
    "title": "Multi-View Clustering on Topological Manifold",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6aaedd4edbb1548887d37023bea6b6302769f7d4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shudong Huang",
      "Ivor Tsang",
      "Zenglin Xu",
      "Jiancheng Lv",
      "Quan-Hui Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20653": {
    "title": "Achieving Counterfactual Fairness for Causal Bandit",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f27dcb447f26b42224bc0b19162b03b8b5c9350c",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Wen Huang",
      "Lu Zhang",
      "Xintao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20654": {
    "title": "Uncertainty-Aware Learning against Label Noise on Imbalanced Datasets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6f963257c57065273e5a7b1723e1f1eb97d18b2c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yingsong Huang",
      "Bing Bai",
      "Shengwei Zhao",
      "Kun Bai",
      "Fei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20655": {
    "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4b3e0f88c23209cee6f910ad01fb0aafa4da514d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Guillermo Infante",
      "Anders Jonsson",
      "Vicenç Gómez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20656": {
    "title": "Causal Discovery in Hawkes Processes by Minimum Description Length",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "10f9f341e03e6e4759de9b3f69cbc0cf305d017a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Amirkasra Jalaldoust",
      "Kateřina Hlaváčková-Schindler",
      "Claudia Plant"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20657": {
    "title": "Group-Aware Threshold Adaptation for Fair Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f74d4fa99ccedfea7a2662fe6944d99f34533912",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Taeuk Jang",
      "Pengyi Shi",
      "Xiaoqian Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20658": {
    "title": "Towards Discriminant Analysis Classifiers Using Online Active Learning via Myoelectric Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e45e004e50be78b00a48b51e0583b87f321ccd69",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres G Jaramillo-Yanez",
      "Marco E. Benalcázar",
      "Sebastian Sardina",
      "Fabio Zambetta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20659": {
    "title": "Label Hallucination for Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8a1deed670e2361dbc74ee90dedcd5efb3b9264e",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Yiren Jian",
      "Lorenzo Torresani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20660": {
    "title": "Learning Expected Emphatic Traces for Deep RL",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c23a0a2bcd953ceb40ee6d1270ffabb5051525b3",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Ray Jiang",
      "Shangtong Zhang",
      "Veronica Chelu",
      "Adam White",
      "Hado   van Hasselt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20661": {
    "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2e12b6fbd312042e1d38b19d7581cc249c59037f",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Shenwang Jiang",
      "Jianan Li",
      "Ying Wang",
      "Bo Huang",
      "Zhang Zhang",
      "Tingfa Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20662": {
    "title": "Fast Graph Neural Tangent Kernel via Kronecker Sketching",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "830ab871618dfebdef7d033382ee5c6bb9f62dbd",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shunhua Jiang",
      "Yunze Man",
      "Zhao Song",
      "Zheng Yu",
      "Danyang Zhuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20663": {
    "title": "Creativity of AI: Automatic Symbolic Option Discovery for Facilitating Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "44c19ed8c8fd70d72155802257453f002af75932",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Mu Jin",
      "Zhihao Ma",
      "Kebing Jin",
      "Hankz Hankui Zhuo",
      "Chen Chen",
      "Chao Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20664": {
    "title": "Adaptive Kernel Graph Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "48075b2824eba15752a2a8afd87d4aee22b349a5",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mingxuan Ju",
      "Shifu Hou",
      "Yujie Fan",
      "Jianan Zhao",
      "Yanfang Ye",
      "Liang Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20665": {
    "title": "Fully Spiking Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "55601656841bf1456105d9b2ebbbf7c49a0a92e4",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hiromichi Kamata",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20666": {
    "title": "Classifying Emails into Human vs Machine Category",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dc791f152c6ab2068c7e08b0cb4b845bccd7c5d9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsung Kang",
      "Hongwei Shang",
      "Jean-Marc Langlois"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20667": {
    "title": "Self-Supervised Enhancement of Latent Discovery in GANs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5d220f0fcfdb2c8f74996dfc0adeb27fdce453d5",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Kappiyath",
      "Silpa Vadakkeeveetil Sreelatha",
      "S. Sumitra"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20668": {
    "title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "318a2253df36ca67ea50ab630439ce6d78cb41ad",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Payam Karisani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20669": {
    "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "79047a44789668db3c4f3bda759f245b0f74a532",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolai Karpov",
      "Qin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20670": {
    "title": "iDECODe: In-Distribution Equivariance for Conformal Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34d35e460b39edb19581ef345c4b32ce45aa9eae",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Ramneet Kaur",
      "Susmit Jha",
      "Anirban Roy",
      "Sangdon Park",
      "Edgar Dobriban",
      "Oleg Sokolsky",
      "Insup Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20671": {
    "title": "Partial Wasserstein Covering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8f4a59bfc9476265f0965416b545e6f57b1b4cf1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Keisuke Kawano",
      "Satoshi Koide",
      "Keisuke Otaki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20672": {
    "title": "Optimal Tensor Transport",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a780e3829eb8048e89aa121ac01b125d70fff1a5",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Kerdoncuff",
      "Rémi Emonet",
      "Michael Perrot",
      "Marc Sebban"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20673": {
    "title": "Dist2Cycle: A Simplicial Neural Network for Homology Localization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "651986c4fd780db2cb866d2f9286254eab90adbb",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Alexandros D Keros",
      "Vidit Nanda",
      "Kartic Subr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20674": {
    "title": "Same State, Different Task: Continual Reinforcement Learning without Interference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "94cefa04e0f834272d85cc425e0adfb27fd17e08",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Samuel Kessler",
      "Jack Parker-Holder",
      "Philip Ball",
      "Stefan Zohren",
      "Stephen J. Roberts"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20675": {
    "title": "Spatial Frequency Bias in Convolutional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3c280679420af34ba8d99b42a951a7c10180fd39",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Mahyar Khayatkhoei",
      "Ahmed Elgammal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20676": {
    "title": "The Effect of Manifold Entanglement and Intrinsic Dimensionality on Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bec922f6e2b99a0a24e853ef4f2ca8770ad49e08",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Daniel Kienitz",
      "Ekaterina Komendantskaya",
      "Michael Lones"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20677": {
    "title": "A Computable Definition of the Spectral Bias",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c5179be7dd7e26daab9fb7514b53a9c1f15ca91f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jonas Kiessling",
      "Filip Thor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20678": {
    "title": "A Nested Bi-level Optimization Framework for Robust Few Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e7eefd2efd63a632269a9c3ab31fbad90ed0a9a0",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Krishnateja Killamsetty",
      "Changbin Li",
      "Chen Zhao",
      "Feng Chen",
      "Rishabh Iyer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20679": {
    "title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ac668af2e58c008636f653a186268afb1167b038",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjun Kim",
      "JeongGil Ko"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20680": {
    "title": "Towards a Rigorous Evaluation of Time-Series Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6e34bc57d8e78322c04733d02bec123febbaf453",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Siwon Kim",
      "Kukjin Choi",
      "Hyun-Soo Choi",
      "Byunghan Lee",
      "Sungroh Yoon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20681": {
    "title": "Introducing Symmetries to Black Box Meta Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "64582e814de758d4ceb7d7f74e0e75fd260ce16b",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Louis Kirsch",
      "Sebastian Flennerhag",
      "Hado   van Hasselt",
      "Abram Friesen",
      "Junhyuk Oh",
      "Yutian Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20682": {
    "title": "Directed Graph Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2669fceff62034643805a7a1fd77d844e17046f7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Georgios Kollias",
      "Vasileios Kalantzis",
      "Tsuyoshi Ide",
      "Aurélie Lozano",
      "Naoki Abe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20683": {
    "title": "HNO: High-Order Numerical Architecture for ODE-Inspired Deep Unfolding Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "41485354058ed154fd8f75fe5351a4ea6ba0d259",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Kong",
      "Wei Sun",
      "Fanhua Shang",
      "Yuanyuan Liu",
      "Hongying Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20684": {
    "title": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features across MDPs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "186cabbc394c560a458f94bf4317a5517edd8041",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ezgi Korkmaz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20685": {
    "title": "Fast Approximations for Job Shop Scheduling: A Lagrangian Dual Deep Learning Method",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f78be2d5ae302a832574cedc9f60b0d0149d39c8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "James Kotary",
      "Ferdinando Fioretto",
      "Pascal Van Hentenryck"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20686": {
    "title": "Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d85a9a6511428beaf0afe5168da542e55c46b47b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yufei Kuang",
      "Miao Lu",
      "Jie Wang",
      "Qi Zhou",
      "Bin Li",
      "Houqiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20687": {
    "title": "Gradient Based Activations for Accurate Bias-Free Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "de3501c3084c98e4deee7ca24ff08b03803f9c19",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinod  K. Kurmi",
      "Rishabh Sharma",
      "Yash Vardhan Sharma",
      "Vinay P Namboodiri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20688": {
    "title": "TrustAL: Trustworthy Active Learning Using Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cfa1d074fc74d673b6511289fbac2fdebcc09e17",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Beong-woo Kwak",
      "Youngwook Kim",
      "Yu Jin Kim",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20689": {
    "title": "Tight Neural Network Verification via Semidefinite Relaxations and Linear Reformulations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "413fd9cdf87ead7eb4161190e4f1ade507ebbdde",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jianglin Lan",
      "Yang Zheng",
      "Alessio Lomuscio"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20690": {
    "title": "Learning Adversarial Markov Decision Processes with Delayed Feedback",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c07876aea9dbab7d79dc4de2dba9b0cfe243eb2",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Tal Lancewicki",
      "Aviv Rosenberg",
      "Yishay Mansour"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20691": {
    "title": "Learning Not to Learn: Nature versus Nurture In Silico",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9293fed98a28ba7b908d914f62c424b882c30590",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Robert Tjarko Lange",
      "Henning Sprekeler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20692": {
    "title": "Optimization for Classical Machine Learning Problems on the GPU",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "183049fedc865cfe5023c257108740c140d1a01a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sören Laue",
      "Mark Blacher",
      "Joachim Giesen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20693": {
    "title": "Interpretable Clustering via Multi-Polytope Machines",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d04185bce9802b9a2076dbc956d1d774f675ba93",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Connor Lawless",
      "Jayant Kalagnanam",
      "Lam M Nguyen",
      "Dzung Phan",
      "Chandra Reddy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20694": {
    "title": "Episodic Policy Gradient Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4e6037db5ec29bb2bdbbaf3bbf7a8c595336bc83",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hung Le",
      "Majid Abdolshah",
      "Thommen K. George",
      "Kien Do",
      "Dung Nguyen",
      "Svetha Venkatesh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20695": {
    "title": "Stability Verification in Stochastic Control Systems via Neural Network Supermartingales",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c5723b6ecc06b9a64b6e3f4f70f01d54fba5cac",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Mathias Lechner",
      "Đorđe Žikelić",
      "Krishnendu Chatterjee",
      "Thomas A. Henzinger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20696": {
    "title": "Learning Losses for Strategic Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "323fe67fda478a3844c90de23cb159d6a3239275",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Tosca Lechner",
      "Ruth Urner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20697": {
    "title": "Differentially Private Normalizing Flows for Synthetic Tabular Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "56a76ae99639c1238059f6d9a19e12880740d01b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jaewoo Lee",
      "Minjung Kim",
      "Yonghyun Jeong",
      "Youngmin Ro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20698": {
    "title": "Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1811020a7c64358543900a5a50673cde7768403a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Tae Lee",
      "Hyunsin Park",
      "Sungrack Yun",
      "Simyung Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20699": {
    "title": "Fast and Efficient MMD-Based Fair PCA via Optimization over Stiefel Manifold",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4a36a00db217fd98f1bd943aa2f2d6303adbc456",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Junghyun Lee",
      "Gwangsu Kim",
      "Mahbod Olfat",
      "Mark Hasegawa-Johnson",
      "Chang D. Yoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20700": {
    "title": "Augmentation-Free Self-Supervised Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2812cc5b514a5c87e29de2e051b3ebadc021897d",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Namkyeong Lee",
      "Junseok Lee",
      "Chanyoung Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20701": {
    "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bca31821e2b68b81f5e5fd1d108d5e20a68346e0",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Sokbae Lee",
      "Yuan Liao",
      "Myung Hwan Seo",
      "Youngki Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20702": {
    "title": "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ccbd2b988ab12549a305f3009da414698a1e4616",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Dan Ley",
      "Umang Bhatt",
      "Adrian Weller"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20703": {
    "title": "Invariant Information Bottleneck for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "014c5aa0a61489f09c224d5f43b6a0eeb47a1687",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Bo Li",
      "Yifei Shen",
      "Yezhen Wang",
      "Wenzhen Zhu",
      "Colorado Reed",
      "Dongsheng Li",
      "Kurt Keutzer",
      "Han Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20704": {
    "title": "Chunk Dynamic Updating for Group Lasso with ODEs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1efa2d794fdfa7d9d95248a437adacd9b92267fc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diyang Li",
      "Bin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20705": {
    "title": "Policy Learning for Robust Markov Decision Process with a Mismatched Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "da33251754e6d7078b651450c4b9c89ea0fd313e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jialian Li",
      "Tongzheng Ren",
      "Dong Yan",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20706": {
    "title": "A Fully Single Loop Algorithm for Bilevel Optimization without Hessian Inverse",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dd6b4b7a1f9d220baaa3256f2fb0976b09ad7cfe",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Junyi Li",
      "Bin Gu",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20707": {
    "title": "A Hybrid Causal Structure Learning Algorithm for Mixed-Type Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b4207ae5e40904de4cbd41e64e94ccf63b02411b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yan Li",
      "Rui Xia",
      "Chunchen Liu",
      "Liang Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20708": {
    "title": "Sharp Analysis of Random Fourier Features in Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8d4715523aaa8bb8573cdd82ab539cdc60fc9346",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zhu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20709": {
    "title": "Zeroth-Order Optimization for Composite Problems with Functional Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "428fc338ec8bb9a9c8278bccad05c68c88e67012",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zichong Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Songtao Lu",
      "Yangyang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20710": {
    "title": "Robust Graph-Based Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2a5c1f511e4a65381eeb2983e1a290a48aecc686",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weixuan Liang",
      "Xinwang Liu",
      "Sihang Zhou",
      "Jiyuan Liu",
      "Siwei Wang",
      "En Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20711": {
    "title": "Conditional Local Convolution for Spatio-Temporal Meteorological Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f5074a176e126f9443ef3fa478599cf5aa2b74f8",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Haitao Lin",
      "Zhangyang Gao",
      "Yongjie Xu",
      "Lirong Wu",
      "Ling Li",
      "Stan  Z. Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20712": {
    "title": "On the Use of Unrealistic Predictions in Hundreds of Papers Evaluating Graph Representations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "62b6e1e833ef2644fe3815d08bd5586e12466da8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Li-Chung Lin",
      "Cheng-Hung Liu",
      "Chih-Ming Chen",
      "Kai-Chin Hsu",
      "I-Feng Wu",
      "Ming-Feng Tsai",
      "Chih-Jen Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20713": {
    "title": "Deep Unsupervised Hashing with Latent Semantic Components",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ee6d8f39c3fb1cd1566130a7d1ebd0836dd5b072",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Qinghong Lin",
      "Xiaojun Chen",
      "Qin Zhang",
      "Shaotian Cai",
      "Wenzhe Zhao",
      "Hongfa Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20714": {
    "title": "SCRIB: Set-Classifier with Class-Specific Risk Bounds for Blackbox Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "132aaf728e5fa8b196fdbfb1293ec8523ced11f8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhen Lin",
      "Lucas Glass",
      "M. Brandon Westover",
      "Cao Xiao",
      "Jimeng Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20715": {
    "title": "RareGAN: Generating Samples for Rare Classes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "163ec520bfc7ef5f068de19c4e061940ad201317",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zinan Lin",
      "Hao Liang",
      "Giulia Fanti",
      "Vyas Sekar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20716": {
    "title": "Conjugated Discrete Distributions for Distributional Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1f28c649eaecf3d72aade439793f1360dda4e243",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Björn Lindenberg",
      "Jonas Nordqvist",
      "Karl-Olof Lindahl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20717": {
    "title": "Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "26bd86d04fc5d741d36868ff4aceeb65db1be441",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Pierre Liotet",
      "Francesco Vidaich",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20718": {
    "title": "Learning Parameterized Task Structure for Generalization to Unseen Entities",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c708330bd81838ae38073220a96cc17f62f0bd7",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Anthony Liu",
      "Sungryull Sohn",
      "Mahdi Qazwini",
      "Honglak Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20719": {
    "title": "Stationary Diffusion State Neural Estimation for Multiview Clustering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3c36f6925f785513653efeb567e0132000b54a9e",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Chenghua Liu",
      "Zhuolin Liao",
      "Yixuan Ma",
      "Kun Zhan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20720": {
    "title": "Deep Amortized Relational Model with Group-Wise Hierarchical Generative Process",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c1a499b99dda7b8b144c0b2cc5251e84ed17f39f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Huafeng Liu",
      "Tong Zhou",
      "Jiaqi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20721": {
    "title": "Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ae25c0c7d3e2de1c90346079e53b9e3e836c7de4",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Jinxin Liu",
      "Donglin Wang",
      "Qiangxing Tian",
      "Zhengyu Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20722": {
    "title": "Transformer with Memory Replay",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f512507e98f20528a21af522b7f63f2d16ba6841",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rui Liu",
      "Barzan Mozafari"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20723": {
    "title": "Efficient One-Pass Multi-View Subspace Clustering with Consensus Anchors",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fd072a57e96ed20fe94d55d0f52b8491df48bf06",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Suyuan Liu",
      "Siwei Wang",
      "Pei Zhang",
      "Kai Xu",
      "Xinwang Liu",
      "Changwang Zhang",
      "Feng Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20724": {
    "title": "Trusted Multi-View Deep Learning with Opinion Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7d60027df0fa6dd56cab08e79bf9a2cf4c81471a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wei Liu",
      "Xiaodong Yue",
      "Yufei Chen",
      "Thierry Denoeux"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20725": {
    "title": "Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "313ca8cb33cdfbeb40f64d7a1aa56322713ca7e3",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Xin Liu",
      "Yangqiu Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20726": {
    "title": "Deep Graph Clustering via Dual Correlation Reduction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8d50200b20e1a49dee65e5ed288a490d14609982",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Yue Liu",
      "Wenxuan Tu",
      "Sihang Zhou",
      "Xinwang Liu",
      "Linxuan Song",
      "Xihong Yang",
      "En Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20727": {
    "title": "Optimistic Initialization for Exploration in Continuous Control",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "60306257fbac5e4fb8b94552b67094e09157f049",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sam Lobel",
      "Omer Gottesman",
      "Cameron Allen",
      "Akhil Bagaria",
      "George Konidaris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20728": {
    "title": "Fast and Data Efficient Reinforcement Learning from Pixels via Non-parametric Value Approximation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ca3b3c89c247729505343262400d47912b52086d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Alexander Long",
      "Alan Blair",
      "Herke van Hoof"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20729": {
    "title": "Frozen Pretrained Transformers as Universal Computation Engines",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3aa2c10dd6c72267ea8a622c8f30b3c9240d5fab",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Kevin Lu",
      "Aditya Grover",
      "Pieter Abbeel",
      "Igor Mordatch"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20730": {
    "title": "Adapt to Environment Sudden Changes by Learning a Context Sensitive Policy",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e2bc67e9a7bf30ba447b9cff051df11027815cfc",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Fan-Ming Luo",
      "Shengyi Jiang",
      "Yang Yu",
      "ZongZhang Zhang",
      "Yi-Feng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20731": {
    "title": "Beyond Shared Subspace: A View-Specific Fusion for Multi-View Multi-Label Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3f27b3603695533eaf4cfe631c69c75654022f17",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Gengyu Lyu",
      "Xiang Deng",
      "Yanan Wu",
      "Songhe Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20732": {
    "title": "Efficient Continuous Control with Double Actors and Regularized Critics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f162abb9f391401df15d7cde9bf52f8da624dfb6",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Jiafei Lyu",
      "Xiaoteng Ma",
      "Jiangpeng Yan",
      "Xiu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20733": {
    "title": "Recursive Reasoning Graph for Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "36054fe91a4a6a5b53c103b626cc86dd2d344d3e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobai Ma",
      "David Isele",
      "Jayesh K. Gupta",
      "Kikuo Fujimura",
      "Mykel J. Kochenderfer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20734": {
    "title": "Sharp Restricted Isometry Property Bounds for Low-Rank Matrix Recovery Problems with Corrupted Measurements",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a1a9c05f2990232746170f38121fb593c409a94c",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Ziye Ma",
      "Yingjie Bi",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20735": {
    "title": "Cross-Lingual Adversarial Domain Adaptation for Novice Programming",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6a0da1cacdd7eaaf6afef2bcc72cd98c1e6dfd0a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ye Mao",
      "Farzaneh Khoshnevisan",
      "Thomas Price",
      "Tiffany Barnes",
      "Min Chi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20736": {
    "title": "Hard to Forget: Poisoning Attacks on Certified Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "036b18662c47dc70a7b69b4184f9dabc81fdb3df",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Neil G. Marchant",
      "Benjamin I. P. Rubinstein",
      "Scott Alfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20737": {
    "title": "Exploring Safer Behaviors for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b75ff79617904f769c5f61cea3693e021757c600",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Enrico Marchesini",
      "Davide Corsi",
      "Alessandro Farinelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20738": {
    "title": "fGOT: Graph Distances Based on Filters and Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ce2d4a62b2273438d67bf25d6b9cbf3467e9a4d0",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hermina Petric Maretic",
      "Mireille El Gheche",
      "Giovanni Chierchia",
      "Pascal Frossard"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20739": {
    "title": "When AI Difficulty Is Easy: The Explanatory Power of Predicting IRT Difficulty",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9e08f1c08b93a8009fdb6b6f5d29931dc3e09c73",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Fernando Martínez-Plumed",
      "David Castellano",
      "Carlos Monserrat-Aranda",
      "José Hernández-Orallo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20740": {
    "title": "Being Friends Instead of Adversaries: Deep Networks Learn from Data Simplified by Other Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "618a6e4668de2379f1dc2356ef4f292f9f1cadbf",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Marullo",
      "Matteo Tiezzi",
      "Marco Gori",
      "Stefano Melacci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20741": {
    "title": "An Experimental Design Approach for Regret Minimization in Logistic Bandits",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8cce269769f18b96558b53df66a68ca41f493a9a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Blake Mason",
      "Kwang-Sung Jun",
      "Lalit Jain"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20742": {
    "title": "Coordinate Descent on the Orthogonal Group for Recurrent Neural Network Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a4d544e09b780b4ec90b6df3deb2f914eaeb0707",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Estelle Massart",
      "Vinayak Abrol"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20743": {
    "title": "Curiosity-Driven Exploration via Latent Bayesian Surprise",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8c2e0392ae194afa0173c26ec92fa6460707f241",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Pietro Mazzaglia",
      "Ozan Catal",
      "Tim Verbelen",
      "Bart Dhoedt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20744": {
    "title": "What Can We Learn Even from the Weakest? Learning Sketches for Programmatic Strategies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f52d46e47878fbaea791aba5d54435fae96d878d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leandro C. Medeiros",
      "David S. Aleixo",
      "Levi H. S. Lelis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20745": {
    "title": "Top-Down Deep Clustering with Multi-Generator GANs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c9ed8382fccf6fa5f84069e89963fab2dfa299e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Daniel P. M. de Mello",
      "Renato M. Assunção",
      "Fabricio Murai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20746": {
    "title": "Temporal Knowledge Graph Completion Using Box Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "918381ce2c8f6afa3a25ac88bf2008733c5a3255",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Johannes Messner",
      "Ralph Abboud",
      "Ismail Ilkan Ceylan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20747": {
    "title": "An Evaluative Measure of Clustering Methods Incorporating Hyperparameter Sensitivity",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2b150b02c2c2daa6103dc42d2fadeb48ac006266",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Siddhartha Mishra",
      "Nicholas Monath",
      "Michael Boratko",
      "Ariel Kobren",
      "Andrew McCallum"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20748": {
    "title": "Simple Unsupervised Graph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3cb7c04c80c1c75cd8df17c5b7ab6a5399563a65",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Yujie Mo",
      "Liang Peng",
      "Jie Xu",
      "Xiaoshuang Shi",
      "Xiaofeng Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20749": {
    "title": "The Role of Adaptive Optimizers for Honest Private Hyperparameter Selection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f90f4b9f3a2159bf94181786a1b11bb4ae01f3f2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Shubhankar Mohapatra",
      "Sajin Sasy",
      "Xi He",
      "Gautam Kamath",
      "Om Thakkar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20750": {
    "title": "Learning Bayesian Networks in the Presence of Structural Side Information",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "293c71bd87af1114868883660040c2d88d785a86",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Ehsan Mokhtarian",
      "Sina Akbari",
      "Fateme Jamshidi",
      "Jalal Etesami",
      "Negar Kiyavash"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20751": {
    "title": "Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "895d4a7449ac55ec259ee4663bf14be4f3c397ca",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungyong Moon",
      "Gaon An",
      "Hyun Oh Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20752": {
    "title": "Provable Guarantees for Understanding Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "075ddf01cb0a722143ae04f49ef7dabcd64f38da",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Peyman Morteza",
      "Yixuan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20753": {
    "title": "Constraint Sampling Reinforcement Learning: Incorporating Expertise for Faster Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "14aa7aa1249075940b8a460d01e8ae821f40ab32",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Tong Mu",
      "Georgios Theocharous",
      "David Arbour",
      "Emma Brunskill"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20754": {
    "title": "Unsupervised Reinforcement Learning in Multiple Environments",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8e63e0ccd18fdfc1f7a73fc9e0d4bc6c36316016",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mirco Mutti",
      "Mattia Mancassola",
      "Marcello Restelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20755": {
    "title": "Is Your Data Relevant?: Dynamic Selection of Relevant Data for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f974a71bc1ca0e211553a3a87b3dc193098ab6cc",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Lokesh Nagalapatti",
      "Ruhi Sharma Mittal",
      "Ramasuri Narayanam"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20756": {
    "title": "A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cfc0683a3db08140c56f848cda6adc74ee9e3fb8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Krishna Prasad Neupane",
      "Ervine Zheng",
      "Yu Kong",
      "Qi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20757": {
    "title": "Out of Distribution Data Detection Using Dropout Bayesian Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6a4a2cbae832beb692565c8a113052e96e33ac63",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Andre T. Nguyen",
      "Fred Lu",
      "Gary  Lopez Munoz",
      "Edward Raff",
      "Charles Nicholas",
      "James Holt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20758": {
    "title": "Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "766c7366ca956634cb15d8238d15cadebe8217c1",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Evgenii Nikishin",
      "Romina Abachi",
      "Rishabh Agarwal",
      "Pierre-Luc Bacon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20759": {
    "title": "Improving Evidential Deep Learning via Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34148f8d11b814739e524bd834d5fe1f4b911d4e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Dongpin Oh",
      "Bonggun Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20760": {
    "title": "Clustering Approach to Solve Hierarchical Classification Problem Complexity",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6ba338ee726a81a05ade8d0e6f35a34332db79b9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aomar Osmani",
      "Massinissa Hamidi",
      "Pegah Alizadeh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20761": {
    "title": "Random Tensor Theory for Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "af4dec5ad76a9e654d94873adca40217600d033d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mohamed Ouerfelli",
      "Mohamed Tamaazousti",
      "Vincent Rivasseau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20762": {
    "title": "Bag Graph: Multiple Instance Learning Using Bayesian Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "228faba0882d6ed2231023cf6e2053e460dea8ed",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Soumyasundar Pal",
      "Antonios Valkanas",
      "Florence Regol",
      "Mark Coates"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20763": {
    "title": "Competing Mutual Information Constraints with Stochastic Competition-Based Activations for Learning Diversified Representations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5a0e4c5bdf03bfaaaf30f736913561b133340789",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Konstantinos P. Panousis",
      "Anastasios Antoniadis",
      "Sotirios Chatzis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20764": {
    "title": "Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8dabfc8f58d4c1067a557fd658cf5c9a82260c11",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Giseung Park",
      "Sungho Choi",
      "Youngchul Sung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20765": {
    "title": "Deformable Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b8a1a2a81383aabfc719548af0b693e06e48b4a0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jinyoung Park",
      "Sungdong Yoo",
      "Jihwan Park",
      "Hyunwoo J. Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20766": {
    "title": "Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ca23d4f34bd48ef1608da43741cbdf78a2f66c06",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Joonhyung Park",
      "June Yong Yang",
      "Jinwoo Shin",
      "Sung Ju Hwang",
      "Eunho Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20767": {
    "title": "Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d3b5cd144e3da102015b79e9f9eb36ff7a30cd92",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Joonhyung Park",
      "Hajin Shim",
      "Eunho Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20768": {
    "title": "CC-CERT: A Probabilistic Approach to Certify General Robustness of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "513de0282764f44c13cc0818d3e12a524b208aad",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mikhail Pautov",
      "Nurislam Tursynbek",
      "Marina Munkhoeva",
      "Nikita Muravev",
      "Aleksandr Petiushko",
      "Ivan Oseledets"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20769": {
    "title": "Covered Information Disentanglement: Model Transparency via Unbiased Permutation Importance",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cdcdc9fa9dfab2de6f3a13fee5c22039d0d0ff63",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "João P. B. Pereira",
      "Erik S. G. Stroes",
      "Aeilko H. Zwinderman",
      "Evgeni Levin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20770": {
    "title": "On the Impossibility of Non-trivial Accuracy in Presence of Fairness Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cb6335d1114a8d31d628f3a9930a9e3e2579462a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Carlos Pinzón",
      "Catuscia Palamidessi",
      "Pablo Piantanida",
      "Frank Valencia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20771": {
    "title": "Spiking Neural Networks with Improved Inherent Recurrence Dynamics for Sequential Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7793027bc575f7ca34643b362e8f64f84e7edfad",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Wachirawit Ponghiran",
      "Kaushik Roy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20772": {
    "title": "How Private Is Your RL Policy? An Inverse RL Based Analysis Framework",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d8b3aa1ca3896cc513b40e9c0a1354e4b7fdd85e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kritika Prakash",
      "Fiza Husain",
      "Praveen Paruchuri",
      "Sujit Gujar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20773": {
    "title": "Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4a07d64c47ef75bef8530874f47f9abde8523605",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xin Qiu",
      "Risto Miikkulainen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20774": {
    "title": "DeepType 2: Superhuman Entity Linking, All You Need Is Type Interactions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f638bf8bbf63ae433223676510d50d9513fdd387",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jonathan Raiman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20775": {
    "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "770144a0d32d5e66982d0cd7f6ec305de2af3b84",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parikshit Ram",
      "Kaushik Sinha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20776": {
    "title": "I-SEA: Importance Sampling and Expected Alignment-Based Deep Distance Metric Learning for Time Series Analysis and Embedding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "47214fcc72a8a1c7624c73d8d0ccf5352baab99f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sirisha Rambhatla",
      "Zhengping Che",
      "Yan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20777": {
    "title": "Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2539ebad34ca8ad2f5e72d2673f2e061302731e1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Anshuka Rangi",
      "Long Tran-Thanh",
      "Haifeng Xu",
      "Massimo Franceschetti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20778": {
    "title": "DISTREAL: Distributed Resource-Aware Learning in Heterogeneous Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1a09171c7a0463183b4846449d178157e68a6828",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Martin Rapp",
      "Ramin Khalili",
      "Kilian Pfeiffer",
      "Jörg Henkel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20779": {
    "title": "Sublinear Time Approximation of Text Similarity Matrices",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8e602cf8e647556bfd2afb565b178795fbf41c2d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Archan Ray",
      "Nicholas Monath",
      "Andrew McCallum",
      "Cameron Musco"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20780": {
    "title": "Decision-Dependent Risk Minimization in Geometrically Decaying Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "740afdd1619d797145b056877865f941891e6a65",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Mitas Ray",
      "Lillian J. Ratliff",
      "Dmitriy Drusvyatskiy",
      "Maryam Fazel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20781": {
    "title": "On Causally Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b4845ef70a5fd4d786038d0360ff32fe2cab8593",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Benin Godfrey L",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20782": {
    "title": "Conditional Loss and Deep Euler Scheme for Time Series Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7da5ff9edf6d3432e0c5ec720cb36adb12b1df02",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Carl Remlinger",
      "Joseph Mikael",
      "Romuald Elie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20783": {
    "title": "Offline Reinforcement Learning as Anti-exploration",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "399806e861a2ef960a81b37b593c2176a728c399",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Shideh Rezaeifar",
      "Robert Dadashi",
      "Nino Vieillard",
      "Léonard Hussenot",
      "Olivier Bachem",
      "Olivier Pietquin",
      "Matthieu Geist"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20784": {
    "title": "Interpretable Neural Subgraph Matching for Graph Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0115465a98cf50988c8246ef3a900ccffcf9b801",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Indradyumna Roy",
      "Venkata Sai Baba Reddy Velugoti",
      "Soumen Chakrabarti",
      "Abir De"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20785": {
    "title": "FedSoft: Soft Clustered Federated Learning with Proximal Local Updating",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d8daa8ea2d9d7e3fd2351601df69661fb3d82370",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Yichen Ruan",
      "Carlee Joe-Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20786": {
    "title": "Knowledge Distillation via Constrained Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ba47986454ed5c0791246c260dc8a2aac4d8e723",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ardavan Saeedi",
      "Yuria Utsumi",
      "Li Sun",
      "Kayhan Batmanghelich",
      "Li-wei Lehman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20787": {
    "title": "Hypergraph Modeling via Spectral Embedding Connection: Hypergraph Cut, Weighted Kernel k-Means, and Heat Kernel",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "46315c539361a20f750a329469f830df99e77ded",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shota Saito"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20788": {
    "title": "Reverse Differentiation via Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0cd8f4c1183cc93078aad4f57004e0e8a3b5bb2f",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Tommaso Salvatori",
      "Yuhang Song",
      "Zhenghua Xu",
      "Thomas Lukasiewicz",
      "Rafal Bogacz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20789": {
    "title": "VACA: Designing Variational Graph Autoencoders for Causal Queries",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e9519d5a27dc9813b6d5d18b4fcab7dab98c7709",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Pablo Sánchez-Martin",
      "Miriam Rateike",
      "Isabel Valera"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20790": {
    "title": "Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3293be154f600be7c89389c0eba2ae22c3dd9dfa",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Christian Schilling",
      "Marcelo Forets",
      "Sebastián Guadalupe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20791": {
    "title": "Scaling Up Influence Functions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ef2a773c3c7848a6cc16b18164be5f8876a310af",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Andrea Schioppa",
      "Polina Zablotskaia",
      "David Vilar",
      "Artem Sokolov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20792": {
    "title": "Chaining Value Functions for Off-Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cdee8ded4b2b8be42f26368dd1fab7ab1eec7bd4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Schmitt",
      "John Shawe-Taylor",
      "Hado   van Hasselt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20793": {
    "title": "Graph Filtration Kernels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3e53ee173a14d3b6b0f2d19b8a4f436d687ed07c",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Till Schulz",
      "Pascal Welke",
      "Stefan Wrobel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20794": {
    "title": "Neural Networks Classify through the Class-Wise Means of Their Representations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "876b613ff4d61dff07ce1277e3e29a9453452cfc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed El Amine Seddik",
      "Mohamed Tamaazousti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20795": {
    "title": "Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "90b0a90fbb88b8031b036b721ed516f8389301a4",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Prithviraj Sen",
      "Breno W. S. R. de Carvalho",
      "Ryan Riegel",
      "Alexander Gray"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20796": {
    "title": "Max-Margin Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2c132fc9c5b99a2ace079856ef191a2a7fd5abec",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Anshul Shah",
      "Suvrit Sra",
      "Rama Chellappa",
      "Anoop Cherian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20797": {
    "title": "Learning to Transfer with von Neumann Conditional Divergence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3d761fa130a935be1326e915bc9af59c9fea5c9e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ammar Shaker",
      "Shujian Yu",
      "Daniel Oñoro-Rubio"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20798": {
    "title": "Online Apprenticeship Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "006b38e300c2e449676f6f3aeb0722a45098f95d",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Lior Shani",
      "Tom Zahavy",
      "Shie Mannor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20799": {
    "title": "HoD-Net: High-Order Differentiable Deep Neural Networks and Applications",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e620d02d658a7c9250cfe09cbcf28f27cc9a6362",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Shen",
      "Tianjia Shao",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Feng Luo",
      "Yin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20800": {
    "title": "Conditional Generative Model Based Predicate-Aware Query Approximation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "26c33ff8ea868b571858fbf37b6cafb10e2cee7d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Nikhil Sheoran",
      "Subrata Mitra",
      "Vibhor Porwal",
      "Siddharth Ghetia",
      "Jatin Varshney",
      "Tung Mai",
      "Anup Rao",
      "Vikas Maddukuri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20801": {
    "title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and the Explanations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c3e7f95e7d1e37003bbef8dca1aee6dcf05a5c16",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Hui Shi",
      "Sicun Gao",
      "Yuandong Tian",
      "Xinyun Chen",
      "Jishen Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20802": {
    "title": "Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "55411c7fb15f5f4f30bd39481a9d7ef28ae17bb8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zhenbo Shi",
      "Zhi Chen",
      "Zhenbo Xu",
      "Wei Yang",
      "Zhidong Yu",
      "Liusheng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20803": {
    "title": "TRF: Learning Kernels with Tuned Random Features",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b14f235352ed8213625a92fdb06e0bba66102e26",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alistair Shilton",
      "Sunil Gupta",
      "Santu Rana",
      "Arun Kumar Venkatesh",
      "Svetha Venkatesh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20804": {
    "title": "Estimation of Local Average Treatment Effect by Data Combination",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7c0bb2463945431c789191c8bec449d015037f57",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kazuhiko Shinoda",
      "Takahiro Hoshino"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20805": {
    "title": "Constraint-Driven Explanations for Black-Box ML Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "113d9fe223d6414709ec6ab80fb068c166f6db24",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Aditya A. Shrotri",
      "Nina Narodytska",
      "Alexey Ignatiev",
      "Kuldeep S Meel",
      "Joao Marques-Silva",
      "Moshe Y. Vardi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20806": {
    "title": "Noise-Robust Learning from Multiple Unsupervised Sources of Inferred Labels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7f53c70a3b5aa22e2843db16c00bb4a3cbb0ba41",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Amila Silva",
      "Ling Luo",
      "Shanika Karunasekera",
      "Christopher Leckie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20807": {
    "title": "QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ea44927c57909af47d5680ff2af256d8f8597ce8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Daniel Silver",
      "Tirthak Patel",
      "Devesh Tiwari"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20808": {
    "title": "EqGNN: Equalized Node Opportunity in Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "48661ae9be42f8be888f7e4577b57c9132c4eb70",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Uriel Singer",
      "Kira Radinsky"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20809": {
    "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "09ce40faf1b73337d83e84f8b449ba7192c6fd82",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mahdi Soleymani",
      "Ramy E. Ali",
      "Hessam Mahdavifar",
      "A. Salman Avestimehr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20810": {
    "title": "Feature Importance Explanations for Temporal Black-Box Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c7f4157152bad59ecbd3f20d1fffa5e1f75238b",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Akshay Sood",
      "Mark Craven"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20811": {
    "title": "Reward-Weighted Regression Converges to a Global Optimum",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "03487212e74fbb61d14ac4217d85ed47764b1c85",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Miroslav Štrupl",
      "Francesco Faccio",
      "Dylan R. Ashley",
      "Rupesh Kumar Srivastava",
      "Jürgen Schmidhuber"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20812": {
    "title": "Gradient-Based Novelty Detection Boosted by Self-Supervised Binary Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6951a59e5e3d608bfb0ef9055bf6807111e028e1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jingbo Sun",
      "Li Yang",
      "Jiaxin Zhang",
      "Frank Liu",
      "Mahantesh Halappanavar",
      "Deliang Fan",
      "Yu Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20813": {
    "title": "Deterministic and Discriminative Imitation (D2-Imitation): Revisiting Adversarial Imitation for Sample Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "01c9b70b02e73aee86a9c67e051c576f4c4135e9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Mingfei Sun",
      "Sam Devlin",
      "Katja Hofmann",
      "Shimon Whiteson"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20814": {
    "title": "Exploiting Mixed Unlabeled Data for Detecting Samples of Seen and Unseen Out-of-Distribution Classes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fe2e2752f6baaf9810ca31f3c2f294a1a565dabb",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yi-Xuan Sun",
      "Wei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20815": {
    "title": "Generalized Equivariance and Preferential Labeling for GNN Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "66b6b1664d15490ea4b2141487817a3d39a25880",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zeyu Sun",
      "Wenjie Zhang",
      "Lili Mou",
      "Qihao Zhu",
      "Yingfei Xiong",
      "Lu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20816": {
    "title": "Explainable and Local Correction of Classification Models Using Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f60716d784941a97f6032deb86c5b581b9e95415",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hirofumi Suzuki",
      "Hiroaki Iwashita",
      "Takuya Takagi",
      "Keisuke Goto",
      "Yuta Fujishige",
      "Satoshi Hara"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20817": {
    "title": "Consistency Regularization for Adversarial Robustness",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "33ca8d34d226e47e0830b6eb73c06e0b85ae7ab7",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Jihoon Tack",
      "Sihyun Yu",
      "Jongheon Jeong",
      "Minseon Kim",
      "Sung Ju Hwang",
      "Jinwoo Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20818": {
    "title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6dba50cad6956da4384538599f610ffdf9e3e98a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aviv Tamar",
      "Daniel Soudry",
      "Ev Zisselman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20819": {
    "title": "FedProto: Federated Prototype Learning across Heterogeneous Clients",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6d3b39d2b08ad848ee94b26c6bf9aa9e424466a7",
    "semantic_title": "",
    "citation_count": 41,
    "authors": [
      "Yue Tan",
      "Guodong Long",
      "LU LIU",
      "Tianyi Zhou",
      "Qinghua Lu",
      "Jing Jiang",
      "Chengqi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20820": {
    "title": "What about Inputting Policy in Value Function: Policy Representation and Policy-Extended Value Function Approximator",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a4e1cf41401674c40174fa8d6073d2b3e9be78d6",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Hongyao Tang",
      "Zhaopeng Meng",
      "Jianye Hao",
      "Chen Chen",
      "Daniel Graves",
      "Dong Li",
      "Changmin Yu",
      "Hangyu Mao",
      "Wulong Liu",
      "Yaodong Yang",
      "Wenyuan Tao",
      "Li Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20821": {
    "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "56b1bb665da6605f1bf99dd451d264cce08d1687",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20822": {
    "title": "With False Friends Like These, Who Can Notice Mistakes?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4dea1701757513fdf7a52e64cb85750dbf0ef662",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Lue Tao",
      "Lei Feng",
      "Jinfeng Yi",
      "Songcan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20823": {
    "title": "Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "151ddaa95cccb7f965be0490c097d9c41bf073a5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ran Tao",
      "Han Zhang",
      "Yutong Zheng",
      "Marios Savvides"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20824": {
    "title": "SMINet: State-Aware Multi-Aspect Interests Representation Network for Cold-Start Users Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "138ee1ae9cca79cb2f79a8b2b344927dbfcec95e",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Wanjie Tao",
      "Yu Li",
      "Liangyue Li",
      "Zulong Chen",
      "Hong Wen",
      "Peilin Chen",
      "Tingting Liang",
      "Quan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20825": {
    "title": "SplitFed: When Federated Learning Meets Split Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0294bd2e6638c9a3619d4baaa63202a3c511dccc",
    "semantic_title": "",
    "citation_count": 146,
    "authors": [
      "Chandra Thapa",
      "Pathum Chamikara Mahawaga Arachchige",
      "Seyit Camtepe",
      "Lichao Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20826": {
    "title": "Listwise Learning to Rank Based on Approximate Rank Indicators",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a890ecfa2eb8b5db0e47fd5e5562b35f92f5220e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Thonet",
      "Yagmur Gizem Cinar",
      "Eric Gaussier",
      "Minghan Li",
      "Jean-Michel Renders"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20827": {
    "title": "PrivateMail: Supervised Manifold Learning of Deep Features with Privacy for Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "645c4e288450712aa929d120c8c3f06d8f04bdcc",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Praneeth Vepakomma",
      "Julia Balla",
      "Ramesh Raskar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20828": {
    "title": "Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "71723f07a8e10db8ea72ad38c11c8922026338d9",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Sahil Verma",
      "Keegan Hines",
      "John P. Dickerson"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20829": {
    "title": "Robust Optimal Classification Trees against Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "017d986cde19758d06fa760d2830129c06363261",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Daniël Vos",
      "Sicco Verwer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20830": {
    "title": "Spline-PINN: Approaching PDEs without Data Using Fast, Physics-Informed Hermite-Spline CNNs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "43c31e66c67259045ea94426159794e0c2b4f5a0",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Nils Wandel",
      "Michael Weinmann",
      "Michael Neidlin",
      "Reinhard Klein"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20831": {
    "title": "Context Uncertainty in Contextual Bandits with Applications to Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "87b59a30d9d50680b20d1f1ff6b42e5981e38d73",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hao Wang",
      "Yifei Ma",
      "Hao Ding",
      "Yuyang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20832": {
    "title": "Demystifying Why Local Aggregation Helps: Convergence Analysis of Hierarchical SGD",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "908c8a84a7a678a5513564fb5c3c99820acc6c2e",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jiayi Wang",
      "Shiqiang Wang",
      "Rong-Rong Chen",
      "Mingyue Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20833": {
    "title": "Learngene: From Open-World to Your Learning Task",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "863f9919063a5385ab1aaa7b3171a9f8fbfd2e40",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Qiu-Feng Wang",
      "Xin Geng",
      "Shu-Xia Lin",
      "Shi-Yu Xia",
      "Lei Qi",
      "Ning Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20834": {
    "title": "Boosting Active Learning via Improving Test Performance",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4ec7a1cac8191d7f67c7159af398af1220fa98fe",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tianyang Wang",
      "Xingjian Li",
      "Pengkun Yang",
      "Guosheng Hu",
      "Xiangrui Zeng",
      "Siyu Huang",
      "Cheng-Zhong Xu",
      "Min Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20835": {
    "title": "Efficient Algorithms for General Isotone Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4d45e7386336bea48a707c78d1cab6f291208af3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwen Wang",
      "Jiaxi Ying",
      "José Vinícius de M. Cardoso",
      "Daniel P. Palomar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20836": {
    "title": "Efficient Causal Structure Learning from Multiple Interventional Datasets with Unknown Targets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fad3ae7839cdc6720dec2af683c036a9461ced63",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yunxia Wang",
      "Fuyuan Cao",
      "Kui Yu",
      "Jiye Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20837": {
    "title": "Continual Learning through Retrieval and Imagination",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "99be6600f10c72c2227f16d6d0eeeb6432a1e61a",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Zhen Wang",
      "Liu Liu",
      "Yiqun Duan",
      "Dacheng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20838": {
    "title": "Max-Min Grouped Bandits",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3cba5b6b50855282db6ffaa4ca2002b5f0b21a50",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhenlin Wang",
      "Jonathan Scarlett"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20839": {
    "title": "Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b2d7ffedc614de190df02ef8613743b5a76c578d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhihai Wang",
      "Jie Wang",
      "Qi Zhou",
      "Bin Li",
      "Houqiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20840": {
    "title": "Controlling Underestimation Bias in Reinforcement Learning via Quasi-median Operation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0437ec501424bf1637993128d0fbd682fb73e33b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Wei Wei",
      "Yujia Zhang",
      "Jiye Liang",
      "Lin Li",
      "Yyuze Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20841": {
    "title": "Symbolic Brittleness in Sequence Models: On Systematic Generalization in Symbolic Mathematics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "aead4418733b998792deb9cbf198a834449e00d2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Sean Welleck",
      "Peter West",
      "Jize Cao",
      "Yejin Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20842": {
    "title": "Prune and Tune Ensembles: Low-Cost Ensemble Learning with Sparse Independent Subnetworks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "502142121f85e78b1c38ece533ddfef016a9b26e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Tim Whitaker",
      "Darrell Whitley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20843": {
    "title": "PluGeN: Multi-Label Conditional Generation from Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c6da67b676d4355b7ce6ac26e5d08bd49a397d5f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maciej Wołczyk",
      "Magdalena Proszewska",
      "Łukasz Maziarka",
      "Maciej Zieba",
      "Patryk Wielopolski",
      "Rafał Kurczab",
      "Marek Smieja"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20844": {
    "title": "Structure Learning-Based Task Decomposition for Reinforcement Learning in Non-stationary Environments",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "476357ee1bdd49355aa2bef957c8d92c7c6f7994",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honguk Woo",
      "Gwangpyo Yoo",
      "Minjong Yoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20845": {
    "title": "An Efficient Combinatorial Optimization Model Using Learning-to-Rank Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c7072f3a6217402fe767c541d8b2d54c7c992d5c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Honguk Woo",
      "Hyunsung Lee",
      "Sangwoo Cho"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20846": {
    "title": "PUMA: Performance Unchanged Model Augmentation for Training Data Removal",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2e2c019217dd187ab43f074f65529b4a3fd3e823",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Ga Wu",
      "Masoud Hashemi",
      "Christopher Srinivasa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20847": {
    "title": "Generalizing Reinforcement Learning through Fusing Self-Supervised Learning into Intrinsic Motivation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "775c6ff63220e9aad98d5aee32788f1d45358075",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Keyu Wu",
      "Min Wu",
      "Zhenghua Chen",
      "Yuecong Xu",
      "Xiaoli Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20848": {
    "title": "AdaLoss: A Computationally-Efficient and Provably Convergent Adaptive Gradient Method",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d7bc06ae35a0cfd0c771eac0ce515a0c3809d7ac",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xiaoxia Wu",
      "Yuege Xie",
      "Simon Shaolei Du",
      "Rachel Ward"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20849": {
    "title": "Towards Off-Policy Learning for Ranking Policies with Logged Feedback",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c0e9ef2dc6bbb464352f3e41c6b5f3bc4854409",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Teng Xiao",
      "Suhang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20850": {
    "title": "Active Learning for Domain Adaptation: An Energy-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0282c031d07bf12d807392601371af86a56cce27",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Binhui Xie",
      "Longhui Yuan",
      "Shuang Li",
      "Chi Harold Liu",
      "Xinjing Cheng",
      "Guoren Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20851": {
    "title": "GearNet: Stepwise Dual Learning for Weakly Supervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d0168d31eb0aa2be477206a85c81fe960ff976ac",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Renchunzi Xie",
      "Hongxin Wei",
      "Lei Feng",
      "Bo An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20852": {
    "title": "Reinforcement Learning Augmented Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7ac85dcb5ae3fe1b19488e189332c95191dd7a13",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Guojun Xiong",
      "Jian Li",
      "Rahul Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20853": {
    "title": "Coordinating Momenta for Cross-Silo Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6a3504e083e728255b51023e021b0afe056380d1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "An Xu",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20854": {
    "title": "Learning-Augmented Algorithms for Online Steiner Tree",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cfc11d07d200cfb9542288127c6f62c31c3e4fbc",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Chenyang Xu",
      "Benjamin Moseley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20855": {
    "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "63572d61745c4c09709463d9c2df00bf9bf8e054",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Haoran Xu",
      "Xianyuan Zhan",
      "Xiangyu Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20856": {
    "title": "Deep Incomplete Multi-View Clustering via Mining Cluster Complementarity",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c37b6fdf9f4cbc27b91c23409501be7fe00f91ed",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Jie Xu",
      "Chao Li",
      "Yazhou Ren",
      "Liang Peng",
      "Yujie Mo",
      "Xiaoshuang Shi",
      "Xiaofeng Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20857": {
    "title": "Linearity-Aware Subspace Clustering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bf5188a93dee4a21e9bb87c22a292d1c567f8b0e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yesong Xu",
      "Shuo Chen",
      "Jun Li",
      "Jianjun Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20858": {
    "title": "Go Wider Instead of Deeper",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ffcd58f453f207d48075627da011f62782334c8f",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Fuzhao Xue",
      "Ziji Shi",
      "Futao Wei",
      "Yuxuan Lou",
      "Yong Liu",
      "Yang You"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20859": {
    "title": "Seizing Critical Learning Periods in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a60c063650e76b29cee119c4979d2bfd4be5b9c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Gang Yan",
      "Hao Wang",
      "Jian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20860": {
    "title": "Learning to Identify Top Elo Ratings: A Dueling Bandits Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e9dad1f76ba2428209b3305923ef9682be1214f3",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xue Yan",
      "Yali Du",
      "Binxin Ru",
      "Jun Wang",
      "Haifeng Zhang",
      "Xu Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20861": {
    "title": "Q-Ball: Modeling Basketball Games Using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f557ce7b93c4f414bc96d86c00d1e996cfddfbea",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chen Yanai",
      "Adir Solomon",
      "Gilad Katz",
      "Bracha Shapira",
      "Lior Rokach"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20862": {
    "title": "Training a Resilient Q-network against Observational Interference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a912cad9f9c7418240da2636c92c54df5d82006b",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Chao-Han Huck Yang",
      "I-Te Danny Hung",
      "Yi Ouyang",
      "Pin-Yu Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20863": {
    "title": "Policy Optimization with Stochastic Mirror Descent",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c415206df64b8fb8e066a6fe680d94d6a0f735f7",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Long Yang",
      "Yu Zhang",
      "Gang Zheng",
      "Qian Zheng",
      "Pengfei Li",
      "Jianhang Huang",
      "Gang Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20864": {
    "title": "Graph Pointer Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "abc881f2c2c12716c4ccc462c74d2d99ca14c601",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Tianmeng Yang",
      "Yujing Wang",
      "Zhihan Yue",
      "Yaming Yang",
      "Yunhai Tong",
      "Jing Bai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20865": {
    "title": "LOGICDEF: An Interpretable Defense Framework against Adversarial Examples via Inductive Scene Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "34a7c662375b0a254c65eff65ab63028441f1023",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yuan Yang",
      "James C Kerce",
      "Faramarz Fekri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20866": {
    "title": "Exploiting Invariance in Training Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "35654f393a7d749c16482a05e5c1db5728e0362e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chengxi Ye",
      "Xiong Zhou",
      "Tristan McKinney",
      "Yanfeng Liu",
      "Qinggang Zhou",
      "Fedor Zhdanov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20867": {
    "title": "Lifelong Generative Modelling Using Dynamic Expansion Graph Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "20233efc7bf13c004b458947fe9c20709c8b34aa",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20868": {
    "title": "Stage Conscious Attention Network (SCAN): A Demonstration-Conditioned Policy for Few-Shot Imitation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2f9e6c49fd4fbab33250b057957cf06fc23b8670",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Fong Yeh",
      "Chi-Ming Chung",
      "Hung-Ting Su",
      "Yi-Ting Chen",
      "Winston H. Hsu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20869": {
    "title": "BATUDE: Budget-Aware Neural Network Compression Based on Tucker Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1779db2f79a70972c533193606dd75e894590fa1",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Miao Yin",
      "Huy Phan",
      "Xiao Zang",
      "Siyu Liao",
      "Bo Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20870": {
    "title": "Distributed Randomized Sketching Kernel Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f5c2e497fb47e746a8e449a84a20e8d3b4df880f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Rong Yin",
      "Yong Liu",
      "Dan Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20871": {
    "title": "AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a3e14674236c633a9188df3bdd45317e884b25a9",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Yihang Yin",
      "Qingzhong Wang",
      "Siyu Huang",
      "Haoyi Xiong",
      "Xiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20872": {
    "title": "BM-NAS: Bilevel Multimodal Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0d0d3d8d5be06b418ca6d5d1eb73d2c96214376f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yihang Yin",
      "Siyu Huang",
      "Xiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20873": {
    "title": "Early-Bird GCNs: Graph-Network Co-optimization towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2eeef005ba04b0d384ad6afb5be2f849d61f2fff",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Haoran You",
      "Zhihan Lu",
      "Zijian Zhou",
      "Yonggan Fu",
      "Yingyan Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20874": {
    "title": "Hindsight Network Credit Assignment: Efficient Credit Assignment in Networks of Discrete Stochastic Units",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "be59a55febff717ee3027e4a11ca03dc890c3b7c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenny Young"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20875": {
    "title": "SAIL: Self-Augmented Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a49f843d9a2ba8dcfb80b1da45e441aae5c942b6",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Lu Yu",
      "Shichao Pei",
      "Lizhong Ding",
      "Jun Zhou",
      "Longfei Li",
      "Chuxu Zhang",
      "Xiangliang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20876": {
    "title": "Natural Black-Box Adversarial Examples against Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0c213604e7cc3942ad551bf51ca2d67ed56548bf",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mengran Yu",
      "Shiliang Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20877": {
    "title": "Regularization Penalty Optimization for Addressing Data Quality Variance in OoD Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c0fc2daaeeddb2cab2039eb32bfc3d5c890c1c53",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Runpeng Yu",
      "Hong Zhu",
      "Kaican Li",
      "Lanqing Hong",
      "Rui Zhang",
      "Nanyang Ye",
      "Shao-Lun Huang",
      "Xiuqiang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20878": {
    "title": "Low-Pass Graph Convolutional Network for Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "da6802b50714c63f216414a5588ce4345f9a5472",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wenhui Yu",
      "Zixin Zhang",
      "Zheng Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20879": {
    "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-Grained Input-Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b6e9f1189fd46dabd6f1059c546cd2f4589fe65d",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Sicheng Li",
      "Chaojian Li",
      "Yingyan Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20880": {
    "title": "Unsupervised Learning of Compositional Scene Representations from Multiple Unspecified Viewpoints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d209d4862d6257ae1cd150d52bcef1f289baf2fa",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jinyang Yuan",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20881": {
    "title": "TS2Vec: Towards Universal Representation of Time Series",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f79a319413b5c014e4a98bcd223e6f65e3f7901b",
    "semantic_title": "",
    "citation_count": 60,
    "authors": [
      "Zhihan Yue",
      "Yujing Wang",
      "Juanyong Duan",
      "Tianmeng Yang",
      "Congrui Huang",
      "Yunhai Tong",
      "Bixiong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20882": {
    "title": "Fractional Adaptive Linear Units",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9e6551105819b844918005aa62130504da16c4a8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Julio Zamora",
      "Anthony D. Rhodes",
      "Lama Nachman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20883": {
    "title": "SimSR: Simple Distance-Based State Representations for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f20c2aee7b91ed3c72d059499a77f335faa45167",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hongyu Zang",
      "Xin Li",
      "Mingzhong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20884": {
    "title": "Efficient Decentralized Stochastic Gradient Descent Method for Nonconvex Finite-Sum Optimization Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3b6393a0e2382d0241f4093dd9c6f5c78b27861e",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Wenkang Zhan",
      "Gang Wu",
      "Hongchang Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20885": {
    "title": "MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5e4c58e1be010eb9e7ea6773e2b082d2095be5f1",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Baoquan Zhang",
      "Xutao Li",
      "Shanshan Feng",
      "Yunming Ye",
      "Rui Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20886": {
    "title": "State Deviation Correction for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3c21b026c7b418335742e14ace107353a945dc12",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hongchang Zhang",
      "Jianzhun Shao",
      "Yuhang Jiang",
      "Shuncheng He",
      "Guanwen Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20887": {
    "title": "Multi-Agent Reinforcement Learning with General Utilities via Decentralized Shadow Reward Actor-Critic",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "18247aad4c2f600c301234b2bacfa48643cc34f4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Junyu Zhang",
      "Amrit Singh Bedi",
      "Mengdi Wang",
      "Alec Koppel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20888": {
    "title": "Co-promotion Predictions of Financing Market and Sales Market: A Cooperative-Competitive Attention Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "883063b6e6f91af4337551cf1abe7c97600e856c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Lei Zhang",
      "Wang Xiang",
      "Chuang Zhao",
      "Hongke Zhao",
      "Rui Li",
      "Runze Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20889": {
    "title": "Categorical Neighbour Correlation Coefficient (CnCor) for Detecting Relationships between Categorical Variables",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "67eb0cfcacc1bb285b8a8e53266f9e073489804f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifeng Zhang",
      "Shimo Yang",
      "Hongxun Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20890": {
    "title": "Interpretable Domain Adaptation for Hidden Subdomain Alignment in the Context of Pre-trained Source Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e7f0679bab93169b824ed12d6939533f5dc1c9b7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Luxin Zhang",
      "Pascal Germain",
      "Yacine Kessaci",
      "Christophe Biernacki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20891": {
    "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1378b2b507fbbc6d475835eeb740f1c7426d7be5",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Matthew S. Zhang",
      "Murat A Erdogdu",
      "Animesh Garg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20892": {
    "title": "Gaussian Process Bandits with Aggregated Feedback",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "42c4a8bf4317d25bc11f438e50d02bd751edc32b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mengyan Zhang",
      "Russell Tsuchida",
      "Cheng Soon Ong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20893": {
    "title": "Rethinking Influence Functions of Neural Networks in the Over-Parameterized Regime",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ee2c7ae4f8c819eaba6427cb1beaccce6c154b40",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Rui Zhang",
      "Shihua Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20894": {
    "title": "A Multi-Agent Reinforcement Learning Approach for Efficient Client Selection in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3385b550a221316a1cd36b9b7a7c82992651a99c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Sai Qian Zhang",
      "Jieyu Lin",
      "Qi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20895": {
    "title": "Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "435cd08e94668fc4a48cb53e31909a6a269ae9ef",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yi Zhang",
      "Mingyuan Chen",
      "Jundong Shen",
      "Chongjun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20896": {
    "title": "Fusion Multiple Kernel K-means",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "45ad9236f2a53c95a85823f5e69853440067e8e6",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yi Zhang",
      "Xinwang Liu",
      "Jiyuan Liu",
      "Sisi Dai",
      "Changwang Zhang",
      "Kai Xu",
      "En Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20897": {
    "title": "Batch Active Learning with Graph Neural Networks via Multi-Agent Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "acda0d4c1b3a63a73849f05cbc73dd283f87edf9",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yuheng Zhang",
      "Hanghang Tong",
      "Yinglong Xia",
      "Yan Zhu",
      "Yuejie Chi",
      "Lei Ying"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20898": {
    "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7de413da6e0a00e14270cfaed2a31666e7c28747",
    "semantic_title": "",
    "citation_count": 33,
    "authors": [
      "Zaixi Zhang",
      "Qi Liu",
      "Hao Wang",
      "Chengqiang Lu",
      "Cheekong Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20899": {
    "title": "Learning to Solve Travelling Salesman Problem with Hardness-Adaptive Curriculum",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1ceefc6af868dadffeecdaace1a023975d8e316f",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zeyang Zhang",
      "Ziwei Zhang",
      "Xin Wang",
      "Wenwu Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20900": {
    "title": "Robust Action Gap Increasing with Clipped Advantage Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0ec532b955b4effd2d6f077e027eddec2f041246",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhang",
      "Yaozhong Gan",
      "Xiaoyang Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20901": {
    "title": "Online Influence Maximization with Node-Level Feedback Using Standard Offline Oracles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6be667f07158d2c9e46ee7443c7ab7756e8b7f8d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Zhijie Zhang",
      "Wei Chen",
      "Xiaoming Sun",
      "Jialin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20902": {
    "title": "CLPA: Clean-Label Poisoning Availability Attacks Using Generative Adversarial Nets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8216096d7268ca560c52eb3ca8cca680725b0c8f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Bingyin Zhao",
      "Yingjie Lao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20903": {
    "title": "FedInv: Byzantine-Robust Federated Learning by Inversing Local Model Updates",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7e1dbebd35df4d7260ebfe56bca0f7a09c4847f5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Bo Zhao",
      "Peng Sun",
      "Tao Wang",
      "Keyu Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20904": {
    "title": "Well-Classified Examples Are Underestimated in Classification with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f8b12f9ce1cfbe13435243bdc7dabd4eb95c05fc",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Guangxiang Zhao",
      "Wenkai Yang",
      "Xuancheng Ren",
      "Lei Li",
      "Yunfang Wu",
      "Xu Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20905": {
    "title": "Error-Based Knockoffs Inference for Controlled Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "22a9307e6108e7357f5198bcf4d33d97ab8e2982",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xuebin Zhao",
      "Hong Chen",
      "Yingjie Wang",
      "Weifu Li",
      "Tieliang Gong",
      "Yulong Wang",
      "Feng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20906": {
    "title": "Online Missing Value Imputation and Change Point Detection with the Gaussian Copula",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "108be462fd92055fd83df07ad5ae3dfac9080702",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yuxuan Zhao",
      "Eric Landgrebe",
      "Eliot Shekhtman",
      "Madeleine Udell"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20907": {
    "title": "LaSSL: Label-Guided Self-Training for Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f25deb30cdc5a1e3ff1b7789e9646a95baca47ed",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Zhen Zhao",
      "Luping Zhou",
      "Lei Wang",
      "Yinghuan Shi",
      "Yang Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20908": {
    "title": "Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e28b9bc26f5f7eb3b0532d823713400202372da2",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Liyuan Zheng",
      "Tanner Fiez",
      "Zane Alumbaugh",
      "Benjamin Chasnov",
      "Lillian J. Ratliff"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20909": {
    "title": "Adaptive Pairwise Weights for Temporal Credit Assignment",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "45b50ef6edf1cedb28f9ae5157ff88db79451558",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zheng",
      "Risto Vuorio",
      "Richard Lewis",
      "Satinder Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20910": {
    "title": "Programmatic Reward Design by Example",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e2eb7309a1bf6162549785c56e3e426918e4fd60",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weichao Zhou",
      "Wenchao Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20911": {
    "title": "Neural Piecewise-Constant Delay Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c6ecbe85ee9b4d1b558028f8048277173630daec",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Qunxi Zhu",
      "Yifei Shen",
      "Dongsheng Li",
      "Wei Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20912": {
    "title": "Structural Landmarking and Interaction Modelling: A \"SLIM\" Network for Graph Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fe963686be76b5da13a3c159bbd74a9815ea95ef",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaokang Zhu",
      "Kai Zhang",
      "Jun Wang",
      "Haibin Ling",
      "Jie Zhang",
      "Hongyuan Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20913": {
    "title": "Invariant Action Effect Model for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1632f51c8e573264730061dc5c9e7db821535bc4",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zheng-Mao Zhu",
      "Shengyi Jiang",
      "Yu-Ren Liu",
      "Yang Yu",
      "Kun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20914": {
    "title": "Self-Adaptive Imitation Learning: Learning Tasks with Delayed Rewards from Sub-optimal Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ee2e080407e112abb764631405df5832934eeb57",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zhuangdi Zhu",
      "Kaixiang Lin",
      "Bo Dai",
      "Jiayu Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/20915": {
    "title": "Locality Matters: A Scalable Value Decomposition Approach for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e6f105477814f500a700ebaaf09df678c3b1250e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Roy Zohar",
      "Shie Mannor",
      "Guy Tennenholtz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21156": {
    "title": "Hedonic Games with Fixed-Size Coalitions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d59be55999511f5ba1ad40e561b9f9739dbe5821",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Vittorio Bilò",
      "Gianpiero Monaco",
      "Luca Moscardelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21158": {
    "title": "Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a0ad7fd42bb4dc9489a4dcf6eddaee8baf4d5f06",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Erdem Biyik",
      "Anusha Lalitha",
      "Rajarshi Saha",
      "Andrea Goldsmith",
      "Dorsa Sadigh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21160": {
    "title": "Fixation Maximization in the Positional Moran Process",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "60b306321e203a35132c4335bb2eb637fe91ac6b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Joachim Brendborg",
      "Panagiotis Karras",
      "Andreas Pavlogiannis",
      "Asger Ullersted Rasmussen",
      "Josef Tkadlec"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21162": {
    "title": "Flex Distribution for Bounded-Suboptimal Multi-Agent Path Finding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1d3345101696bb414664cde830746e3ea6d4787e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shao-Hung Chan",
      "Jiaoyang Li",
      "Graeme Gange",
      "Daniel Harabor",
      "Peter J. Stuckey",
      "Sven Koenig"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21163": {
    "title": "Participatory Budgeting with Donations and Diversity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "99f50bfdf75144dc6ea1aab6093621171d1b6222",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jiehua Chen",
      "Martin Lackner",
      "Jan Maly"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21164": {
    "title": "Pretrained Cost Model for Distributed Constraint Optimization Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5012ecfad59793a330ec2c6311234acf590667b9",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yanchen Deng",
      "Shufeng Kong",
      "Bo An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21165": {
    "title": "Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7bac30e24036a111eb0a20bf8e4a9cef3cb79559",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Qingxu Fu",
      "Tenghai Qiu",
      "Jianqiang Yi",
      "Zhiqiang Pu",
      "Shiguang Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21166": {
    "title": "Cooperative Multi-Agent Fairness and Equivariant Policies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f740690d5044901b959f93659999355f6a6825fc",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Niko A. Grupen",
      "Bart Selman",
      "Daniel D. Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21167": {
    "title": "Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e968c08085aa917586620521249ed0683065c6ce",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Mingyu Guo",
      "Jialiang Li",
      "Aneta Neumann",
      "Frank Neumann",
      "Hung Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21168": {
    "title": "Anytime Multi-Agent Path Finding via Machine Learning-Guided Large Neighborhood Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "81e43d43c6ae0fb9696b39c2d29cc1987a2b58c8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Taoan Huang",
      "Jiaoyang Li",
      "Sven Koenig",
      "Bistra Dilkina"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21169": {
    "title": "MDPGT: Momentum-Based Decentralized Policy Gradient Tracking",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b8a34431cbe4d374a49f993e68337e7c92533839",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhanhong Jiang",
      "Xian Yeow Lee",
      "Sin Yong Tan",
      "Kai Liang Tan",
      "Aditya Balu",
      "Young M Lee",
      "Chinmay Hegde",
      "Soumik Sarkar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21170": {
    "title": "Shard Systems: Scalable, Robust and Persistent Multi-Agent Path Finding with Performance Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d3bb2a4451c8ff90c7a5dced062b21cfec641421",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Christopher Leet",
      "Jiaoyang Li",
      "Sven Koenig"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21171": {
    "title": "A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2fdd6e9f639fee9cf722a1a35b0cf8b306e59310",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Xueguang Lyu",
      "Andrea Baisero",
      "Yuchen Xiao",
      "Christopher Amato"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21172": {
    "title": "When Can the Defender Effectively Deceive Attackers in Security Games?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fb1d06a595cca5eced44b69abcb060ac4341aa68",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Nguyen",
      "Haifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21173": {
    "title": "Generalization in Mean Field Games by Learning Master Policies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "51e1f805fa2b04d7a7755eac55ac5086385e52c0",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Sarah Perrin",
      "Mathieu Laurière",
      "Julien Pérolat",
      "Romuald Élie",
      "Matthieu Geist",
      "Olivier Pietquin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21174": {
    "title": "Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems: Complexity, Special Case Algorithms and Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "551cde23cafa0c926f8ae067a14dfeacfb3aa488",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirou Qiu",
      "Chen Chen",
      "Madhav Marathe",
      "S.S. Ravi",
      "Daniel J. Rosenkrantz",
      "Richard Stearns",
      "Anil Vullikanti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21175": {
    "title": "How Many Representatives Do We Need? The Optimal Size of a Congress Voting on Binary Issues",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "26d8ef467f834004ca6cbb59f03f1b13168d604d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manon Revel",
      "Tao Lin",
      "Daniel Halpern"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21176": {
    "title": "Decentralized Mean Field Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "349df5e4a33588057bb4dedb3ff6dbef039d7262",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Sriram Ganapathi Subramanian",
      "Matthew E. Taylor",
      "Mark Crowley",
      "Pascal Poupart"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21177": {
    "title": "Incentivizing Collaboration in Machine Learning via Synthetic Data Rewards",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8f181c54d3d7ca79218207471f00c368392b76d1",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sebastian Shenghong Tay",
      "Xinyi Xu",
      "Chuan Sheng Foo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21178": {
    "title": "Learning the Optimal Recommendation from Explorative Users",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c2a8c6c03041222274799be59cb7ade30a68193a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Fan Yao",
      "Chuanhao Li",
      "Denis Nekipelov",
      "Hongning Wang",
      "Haifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21179": {
    "title": "Multi-Agent Incentive Communication via Decentralized Teammate Modeling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fc8e507b1ff14de57157c6c5c730247372f961f0",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Lei Yuan",
      "Jianhao Wang",
      "Fuxiang Zhang",
      "Chenghe Wang",
      "ZongZhang Zhang",
      "Yang Yu",
      "Chongjie Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21180": {
    "title": "MLink: Linking Black-Box Models for Collaborative Multi-Model Inference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "89ce6565222c245826b454ca19050623fd1c5d0b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mu Yuan",
      "Lan Zhang",
      "Xiang-Yang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21181": {
    "title": "Equilibrium Finding in Normal-Form Games via Greedy Regret Minimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ece043304f2b608d776d3cd695a58d8ffc7a80a1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hugh Zhang",
      "Adam Lerer",
      "Noam Brown"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21182": {
    "title": "Why Fair Labels Can Yield Unfair Predictions: Graphical Conditions for Introduced Unfairness",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5bde6e5180841063adb15b0d041d7e5858df399e",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Carolyn Ashurst",
      "Ryan Carey",
      "Silvia Chiappa",
      "Tom Everitt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21183": {
    "title": "Incorporating Item Frequency for Differentially Private Set Union",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9fe931de36bb4bb83297de4edaa29d233044cae4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ricardo Silva Carvalho",
      "Ke Wang",
      "Lovedeep Singh Gondara"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21184": {
    "title": "Cosine Model Watermarking against Ensemble Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "83a144e7425ee02c66f187e3e531c526620f310f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Laurent Charette",
      "Lingyang Chu",
      "Yizhou Chen",
      "Jian Pei",
      "Lanjun Wang",
      "Yong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21185": {
    "title": "Towards Debiasing DNN Models from Spurious Feature Influence",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4510c8e4e13fe1ab99c600ead23d2da0c00de04a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mengnan Du",
      "Ruixiang Tang",
      "Weijie Fu",
      "Xia Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21186": {
    "title": "Path-Specific Objectives for Safer Agent Incentives",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6d624cb1e7126e3245d55c7208ef6f04141e5895",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sebastian Farquhar",
      "Ryan Carey",
      "Tom Everitt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21187": {
    "title": "Algorithmic Fairness Verification with Graphical Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9d4c4638bd37007218a3d02121e633e1fb92bd4a",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S Meel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21188": {
    "title": "Achieving Long-Term Fairness in Sequential Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "36c6a7fcf4917c1fc345b1a3540acef1309a36ba",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yaowei Hu",
      "Lu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21189": {
    "title": "Fairness without Imputation: A Decision Tree Approach for Fair Prediction with Missing Values",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2220357eda604c56232949a3b18ad47075db2554",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Haewon Jeong",
      "Hao Wang",
      "Flavio P. Calmon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21190": {
    "title": "Shaping Noise for Robust Attributions in Neural Stochastic Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "876a63ab3da9b93cc15e70e34049b5f2e14050a1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sumit Kumar Jha",
      "Rickard Ewetz",
      "Alvaro Velasquez",
      "Arvind Ramanathan",
      "Susmit Jha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21191": {
    "title": "Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cfdae0661a917d8d06a32ab52b2ffac73637b616",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Jinyuan Jia",
      "Yupei Liu",
      "Xiaoyu Cao",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21192": {
    "title": "On the Fairness of Causal Algorithmic Recourse",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6b0a1a69ba65618a104b421cd434be08cca9a8eb",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Julius von Kügelgen",
      "Amir-Hossein Karimi",
      "Umang Bhatt",
      "Isabel Valera",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21193": {
    "title": "DeepAuth: A DNN Authentication Framework by Model-Unique and Fragile Signature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b94924311f9b0945f50e3881ac355a89b1141464",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yingjie Lao",
      "Weijie Zhao",
      "Peng Yang",
      "Ping Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21194": {
    "title": "Fast Sparse Decision Tree Optimization via Reference Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b563aa48719abbafa19ba4169a96ecc7be4aa1f6",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Hayden McTavish",
      "Chudi Zhong",
      "Reto Achermann",
      "Ilias Karimalis",
      "Jacques Chen",
      "Cynthia Rudin",
      "Margo Seltzer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21195": {
    "title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-Box Model Explanation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5697da09cdfd3ce7768b126ced6b5a0b82e4e884",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Thien Q Tran",
      "Kazuto Fukuchi",
      "Youhei Akimoto",
      "Jun Sakuma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21196": {
    "title": "Do Feature Attribution Methods Correctly Attribute Features?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "426734685283b4a0c08b34cd9e996e2e30e7f7ee",
    "semantic_title": "",
    "citation_count": 53,
    "authors": [
      "Yilun Zhou",
      "Serena Booth",
      "Marco Tulio Ribeiro",
      "Julie Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21197": {
    "title": "Formal Semantics and Formally Verified Validation for Temporal Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "464851deaa2ff15a76e782fefc87ef2be4404dac",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Mohammad Abdulaziz",
      "Lukas Koller"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21198": {
    "title": "Goal Recognition as Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "412b6cb8c965474a2d98ea7e4856a4463d17c429",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Leonardo Amado",
      "Reuth Mirsky",
      "Felipe Meneguzzi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21199": {
    "title": "Online Search with Best-Price and Query-Based Predictions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fe4fbe8a56c7b5c5c2663ee37419c90380ddffd8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Spyros Angelopoulos",
      "Shahin Kamali",
      "Dehou Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21200": {
    "title": "Extended Goal Recognition Design with First-Order Computation Tree Logic",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d8a8791133ed1b7d3ee4ce885d6eb661b235126e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsz-Chiu Au"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21201": {
    "title": "Sampling-Based Robust Control of Autonomous Systems with Non-Gaussian Noise",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ebc8a76ccae6b28b60fec99455560fc6ee2ae965",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Thom S. Badings",
      "Alessandro Abate",
      "Nils Jansen",
      "David Parker",
      "Hasan A. Poonawala",
      "Marielle Stoelinga"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21202": {
    "title": "Synthesis from Satisficing and Temporal Goals",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a409ef38d9043b53881557dab68f93bef1dfabc3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Suguman Bansal",
      "Lydia Kavraki",
      "Moshe Y. Vardi",
      "Andrew Wells"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21203": {
    "title": "Making Translations to Classical Planning Competitive with Other HTN Planners",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d4c723aea136ce2164e626fbff58bbafa262c91d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Gregor Behnke",
      "Florian Pollitt",
      "Daniel Höller",
      "Pascal Bercher",
      "Ron Alford"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21204": {
    "title": "PlanVerb: Domain-Independent Verbalization and Summary of Task Plans",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8c3498d55e606b4ab207ec83ae57774d80047054",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Gerard Canal",
      "Senka Krivić",
      "Paul Luff",
      "Andrew Coles"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21205": {
    "title": "Competing for Resources: Estimating Adversary Strategy for Effective Plan Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4dda064b1c25cd774882cf7ac9c28e4d8b8e7d9b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukáš Chrpa",
      "Pavel Rytíř",
      "Rostislav Horčík",
      "Stefan Edelkamp"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21206": {
    "title": "The FF Heuristic for Lifted Classical Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68c9387811d3975cf506fc80ef909c865126f8ab",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Augusto B. Corrêa",
      "Florian Pommerening",
      "Malte Helmert",
      "Guillem Francès"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21207": {
    "title": "Inconsistent Planning: When in Doubt, Toss a Coin!",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8d53489cd07aeef21f2770e47fbb834ca8d9c9c7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuriy Dementiev",
      "Fedor Fomin",
      "Artur Ignatiev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21208": {
    "title": "Robustification of Online Graph Exploration Methods",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "669324705305e48e62105194fa927489e229dcde",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Franziska Eberle",
      "Alexander Lindermayr",
      "Nicole Megow",
      "Lukas Nölke",
      "Jens Schlöter"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21209": {
    "title": "Explainable Planner Selection for Classical Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8805ba0b92fe0be5903adec89926b0ca706d30b6",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Ferber",
      "Jendrik Seipp"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21210": {
    "title": "Operator-Potential Heuristics for Symbolic Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "94a3e7d9064adb177c3fb404e61193dfc25a4594",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Daniel Fišer",
      "Álvaro Torralba",
      "Jörg Hoffmann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21211": {
    "title": "Reconfiguring Shortest Paths in Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9a40abe73a1f1c45d6d1192a8c7b5358a62c79be",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Kshitij Gajjar",
      "Agastya Vibhuti Jha",
      "Manish Kumar",
      "Abhiruk Lahiri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21212": {
    "title": "Homomorphisms of Lifted Planning Tasks: The Case for Delete-Free Relaxation Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b0ad5bf16cb43807489bfa297ee985f31bcabc67",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Rostislav Horčík",
      "Daniel Fišer",
      "Álvaro Torralba"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21213": {
    "title": "Speeding Up the RUL¯ Dynamic-Controllability-Checking Algorithm for Simple Temporal Networks with Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e5ac2bf9421edc24aa43b85fe637ce5048d354d1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Hunsberger",
      "Roberto Posenato"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21214": {
    "title": "Learning to Solve Routing Problems via Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9f6f85e59bf929a9c15055d6b73291f5395e94bd",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yuan Jiang",
      "Yaoxin Wu",
      "Zhiguang Cao",
      "Jie Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21215": {
    "title": "Learning Probably Approximately Complete and Safe Action Models for Stochastic Worlds",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cf62922723b788bcfa73dd11c4bd8ba46a946b04",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brendan Juba",
      "Roni Stern"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21216": {
    "title": "Bounding Quality in Diverse Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dddef96dd045b3676c3c6e1794fe647b51469b85",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Michael Katz",
      "Shirin Sohrabi",
      "Octavian Udrea"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21217": {
    "title": "A* Search and Bound-Sensitive Heuristics for Oversubscription Planning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "70898c858aa72ec68e9c1fc5effd28baa5db10b1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Michael Katz",
      "Emil Keyder"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21218": {
    "title": "NICE: Robust Scheduling through Reinforcement Learning-Guided Integer Programming",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "eb1ed3e65130fb14e86a4a3a1c6c70bae113e4dd",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Luke Kenworthy",
      "Siddharth Nayak",
      "Christopher Chin",
      "Hamsa Balakrishnan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21219": {
    "title": "Planning to Avoid Side Effects",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "09de97b634c6b09964cef32fc5c1e4b1c5c99811",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Toryn Q. Klassen",
      "Sheila A. McIlraith",
      "Christian Muise",
      "Jarvis Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21220": {
    "title": "Sample-Efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fc49c529169a12f70be3112284721a28491224af",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar",
      "Scott Sanner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21221": {
    "title": "Bridging LTLf Inference to GNN Inference for Learning LTLf Formulae",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ec90d9e5ec6e2b11d8076e274d4dec4744fef95c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weilin Luo",
      "Pingjia Liang",
      "Jianfeng Du",
      "Hai Wan",
      "Bo Peng",
      "Delong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21222": {
    "title": "Risk-Aware Stochastic Shortest Path",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a61a536c3a2de48d27152c8f4c14d7099c63029b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Tobias Meggendorfer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21223": {
    "title": "Differential Assessment of Black-Box AI Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b96452dd9f472a24b4a918ccca4032e9fabb44be",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Rashmeet Kaur Nayyar",
      "Pulkit Verma",
      "Siddharth Srivastava"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21224": {
    "title": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability Using Tree Search and Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4fda71d3c53f1dea835ae181921bcf259de59ebb",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kevin Osanlou",
      "Jeremy Frank",
      "Andrei Bursuc",
      "Tristan Cazenave",
      "Eric Jacopin",
      "Christophe Guettier",
      "J. Benton"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21225": {
    "title": "Deciding Unsolvability in Temporal Planning under Action Non-Self-Overlapping",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "44b8f26128ae7cfe86c16d198856587e27155ecf",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Panjkovic",
      "Andrea Micheli",
      "Alessandro Cimatti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21226": {
    "title": "A Distributional Framework for Risk-Sensitive End-to-End Planning in Continuous MDPs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "695473d49e5b6876fc6fef799ce82a6c74f77037",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Patton",
      "Jihwan Jeong",
      "Mike Gimelfarb",
      "Scott Sanner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21227": {
    "title": "Formula Synthesis in Propositional Dynamic Logic with Shuffle",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e49eb4c9c3f41491ad6ec2acd9d6b537e84bc0d7",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sophie Pinchinat",
      "Sasha Rubin",
      "François Schwarzentruber"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21228": {
    "title": "Efficient Encoding of Cost Optimal Delete-Free Planning as SAT",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cd5bcf5e8e2660d8e38fd0699fdccae4c8ce93c2",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Masood Feyzbakhsh Rankooh",
      "Jussi Rintanen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21229": {
    "title": "Optimal Admission Control for Multiclass Queues with Time-Varying Arrival Rates via State Abstraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b3c36ea2d00e2bcf5ea55b75df92e73deee5c901",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Rigter",
      "Danial Dervovic",
      "Parisa Hassanzadeh",
      "Jason Long",
      "Parisa Zehtabi",
      "Daniele Magazzeni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21230": {
    "title": "Enhancing Column Generation by a Machine-Learning-Based Pricing Heuristic for Graph Coloring",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "267a19fcc41cb964faaa6114aefd4391c5c1c98b",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yunzhuang Shen",
      "Yuan Sun",
      "Xiaodong Li",
      "Andrew Eberhard",
      "Andreas Ernst"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21231": {
    "title": "Qubit Routing Using Graph Neural Network Aided Monte Carlo Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5648551b47561917a796db2694f01cb5232fcdd0",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Animesh Sinha",
      "Utkarsh Azad",
      "Harjinder Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21232": {
    "title": "Classical Planning with Avoid Conditions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c055ab009b82dbc00d2ea2e3d56682c053ec0c7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Steinmetz",
      "Jörg Hoffmann",
      "Alisa Kovtunova",
      "Stefan Borgwardt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21233": {
    "title": "Stochastic Goal Recognition Design Problems with Suboptimal Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68520e8162d1f725620e8b92ae9a06c9a4411785",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christabel Wayllace",
      "William Yeoh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21234": {
    "title": "Equity Promotion in Online Resource Allocation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1df817967454019f9a9392e210c9b2254ff52621",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pan Xu",
      "Yifan Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21235": {
    "title": "Efficient Device Scheduling with Multi-Job Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5b0f61e0af7abf2c54b6601d8c40a69f09b49823",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Chendi Zhou",
      "Ji Liu",
      "Juncheng Jia",
      "Jingbo Zhou",
      "Yang Zhou",
      "Huaiyu Dai",
      "Dejing Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21236": {
    "title": "MAPDP: Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2f1df6c61e66081e91defe3b0815e9854cae1589",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zefang Zong",
      "Meng Zheng",
      "Yong Li",
      "Depeng Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21237": {
    "title": "Entropy Estimation via Normalizing Flow",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "14e5883b9d4e664deb20ca82e4efad476310098c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ziqiao Ao",
      "Jinglai  Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21238": {
    "title": "Fast and More Powerful Selective Inference for Sparse High-Order Interaction Model",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b4db4ce28b81d208519b66d582a4dff5697324c3",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Diptesh Das",
      "Vo Nguyen Le Duy",
      "Hiroyuki Hanada",
      "Koji Tsuda",
      "Ichiro Takeuchi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21239": {
    "title": "Generalized Stochastic Matching",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "56b69d4abba205fe537a4d5ea29eb37778dca76a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Farhadi",
      "Jacob Gilbert",
      "MohammadTaghi Hajiaghayi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21240": {
    "title": "Robust Tests in Online Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f0fbb0627602cdfb07e779299eed051bb63875ce",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gi-Soo Kim",
      "Jane P Kim",
      "Hyun-Joon Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21241": {
    "title": "Local Differential Privacy for Belief Functions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2f6cefb1cc06a63b99af19eb57da9192143b7219",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Li",
      "Chunlai Zhou",
      "Biao Qin",
      "Zhiqiang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21242": {
    "title": "A Complete Criterion for Value of Information in Soluble Influence Diagrams",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a43196ca18c750472479c4d9a4ffe09659e5e3cd",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris van Merwijk",
      "Ryan Carey",
      "Tom Everitt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21243": {
    "title": "Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7b5a53106c08dcbb80c8efa6325911ee022abebe",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Lu Mi",
      "Hao Wang",
      "Yonglong Tian",
      "Hao He",
      "Nir N Shavit"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21244": {
    "title": "On the Impact of Spurious Correlation for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a98cc4d37b0af9be710c5b6f4a3579331e6fcfbe",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yifei Ming",
      "Hang Yin",
      "Yixuan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21245": {
    "title": "Inference and Learning with Model Uncertainty in Probabilistic Logic Programs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "84e4f7b543d129678cf04b8adbbdd6decfb0722b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Verreet",
      "Vincent Derkinderen",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21246": {
    "title": "Domain-Lifted Sampling for Universal Two-Variable Logic and Extensions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "632d8a8b73b851e5083f430c3c941befde5112a7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yuanhong Wang",
      "Timothy van Bremen",
      "Yuyi Wang",
      "Ondřej Kuželka"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21247": {
    "title": "Identifiability of Linear AMP Chain Graph Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b08649b7b285cf9dfc189823715ee23ff8f28d07",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yuhao Wang",
      "Arnab Bhattacharyya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21248": {
    "title": "DeepStochLog: Neural Stochastic Logic Programming",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ef01e99f73593bcc53cb9251928d51c551068e8e",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Thomas Winters",
      "Giuseppe Marra",
      "Robin Manhaeve",
      "Luc De Raedt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21249": {
    "title": "Towards Robust Off-Policy Learning for Runtime Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f7ae0a0a8d36ae99368aafff53240c69aa1cf680",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Da Xu",
      "Yuting Ye",
      "Chuanwei Ruan",
      "Bo Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21250": {
    "title": "Improving Bayesian Neural Networks by Adversarial Sampling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ce5b1650af21f006b9ec4debba2a1aab78b98feb",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jiaru Zhang",
      "Yang Hua",
      "Tao Song",
      "Hao Wang",
      "Zhengui Xue",
      "Ruhui Ma",
      "Haibing Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21251": {
    "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e2179a9911d30cb5cbde20506addd62088818e3c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Dongsheng An",
      "Na Lei",
      "Xiaoyin Xu",
      "Xianfeng Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21252": {
    "title": "Local and Global Linear Convergence of General Low-Rank Matrix Recovery Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "aeb960d8620c21f87afe144b3c87304844272c74",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yingjie Bi",
      "Haixiang Zhang",
      "Javad Lavaei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21253": {
    "title": "A*+BFHS: A Hybrid Heuristic Search Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cae96a1ed6d8129c8ee433b8754e059eb1d4af6c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhaoxing Bu",
      "Richard E. Korf"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21254": {
    "title": "NukCP: An Improved Local Search Algorithm for Maximum k-Club Problem",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4145186c7be17a6c4ab28def4db7596fe8ea6f18",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jiejiang Chen",
      "Yiyuan Wang",
      "Shaowei Cai",
      "Minghao Yin",
      "Yupeng Zhou",
      "Jieyu Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21255": {
    "title": "Fourier Representations for Black-Box Optimization over Categorical Variables",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a6fd6aa5e7b26a400a104bb3ef522d88bbe69cf7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hamid Dadkhahi",
      "Jesus Rios",
      "Karthikeyan Shanmugam",
      "Payel Das"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21256": {
    "title": "New Results in Bounded-Suboptimal Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bfb5c87deb4e081d48ceee7bf3fce76070b404d6",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Maximilian Fickert",
      "Tianyi Gu",
      "Wheeler Ruml"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21257": {
    "title": "An Exact Algorithm with New Upper Bounds for the Maximum k-Defective Clique Problem in Massive Sparse Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e2e36d9085fc3e6ed04c16bec800f02d7dbd923e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jian Gao",
      "Zhenghang Xu",
      "Ruizhi Li",
      "Minghao Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21258": {
    "title": "Learning from Mistakes – a Framework for Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c2bbd54bc227d630c2018725d4edb52ea8fa81e3",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Bhanu Garg",
      "Li Zhang",
      "Pradyumna Sridhara",
      "Ramtin Hosseini",
      "Eric Xing",
      "Pengtao Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21259": {
    "title": "The Complexity of Temporal Vertex Cover in Small-Degree Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "54536fe6b21770eabc14a3315bff3ddab6d29bbb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Thekla Hamm",
      "Nina Klobas",
      "George B. Mertzios",
      "Paul G. Spirakis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21260": {
    "title": "Provable Sensor Sets for Epidemic Detection over Networks with Minimum Delay",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c6b4e98e8e2b0bb3c54df31d64a5a2d10eba1fca",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jack Heavey",
      "Jiaming Cui",
      "Chen Chen",
      "B. Aditya Prakash",
      "Anil Vullikanti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21261": {
    "title": "Towards Automated Discovery of God-Like Folk Algorithms for Rubik's Cube",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c81d5ee77ca18c6becdb867510b1144c9553b2f8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Garrett E. Katz",
      "Naveed Tahir"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21262": {
    "title": "MIP-GNN: A Data-Driven Framework for Guiding Combinatorial Solvers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9cd1a80a5534d9288fccf1852503c89c9b57ac9b",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Elias B. Khalil",
      "Christopher Morris",
      "Andrea Lodi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21263": {
    "title": "Bandit Limited Discrepancy Search and Application to Machine Learning Pipeline Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "81c004a5f4d0da6d5f61662b4b70cc6d368e2783",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Akihiro Kishimoto",
      "Djallel Bouneffouf",
      "Radu Marinescu",
      "Parikshit Ram",
      "Ambrish Rawat",
      "Martin Wistuba",
      "Paulito Palmes",
      "Adi Botea"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21264": {
    "title": "PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Data Subset Selection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b49a2e372010577658e98abedc4fc0adc8e5f923",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Suraj Kothawade",
      "Vishal Kaushal",
      "Ganesh Ramakrishnan",
      "Jeff Bilmes",
      "Rishabh Iyer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21265": {
    "title": "Split Moves for Monte-Carlo Tree Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b7458f2cb41afb73d87029228c9509ae5435506f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Kowalski",
      "Maksymilian Mika",
      "Wojciech Pawlik",
      "Jakub Sutowicz",
      "Marek Szykuła",
      "Mark H. M. Winands"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21266": {
    "title": "MAPF-LNS2: Fast Repairing for Multi-Agent Path Finding via Large Neighborhood Search",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d47c6e9b5491b05f3b6da2eaf51e748728d203a6",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Jiaoyang Li",
      "Zhe Chen",
      "Daniel Harabor",
      "Peter J. Stuckey",
      "Sven Koenig"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21267": {
    "title": "Local and Global Convergence of General Burer-Monteiro Tensor Optimizations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "caddb6bdff9d3202cad5b5a900fb6d6ec60fc9bd",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shuang Li",
      "Qiuwei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21268": {
    "title": "Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b667a1d5943c34f1e7bc454f2566c085b871d64b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tieying Li",
      "Xiaochun Yang",
      "Bin Wang",
      "Chong Xi",
      "Hanzhong Zheng",
      "Xiangmin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21269": {
    "title": "Improving Local Search Algorithms via Probabilistic Configuration Checking",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d5a447af4d3a9803ff3791325e94b6782971880d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weilin Luo",
      "Rongzhen Ye",
      "Hai Wan",
      "Shaowei Cai",
      "Biqing Fang",
      "Delong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21270": {
    "title": "PEA*+IDA*: An Improved Hybrid Memory-Restricted Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b4ffef1e4be4d4a52477077f37648c2a41cb7ace",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederico Messa",
      "André Grahl Pereira"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21271": {
    "title": "Search Strategies for Topological Network Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c7a927ba3ea073f847b3f4de0512f2c95f10fc7a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael D. Moffitt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21272": {
    "title": "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "89e9872cf16e058499877efbd578646ebb04db87",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Rui Ning",
      "Jiang Li",
      "Chunsheng Xin",
      "Hongyi Wu",
      "Chonggang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21273": {
    "title": "Planning with Explanations for Finding Desired Meeting Points on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "781553b6576ea32de96f92681215b34f0599c42c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keisuke Otaki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21274": {
    "title": "A Fast Local Search Algorithm for the Latin Square Completion Problem",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e33e9a26819460e4a7f7ab7a072a7270b69c0f47",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Pan",
      "Yiyuan Wang",
      "Minghao Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21275": {
    "title": "Sparsification of Decomposable Submodular Functions",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "01d395d2192e2d137c94bb4491e559067817c421",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Akbar Rafiey",
      "Yuichi Yoshida"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21276": {
    "title": "Subset Approximation of Pareto Regions with Bi-objective A*",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3dd3490cf590b90c3a8ff42d63e484675908728c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nicolás Rivera",
      "Jorge A. Baier",
      "Carlos Hernández"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21277": {
    "title": "On Probabilistic Generalization of Backdoors in Boolean Satisfiability",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fd2df3e9941353581aaaf0151fec27332370022e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Alexander Semenov",
      "Artem Pavlenko",
      "Daniil Chivilikhin",
      "Stepan Kochemazov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21278": {
    "title": "A Novel Approach to Solving Goal-Achieving Problems for Board Games",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d83cd7ca6622910c38ecbe8adf5d7509351852af",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chung-Chin Shih",
      "Ti-Rong Wu",
      "Ting Han Wei",
      "I-Chen Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21279": {
    "title": "Machine Learning for Online Algorithm Selection under Censored Feedback",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "83cc6596965705a8657961f97d1ce4cfc90f29bc",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Alexander Tornede",
      "Viktor Bengs",
      "Eyke Hüllermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21280": {
    "title": "Procrastinated Tree Search: Black-Box Optimization with Delayed, Noisy, and Multi-Fidelity Feedback",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e7346fd7f63d61f20aa1b3af5f2effc49da77b85",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiong Wang",
      "Debabrota Basu",
      "Immanuel Trummer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21281": {
    "title": "DPCD: Discrete Principal Coordinate Descent for Binary Variable Problems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "71d8807013ce356245f70d41ab35437224c505c3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21282": {
    "title": "Optimize What You Evaluate With: Search Result Diversification Based on Metric Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9013d208a2b257dc406ff894eb6a8929a40b495b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hai-Tao Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21283": {
    "title": "A First Mathematical Runtime Analysis of the Non-dominated Sorting Genetic Algorithm II (NSGA-II)",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "176e33373baf0c0417fb18002c3da33a939fcbca",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Weijie Zheng",
      "Yufei Liu",
      "Benjamin Doerr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21284": {
    "title": "Pinpointing Fine-Grained Relationships between Hateful Tweets and Replies",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8a2af9b73e5044f820e45080566be3d258a2554a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Albanyan",
      "Eduardo Blanco"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21285": {
    "title": "Cross-Modal Coherence for Text-to-Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "315004717da2a6bdb122622b8306af01542d714e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Malihe Alikhani",
      "Fangda Han",
      "Hareesh Ravi",
      "Mubbasir Kapadia",
      "Vladimir Pavlovic",
      "Matthew Stone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21286": {
    "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fc92ed618ded1d1463f7ec736093f420b05827db",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Berkeley R Andrus",
      "Yeganeh Nasiri",
      "Shilong Cui",
      "Benjamin Cullen",
      "Nancy Fulda"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21287": {
    "title": "Diagnostics-Guided Explanation Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d592d56418c4df24b443c038a88ec5606a615cde",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Pepa Atanasova",
      "Jakob Grue Simonsen",
      "Christina Lioma",
      "Isabelle Augenstein"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21288": {
    "title": "Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "98f19ca97512361b12475b42b67a617de14d33a1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bibo Cai",
      "Xiao Ding",
      "Bowen Chen",
      "Li Du",
      "Ting Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21289": {
    "title": "Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1afc53f501e0801a7385084965aed84d28d35cf4",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Hanjie Chen",
      "Yangfeng Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21290": {
    "title": "Unsupervised Editing for Counterfactual Stories",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "76a4ad0a90c9374fd81ed4b50621727e87ff0f4f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jiangjie Chen",
      "Chun Gan",
      "Sijie Cheng",
      "Hao Zhou",
      "Yanghua Xiao",
      "Lei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21291": {
    "title": "LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0177d4e486872f2b95d018dc836dea98f686dd2b",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jiangjie Chen",
      "Qiaoben Bao",
      "Changzhi Sun",
      "Xinbo Zhang",
      "Jiaze Chen",
      "Hao Zhou",
      "Yanghua Xiao",
      "Lei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21292": {
    "title": "ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5657a053bb3081be8b3974c33cb6d73c4c347c15",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Junfan Chen",
      "Richong Zhang",
      "Yongyi Mao",
      "Jie Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21293": {
    "title": "From Good to Best: Two-Stage Training for Cross-Lingual Machine Reading Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "01b35b50a5c3df50477382b6ef5ff28cc8776482",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Nuo Chen",
      "Linjun Shou",
      "Ming Gong",
      "Jian Pei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21294": {
    "title": "Probing Linguistic Information for Logical Inference in Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9b17e77056110abc1beb1ac271f76f48508bbc0b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zeming Chen",
      "Qiyue Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21295": {
    "title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Cheng-Han Chiang",
      "Hung-yi Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21296": {
    "title": "C2L: Causally Contrastive Learning for Robust Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a04b3744834b1b167a590bbb3b6230a75e20accc",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Seungtaek Choi",
      "Myeongho Jeong",
      "Hojae Han",
      "Seung-won Hwang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21297": {
    "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "23e8ed7568454e11d9a6fecb8242e1d16b1828d5",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Jishnu Ray Chowdhury",
      "Yong Zhuang",
      "Shuyi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21298": {
    "title": "Flexible Instance-Specific Rationalization of NLP Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a88eb2104efba1a421700501eb56a995b3bb2b1c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "George Chrysostomou",
      "Nikolaos Aletras"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21299": {
    "title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ae36bf4d3f6167f89149a3a7e9da9a602a768cde",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Pierre Jean A. Colombo",
      "Chloé Clavel",
      "Pablo Piantanida"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21300": {
    "title": "Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "be97ecf88d6bdf0c35d6bc00751bb6c8d6a0c484",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Poorav Desai",
      "Tanmoy Chakraborty",
      "Md Shad Akhtar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21301": {
    "title": "Zero-Shot Commonsense Question Answering with Cloze Translation and Consistency Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c3a454e50ec0610f1380d55b1988a5eb5d45207b",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zi-Yi Dou",
      "Nanyun Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21302": {
    "title": "Synthetic Disinformation Attacks on Automated Fact Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "536dd3e54ad83e64d27217485db230ea09b19f51",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yibing Du",
      "Antoine Bosselut",
      "Christopher D. Manning"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21303": {
    "title": "Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "139a83b5eab873dd464e2cf897887a95c8d07e13",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yichao Du",
      "Zhirui Zhang",
      "Weizhi Wang",
      "Boxing Chen",
      "Jun Xie",
      "Tong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21304": {
    "title": "Play the Shannon Game with Language Models: A Human-Free Approach to Summary Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c64f56e46b74540038cf79c0f35ac20e1b91f521",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Nicholas Egan",
      "Oleg Vasilyev",
      "John Bohannon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21305": {
    "title": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3013bf2c987db18ec6fb4c6f28ecd1c9f1f669f4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Liat Ein-Dor",
      "Ilya Shnayderman",
      "Artem Spector",
      "Lena Dankin",
      "Ranit Aharonov",
      "Noam Slonim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21306": {
    "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "299983121dec88d4cc8e4ea2aa06514787d8d878",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Steven Y. Feng",
      "Kevin Lu",
      "Zhuofu Tao",
      "Malihe Alikhani",
      "Teruko Mitamura",
      "Eduard Hovy",
      "Varun Gangal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21307": {
    "title": "Language Model Priming for Cross-Lingual Event Extraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "e500a0500a42e4e7fe919a46e88524a5148eb1a8",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Steven Fincke",
      "Shantanu Agarwal",
      "Scott Miller",
      "Elizabeth Boschee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21308": {
    "title": "Language Modelling via Learning to Rank",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f21be3f230cb2721904671c7747165edad8bd033",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Arvid Frydenlund",
      "Gagandeep Singh",
      "Frank Rudzicz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21309": {
    "title": "NAREOR: The Narrative Reordering Problem",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a14e36108aca336e37b28d89a6f4c529f73954c",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Varun Gangal",
      "Steven Y. Feng",
      "Malihe Alikhani",
      "Teruko Mitamura",
      "Eduard Hovy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21310": {
    "title": "UNISON: Unpaired Cross-Lingual Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a39e6c647c483721dbc2843ccddb60d2f247c85b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jiahui Gao",
      "Yi Zhou",
      "Philip L. H. Yu",
      "Shafiq Joty",
      "Jiuxiang Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21311": {
    "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "47354f49a4768719add414ea853977cb868faf25",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Jiahui Gao",
      "Hang Xu",
      "Han Shi",
      "Xiaozhe Ren",
      "Philip L. H. Yu",
      "Xiaodan Liang",
      "Xin Jiang",
      "Zhenguo Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21312": {
    "title": "ISEEQ: Information Seeking Question Generation Using Dynamic Meta-Information Retrieval and Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "77d2456630d7b22efe84bffcc7d4ad495ce50a6d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Manas Gaur",
      "Kalpa Gunaratna",
      "Vijay Srinivasan",
      "Hongxia Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21313": {
    "title": "Explainable Metaphor Identification Inspired by Conceptual Metaphor Theory",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "57be279d11ad05f8268e680c3c255cee25c50bc1",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Mengshi Ge",
      "Rui Mao",
      "Erik Cambria"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21314": {
    "title": "Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "30b123fbae05d5f3fdea81b69178041d6b2aff49",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yantao Gong",
      "Cao Liu",
      "Fan Yang",
      "Xunliang Cai",
      "Guanglu Wan",
      "Jiansong Chen",
      "Weipeng Zhang",
      "Houfeng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21315": {
    "title": "SSAST: Self-Supervised Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "12601dd02aa624c4c676f42c8f7b034fa3457e37",
    "semantic_title": "",
    "citation_count": 73,
    "authors": [
      "Yuan Gong",
      "Cheng-I Lai",
      "Yu-An Chung",
      "James Glass"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21316": {
    "title": "Block-Skim: Efficient Question Answering for Transformer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f5a3dbc0518df5ca1b6333ae93244dde7f793736",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yue Guan",
      "Zhengyi Li",
      "Zhouhan Lin",
      "Yuhao Zhu",
      "Jingwen Leng",
      "Minyi Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21317": {
    "title": "Deep Clustering of Text Representations for Supervision-Free Probing of Syntax",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c38184c7ed9d798c83dbb48c8231e5a950a9b420",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Vikram Gupta",
      "Haoyue Shi",
      "Kevin Gimpel",
      "Mrinmaya Sachan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21318": {
    "title": "Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ad18b16df23000dc15fa70bcde93125bc4b4a486",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Momchil Hardalov",
      "Arnav Arora",
      "Preslav Nakov",
      "Isabelle Augenstein"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21319": {
    "title": "Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bccfcca7b28c4792bd59659c56e1d7843542dfe9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Dilek Hakkani-Tür"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21320": {
    "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "127ffc8697630a76b1b4149c24d1350f69205f41",
    "semantic_title": "",
    "citation_count": 58,
    "authors": [
      "Wanwei He",
      "Yinpei Dai",
      "Yinhe Zheng",
      "Yuchuan Wu",
      "Zheng Cao",
      "Dermot Liu",
      "Peng Jiang",
      "Min Yang",
      "Fei Huang",
      "Luo Si",
      "Jian Sun",
      "Yongbin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21321": {
    "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2569a7309142e40815cf556b6417059df9abbda8",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Xuanli He",
      "Qiongkai Xu",
      "Lingjuan Lyu",
      "Fangzhao Wu",
      "Chenguang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21322": {
    "title": "BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a0e4521d440e8013d09fcea3d9c6d4b24bbdefc7",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Teakgyu Hong",
      "DongHyun Kim",
      "Mingi Ji",
      "Wonseok Hwang",
      "Daehyun Nam",
      "Sungrae Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21323": {
    "title": "Non-autoregressive Translation with Layer-Wise Prediction and Deep Supervision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3db8de1e088e1837146c4f2f3c9d13a6c40434da",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Chenyang Huang",
      "Hao Zhou",
      "Osmar R. Zaïane",
      "Lili Mou",
      "Lei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21324": {
    "title": "Word Level Robustness Enhancement: Fight Perturbation with Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2ee1bbce0238092e8087dccd2e26b6ecb293367c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pei Huang",
      "Yuting Yang",
      "Fuqi Jia",
      "Minghao Liu",
      "Feifei Ma",
      "Jian Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21325": {
    "title": "Predicting Above-Sentence Discourse Structure Using Distant Supervision from Topic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c88f908e9e7c9a6f839f72920322bf86289bb99b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Patrick Huber",
      "Linzi Xing",
      "Giuseppe Carenini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21326": {
    "title": "Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c65c96cfc8b448fe9fd6bbfe1aaaea727515f4f7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yoonna Jang",
      "Jungwoo Lim",
      "Yuna Hur",
      "Dongsuk Oh",
      "Suhyune Son",
      "Yeonsoo Lee",
      "Donghoon Shin",
      "Seungryong Kim",
      "Heuiseok Lim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21327": {
    "title": "Towards Building ASR Systems for the Next Billion Users",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a2d2ec16aa26226c3f59fcbf227ac869bbb0f999",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Tahir Javed",
      "Sumanth Doddapaneni",
      "Abhigyan Raman",
      "Kaushal Santosh Bhogale",
      "Gowtham Ramesh",
      "Anoop Kunchukuttan",
      "Pratyush Kumar",
      "Mitesh M. Khapra"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21328": {
    "title": "Span-Based Semantic Role Labeling with Argument Pruning and Second-Order Inference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0fd98f53c596cef4e0da0f0716146e50e4268ef8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zixia Jia",
      "Zhaohui Yan",
      "Haoyi Wu",
      "Kewei Tu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21329": {
    "title": "Incorporating Constituent Syntax for Coreference Resolution",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b0471deb442c048e3b2bb6b109d3d45ea9576f4d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Fan Jiang",
      "Trevor Cohn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21330": {
    "title": "XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "607fa844e35b70a3a72d86c639312fe8670ba342",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Xiaoze Jiang",
      "Yaobo Liang",
      "Weizhu Chen",
      "Nan Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21331": {
    "title": "Hierarchical Context Tagging for Utterance Rewriting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f327a27515c72d0c7c92e8d2e83475477e68f877",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Lisa Jin",
      "Linfeng Song",
      "Lifeng Jin",
      "Dong Yu",
      "Daniel Gildea"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21332": {
    "title": "Search and Learn: Improving Semantic Coverage for Data-to-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3aaa5a41a3d6cc7e465163a2529cecaf2a988f74",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shailza Jolly",
      "Zi Xuan Zhang",
      "Andreas Dengel",
      "Lili Mou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21333": {
    "title": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6e399a6840c73013c389c2212bef5a87bdf2e996",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Aditya Kalyanpur",
      "Tom Breloff",
      "David A Ferrucci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21334": {
    "title": "Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "09f61bf7b7d02112166620571fa8d958ba6cd7b5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yu Kang",
      "Tianqiao Liu",
      "Hang Li",
      "Yang Hao",
      "Wenbiao Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21335": {
    "title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "844a83ef632af2b6ab8a83e56055a1ee2c65c7d6",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Gaoussou Youssouf Kebe",
      "Luke E. Richards",
      "Edward Raff",
      "Francis Ferraro",
      "Cynthia Matuszek"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21336": {
    "title": "ALP: Data Augmentation Using Lexicalized PCFGs for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1c740daa31bb2b68a4ac85c4e5c1b5b9367252c7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hazel H. Kim",
      "Daecheol Woo",
      "Seong Joon Oh",
      "Jeong-Won Cha",
      "Yo-Sub Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21337": {
    "title": "CAISE: Conversational Agent for Image Search and Editing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5c01c60319cc81e4ba2486de9af4ce5580e5df48",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyounghun Kim",
      "Doo Soon Kim",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Trung Bui",
      "Mohit Bansal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21338": {
    "title": "Dual Task Framework for Improving Persona-Grounded Dialogue Dataset",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1afeaef110134ebbba11604a206a54be547fdee0",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Minju Kim",
      "Beong-woo Kwak",
      "Youngwook Kim",
      "Hong-in Lee",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21339": {
    "title": "Minimally-Supervised Joint Learning of Event Volitionality and Subject Animacy Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d6155f8eaf8c4a70c809d9a8c5eaaa160e4e0724",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hirokazu Kiyomaru",
      "Sadao Kurohashi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21340": {
    "title": "From Fully Trained to Fully Random Embeddings: Improving Neural Machine Translation with Compact Word Embedding Tables",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "db3f38d1499ac678fa237ca70991144eaeff1636",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Krtin Kumar",
      "Peyman Passban",
      "Mehdi Rezagholizadeh",
      "Yiusing Lau",
      "Qun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21341": {
    "title": "SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3e9044c80d65a0c9aae8bcec4ed5c8997bcf7e2e",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Harrison Lee",
      "Raghav Gupta",
      "Abhinav Rastogi",
      "Yuan Cao",
      "Bin Zhang",
      "Yonghui Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21342": {
    "title": "Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "876c32f0648e1cbcea525a5cc76bd9e15095f81e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongfang Li",
      "Baotian Hu",
      "Qingcai Chen",
      "Tujie Xu",
      "Jingcong Tao",
      "Yunan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21343": {
    "title": "Text Revision By On-the-Fly Representation Optimization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "40cabeaeea1c7d7688f9834b7c8081564ece6664",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jingjing Li",
      "Zichao Li",
      "Tao Ge",
      "Irwin King",
      "Michael R. Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21344": {
    "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7915f5b00d5fba4806184d2afa2ab96435ad1078",
    "semantic_title": "",
    "citation_count": 49,
    "authors": [
      "Jingye Li",
      "Hao Fei",
      "Jiang Liu",
      "Shengqiong Wu",
      "Meishan Zhang",
      "Chong Teng",
      "Donghong Ji",
      "Fei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21345": {
    "title": "Sequence-to-Action: Grammatical Error Correction with Action Guided Sequence Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fd39b8c41ef56a25b38acec3f5372671cfa18b47",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jiquan Li",
      "Junliang Guo",
      "Yongxin Zhu",
      "Xin Sheng",
      "Deqiang Jiang",
      "Bo Ren",
      "Linli Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21346": {
    "title": "Dynamic Key-Value Memory Enhanced Multi-Step Graph Reasoning for Knowledge-Based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "18ec13d5c66f6ba6c38ede6ba9b451f89e986432",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mingxiao Li",
      "Marie-Francine Moens"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21347": {
    "title": "Knowledge Bridging for Empathetic Dialogue Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "01caaf3a67ad31c93048a29fff90e62ad3dac167",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Qintong Li",
      "Piji Li",
      "Zhaochun Ren",
      "Pengjie Ren",
      "Zhumin Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21348": {
    "title": "Contrast and Generation Make BART a Good Dialogue Emotion Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fe67243b9929f820c637cd9836deff82713290c8",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Shimin Li",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21349": {
    "title": "A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3cd02d5e8421612aff2525c1fb294ed229cfaa68",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Lin",
      "Hwee Tou Ng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21350": {
    "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fe92f3f7ceec008118842d42b578dc25bcba63f9",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Jinglin Liu",
      "Chengxi Li",
      "Yi Ren",
      "Feiyang Chen",
      "Zhou Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21351": {
    "title": "KGR4: Retrieval, Retrospect, Refine and Rethink for Commonsense Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "dacc2d8ec6b8eaeda511d7b08e526565a9a2618c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xin Liu",
      "Dayiheng Liu",
      "Baosong Yang",
      "Haibo Zhang",
      "Junwei Ding",
      "Wenqing Yao",
      "Weihua Luo",
      "Haiying Zhang",
      "Jinsong Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21352": {
    "title": "Improving Biomedical Information Retrieval with Neural Retrievers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "15031e6a94f9d6d19f74740c224a5523ec64d975",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Man Luo",
      "Arindam Mitra",
      "Tejas Gokhale",
      "Chitta Baral"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21353": {
    "title": "The King Is Naked: On the Notion of Robustness for Natural Language Processing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5e86c1adbe5b7cfccf2201e1c34400c819cdcdab",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Emanuele La Malfa",
      "Marta Kwiatkowska"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21354": {
    "title": "Selecting Optimal Context Sentences for Event-Event Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "caf084d6f1f46f1aad60691efbb1b9bb0a77d257",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Hieu Man",
      "Nghia Trung Ngo",
      "Linh Ngo Van",
      "Thien  Huu Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21355": {
    "title": "Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-Based Encoder",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6bb6a37a0754093035734d53ddc36df8803d4f47",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Elman Mansimov",
      "Yi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21356": {
    "title": "CINS: Comprehensive Instruction for Few-Shot Learning in Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "20da8033ed8b696e2e27ec40b1aa8a0ab82b964c",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Fei Mi",
      "Yasheng Wang",
      "Yitong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21357": {
    "title": "Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4eb45f33446018175e266738be22f4d830ed697e",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Gianluca Moro",
      "Luca Ragazzi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21358": {
    "title": "Eye of the Beholder: Improved Relation Generalization for Text-Based Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0529d32254a10195ab45e3f04e823113b284c074",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Keerthiram Murugesan",
      "Subhajit Chaudhury",
      "Kartik Talamadupula"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21359": {
    "title": "Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "12b744b5a14048c07ede827181d80617edbfe858",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Thong Thanh Nguyen",
      "Anh Tuan Luu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21360": {
    "title": "HiTKG: Towards Goal-Oriented Conversations via Multi-Hierarchy Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "68b34d61ea71ab60129fae0e27f37493a1076674",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Jinjie Ni",
      "Vlad Pandelea",
      "Tom Young",
      "Haicang Zhou",
      "Erik Cambria"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21361": {
    "title": "Is Discourse Role Important for Emotion Recognition in Conversation?",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "37fd19b0daa4401ea95cd774c1d047ccbb6bc1b5",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donovan Ong",
      "Jian Su",
      "Bin Chen",
      "Anh Tuan Luu",
      "Ashok Narendranath",
      "Yue Li",
      "Shuqi Sun",
      "Yingzhan Lin",
      "Haifeng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21362": {
    "title": "Improved Text Classification via Contrastive Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "35db4af3d2311547dc57ed41e7e13a8a7bbffc52",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Lin Pan",
      "Chung-Wei Hang",
      "Avirup Sil",
      "Saloni Potdar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21363": {
    "title": "LeSICiN: A Heterogeneous Graph-Based Approach for Automatic Legal Statute Identification from Indian Legal Documents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "499b172ae03e36dcad9f584983d18d70bfd2cdf2",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Shounak Paul",
      "Pawan Goyal",
      "Saptarshi Ghosh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21364": {
    "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3f7fae7ace36c562158c2e9b24f02d824f4a203c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jiahuan Pei",
      "Cheng Wang",
      "György Szarvas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21365": {
    "title": "STEPS: Semantic Typing of Event Processes with a Sequence-to-Sequence Approach",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1b5285a955ecc0e3b2aeb718e3b9720275ea82b1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sveva Pepe",
      "Edoardo Barba",
      "Rexhina Blloshmi",
      "Roberto Navigli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21366": {
    "title": "Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7805cb9b197433ae3739c67580cfcae6e00d6a19",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yinhua Piao",
      "Sangseon Lee",
      "Dohoon Lee",
      "Sun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21367": {
    "title": "STEM: Unsupervised STructural EMbedding for Stance Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "109217e18523c50ace18d72fd10574265eb5526d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ron Korenblum Pick",
      "Vladyslav Kozhukhov",
      "Dan Vilenchik",
      "Oren Tsur"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21368": {
    "title": "ValueNet: A New Dataset for Human Value Driven Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3af37400f1f9a4f4f211c4a472e18963edc2b34f",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Liang Qiu",
      "Yizhou Zhao",
      "Jinchao Li",
      "Pan Lu",
      "Baolin Peng",
      "Jianfeng Gao",
      "Song-Chun Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21369": {
    "title": "Post-OCR Document Correction with Large Ensembles of Character Sequence-to-Sequence Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "039f9c4ffdf0727fd6e8de756498ece0d87a362f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Juan Antonio Ramirez-Orta",
      "Eduardo Xamena",
      "Ana Maguitman",
      "Evangelos Milios",
      "Axel J. Soto"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21370": {
    "title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bba57c53ab9b600f71d888601ed0aa03812c8199",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Revant Gangi Reddy",
      "Xilin Rui",
      "Manling Li",
      "Xudong Lin",
      "Haoyang Wen",
      "Jaemin Cho",
      "Lifu Huang",
      "Mohit Bansal",
      "Avirup Sil",
      "Shih-Fu Chang",
      "Alexander Schwing",
      "Heng Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21371": {
    "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "63f17017257063ee034c4082d93005dc4b25d42d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Kyle Richardson",
      "Ashish Sabharwal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21372": {
    "title": "SFSRNet: Super-resolution for Single-Channel Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "da1d7c3ac8799dee652e5dc9136ca8d0c0c463a0",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Joel Rixen",
      "Matthias Renz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21373": {
    "title": "CEM: Commonsense-Aware Empathetic Response Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "604d426c0100ca35b7a0fe3a2279679a11cac675",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Sahand Sabour",
      "Chujie Zheng",
      "Minlie Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21374": {
    "title": "Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning over Text",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a48b1b5390d7c8621c0dbb55d6e675da83a2027a",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Amrita Saha",
      "Shafiq Joty",
      "Steven C.H. Hoi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21375": {
    "title": "Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8a73b0775d36d3c707ec582d19573a38a1196673",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Emmanuelle Salin",
      "Badreddine Farah",
      "Stéphane Ayache",
      "Benoit Favre"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21376": {
    "title": "Entailment Relation Aware Paraphrase Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c44a192fc5bd431f0b30691b92f6c72207c8b8eb",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abhilasha Sancheti",
      "Balaji Vasan Srinivasan",
      "Rachel Rudinger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21377": {
    "title": "Visual Definition Modeling: Challenging Vision & Language Models to Define Words and Objects",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b2a68e0d59c0f0ddd3eb63b02290a8b166eb085a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bianca Scarlini",
      "Tommaso Pasini",
      "Roberto Navigli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21378": {
    "title": "Active Learning on Pre-trained Language Model with Task-Independent Triplet Loss",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4496f4817a91e5002207da28be3b14d55e51ca6a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Seungmin Seo",
      "Donghyun Kim",
      "Youbin Ahn",
      "Kyong-Ho Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21379": {
    "title": "OneRel: Joint Entity and Relation Extraction with One Module in One Step",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "80caf669f7369bcd13a9dac8507328492e5f138f",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yu-Ming Shang",
      "Heyan Huang",
      "Xianling Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21380": {
    "title": "KATG: Keyword-Bias-Aware Adversarial Text Generation for Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "95db3a56b925c2e599a673d7bf1a146e7ae23394",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Lingfeng Shen",
      "Shoushan Li",
      "Ying Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21381": {
    "title": "Unsupervised Deep Keyphrase Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "de0f5298fe818745db3e9a1787536d95bc076e30",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Xianjie Shen",
      "Yinghan Wang",
      "Rui Meng",
      "Jingbo Shang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21382": {
    "title": "Generation-Focused Table-Based Intermediate Pre-training for Free-Form Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ea98db542a68f6e4dafe7a51eab32486586524b1",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Peng Shi",
      "Patrick Ng",
      "Feng Nan",
      "Henghui Zhu",
      "Jun Wang",
      "Jiarong Jiang",
      "Alexander Hanbo Li",
      "Rishav Chakravarti",
      "Donald Weidner",
      "Bing Xiang",
      "Zhiguo Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21383": {
    "title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2a1acf6dce142d9a87d353c7edbda0127dbe69d7",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Zhengxiang Shi",
      "Qiang Zhang",
      "Aldo Lipani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21384": {
    "title": "MINIMAL: Mining Models for Universal Adversarial Triggers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "08275408b5975e35788e7e88b055f657a590235d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaman Kumar Singla",
      "Swapnil Parekh",
      "Somesh Singh",
      "Changyou Chen",
      "Balaji Krishnamurthy",
      "Rajiv Ratn Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21385": {
    "title": "Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "998234184c6b6188483b2b27c06e7a2e5095fb8f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zixing Song",
      "Irwin King"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21386": {
    "title": "Supervising Model Attention with Human Explanations for Robust Natural Language Inference",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "11d57e1af3c1f939efcee67640ccbf9814290016",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Joe Stacey",
      "Yonatan Belinkov",
      "Marek Rei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21387": {
    "title": "Hyperbolic Disentangled Representation for Fine-Grained Aspect Extraction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "63d3d493b0da02da23bbaf4aa3775dcb0646907e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chang-Yu Tai",
      "Ming-Yao Li",
      "Lun-Wei Ku"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21388": {
    "title": "Procedural Text Understanding via Scene-Wise Evolution",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "110743395cec1dcb0b7d2e87e692530e615a1f4c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Tang",
      "Hongyu Lin",
      "Meng Liao",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun",
      "Weijian Xie",
      "Jin Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21389": {
    "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d1eb051c6b13eba8a9b333d5ee0a55250717195d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Bing Tian",
      "Yixin Cao",
      "Yong Zhang",
      "Chunxiao Xing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21390": {
    "title": "Chess as a Testbed for Language Model State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "453fc588d97958c6fefad96e79edd896873b3e09",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shubham Toshniwal",
      "Sam Wiseman",
      "Karen Livescu",
      "Kevin Gimpel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21391": {
    "title": "Contrast-Enhanced Semi-supervised Text Classification with Few Labels",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1d091415de4f1a4b369d72cdbf4a3545d9f74cb7",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Austin Cheng-Yun Tsai",
      "Sheng-Ya Lin",
      "Li-Chen Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21392": {
    "title": "Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9fac50713a0597f40416e753d0f3b6fe3b19ec1c",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Marco Valentino",
      "Mokanarangan Thayaparan",
      "Deborah Ferreira",
      "André Freitas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21393": {
    "title": "DetIE: Multilingual Open Information Extraction Inspired by Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "6ee1df036087234d003ad42737f3f6d54629644a",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Michael Vasilkovsky",
      "Anton Alekseev",
      "Valentin Malykh",
      "Ilya Shenbin",
      "Elena Tutubalina",
      "Dmitriy Salikhov",
      "Mikhail Stepnov",
      "Andrey Chertok",
      "Sergey Nikolenko"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21394": {
    "title": "Hybrid Neural Networks for On-Device Directional Hearing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5b353f3f9f67057fb164f8ea232916aa2905cfd3",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Anran Wang",
      "Maruchi Kim",
      "Hao Zhang",
      "Shyamnath Gollakota"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21395": {
    "title": "Non-parametric Online Learning from Human Feedback for Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "41da1bf3b4a328cc32193000ffc75a3797c49f4a",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Dongqi Wang",
      "Haoran Wei",
      "Zhirui Zhang",
      "Shujian Huang",
      "Jun Xie",
      "Jiajun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21396": {
    "title": "Parameter Differentiation Based Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b6d553cfdaa31456cb17259c3190dd0cf68e0b30",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Qian Wang",
      "Jiajun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21397": {
    "title": "DisenCite: Graph-Based Disentangled Representation Learning for Context-Specific Citation Generation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f4749ec7b46d0c04ea7ca9b64e2317d877b901c0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yifan Wang",
      "Yiping Song",
      "Shuai Li",
      "Chaoran Cheng",
      "Wei Ju",
      "Ming Zhang",
      "Sheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21398": {
    "title": "HEAL: A Knowledge Graph for Distress Management Conversations",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "5962e4c4ec6cc2bec0f7f35e1aef189019b1278c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Anuradha Welivita",
      "Pearl Pu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21399": {
    "title": "Deep Fusing Pre-trained Models into Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "698113eac0df3bd6d8cf724ea6b5fa94a5bc57d9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongxiang Weng",
      "Heng Yu",
      "Weihua Luo",
      "Min Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21400": {
    "title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bce35b3d1cba7a8dd7bb350213780202a2922ec7",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21401": {
    "title": "A Label Dependence-Aware Sequence Generation Model for Multi-Level Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b0db941b5f9cd3a9c1ace0cb9fd9b65acb7dd219",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Changxing Wu",
      "Liuwen Cao",
      "Yubin Ge",
      "Yang Liu",
      "Min Zhang",
      "Jinsong Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21402": {
    "title": "Fast and Constrained Absent Keyphrase Generation by Prompt-Based Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "1ca0b2ccd7e9ade375c780b8bc9c7266dd34039d",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Huanqin Wu",
      "Baijiaxin Ma",
      "Wei Liu",
      "Tao Chen",
      "Dan Nie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21403": {
    "title": "GraphMemDialog: Optimizing End-to-End Task-Oriented Dialog Systems Using Graph Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cd4a820130d715effe7dace59376e6ac0d77f05b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jie Wu",
      "Ian G Harris",
      "Hongzhi Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21404": {
    "title": "Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8724ec52e653d92dc6b867a0b0d56c09b7a60253",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Fei Li",
      "Meishan Zhang",
      "Yijiang Liu",
      "Chong Teng",
      "Donghong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21405": {
    "title": "A Graph Convolutional Network with Adaptive Graph Generation and Channel Selection for Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4262331838e5071f93783f686cef07f7825f3742",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhipeng Xie",
      "Yumin Tu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21406": {
    "title": "Leashing the Inner Demons: Self-Detoxification for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "7ccbc36d047eb61ab1deb743dba10f2ec7853151",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Canwen Xu",
      "Zexue He",
      "Zhankui He",
      "Julian McAuley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21407": {
    "title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-sentence Dependency Graph",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "494c1f745dbb7625e86e9a222c480e40949b8dad",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Liyan Xu",
      "Xuchao Zhang",
      "Bo Zong",
      "Yanchi Liu",
      "Wei Cheng",
      "Jingchao Ni",
      "Haifeng Chen",
      "Liang Zhao",
      "Jinho D. Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21408": {
    "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "08c2e7812ff224db1c877b4d14730d6288d529aa",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Runxin Xu",
      "Fuli Luo",
      "Chengyu Wang",
      "Baobao Chang",
      "Jun Huang",
      "Songfang Huang",
      "Fei Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21409": {
    "title": "Sequence Level Contrastive Learning for Text Summarization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4b2af8e5da894a72f6236ab9347753760cfea7fd",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Shusheng Xu",
      "Xingxing Zhang",
      "Yi Wu",
      "Furu Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21410": {
    "title": "Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "bf2e9a6e40f2e7d797b59285b68b8abd54fb58c6",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Wenda Xu",
      "Michael Saxon",
      "Misha Sra",
      "William Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21411": {
    "title": "Text Is No More Enough! A Benchmark for Profile-Based Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "073a404154f66e57e029af6618d0e171d0c74fa8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Xu",
      "Libo Qin",
      "Kaiji Chen",
      "Guoxing Wu",
      "Linlin Li",
      "Wanxiang Che"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21412": {
    "title": "SAS: Self-Augmentation Strategy for Language Model Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4757b2dcb1d63b871ecce014ce8560a649652b4b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xu",
      "Jingqiao Zhang",
      "Ru He",
      "Liangzhu Ge",
      "Chao Yang",
      "Cheng Yang",
      "Ying Nian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21413": {
    "title": "Hybrid Curriculum Learning for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c829e17951ad1979e4ef724f674cc886b4870e00",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Lin Yang",
      "YI Shen",
      "Yue Mao",
      "Longjun Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21414": {
    "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b1f10c9fbb8dceff45e9c5aa9d7ca925b742d9ac",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Linyi Yang",
      "Jiazheng Li",
      "Ruihai Dong",
      "Yue Zhang",
      "Barry Smyth"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21415": {
    "title": "Tracing Text Provenance via Context-Aware Lexical Substitution",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9b79eb8d21c8a832daedbfc6d8c31bebe0da3ed5",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xi Yang",
      "Jie Zhang",
      "Kejiang Chen",
      "Weiming Zhang",
      "Zehua Ma",
      "Feng Wang",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21416": {
    "title": "Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "937c037176c33b97131d9e92823119442a416993",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Tom Young",
      "Frank Xing",
      "Vlad Pandelea",
      "Jinjie Ni",
      "Erik Cambria"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21417": {
    "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "37bf0bf34603145246c3311df19e2afdf6e0270a",
    "semantic_title": "",
    "citation_count": 56,
    "authors": [
      "Donghan Yu",
      "Chenguang Zhu",
      "Yiming Yang",
      "Michael Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21418": {
    "title": "KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "24a95ff7a4f37d7d05f30e60dae40a576f49eeda",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhe Yuan",
      "Pengfei Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21419": {
    "title": "Reference-Based Speech Enhancement via Feature Alignment and Fusion Network",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "87a13e6d7aec847d521c97dbfac9382b6c9f090a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Huanjing Yue",
      "Wenxin Duo",
      "Xiulian Peng",
      "Jingyu Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21420": {
    "title": "MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0ba23c847d2ca087887b60ea92ce56c71f0425b2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Thomas Friedrichs",
      "Haizhou Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21421": {
    "title": "Efficient Dialog Policy Learning by Reasoning with Contextual Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c1da0bb8db38642a28bea077604897237bf67946",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Haodi Zhang",
      "Zhichao Zeng",
      "Keting Lu",
      "Kaishun Wu",
      "Shiqi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21422": {
    "title": "Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "30675cc0f831ccc418c02a9f7f9f2d2bdaaf68de",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Litian Zhang",
      "Xiaoming Zhang",
      "Junshu Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21423": {
    "title": "Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8dd49f94b213834e580eeea18098f402b1be1226",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Minjia Zhang",
      "Niranjan Uma Naresh",
      "Yuxiong He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21424": {
    "title": "Text-Based Interactive Recommendation via Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "b88c84a90160393a175c608e10eef9222ff69990",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ruiyi Zhang",
      "Tong Yu",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21425": {
    "title": "DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2fab75cfd8394de70bca365572bc5bb04a1b1eb5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Taolin Zhang",
      "Chengyu Wang",
      "Nan Hu",
      "Minghui Qiu",
      "Chengguang Tang",
      "Xiaofeng He",
      "Jun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21426": {
    "title": "Frequency-Aware Contrastive Learning for Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a821453809f36a23402ce93b6042c76d85a720a6",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Tong Zhang",
      "Wei Ye",
      "Baosong Yang",
      "Long Zhang",
      "Xingzhang Ren",
      "Dayiheng Liu",
      "Jinan Sun",
      "Shikun Zhang",
      "Haibo Zhang",
      "Wen Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21427": {
    "title": "Probing Word Syntactic Representations in the Brain by a Feature Elimination Method",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c9bd83a75817a5aef817a704b398ea920bf3bfce",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Xiaohan Zhang",
      "Shaonan Wang",
      "Nan Lin",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21428": {
    "title": "Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ff5c0e3e23a79fbcca95aa6dba1ec7ba71baf204",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yanzhao Zhang",
      "Richong Zhang",
      "Samuel Mensah",
      "Xudong Liu",
      "Yongyi Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21429": {
    "title": "RetGen: A Joint Framework for Retrieval and Grounded Text Generation Modeling",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "39a9f750c9b79fba4a0404179fdac6a7cb922838",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yizhe Zhang",
      "Siqi Sun",
      "Xiang Gao",
      "Yuwei Fang",
      "Chris Brockett",
      "Michel Galley",
      "Jianfeng Gao",
      "Bill Dolan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21430": {
    "title": "BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "4f7c4a9d73f6d18f4bbb4424c7f8a16df45df474",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yunxiang Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21431": {
    "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "cfeb9181a23c13d1d3ab85985038110a14f97c7d",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zhengkun Zhang",
      "Xiaojun Meng",
      "Yasheng Wang",
      "Xin Jiang",
      "Qun Liu",
      "Zhenglu Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21432": {
    "title": "DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ac95a18762133d4065ac8af518c33084d83c5582",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Ming Zhong",
      "Yang Liu",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21433": {
    "title": "Idiomatic Expression Paraphrasing without Strong Supervision",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "c1ce047eb0911babdfd160071981bb82a41664fb",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jianing Zhou",
      "Ziheng Zeng",
      "Hongyu Gong",
      "Suma Bhat"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/21434": {
    "title": "Multilingual Code Snippets Training for Program Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "62851a515ea0ee3a547d94e8a493d978c22d0be9",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ming Zhu",
      "Karthik Suresh",
      "Chandan K Reddy"
    ]
  }
}