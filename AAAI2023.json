{
  "https://ojs.aaai.org/index.php/AAAI/article/view/25070": {
    "title": "Back to the Future: Toward a Hybrid Architecture for Ad Hoc Teamwork",
    "volume": "main",
    "abstract": "State of the art methods for ad hoc teamwork, i.e., for collaboration without prior coordination, often use a long history of prior observations to model the behavior of other agents (or agent types) and to determine the ad hoc agent's behavior. In many practical domains, it is difficult to obtain large training datasets, and necessary to quickly revise the existing models to account for changes in team composition or domain attributes. Our architecture builds on the principles of step-wise refinement and ecological rationality to enable an ad hoc agent to perform non-monotonic logical reasoning with prior commonsense domain knowledge and models learned rapidly from limited examples to predict the behavior of other agents. In the simulated multiagent collaboration domain Fort Attack, we experimentally demonstrate that our architecture enables an ad hoc agent to adapt to changes in the behavior of other agents, and provides enhanced transparency and better performance than a state of the art data-driven baseline",
    "checked": true,
    "id": "118fd51b49e4f344873b8246bb051afb66c0c8d9",
    "semantic_title": "back to the future: toward a hybrid architecture for ad hoc teamwork",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25071": {
    "title": "Reducing ANN-SNN Conversion Error through Residual Membrane Potential",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have received extensive academic attention due to the unique properties of low power consumption and high-speed computing on neuromorphic chips. Among various training methods of SNNs, ANN-SNN conversion has shown the equivalent level of performance as ANNs on large-scale datasets. However, unevenness error, which refers to the deviation caused by different temporal sequences of spike arrival on activation layers, has not been effectively resolved and seriously suffers the performance of SNNs under the condition of short time-steps. In this paper, we make a detailed analysis of unevenness error and divide it into four categories. We point out that the case of the ANN output being zero while the SNN output being larger than zero accounts for the largest percentage. Based on this, we theoretically prove the sufficient and necessary conditions of this case and propose an optimization strategy based on residual membrane potential to reduce unevenness error. The experimental results show that the proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets. For example, we reach top-1 accuracy of 64.32% on ImageNet with 10-steps. To the best of our knowledge, this is the first time ANN-SNN conversion can simultaneously achieve high accuracy and ultra-low-latency on the complex dataset. Code is available at https://github.com/hzc1208/ANN2SNN_SRP",
    "checked": true,
    "id": "366a99da9ddef081aacd362d3da6668dfff04b2d",
    "semantic_title": "reducing ann-snn conversion error through residual membrane potential",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25072": {
    "title": "Hierarchical ConViT with Attention-Based Relational Reasoner for Visual Analogical Reasoning",
    "volume": "main",
    "abstract": "Raven's Progressive Matrices (RPMs) have been widely used to evaluate the visual reasoning ability of humans. To tackle the challenges of visual perception and logic reasoning on RPMs, we propose a Hierarchical ConViT with Attention-based Relational Reasoner (HCV-ARR). Traditional solution methods often apply relatively shallow convolution networks to visually perceive shape patterns in RPM images, which may not fully model the long-range dependencies of complex pattern combinations in RPMs. The proposed ConViT consists of a convolutional block to capture the low-level attributes of visual patterns, and a transformer block to capture the high-level image semantics such as pattern formations. Furthermore, the proposed hierarchical ConViT captures visual features from multiple receptive fields, where the shallow layers focus on the image fine details while the deeper layers focus on the image semantics. To better model the underlying reasoning rules embedded in RPM images, an Attention-based Relational Reasoner (ARR) is proposed to establish the underlying relations among images. The proposed ARR well exploits the hidden relations among question images through the developed element-wise attentive reasoner. Experimental results on three RPM datasets demonstrate that the proposed HCV-ARR achieves a significant performance gain compared with the state-of-the-art models. The source code is available at: https://github.com/wentaoheunnc/HCV-ARR",
    "checked": true,
    "id": "58f92031574529f13dfa8455c8bb539f43c722a0",
    "semantic_title": "hierarchical convit with attention-based relational reasoner for visual analogical reasoning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25073": {
    "title": "Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse",
    "volume": "main",
    "abstract": "Deep artificial neural networks (ANNs) play a major role in modeling the visual pathways of primate and rodent. However, they highly simplify the computational properties of neurons compared to their biological counterparts. Instead, Spiking Neural Networks (SNNs) are more biologically plausible models since spiking neurons encode information with time sequences of spikes, just like biological neurons do. However, there is a lack of studies on visual pathways with deep SNNs models. In this study, we model the visual cortex with deep SNNs for the first time, and also with a wide range of state-of-the-art deep CNNs and ViTs for comparison. Using three similarity metrics, we conduct neural representation similarity experiments on three neural datasets collected from two species under three types of stimuli. Based on extensive similarity analyses, we further investigate the functional hierarchy and mechanisms across species. Almost all similarity scores of SNNs are higher than their counterparts of CNNs with an average of 6.6%. Depths of the layers with the highest similarity scores exhibit little differences across mouse cortical regions, but vary significantly across macaque regions, suggesting that the visual processing structure of mice is more regionally homogeneous than that of macaques. Besides, the multi-branch structures observed in some top mouse brain-like neural networks provide computational evidence of parallel processing streams in mice, and the different performance in fitting macaque neural representations under different stimuli exhibits the functional specialization of information processing in macaques. Taken together, our study demonstrates that SNNs could serve as promising candidates to better model and explain the functional hierarchy and mechanisms of the visual system",
    "checked": true,
    "id": "462b999f9e47915c89a0c70d797d3e82276f8410",
    "semantic_title": "deep spiking neural networks with high representation similarity model visual pathways of macaque and mouse",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25074": {
    "title": "A Semi-parametric Model for Decision Making in High-Dimensional Sensory Discrimination Tasks",
    "volume": "main",
    "abstract": "Psychometric functions typically characterize binary sensory decisions along a single stimulus dimension. However, real-life sensory tasks vary along a greater variety of dimensions (e.g. color, contrast and luminance for visual stimuli). Approaches to characterizing high-dimensional sensory spaces either require strong parametric assumptions about these additional contextual dimensions, or fail to leverage known properties of classical psychometric curves. We overcome both limitations by introducing a semi-parametric model of sensory discrimination that applies traditional psychophysical models along a stimulus intensity dimension, but puts Gaussian process (GP) priors on the parameters of these models with respect to the remaining dimensions. By combining the flexibility of the GP with the deep literature on parametric psychophysics, our semi-parametric models achieve good performance with much less data than baselines on both synthetic and real-world, high-dimensional psychophysics datasets. We additionally show strong performance in a Bayesian active learning setting, and present a novel active learning paradigm for the semi-parametric model",
    "checked": true,
    "id": "80df6f7525f24cce6d3d4c5f1a17566613a6f60d",
    "semantic_title": "a semi-parametric model for decision making in high-dimensional sensory discrimination tasks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25075": {
    "title": "A Machine with Short-Term, Episodic, and Semantic Memory Systems",
    "volume": "main",
    "abstract": "Inspired by the cognitive science theory of the explicit human memory systems, we have modeled an agent with short-term, episodic, and semantic memory systems, each of which is modeled with a knowledge graph. To evaluate this system and analyze the behavior of this agent, we designed and released our own reinforcement learning agent environment, \"the Room\", where an agent has to learn how to encode, store, and retrieve memories to maximize its return by answering questions. We show that our deep Q-learning based agent successfully learns whether a short-term memory should be forgotten, or rather be stored in the episodic or semantic memory systems. Our experiments indicate that an agent with human-like memory systems can outperform an agent without this memory structure in the environment",
    "checked": true,
    "id": "d823694648cff737e14a613d5c743ac2b6cf39bf",
    "semantic_title": "a machine with short-term, episodic, and semantic memory systems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25076": {
    "title": "Persuasion Strategies in Advertisements",
    "volume": "main",
    "abstract": "Modeling what makes an advertisement persuasive, i.e., eliciting the desired response from consumer, is critical to the study of propaganda, social psychology, and marketing. Despite its importance, computational modeling of persuasion in computer vision is still in its infancy, primarily due to the lack of benchmark datasets that can provide persuasion-strategy labels associated with ads. Motivated by persuasion literature in social psychology and marketing, we introduce an extensive vocabulary of persuasion strategies and build the first ad image corpus annotated with persuasion strategies. We then formulate the task of persuasion strategy prediction with multi-modal learning, where we design a multi-task attention fusion model that can leverage other ad-understanding tasks to predict persuasion strategies. The dataset also provides image segmentation masks, which labels persuasion strategies in the corresponding ad images on the test split. We publicly release our code and dataset at https://midas-research.github.io/persuasion-advertisements/",
    "checked": false,
    "id": "1bd3def5257e992cecbcd5381771d77a95cc8200",
    "semantic_title": "persuasion strategies in advertisements: dataset, modeling, and baselines",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25077": {
    "title": "Intensity-Aware Loss for Dynamic Facial Expression Recognition in the Wild",
    "volume": "main",
    "abstract": "Compared with the image-based static facial expression recognition (SFER) task, the dynamic facial expression recognition (DFER) task based on video sequences is closer to the natural expression recognition scene. However, DFER is often more challenging. One of the main reasons is that video sequences often contain frames with different expression intensities, especially for the facial expressions in the real-world scenarios, while the images in SFER frequently present uniform and high expression intensities. Nevertheless, if the expressions with different intensities are treated equally, the features learned by the networks will have large intra-class and small inter-class differences, which are harmful to DFER. To tackle this problem, we propose the global convolution-attention block (GCA) to rescale the channels of the feature maps. In addition, we introduce the intensity-aware loss (IAL) in the training process to help the network distinguish the samples with relatively low expression intensities. Experiments on two in-the-wild dynamic facial expression datasets (i.e., DFEW and FERV39k) indicate that our method outperforms the state-of-the-art DFER approaches. The source code will be available at https://github.com/muse1998/IAL-for-Facial-Expression-Recognition",
    "checked": true,
    "id": "2a78605f7d9372504eee3a0d9b86f0219a29d115",
    "semantic_title": "intensity-aware loss for dynamic facial expression recognition in the wild",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25078": {
    "title": "AVCAffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work",
    "volume": "main",
    "abstract": "We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive load and Affect attributes. We record AVCAffe by simulating remote work scenarios over a video-conferencing platform, where subjects collaborate to complete a number of cognitively engaging tasks. AVCAffe is the largest originally collected (not collected from the Internet) affective dataset in English language. We recruit 106 participants from 18 different countries of origin, spanning an age range of 18 to 57 years old, with a balanced male-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent to more than 58,000 clips along with task-based self-reported ground truth labels for arousal, valence, and cognitive load attributes such as mental demand, temporal demand, effort, and a few others. We believe AVCAffe would be a challenging benchmark for the deep learning research community given the inherent difficulty of classifying affect and cognitive load in particular. Moreover, our dataset fills an existing timely gap by facilitating the creation of learning systems for better self-management of remote work meetings, and further study of hypotheses regarding the impact of remote work on cognitive load and affective states",
    "checked": true,
    "id": "e4dbc668686f89830de4c67203db0709d9c4dda2",
    "semantic_title": "avcaffe: a large scale audio-visual dataset of cognitive load and affect for remote work",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25079": {
    "title": "ESL-SNNs: An Evolutionary Structure Learning Strategy for Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) have manifested remarkable advantages in power consumption and event-driven property during the inference process. To take full advantage of low power consumption and improve the efficiency of these models further, the pruning methods have been explored to find sparse SNNs without redundancy connections after training. However, parameter redundancy still hinders the efficiency of SNNs during training. In the human brain, the rewiring process of neural networks is highly dynamic, while synaptic connections maintain relatively sparse during brain development. Inspired by this, here we propose an efficient evolutionary structure learning (ESL) framework for SNNs, named ESL-SNNs, to implement the sparse SNN training from scratch. The pruning and regeneration of synaptic connections in SNNs evolve dynamically during learning, yet keep the structural sparsity at a certain level. As a result, the ESL-SNNs can search for optimal sparse connectivity by exploring all possible parameters across time. Our experiments show that the proposed ESL-SNNs framework is able to learn SNNs with sparse structures effectively while reducing the limited accuracy. The ESL-SNNs achieve merely 0.28% accuracy loss with 10% connection density on the DVS-Cifar10 dataset. Our work presents a brand-new approach for sparse training of SNNs from scratch with biologically plausible evolutionary mechanisms, closing the gap in the expressibility between sparse training and dense training. Hence, it has great potential for SNN lightweight training and inference with low power consumption and small memory usage",
    "checked": true,
    "id": "f4e685992c8635225acabbe87d3900d104c9e78e",
    "semantic_title": "esl-snns: an evolutionary structure learning strategy for spiking neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25080": {
    "title": "Zero-Shot Linear Combinations of Grounded Social Interactions with Linear Social MDPs",
    "volume": "main",
    "abstract": "Humans and animals engage in rich social interactions. It is often theorized that a relatively small number of basic social interactions give rise to the full range of behavior observed. But no computational theory explaining how social interactions combine together has been proposed before. We do so here. We take a model, the Social MDP, which is able to express a range of social interactions, and extend it to represent linear combinations of social interactions. Practically for robotics applications, such models are now able to not just express that an agent should help another agent, but to express goal-centric social interactions. Perhaps an agent is helping someone get dressed, but preventing them from falling, and is happy to exchange stories in the meantime. How an agent responds socially, should depend on what it thinks the other agent is doing at that point in time. To encode this notion, we take linear combinations of social interactions as defined in Social MDPs, and compute the weights on those combinations on the fly depending on the estimated goals of other agents. This new model, the Linear Social MDP, enables zero-shot reasoning about complex social interactions, provides a mathematical basis for the long-standing intuition that social interactions should compose, and leads to interesting new behaviors that we validate using human observers. Complex social interactions are part of the future of intelligent agents, and having principled mathematical models built on a foundation like MDPs will make it possible to bring social interactions to every robotic application",
    "checked": true,
    "id": "e09dc84599f5b9165eb38641ac7167b6fcfc450c",
    "semantic_title": "zero-shot linear combinations of grounded social interactions with linear social mdps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25081": {
    "title": "Complex Dynamic Neurons Improved Spiking Transformer Network for Efficient Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The spiking neural network (SNN) using leaky-integrated-and-fire (LIF) neurons has been commonly used in automatic speech recognition (ASR) tasks. However, the LIF neuron is still relatively simple compared to that in the biological brain. Further research on more types of neurons with different scales of neuronal dynamics is necessary. Here we introduce four types of neuronal dynamics to post-process the sequential patterns generated from the spiking transformer to get the complex dynamic neuron improved spiking transformer neural network (DyTr-SNN). We found that the DyTr-SNN could handle the non-toy automatic speech recognition task well, representing a lower phoneme error rate, lower computational cost, and higher robustness. These results indicate that the further cooperation of SNNs and neural dynamics at the neuron and network scales might have much in store for the future, especially on the ASR tasks",
    "checked": true,
    "id": "ced7e9eecfa29e4d5a4be0a2a649efd2ab119ed6",
    "semantic_title": "complex dynamic neurons improved spiking transformer network for efficient automatic speech recognition",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25082": {
    "title": "Self-Supervised Graph Learning for Long-Tailed Cognitive Diagnosis",
    "volume": "main",
    "abstract": "Cognitive diagnosis is a fundamental yet critical research task in the field of intelligent education, which aims to discover the proficiency level of different students on specific knowledge concepts. Despite the effectiveness of existing efforts, previous methods always considered the mastery level on the whole students, so they still suffer from the Long Tail Effect. A large number of students who have sparse interaction records are usually wrongly diagnosed during inference. To relieve the situation, we proposed a Self-supervised Cognitive Diagnosis (SCD) framework which leverages the self-supervised manner to assist the graph-based cognitive diagnosis, then the performance on those students with sparse data can be improved. Specifically, we came up with a graph confusion method that drops edges under some special rules to generate different sparse views of the graph. By maximizing the cross-view consistency of node representations, our model could pay more attention on long-tailed students. Additionally, we proposed an importance-based view generation rule to improve the influence of long-tailed students. Extensive experiments on real-world datasets show the effectiveness of our approach, especially on the students with much sparser interaction records. Our code is available at https://github.com/zeng-zhen/SCD",
    "checked": true,
    "id": "e1126255b496a35ff2f04bfe33d6eedc765b649d",
    "semantic_title": "self-supervised graph learning for long-tailed cognitive diagnosis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25083": {
    "title": "CMNet: Contrastive Magnification Network for Micro-Expression Recognition",
    "volume": "main",
    "abstract": "Micro-Expression Recognition (MER) is challenging because the Micro-Expressions' (ME) motion is too weak to distinguish. This hurdle can be tackled by enhancing intensity for a more accurate acquisition of movements. However, existing magnification strategies tend to use the features of facial images that include not only intensity clues as intensity features, leading to the intensity representation deficient of credibility. In addition, the intensity variation over time, which is crucial for encoding movements, is also neglected. To this end, we provide a reliable scheme to extract intensity clues while considering their variation on the time scale. First, we devise an Intensity Distillation (ID) loss to acquire the intensity clues by contrasting the difference between frames, given that the difference in the same video lies only in the intensity. Then, the intensity clues are calibrated to follow the trend of the original video. Specifically, due to the lack of truth intensity annotation of the original video, we build the intensity tendency by setting each intensity vacancy an uncertain value, which guides the extracted intensity clues to converge towards this trend rather some fixed values. A Wilcoxon rank sum test (Wrst) method is enforced to implement the calibration. Experimental results on three public ME databases i.e. CASME II, SAMM, and SMIC-HS validate the superiority against state-of-the-art methods",
    "checked": true,
    "id": "f9ad4ac41b1ee5e150cb5adfc16b7c370d265d1c",
    "semantic_title": "cmnet: contrastive magnification network for micro-expression recognition",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25084": {
    "title": "Disentangling Reafferent Effects by Doing Nothing",
    "volume": "main",
    "abstract": "An agent's ability to distinguish between sensory effects that are self-caused, and those that are not, is instrumental in the achievement of its goals. This ability is thought to be central to a variety of functions in biological organisms, from perceptual stabilisation and accurate motor control, to higher level cognitive functions such as planning, mirroring and the sense of agency. Although many of these functions are well studied in AI, this important distinction is rarely made explicit and the focus tends to be on the associational relationship between action and sensory effect or success. Toward the development of more general agents, we develop a framework that enables agents to disentangle self-caused and externally-caused sensory effects. Informed by relevant models and experiments in robotics, and in the biological and cognitive sciences, we demonstrate the general applicability of this framework through an extensive experimental evaluation over three different environments",
    "checked": true,
    "id": "6908360d349167eaec8c4007a56d84cea9397dc7",
    "semantic_title": "disentangling reafferent effects by doing nothing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25085": {
    "title": "Learning Temporal-Ordered Representation for Spike Streams Based on Discrete Wavelet Transforms",
    "volume": "main",
    "abstract": "Spike camera, a new type of neuromorphic visual sensor that imitates the sampling mechanism of the primate fovea, can capture photons and output 40000 Hz binary spike streams. Benefiting from the asynchronous sampling mechanism, the spike camera can record fast-moving objects and clear images can be recovered from the spike stream at any specified timestamps without motion blurring. Despite these, due to the dense time sequence information of the discrete spike stream, it is not easy to directly apply the existing algorithms of traditional cameras to the spike camera. Therefore, it is necessary and interesting to explore a universally effective representation of dense spike streams to better fit various network architectures. In this paper, we propose to mine temporal-robust features of spikes in time-frequency space with wavelet transforms. We present a novel Wavelet-Guided Spike Enhancing (WGSE) paradigm consisting of three consecutive steps: multi-level wavelet transform, CNN-based learnable module, and inverse wavelet transform. With the assistance of WGSE, the new streaming representation of spikes can be learned. We demonstrate the effectiveness of WGSE on two downstream tasks, achieving state-of-the-art performance on the image reconstruction task and getting considerable performance on semantic segmentation. Furthermore, We build a new spike-based synthesized dataset for semantic segmentation. Code and Datasets are available at https://github.com/Leozhangjiyuan/WGSE-SpikeCamera",
    "checked": true,
    "id": "fbf899db663418a2fad1aa244e8d469d89138f0c",
    "semantic_title": "learning temporal-ordered representation for spike streams based on discrete wavelet transforms",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25086": {
    "title": "ScatterFormer: Locally-Invariant Scattering Transformer for Patient-Independent Multispectral Detection of Epileptiform Discharges",
    "volume": "main",
    "abstract": "Patient-independent detection of epileptic activities based on visual spectral representation of continuous EEG (cEEG) has been widely used for diagnosing epilepsy. However, precise detection remains a considerable challenge due to subtle variabilities across subjects, channels and time points. Thus, capturing fine-grained, discriminative features of EEG patterns, which is associated with high-frequency textural information, is yet to be resolved. In this work, we propose Scattering Transformer (ScatterFormer), an invariant scattering transform-based hierarchical Transformer that specifically pays attention to subtle features. In particular, the disentangled frequency-aware attention (FAA) enables the Transformer to capture clinically informative high-frequency components, offering a novel clinical explainability based on visual encoding of multichannel EEG signals. Evaluations on two distinct tasks of epileptiform detection demonstrate the effectiveness our method. Our proposed model achieves median AUCROC and accuracy of 98.14%, 96.39% in patients with Rolandic epilepsy. On a neonatal seizure detection benchmark, it outperforms the state-of-the-art by 9% in terms of average AUCROC",
    "checked": true,
    "id": "7fa2d8608e03c22ee025a27c7d6f81cc872735c6",
    "semantic_title": "scatterformer: locally-invariant scattering transformer for patient-independent multispectral detection of epileptiform discharges",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25087": {
    "title": "Progress and Limitations of Deep Networks to Recognize Objects in Unusual Poses",
    "volume": "main",
    "abstract": "Deep networks should be robust to rare events if they are to be successfully deployed in high-stakes real-world applications. Here we study the capability of deep networks to recognize objects in unusual poses. We create a synthetic dataset of images of objects in unusual orientations, and evaluate the robustness of a collection of 38 recent and competitive deep networks for image classification. We show that classifying these images is still a challenge for all networks tested, with an average accuracy drop of 29.5% compared to when the objects are presented upright. This brittleness is largely unaffected by various design choices, such as training losses, architectures, dataset modalities, and data-augmentation schemes. However, networks trained on very large datasets substantially outperform others, with the best network tested—Noisy Student trained on JFT-300M—showing a relatively small accuracy drop of only 14.5% on unusual poses. Nevertheless, a visual inspection of the failures of Noisy Student reveals a remaining gap in robustness with humans. Furthermore, combining multiple object transformations—3D-rotations and scaling—further degrades the performance of all networks. Our results provide another measurement of the robustness of deep networks to consider when using them in the real world. Code and datasets are available at https://github.com/amro-kamal/ObjectPose",
    "checked": true,
    "id": "6fbdc73ee62c32dc61cdddfb73410e1ec65cf35c",
    "semantic_title": "progress and limitations of deep networks to recognize objects in unusual poses",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25088": {
    "title": "Denoising after Entropy-Based Debiasing a Robust Training Method for Dataset Bias with Noisy Labels",
    "volume": "main",
    "abstract": "Improperly constructed datasets can result in inaccurate inferences. For instance, models trained on biased datasets perform poorly in terms of generalization (i.e., dataset bias). Recent debiasing techniques have successfully achieved generalization performance by underestimating easy-to-learn samples (i.e., bias-aligned samples) and highlighting difficult-to-learn samples (i.e., bias-conflicting samples). However, these techniques may fail owing to noisy labels, because the trained model recognizes noisy labels as difficult-to-learn and thus highlights them. In this study, we find that earlier approaches that used the provided labels to quantify difficulty could be affected by the small proportion of noisy labels. Furthermore, we find that running denoising algorithms before debiasing is ineffective because denoising algorithms reduce the impact of difficult-to-learn samples, including valuable bias-conflicting samples. Therefore, we propose an approach called denoising after entropy-based debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is trained by emphasizing (bias-aligned, clean) samples, which are selected using a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of the prejudice model, the sampling probability of each sample that is proportional to the entropy is computed. (3) The final model is trained using existing denoising algorithms with the mini-batches constructed by following the computed sampling probability. Compared to existing debiasing and denoising algorithms, our method achieves better debiasing performance on multiple benchmarks",
    "checked": true,
    "id": "9fc21e2c94cb2ed51809a9c96eccce810ef22520",
    "semantic_title": "denoising after entropy-based debiasing a robust training method for dataset bias with noisy labels",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25089": {
    "title": "Rethinking Interpretation: Input-Agnostic Saliency Mapping of Deep Visual Classifiers",
    "volume": "main",
    "abstract": "Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use `general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understandable concept, e.g., class label for classifiers. With a systematic projection, scaling and refinement process, this information is transformed into an interpretable visualization without compromising its model-fidelity. The visualization serves as a stand-alone qualitative interpretation. With an extensive evaluation, we not only demonstrate successful visualizations for a variety of concepts for large-scale models, but also showcase an interesting utility of this new form of saliency mapping by identifying backdoor signatures in compromised classifiers",
    "checked": true,
    "id": "aed3b3d9809b0b3847d1853601eee97a9798257d",
    "semantic_title": "rethinking interpretation: input-agnostic saliency mapping of deep visual classifiers",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25090": {
    "title": "Deep Digging into the Generalization of Self-Supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Self-supervised monocular depth estimation has been widely studied recently. Most of the work has focused on improving performance on benchmark datasets, such as KITTI, but has offered a few experiments on generalization performance. In this paper, we investigate the backbone networks (e.g., CNNs, Transformers, and CNN-Transformer hybrid models) toward the generalization of monocular depth estimation. We first evaluate state-of-the-art models on diverse public datasets, which have never been seen during the network training. Next, we investigate the effects of texture-biased and shape-biased representations using the various texture-shifted datasets that we generated. We observe that Transformers exhibit a strong shape bias and CNNs do a strong texture-bias. We also find that shape-biased models show better generalization performance for monocular depth estimation compared to texture-biased models. Based on these observations, we newly design a CNN-Transformer hybrid network with a multi-level adaptive feature fusion module, called MonoFormer. The design intuition behind MonoFormer is to increase shape bias by employing Transformers while compensating for the weak locality bias of Transformers by adaptively fusing multi-level representations. Extensive experiments show that the proposed method achieves state-of-the-art performance with various public datasets. Our method also shows the best generalization ability among the competitive methods",
    "checked": true,
    "id": "4d7ce2e45a56c6cae56c4c9f0011f1ed739defc7",
    "semantic_title": "deep digging into the generalization of self-supervised monocular depth estimation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25091": {
    "title": "Self-Contrastive Learning: Single-Viewed Supervised Contrastive Framework Using Sub-network",
    "volume": "main",
    "abstract": "Contrastive loss has significantly improved performance in supervised classification tasks by using a multi-viewed framework that leverages augmentation and label information. The augmentation enables contrast with another view of a single image but enlarges training time and memory usage. To exploit the strength of multi-views while avoiding the high computation cost, we introduce a multi-exit architecture that outputs multiple features of a single image in a single-viewed framework. To this end, we propose Self-Contrastive (SelfCon) learning, which self-contrasts within multiple outputs from the different levels of a single network. The multi-exit architecture efficiently replaces multi-augmented images and leverages various information from different layers of a network. We demonstrate that SelfCon learning improves the classification performance of the encoder network, and empirically analyze its advantages in terms of the single-view and the sub-network. Furthermore, we provide theoretical evidence of the performance increase based on the mutual information bound. For ImageNet classification on ResNet-50, SelfCon improves accuracy by +0.6% with 59% memory and 48% time of Supervised Contrastive learning, and a simple ensemble of multi-exit outputs boosts performance up to +1.5%. Our code is available at https://github.com/raymin0223/self-contrastive-learning",
    "checked": true,
    "id": "ef6fe11aa4b7dfec9f4d8da4e039f9c8d1839b0f",
    "semantic_title": "self-contrastive learning: single-viewed supervised contrastive framework using sub-network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25092": {
    "title": "Layout Representation Learning with Spatial and Structural Hierarchies",
    "volume": "main",
    "abstract": "We present a novel hierarchical modeling method for layout representation learning, the core of design documents (e.g., user interface, poster, template). Existing works on layout representation often ignore element hierarchies, which is an important facet of layouts, and mainly rely on the spatial bounding boxes for feature extraction. This paper proposes a Spatial-Structural Hierarchical Auto-Encoder (SSH-AE) that learns hierarchical representation by treating a hierarchically annotated layout as a tree format. On the one side, we model SSH-AE from both spatial (semantic views) and structural (organization and relationships) perspectives, which are two complementary aspects to represent a layout. On the other side, the semantic/geometric properties are associated at multiple resolutions/granularities, naturally handling complex layouts. Our learned representations are used for effective layout search from both spatial and structural similarity perspectives. We also newly involve the tree-edit distance (TED) as an evaluation metric to construct a comprehensive evaluation protocol for layout similarity assessment, which benefits a systematic and customized layout search. We further present a new dataset of POSTER layouts which we believe will be useful for future layout research. We show that our proposed SSH-AE outperforms the existing methods achieving state-of-the-art performance on two benchmark datasets. Code is available at github.com/yueb17/SSH-AE",
    "checked": true,
    "id": "f27d51387a1e00921c526cf0b44d8f41dc323e21",
    "semantic_title": "layout representation learning with spatial and structural hierarchies",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25093": {
    "title": "Cross-Modal Label Contrastive Learning for Unsupervised Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "This paper for the first time explores audio-visual event localization in an unsupervised manner. Previous methods tackle this problem in a supervised setting and require segment-level or video-level event category ground-truth to train the model. However, building large-scale multi-modality datasets with category annotations is human-intensive and thus not scalable to real-world applications. To this end, we propose cross-modal label contrastive learning to exploit multi-modal information among unlabeled audio and visual streams as self-supervision signals. At the feature representation level, multi-modal representations are collaboratively learned from audio and visual components by using self-supervised representation learning. At the label level, we propose a novel self-supervised pretext task i.e. label contrasting to self-annotate videos with pseudo-labels for localization model training. Note that irrelevant background would hinder the acquisition of high-quality pseudo-labels and thus lead to an inferior localization model. To address this issue, we then propose an expectation-maximization algorithm that optimizes the pseudo-label acquisition and localization model in a coarse-to-fine manner. Extensive experiments demonstrate that our unsupervised approach performs reasonably well compared to the state-of-the-art supervised methods",
    "checked": true,
    "id": "ad6b3041cb994c57e26c4a8fe0203ffb1f0c8ea0",
    "semantic_title": "cross-modal label contrastive learning for unsupervised audio-visual event localization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25094": {
    "title": "Multi-Level Compositional Reasoning for Interactive Instruction Following",
    "volume": "main",
    "abstract": "Robotic agents performing domestic chores by natural language directives are required to master the complex job of navigating environment and interacting with objects in the environments. The tasks given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks, e.g., bring a cup of coffee. To address the challenge, we propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction. We call it Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, we learn a three-level action policy. At the highest level, we infer a sequence of human-interpretable subgoals to be executed based on language instructions by a high-level policy composition controller. At the middle level, we discriminatively control the agent's navigation by a master policy by alternating between a navigation policy and various independent interaction policies. Finally, at the lowest level, we infer manipulation actions with the corresponding object masks using the appropriate interaction policy. Our approach not only generates human interpretable subgoals but also achieves 2.03% absolute gain to comparable state of the arts in the efficiency metric (PLWSR in unseen set) without using rule-based planning or a semantic spatial memory. The code is available at https://github.com/yonseivnl/mcr-agent",
    "checked": true,
    "id": "da804b058006e9b9ccda1776f437ceda9e869363",
    "semantic_title": "multi-level compositional reasoning for interactive instruction following",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25095": {
    "title": "Self-Supervised Image Local Forgery Detection by JPEG Compression Trace",
    "volume": "main",
    "abstract": "For image local forgery detection, the existing methods require a large amount of labeled data for training, and most of them cannot detect multiple types of forgery simultaneously. In this paper, we firstly analyzed the JPEG compression traces which are mainly caused by different JPEG compression chains, and designed a trace extractor to learn such traces. Then, we utilized the trace extractor as the backbone and trained self-supervised to strengthen the discrimination ability of learned traces. With its benefits, regions with different JPEG compression chains can easily be distinguished within a forged image. Furthermore, our method does not rely on a large amount of training data, and even does not require any forged images for training. Experiments show that the proposed method can detect image local forgery on different datasets without re-training, and keep stable performance over various types of image local forgery",
    "checked": true,
    "id": "b8d8f2834294897104e86f68a2f492058bfe5ccb",
    "semantic_title": "self-supervised image local forgery detection by jpeg compression trace",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25096": {
    "title": "VASR: Visual Analogies of Situation Recognition",
    "volume": "main",
    "abstract": "A core process in human cognition is analogical mapping: the ability to identify a similar relational structure between different situations. We introduce a novel task, Visual Analogies of Situation Recognition, adapting the classical word-analogy task into the visual domain. Given a triplet of images, the task is to select an image candidate B' that completes the analogy (A to A' is like B to what?). Unlike previous work on visual analogy that focused on simple image transformations, we tackle complex analogies requiring understanding of scenes. We leverage situation recognition annotations and the CLIP model to generate a large set of 500k candidate analogies. Crowdsourced annotations for a sample of the data indicate that humans agree with the dataset label ~80% of the time (chance level 25%). Furthermore, we use human annotations to create a gold-standard dataset of 3,820 validated analogies. Our experiments demonstrate that state-of-the-art models do well when distractors are chosen randomly (~86%), but struggle with carefully chosen distractors (~53%, compared to 90% human accuracy). We hope our dataset will encourage the development of new analogy-making models. Website: https://vasr-dataset.github.io/",
    "checked": true,
    "id": "b48eb1a32dcc4dcd122a5234c0fc055b71752e98",
    "semantic_title": "vasr: visual analogies of situation recognition",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25097": {
    "title": "Parametric Surface Constrained Upsampler Network for Point Cloud",
    "volume": "main",
    "abstract": "Designing a point cloud upsampler, which aims to generate a clean and dense point cloud given a sparse point representation, is a fundamental and challenging problem in computer vision. A line of attempts achieves this goal by establishing a point-to-point mapping function via deep neural networks. However, these approaches are prone to produce outlier points due to the lack of explicit surface-level constraints. To solve this problem, we introduce a novel surface regularizer into the upsampler network by forcing the neural network to learn the underlying parametric surface represented by bicubic functions and rotation functions, where the new generated points are then constrained on the underlying surface. These designs are integrated into two different networks for two tasks that take advantages of upsampling layers -- point cloud upsampling and point cloud completion for evaluation. The state-of-the-art experimental results on both tasks demonstrate the effectiveness of the proposed method. The implementation code will be available at https://github.com/corecai163/PSCU",
    "checked": true,
    "id": "3b1f3354487bdc26a6eefef82960d085e9b78bb1",
    "semantic_title": "parametric surface constrained upsampler network for point cloud",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25098": {
    "title": "Explicit Invariant Feature Induced Cross-Domain Crowd Counting",
    "volume": "main",
    "abstract": "Cross-domain crowd counting has shown progressively improved performance. However, most methods fail to explicitly consider the transferability of different features between source and target domains. In this paper, we propose an innovative explicit Invariant Feature induced Cross-domain Knowledge Transformation framework to address the inconsistent domain-invariant features of different domains. The main idea is to explicitly extract domain-invariant features from both source and target domains, which builds a bridge to transfer more rich knowledge between two domains. The framework consists of three parts, global feature decoupling (GFD), relation exploration and alignment (REA), and graph-guided knowledge enhancement (GKE). In the GFD module, domain-invariant features are efficiently decoupled from domain-specific ones in two domains, which allows the model to distinguish crowds features from backgrounds in the complex scenes. In the REA module both inter-domain relation graph (Inter-RG) and intra-domain relation graph (Intra-RG) are built. Specifically, Inter-RG aggregates multi-scale domain-invariant features between two domains and further aligns local-level invariant features. Intra-RG preserves taskrelated specific information to assist the domain alignment. Furthermore, GKE strategy models the confidence of pseudolabels to further enhance the adaptability of the target domain. Various experiments show our method achieves state-of-theart performance on the standard benchmarks. Code is available at https://github.com/caiyiqing/IF-CKT",
    "checked": true,
    "id": "c7a2c9710df23afea3dfb8a7639fba8273c066cf",
    "semantic_title": "explicit invariant feature induced cross-domain crowd counting",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25099": {
    "title": "Painterly Image Harmonization in Dual Domains",
    "volume": "main",
    "abstract": "Image harmonization aims to produce visually harmonious composite images by adjusting the foreground appearance to be compatible with the background. When the composite image has photographic foreground and painterly background, the task is called painterly image harmonization. There are only few works on this task, which are either time-consuming or weak in generating well-harmonized results. In this work, we propose a novel painterly harmonization network consisting of a dual-domain generator and a dual-domain discriminator, which harmonizes the composite image in both spatial domain and frequency domain. The dual-domain generator performs harmonization by using AdaIN modules in the spatial domain and our proposed ResFFT modules in the frequency domain. The dual-domain discriminator attempts to distinguish the inharmonious patches based on the spatial feature and frequency feature of each patch, which can enhance the ability of generator in an adversarial manner. Extensive experiments on the benchmark dataset show the effectiveness of our method. Our code and model are available at https://github.com/bcmi/PHDNet-Painterly-Image-Harmonization",
    "checked": true,
    "id": "04f9cc4d25baf7df5b14aa3e4bcdb91542f75d01",
    "semantic_title": "painterly image harmonization in dual domains",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25100": {
    "title": "MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation",
    "volume": "main",
    "abstract": "Automatic medical report generation is an essential task in applying artificial intelligence to the medical domain, which can lighten the workloads of doctors and promote clinical automation. The state-of-the-art approaches employ Transformer-based encoder-decoder architectures to generate reports for medical images. However, they do not fully explore the relationships between multi-modal medical data, and generate inaccurate and inconsistent reports. To address these issues, this paper proposes a Multi-modal Memory Transformer Network (MMTN) to cope with multi-modal medical data for generating image-report consistent medical reports. On the one hand, MMTN reduces the occurrence of image-report inconsistencies by designing a unique encoder to associate and memorize the relationship between medical images and medical terminologies. On the other hand, MMTN utilizes the cross-modal complementarity of the medical vision and language for the word prediction, which further enhances the accuracy of generating medical reports. Extensive experiments on three real datasets show that MMTN achieves significant effectiveness over state-of-the-art approaches on both automatic metrics and human evaluation",
    "checked": true,
    "id": "7cb09c77aab947c376df4fbcc4f2d877960f35d6",
    "semantic_title": "mmtn: multi-modal memory transformer network for image-report consistent medical report generation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25101": {
    "title": "KT-Net: Knowledge Transfer for Unpaired 3D Shape Completion",
    "volume": "main",
    "abstract": "Unpaired 3D object completion aims to predict a complete 3D shape from an incomplete input without knowing the correspondence between the complete and incomplete shapes. In this paper, we propose the novel KTNet to solve this task from the new perspective of knowledge transfer. KTNet elaborates a teacher-assistant-student network to establish multiple knowledge transfer processes. Specifically, the teacher network takes complete shape as input and learns the knowledge of complete shape. The student network takes the incomplete one as input and restores the corresponding complete shape. And the assistant modules not only help to transfer the knowledge of complete shape from the teacher to the student, but also judge the learning effect of the student network. As a result, KTNet makes use of a more comprehensive understanding to establish the geometric correspondence between complete and incomplete shapes in a perspective of knowledge transfer, which enables more detailed geometric inference for generating high-quality complete shapes. We conduct comprehensive experiments on several datasets, and the results show that our method outperforms previous methods of unpaired point cloud completion by a large margin. Code is available at https://github.com/a4152684/KT-Net",
    "checked": true,
    "id": "2c407b4721a7aab2b198d73b373c53edfd5604d5",
    "semantic_title": "kt-net: knowledge transfer for unpaired 3d shape completion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25102": {
    "title": "Deconstructed Generation-Based Zero-Shot Model",
    "volume": "main",
    "abstract": "Recent research on Generalized Zero-Shot Learning (GZSL) has focused primarily on generation-based methods. However, current literature has overlooked the fundamental principles of these methods and has made limited progress in a complex manner. In this paper, we aim to deconstruct the generator-classifier framework and provide guidance for its improvement and extension. We begin by breaking down the generator-learned unseen class distribution into class-level and instance-level distributions. Through our analysis of the role of these two types of distributions in solving the GZSL problem, we generalize the focus of the generation-based approach, emphasizing the importance of (i) attribute generalization in generator learning and (ii) independent classifier learning with partially biased data. We present a simple method based on this analysis that outperforms SotAs on four public GZSL datasets, demonstrating the validity of our deconstruction. Furthermore, our proposed method remains effective even without a generative model, representing a step towards simplifying the generator-classifier structure. Our code is available at https://github.com/cdb342/DGZ",
    "checked": true,
    "id": "50b3996ca86a55f9d3af68cb43c79659b1429daa",
    "semantic_title": "deconstructed generation-based zero-shot model",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25103": {
    "title": "Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild",
    "volume": "main",
    "abstract": "In this work, we tackle the challenging task of jointly tracking hand object poses and reconstructing their shapes from depth point cloud sequences in the wild, given the initial poses at frame 0. We for the first time propose a point cloud-based hand joint tracking network, HandTrackNet, to estimate the inter-frame hand joint motion. Our HandTrackNet proposes a novel hand pose canonicalization module to ease the tracking task, yielding accurate and robust hand joint tracking. Our pipeline then reconstructs the full hand via converting the predicted hand joints into a MANO hand. For object tracking, we devise a simple yet effective module that estimates the object SDF from the first frame and performs optimization-based tracking. Finally, a joint optimization step is adopted to perform joint hand and object reasoning, which alleviates the occlusion-induced ambiguity and further refines the hand pose. During training, the whole pipeline only sees purely synthetic data, which are synthesized with sufficient variations and by depth simulation for the ease of generalization. The whole pipeline is pertinent to the generalization gaps and thus directly transferable to real in-the-wild data. We evaluate our method on two real hand object interaction datasets, e.g. HO3D and DexYCB, without any fine-tuning. Our experiments demonstrate that the proposed method significantly outperforms the previous state-of-the-art depth-based hand and object pose estimation and tracking methods, running at a frame rate of 9 FPS. We have released our code on https://github.com/PKU-EPIC/HOTrack",
    "checked": true,
    "id": "5ef69136aa88675e3cf8aa33d87931d44a0615b2",
    "semantic_title": "tracking and reconstructing hand object interactions from point cloud sequences in the wild",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25104": {
    "title": "Amodal Instance Segmentation via Prior-Guided Expansion",
    "volume": "main",
    "abstract": "Amodal instance segmentation aims to infer the amodal mask, including both the visible part and occluded part of each object instance. Predicting the occluded parts is challenging. Existing methods often produce incomplete amodal boxes and amodal masks, probably due to lacking visual evidences to expand the boxes and masks. To this end, we propose a prior-guided expansion framework, which builds on a two-stage segmentation model (i.e., Mask R-CNN) and performs box-level (resp., pixel-level) expansion for amodal box (resp., mask) prediction, by retrieving regression (resp., flow) transformations from a memory bank of expansion prior. We conduct extensive experiments on KINS, D2SA, and COCOA cls datasets, which show the effectiveness of our method",
    "checked": true,
    "id": "0c9b776483a0de5d919842c05b48c029a78b7399",
    "semantic_title": "amodal instance segmentation via prior-guided expansion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25105": {
    "title": "SwinRDM: Integrate SwinRNN with Diffusion Model towards High-Resolution and High-Quality Weather Forecasting",
    "volume": "main",
    "abstract": "Data-driven medium-range weather forecasting has attracted much attention in recent years. However, the forecasting accuracy at high resolution is unsatisfactory currently. Pursuing high-resolution and high-quality weather forecasting, we develop a data-driven model SwinRDM which integrates an improved version of SwinRNN with a diffusion model. SwinRDM performs predictions at 0.25-degree resolution and achieves superior forecasting accuracy to IFS (Integrated Forecast System), the state-of-the-art operational NWP model, on representative atmospheric variables including 500 hPa geopotential (Z500), 850 hPa temperature (T850), 2-m temperature (T2M), and total precipitation (TP), at lead times of up to 5 days. We propose to leverage a two-step strategy to achieve high-resolution predictions at 0.25-degree considering the trade-off between computation memory and forecasting accuracy. Recurrent predictions for future atmospheric fields are firstly performed at 1.40625-degree resolution, and then a diffusion-based super-resolution model is leveraged to recover the high spatial resolution and finer-scale atmospheric details. SwinRDM pushes forward the performance and potential of data-driven models for a large margin towards operational applications",
    "checked": true,
    "id": "9d874861ec9f6b58166dcfcf05caf7a4a6bd032c",
    "semantic_title": "swinrdm: integrate swinrnn with diffusion model towards high-resolution and high-quality weather forecasting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25106": {
    "title": "Take Your Model Further: A General Post-refinement Network for Light Field Disparity Estimation via BadPix Correction",
    "volume": "main",
    "abstract": "Most existing light field (LF) disparity estimation algorithms focus on handling occlusion, texture-less or other areas that harm LF structure to improve accuracy, while ignoring other potential modeling ideas. In this paper, we propose a novel idea called Bad Pixel (BadPix) correction for method modeling, then implement a general post-refinement network for LF disparity estimation: Bad-pixel Correction Network (BpCNet). Given an initial disparity map generated by a specific algorithm, we assume that all BadPixs on it are in a small range. Then BpCNet is modeled as a fine-grained search strategy, and a more accurate result can be obtained by evaluating the consistency of LF images in this limited range. Due to the assumption and the consistency between input and output, BpCNet can perform as a general post-refinement network, and can work on almost all existing algorithms iteratively. We demonstrate the feasibility of our theory through extensive experiments, and achieve remarkable performance on the HCI 4D Light Field Benchmark",
    "checked": true,
    "id": "e7f81eb842623b41dd0bb09334506e148b076e66",
    "semantic_title": "take your model further: a general post-refinement network for light field disparity estimation via badpix correction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25107": {
    "title": "Improving Dynamic HDR Imaging with Fusion Transformer",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1aea87fb6d103ecfc92b309b8c9ff9f10df47b7d",
    "semantic_title": "improving dynamic hdr imaging with fusion transformer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25108": {
    "title": "Self-Supervised Joint Dynamic Scene Reconstruction and Optical Flow Estimation for Spiking Camera",
    "volume": "main",
    "abstract": "Spiking camera, a novel retina-inspired vision sensor, has shown its great potential for capturing high-speed dynamic scenes with a sampling rate of 40,000 Hz. The spiking camera abandons the concept of exposure window, with each of its photosensitive units continuously capturing photons and firing spikes asynchronously. However, the special sampling mechanism prevents the frame-based algorithm from being used to spiking camera. It remains to be a challenge to reconstruct dynamic scenes and perform common computer vision tasks for spiking camera. In this paper, we propose a self-supervised joint learning framework for optical flow estimation and reconstruction of spiking camera. The framework reconstructs clean frame-based spiking representations in a self-supervised manner, and then uses them to train the optical flow networks. We also propose an optical flow based inverse rendering process to achieve self-supervision by minimizing the difference with respect to the original spiking temporal aggregation image. The experimental results demonstrate that our method bridges the gap between synthetic and real-world scenes and achieves desired results in real-world scenarios. To the best of our knowledge, this is the first attempt to jointly reconstruct dynamic scenes and estimate optical flow for spiking camera from a self-supervised learning perspective",
    "checked": true,
    "id": "b10dd9f4a5dfcc782c0a0892d43fea21cbb2eaea",
    "semantic_title": "self-supervised joint dynamic scene reconstruction and optical flow estimation for spiking camera",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25109": {
    "title": "Bidirectional Optical Flow NeRF: High Accuracy and High Quality under Fewer Views",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) can implicitly represent 3D-consistent RGB images and geometric by optimizing an underlying continuous volumetric scene function using a sparse set of input views, which has greatly benefited view synthesis tasks. However, NeRF fails to estimate correct geometry when given fewer views, resulting in failure to synthesize novel views. Existing works rely on introducing depth images or adding depth estimation networks to resolve the problem of poor synthetic view in NeRF with fewer views. However, due to the lack of spatial consistency of the single-depth image and the poor performance of depth estimation with fewer views, the existing methods still have challenges in addressing this problem. So this paper proposes Bidirectional Optical Flow NeRF(BOF-NeRF), which addresses this problem by mining optical flow information between 2D images. Our key insight is that utilizing 2D optical flow images to design a loss can effectively guide NeRF to learn the correct geometry and synthesize the right novel view. We also propose a view-enhanced fusion method based on geometry and color consistency to solve the problem of novel view details loss in NeRF. We conduct extensive experiments on the NeRF-LLFF and DTU MVS benchmarks for novel view synthesis tasks with fewer images in different complex real scenes. We further demonstrate the robustness of BOF-NeRF under different baseline distances on the Middlebury dataset. In all cases, BOF-NeRF outperforms current state-of-the-art baselines for novel view synthesis and scene geometry estimation",
    "checked": true,
    "id": "dbb56d48ec9efeacf1d4dd31b76e23d5fb2c84b3",
    "semantic_title": "bidirectional optical flow nerf: high accuracy and high quality under fewer views",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25110": {
    "title": "Scalable Spatial Memory for Scene Rendering and Navigation",
    "volume": "main",
    "abstract": "Neural scene representation and rendering methods have shown promise in learning the implicit form of scene structure without supervision. However, the implicit representation learned in most existing methods is non-expandable and cannot be inferred online for novel scenes, which makes the learned representation difficult to be applied across different reinforcement learning (RL) tasks. In this work, we introduce Scene Memory Network (SMN) to achieve online spatial memory construction and expansion for view rendering in novel scenes. SMN models the camera projection and back-projection as spatially aware memory control processes, where the memory values store the information of the partial 3D area, and the memory keys indicate the position of that area. The memory controller can learn the geometry property from observations without the camera's intrinsic parameters and depth supervision. We further apply the memory constructed by SMN to exploration and navigation tasks. The experimental results reveal the generalization ability of our proposed SMN in large-scale scene synthesis and its potential to improve the performance of spatial RL tasks",
    "checked": true,
    "id": "532cb7ca76a0e4f4993eae3e0cd31be73bd7e117",
    "semantic_title": "scalable spatial memory for scene rendering and navigation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25111": {
    "title": "Hybrid CNN-Transformer Feature Fusion for Single Image Deraining",
    "volume": "main",
    "abstract": "Since rain streaks exhibit diverse geometric appearances and irregular overlapped phenomena, these complex characteristics challenge the design of an effective single image deraining model. To this end, rich local-global information representations are increasingly indispensable for better satisfying rain removal. In this paper, we propose a lightweight Hybrid CNN-Transformer Feature Fusion Network (dubbed as HCT-FFN) in a stage-by-stage progressive manner, which can harmonize these two architectures to help image restoration by leveraging their individual learning strengths. Specifically, we stack a sequence of the degradation-aware mixture of experts (DaMoE) modules in the CNN-based stage, where appropriate local experts adaptively enable the model to emphasize spatially-varying rain distribution features. As for the Transformer-based stage, a background-aware vision Transformer (BaViT) module is employed to complement spatially-long feature dependencies of images, so as to achieve global texture recovery while preserving the required structure. Considering the indeterminate knowledge discrepancy among CNN features and Transformer features, we introduce an interactive fusion branch at adjacent stages to further facilitate the reconstruction of high-quality deraining results. Extensive evaluations show the effectiveness and extensibility of our developed HCT-FFN. The source code is available at https://github.com/cschenxiang/HCT-FFN",
    "checked": true,
    "id": "44c091514f3397740a1cbe72f86b53f2f409bc5b",
    "semantic_title": "hybrid cnn-transformer feature fusion for single image deraining",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25112": {
    "title": "MGFN: Magnitude-Contrastive Glance-and-Focus Network for Weakly-Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Weakly supervised detection of anomalies in surveillance videos is a challenging task. Going beyond existing works that have deficient capabilities to localize anomalies in long videos, we propose a novel glance and focus network to effectively integrate spatial-temporal information for accurate anomaly detection. In addition, we empirically found that existing approaches that use feature magnitudes to represent the degree of anomalies typically ignore the effects of scene variations, and hence result in sub-optimal performance due to the inconsistency of feature magnitudes across scenes. To address this issue, we propose the Feature Amplification Mechanism and a Magnitude Contrastive Loss to enhance the discriminativeness of feature magnitudes for detecting anomalies. Experimental results on two large-scale benchmarks UCF-Crime and XD-Violence manifest that our method outperforms state-of-the-art approaches",
    "checked": true,
    "id": "9d60265ea72d0344e991e71dfb13b4e1f15d4d4d",
    "semantic_title": "mgfn: magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25113": {
    "title": "Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval",
    "volume": "main",
    "abstract": "Vision-language alignment learning for video-text retrieval arouses a lot of attention in recent years. Most of the existing methods either transfer the knowledge of image-text pretraining model to video-text retrieval task without fully exploring the multi-modal information of videos, or simply fuse multi-modal features in a brute force manner without explicit guidance. In this paper, we integrate multi-modal information in an explicit manner by tagging, and use the tags as the anchors for better video-text alignment. Various pretrained experts are utilized for extracting the information of multiple modalities, including object, person, motion, audio, etc. To take full advantage of these information, we propose the TABLE (TAgging Before aLignmEnt) network, which consists of a visual encoder, a tag encoder, a text encoder, and a tag-guiding cross-modal encoder for jointly encoding multi-frame visual features and multi-modal tags information. Furthermore, to strengthen the interaction between video and text, we build a joint cross-modal encoder with the triplet input of [vision, tag, text] and perform two additional supervised tasks, Video Text Matching (VTM) and Masked Language Modeling (MLM). Extensive experimental results demonstrate that the TABLE model is capable of achieving State-Of-The-Art (SOTA) performance on various video-text retrieval benchmarks, including MSR-VTT, MSVD, LSMDC and DiDeMo",
    "checked": true,
    "id": "2e9525ebe76a1d37533539ad2f560b1b453e66f6",
    "semantic_title": "tagging before alignment: integrating multi-modal tags for video-text retrieval",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25114": {
    "title": "DUET: Cross-Modal Semantic Grounding for Contrastive Zero-Shot Learning",
    "volume": "main",
    "abstract": "Zero-shot learning (ZSL) aims to predict unseen classes whose samples have never appeared during training. One of the most effective and widely used semantic information for zero-shot image classification are attributes which are annotations for class-level visual characteristics. However, the current methods often fail to discriminate those subtle visual distinctions between images due to not only the shortage of fine-grained annotations, but also the attribute imbalance and co-occurrence. In this paper, we present a transformer-based end-to-end ZSL method named DUET, which integrates latent semantic knowledge from the pre-trained language models (PLMs) via a self-supervised multi-modal learning paradigm. Specifically, we (1) developed a cross-modal semantic grounding network to investigate the model's capability of disentangling semantic attributes from the images; (2) applied an attribute-level contrastive learning strategy to further enhance the model's discrimination on fine-grained visual characteristics against the attribute co-occurrence and imbalance; (3) proposed a multi-task learning policy for considering multi-model objectives. We find that our DUET can achieve state-of-the-art performance on three standard ZSL benchmarks and a knowledge graph equipped ZSL benchmark. Its components are effective and its predictions are interpretable",
    "checked": true,
    "id": "13d8ce3d2ac01cc5e108c1b89e79049428a53ad5",
    "semantic_title": "duet: cross-modal semantic grounding for contrastive zero-shot learning",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25115": {
    "title": "Imperceptible Adversarial Attack via Invertible Neural Networks",
    "volume": "main",
    "abstract": "Adding perturbations via utilizing auxiliary gradient information or discarding existing details of the benign images are two common approaches for generating adversarial examples. Though visual imperceptibility is the desired property of adversarial examples, conventional adversarial attacks still generate traceable adversarial perturbations. In this paper, we introduce a novel Adversarial Attack via Invertible Neural Networks (AdvINN) method to produce robust and imperceptible adversarial examples. Specifically, AdvINN fully takes advantage of the information preservation property of Invertible Neural Networks and thereby generates adversarial examples by simultaneously adding class-specific semantic information of the target class and dropping discriminant information of the original class. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that the proposed AdvINN method can produce less imperceptible adversarial images than the state-of-the-art methods and AdvINN yields more robust adversarial examples with high confidence compared to other adversarial attacks. Code is available at https://github.com/jjhuangcs/AdvINN",
    "checked": true,
    "id": "85af65c9355dc1ff8585354e014543751b498ee3",
    "semantic_title": "imperceptible adversarial attack via invertible neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25116": {
    "title": "Cross-Modality Person Re-identification with Memory-Based Contrastive Embedding",
    "volume": "main",
    "abstract": "Visible-infrared person re-identification (VI-ReID) aims to retrieve the person images of the same identity from the RGB to infrared image space, which is very important for real-world surveillance system. In practice, VI-ReID is more challenging due to the heterogeneous modality discrepancy, which further aggravates the challenges of traditional single-modality person ReID problem, i.e., inter-class confusion and intra-class variations. In this paper, we propose an aggregated memory-based cross-modality deep metric learning framework, which benefits from the increasing number of learned modality-aware and modality-agnostic centroid proxies for cluster contrast and mutual information learning. Furthermore, to suppress the modality discrepancy, the proposed cross-modality alignment objective simultaneously utilizes both historical and up-to-date learned cluster proxies for enhanced cross-modality association. Such training mechanism helps to obtain hard positive references through increased diversity of learned cluster proxies, and finally achieves stronger ``pulling close'' effect between cross-modality image features. Extensive experiment results demonstrate the effectiveness of the proposed method, surpassing state-of-the-art works significantly by a large margin on the commonly used VI-ReID datasets",
    "checked": true,
    "id": "0e0e103a2a1ed0241df544928d09d2a6b55c766e",
    "semantic_title": "cross-modality person re-identification with memory-based contrastive embedding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25117": {
    "title": "User-Controllable Arbitrary Style Transfer via Entropy Regularization",
    "volume": "main",
    "abstract": "Ensuring the overall end-user experience is a challenging task in arbitrary style transfer (AST) due to the subjective nature of style transfer quality. A good practice is to provide users many instead of one AST result. However, existing approaches require to run multiple AST models or inference a diversified AST (DAST) solution multiple times, and thus they are either slow in speed or limited in diversity. In this paper, we propose a novel solution ensuring both efficiency and diversity for generating multiple user-controllable AST results by systematically modulating AST behavior at run-time. We begin with reformulating three prominent AST methods into a unified assign-and-mix problem and discover that the entropies of their assignment matrices exhibit a large variance. We then solve the unified problem in an optimal transport framework using the Sinkhorn-Knopp algorithm with a user input ε to control the said entropy and thus modulate stylization. Empirical results demonstrate the superiority of the proposed solution, with speed and stylization quality comparable to or better than existing AST and significantly more diverse than previous DAST works. Code is available at https://github.com/cplusx/eps-Assign-and-Mix",
    "checked": true,
    "id": "7dc98ae2967d6ad9c115ccaa705540b7489e0d40",
    "semantic_title": "user-controllable arbitrary style transfer via entropy regularization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25118": {
    "title": "Neural Architecture Search for Wide Spectrum Adversarial Robustness",
    "volume": "main",
    "abstract": "One major limitation of CNNs is that they are vulnerable to adversarial attacks. Currently, adversarial robustness in neural networks is commonly optimized with respect to a small pre-selected adversarial noise strength, causing them to have potentially limited performance when under attack by larger adversarial noises in real-world scenarios. In this research, we aim to find Neural Architectures that have improved robustness on a wide range of adversarial noise strengths through Neural Architecture Search. In detail, we propose a lightweight Adversarial Noise Estimator to reduce the high cost of generating adversarial noise with respect to different strengths. Besides, we construct an Efficient Wide Spectrum Searcher to reduce the cost of adjusting network architecture with the large adversarial validation set during the search. With the two components proposed, the number of adversarial noise strengths searched can be increased significantly while having a limited increase in search time. Extensive experiments on benchmark datasets such as CIFAR and ImageNet demonstrate that with a significantly richer search signal in robustness, our method can find architectures with improved overall robustness while having a limited impact on natural accuracy and around 40% reduction in search time compared with the naive approach of searching. Codes available at: https://github.com/zhicheng2T0/Wsr-NAS.git",
    "checked": true,
    "id": "d3eb76059d9115f2f746b06dfcde7f8137147c59",
    "semantic_title": "neural architecture search for wide spectrum adversarial robustness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25119": {
    "title": "Adversarial Alignment for Source Free Object Detection",
    "volume": "main",
    "abstract": "Source-free object detection (SFOD) aims to transfer a detector pre-trained on a label-rich source domain to an unlabeled target domain without seeing source data. While most existing SFOD methods generate pseudo labels via a source-pretrained model to guide training, these pseudo labels usually contain high noises due to heavy domain discrepancy. In order to obtain better pseudo supervisions, we divide the target domain into source-similar and source-dissimilar parts and align them in the feature space by adversarial learning.Specifically, we design a detection variance-based criterion to divide the target domain. This criterion is motivated by a finding that larger detection variances denote higher recall and larger similarity to the source domain. Then we incorporate an adversarial module into a mean teacher framework to drive the feature spaces of these two subsets indistinguishable. Extensive experiments on multiple cross-domain object detection datasets demonstrate that our proposed method consistently outperforms the compared SFOD methods. Our implementation is available at https://github.com/ChuQiaosong",
    "checked": true,
    "id": "693a942fa34028de582d18642d73d57c70842303",
    "semantic_title": "adversarial alignment for source free object detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25120": {
    "title": "Weakly Supervised 3D Multi-Person Pose Estimation for Large-Scale Scenes Based on Monocular Camera and Single LiDAR",
    "volume": "main",
    "abstract": "Depth estimation is usually ill-posed and ambiguous for monocular camera-based 3D multi-person pose estimation. Since LiDAR can capture accurate depth information in long-range scenes, it can benefit both the global localization of individuals and the 3D pose estimation by providing rich geometry features. Motivated by this, we propose a monocular camera and single LiDAR-based method for 3D multi-person pose estimation in large-scale scenes, which is easy to deploy and insensitive to light. Specifically, we design an effective fusion strategy to take advantage of multi-modal input data, including images and point cloud, and make full use of temporal information to guide the network to learn natural and coherent human motions. Without relying on any 3D pose annotations, our method exploits the inherent geometry constraints of point cloud for self-supervision and utilizes 2D keypoints on images for weak supervision. Extensive experiments on public datasets and our newly collected dataset demonstrate the superiority and generalization capability of our proposed method. Project homepage is at \\url{https://github.com/4DVLab/FusionPose.git}",
    "checked": true,
    "id": "39dfc2eeb83e8694b1adc0b484775e9d47fee37a",
    "semantic_title": "weakly supervised 3d multi-person pose estimation for large-scale scenes based on monocular camera and single lidar",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25121": {
    "title": "OctFormer: Efficient Octree-Based Transformer for Point Cloud Compression with Local Enhancement",
    "volume": "main",
    "abstract": "Point cloud compression with a higher compression ratio and tiny loss is essential for efficient data transportation. However, previous methods that depend on 3D convolution or frequent multi-head self-attention operations bring huge computations. To address this problem, we propose an octree-based Transformer compression method called OctFormer, which does not rely on the occupancy information of sibling nodes. Our method uses non-overlapped context windows to construct octree node sequences and share the result of a multi-head self-attention operation among a sequence of nodes. Besides, we introduce a locally-enhance module for exploiting the sibling features and a positional encoding generator for enhancing the translation invariance of the octree node sequence. Compared to the previous state-of-the-art works, our method obtains up to 17% Bpp savings compared to the voxel-context-based baseline and saves an overall 99% coding time compared to the attention-based baseline",
    "checked": true,
    "id": "a7acd42a6df03f98b6760c2f22b2bd2ce90695a0",
    "semantic_title": "octformer: efficient octree-based transformer for point cloud compression with local enhancement",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25122": {
    "title": "Dual-Domain Attention for Image Deblurring",
    "volume": "main",
    "abstract": "As a long-standing and challenging task, image deblurring aims to reconstruct the latent sharp image from its degraded counterpart. In this study, to bridge the gaps between degraded/sharp image pairs in the spatial and frequency domains simultaneously, we develop the dual-domain attention mechanism for image deblurring. Self-attention is widely used in vision tasks, however, due to the quadratic complexity, it is not applicable to image deblurring with high-resolution images. To alleviate this issue, we propose a novel spatial attention module by implementing self-attention in the style of dynamic group convolution for integrating information from the local region, enhancing the representation learning capability and reducing computational burden. Regarding frequency domain learning, many frequency-based deblurring approaches either treat the spectrum as a whole or decompose frequency components in a complicated manner. In this work, we devise a frequency attention module to compactly decouple the spectrum into distinct frequency parts and accentuate the informative part with extremely lightweight learnable parameters. Finally, we incorporate attention modules into a U-shaped network. Extensive comparisons with prior arts on the common benchmarks show that our model, named Dual-domain Attention Network (DDANet), obtains comparable results with a significantly improved inference speed",
    "checked": true,
    "id": "2ea2350d2bbafc07267d45aac410985376a6a332",
    "semantic_title": "dual-domain attention for image deblurring",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25123": {
    "title": "Multi-Resolution Monocular Depth Map Fusion by Self-Supervised Gradient-Based Composition",
    "volume": "main",
    "abstract": "Monocular depth estimation is a challenging problem on which deep neural networks have demonstrated great potential. However, depth maps predicted by existing deep models usually lack fine-grained details due to convolution operations and down-samplings in networks. We find that increasing input resolution is helpful to preserve more local details while the estimation at low resolution is more accurate globally. Therefore, we propose a novel depth map fusion module to combine the advantages of estimations with multi-resolution inputs. Instead of merging the low- and high-resolution estimations equally, we adopt the core idea of Poisson fusion, trying to implant the gradient domain of high-resolution depth into the low-resolution depth. While classic Poisson fusion requires a fusion mask as supervision, we propose a self-supervised framework based on guided image filtering. We demonstrate that this gradient-based composition performs much better at noisy immunity, compared with the state-of-the-art depth map fusion method. Our lightweight depth fusion is one-shot and runs in real-time, making it 80X faster than a state-of-the-art depth fusion method. Quantitative evaluations demonstrate that the proposed method can be integrated into many fully convolutional monocular depth estimation backbones with a significant performance boost, leading to state-of-the-art results of detail enhancement on depth maps. Codes are released at https://github.com/yuinsky/gradient-based-depth-map-fusion",
    "checked": true,
    "id": "1527361af4eff8d52914a9e83f43471209c7de07",
    "semantic_title": "multi-resolution monocular depth map fusion by self-supervised gradient-based composition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25124": {
    "title": "Improving Crowded Object Detection via Copy-Paste",
    "volume": "main",
    "abstract": "Crowdedness caused by overlapping among similar objects is a ubiquitous challenge in the field of 2D visual object detection. In this paper, we first underline two main effects of the crowdedness issue: 1) IoU-confidence correlation disturbances (ICD) and 2) confused de-duplication (CDD). Then we explore a pathway of cracking these nuts from the perspective of data augmentation. Primarily, a particular copy- paste scheme is proposed towards making crowded scenes. Based on this operation, we first design a \"consensus learning\" method to further resist the ICD problem and then find out the pasting process naturally reveals a pseudo \"depth\" of object in the scene, which can be potentially used for alleviating CDD dilemma. Both methods are derived from magical using of the copy-pasting without extra cost for hand-labeling. Experiments show that our approach can easily improve the state-of-the-art detector in typical crowded detection task by more than 2% without any bells and whistles. Moreover, this work can outperform existing data augmentation strategies in crowded scenario",
    "checked": true,
    "id": "ea6e1e2254a95a7f5413f9fcfa4b1c0d63de46a0",
    "semantic_title": "improving crowded object detection via copy-paste",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25125": {
    "title": "Defending Backdoor Attacks on Vision Transformer via Patch Processing",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have a radically different architecture with significantly less inductive bias than Convolutional Neural Networks. Along with the improvement in performance, security and robustness of ViTs are also of great importance to study. In contrast to many recent works that exploit the robustness of ViTs against adversarial examples, this paper investigates a representative causative attack, i.e., backdoor. We first examine the vulnerability of ViTs against various backdoor attacks and find that ViTs are also quite vulnerable to existing attacks. However, we observe that the clean-data accuracy and backdoor attack success rate of ViTs respond distinctively to patch transformations before the positional encoding. Then, based on this finding, we propose an effective method for ViTs to defend both patch-based and blending-based trigger backdoor attacks via patch processing. The performances are evaluated on several benchmark datasets, including CIFAR10, GTSRB, and TinyImageNet, which show the proposedds defense is very successful in mitigating backdoor attacks for ViTs. To the best of our knowledge, this paper presents the first defensive strategy that utilizes a unique characteristic of ViTs against backdoor attacks",
    "checked": true,
    "id": "95cb1fe9ad477b0ee3dff5659887f3ea0347e5e9",
    "semantic_title": "defending backdoor attacks on vision transformer via patch processing",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25126": {
    "title": "Head-Free Lightweight Semantic Segmentation with Linear Transformer",
    "volume": "main",
    "abstract": "Existing semantic segmentation works have been mainly focused on designing effective decoders; however, the computational load introduced by the overall structure has long been ignored, which hinders their applications on resource-constrained hardwares. In this paper, we propose a head-free lightweight architecture specifically for semantic segmentation, named Adaptive Frequency Transformer (AFFormer). AFFormer adopts a parallel architecture to leverage prototype representations as specific learnable local descriptions which replaces the decoder and preserves the rich image semantics on high-resolution features. Although removing the decoder compresses most of the computation, the accuracy of the parallel structure is still limited by low computational resources. Therefore, we employ heterogeneous operators (CNN and vision Transformer) for pixel embedding and prototype representations to further save computational costs. Moreover, it is very difficult to linearize the complexity of the vision Transformer from the perspective of spatial domain. Due to the fact that semantic segmentation is very sensitive to frequency information, we construct a lightweight prototype learning block with adaptive frequency filter of complexity O(n) to replace standard self attention with O(n^2). Extensive experiments on widely adopted datasets demonstrate that AFFormer achieves superior accuracy while retaining only 3M parameters. On the ADE20K dataset, AFFormer achieves 41.8 mIoU and 4.6 GFLOPs, which is 4.4 mIoU higher than Segformer, with 45% less GFLOPs. On the Cityscapes dataset, AFFormer achieves 78.7 mIoU and 34.4 GFLOPs, which is 2.5 mIoU higher than Segformer with 72.5% less GFLOPs. Code is available at https://github.com/dongbo811/AFFormer",
    "checked": true,
    "id": "5fbc38ed3aa2de8eb22bc263e1c4d5091b7ce05a",
    "semantic_title": "head-free lightweight semantic segmentation with linear transformer",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25127": {
    "title": "Hierarchical Contrast for Unsupervised Skeleton-Based Action Representation Learning",
    "volume": "main",
    "abstract": "This paper targets unsupervised skeleton-based action representation learning and proposes a new Hierarchical Contrast (HiCo) framework. Different from the existing contrastive-based solutions that typically represent an input skeleton sequence into instance-level features and perform contrast holistically, our proposed HiCo represents the input into multiple-level features and performs contrast in a hierarchical manner. Specifically, given a human skeleton sequence, we represent it into multiple feature vectors of different granularities from both temporal and spatial domains via sequence-to-sequence (S2S) encoders and unified downsampling modules. Besides, the hierarchical contrast is conducted in terms of four levels: instance level, domain level, clip level, and part level. Moreover, HiCo is orthogonal to the S2S encoder, which allows us to flexibly embrace state-of-the-art S2S encoders. Extensive experiments on four datasets, i.e., NTU-60, NTU-120, PKU-I and PKU-II, show that HiCo achieves a new state-of-the-art for unsupervised skeleton-based action representation learning in two downstream tasks including action recognition and retrieval, and its learned action representation is of good transferability. Besides, we also show that our framework is effective for semi-supervised skeleton-based action recognition. Our code is available at https://github.com/HuiGuanLab/HiCo",
    "checked": true,
    "id": "52740e6b55a27ed79397adc59928ca90e739a53a",
    "semantic_title": "hierarchical contrast for unsupervised skeleton-based action representation learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25128": {
    "title": "Exploring Tuning Characteristics of Ventral Stream's Neurons for Few-Shot Image Classification",
    "volume": "main",
    "abstract": "Human has the remarkable ability of learning novel objects by browsing extremely few examples, which may be attributed to the generic and robust feature extracted in the ventral stream of our brain for representing visual objects. In this sense, the tuning characteristics of ventral stream's neurons can be useful prior knowledge to improve few-shot classification. Specifically, we computationally model two groups of neurons found in ventral stream which are respectively sensitive to shape cues and color cues. Then we propose the hierarchical feature regularization method with these neuron models to regularize the backbone of a few-shot model, thus making it produce more generic and robust features for few-shot classification. In addition, to simulate the tuning characteristic that neuron firing at a higher rate in response to foreground stimulus elements compared to background elements, which we call belongingness, we design a foreground segmentation algorithm based on the observation that the foreground object usually does not appear at the edge of the picture, then multiply the foreground mask with the backbone of few-shot model. Our method is model-agnostic and can be applied to few-shot models with different backbones, training paradigms and classifiers",
    "checked": true,
    "id": "f04d225453e19cda73c0de049a59a8acd7768bec",
    "semantic_title": "exploring tuning characteristics of ventral stream's neurons for few-shot image classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25129": {
    "title": "Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning",
    "volume": "main",
    "abstract": "Incremental few-shot object detection aims at detecting novel classes without forgetting knowledge of the base classes with only a few labeled training data from the novel classes. Most related prior works are on incremental object detection that rely on the availability of abundant training samples per novel class that substantially limits the scalability to real-world setting where novel data can be scarce. In this paper, we propose the Incremental-DETR that does incremental few-shot object detection via fine-tuning and self-supervised learning on the DETR object detector. To alleviate severe over-fitting with few novel class data, we first fine-tune the class-specific components of DETR with self-supervision from additional object proposals generated using Selective Search as pseudo labels. We further introduce an incremental few-shot fine-tuning strategy with knowledge distillation on the class-specific components of DETR to encourage the network in detecting novel classes without forgetting the base classes. Extensive experiments conducted on standard incremental object detection and incremental few-shot object detection settings show that our approach significantly outperforms state-of-the-art methods by a large margin. Our source code is available at https://github.com/dongnana777/Incremental-DETR",
    "checked": true,
    "id": "df11b7fd7c6f627ac8717e91e956e4611746afe3",
    "semantic_title": "incremental-detr: incremental few-shot object detection via self-supervised learning",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25130": {
    "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers",
    "volume": "main",
    "abstract": "This paper explores a better prediction target for BERT pre-training of vision transformers. We observe that current prediction targets disagree with human perception judgment. This contradiction motivates us to learn a perceptual prediction target. We argue that perceptually similar images should stay close to each other in the prediction target space. We surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. Moreover, we adopt a self-supervised transformer model for deep feature extraction and show that it works well for calculating perceptual similarity. We demonstrate that such learned visual tokens indeed exhibit better semantic meanings, and help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve 84.5% Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by +1.3% under the same pre-training epochs. Our approach also gets significant improvement on object detection and segmentation on COCO and semantic segmentation on ADE20K. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art ImageNet accuracy (88.3%) among methods using only ImageNet-1K data",
    "checked": true,
    "id": "3e38f4b4055abecbac2e618df2ecb33554073e08",
    "semantic_title": "peco: perceptual codebook for bert pre-training of vision transformers",
    "citation_count": 145
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25131": {
    "title": "Domain-General Crowd Counting in Unseen Scenarios",
    "volume": "main",
    "abstract": "Domain shift across crowd data severely hinders crowd counting models to generalize to unseen scenarios. Although domain adaptive crowd counting approaches close this gap to a certain extent, they are still dependent on the target domain data to adapt (e.g. finetune) their models to the specific domain. In this paper, we instead target to train a model based on a single source domain which can generalize well on any unseen domain. This falls into the realm of domain generalization that remains unexplored in crowd counting. We first introduce a dynamic sub-domain division scheme which divides the source domain into multiple sub-domains such that we can initiate a meta-learning framework for domain generalization. The sub-domain division is dynamically refined during the meta-learning. Next, in order to disentangle domain-invariant information from domain-specific information in image features, we design the domain-invariant and -specific crowd memory modules to re-encode image features. Two types of losses, i.e. feature reconstruction and orthogonal losses, are devised to enable this disentanglement. Extensive experiments on several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show the strong generalizability of our method. Our code is available at: https://github.com/ZPDu/Domain-general-Crowd-Counting-in-Unseen-Scenarios",
    "checked": true,
    "id": "f8232b83e87aef507a21b7afccecdbe511e46999",
    "semantic_title": "domain-general crowd counting in unseen scenarios",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25132": {
    "title": "Few-Shot Defect Image Generation via Defect-Aware Feature Manipulation",
    "volume": "main",
    "abstract": "The performances of defect inspection have been severely hindered by insufficient defect images in industries, which can be alleviated by generating more samples as data augmentation. We propose the first defect image generation method in the challenging few-shot cases. Given just a handful of defect images and relatively more defect-free ones, our goal is to augment the dataset with new defect images. Our method consists of two training stages. First, we train a data-efficient StyleGAN2 on defect-free images as the backbone. Second, we attach defect-aware residual blocks to the backbone, which learn to produce reasonable defect masks and accordingly manipulate the features within the masked regions by training the added modules on limited defect images. Extensive experiments on MVTec AD dataset not only validate the effectiveness of our method in generating realistic and diverse defect images, but also manifest the benefits it brings to downstream defect inspection tasks. Codes are available at https://github.com/Ldhlwh/DFMGAN",
    "checked": true,
    "id": "bf39a82f65d9047e676f2f85f700c73d427b4189",
    "semantic_title": "few-shot defect image generation via defect-aware feature manipulation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25133": {
    "title": "Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have shown great potential for high-quality image synthesis. However, when it comes to producing images with complex scenes, how to properly describe both image global structures and object details remains a challenging task. In this paper, we present Frido, a Feature Pyramid Diffusion model performing a multi-scale coarse-to-fine denoising process for image synthesis. Our model decomposes an input image into scale-dependent vector quantized features, followed by a coarse-to-fine gating for producing image output. During the above multi-scale representation learning stage, additional input conditions like text, scene graph, or image layout can be further exploited. Thus, Frido can be also applied for conditional or cross-modality image synthesis. We conduct extensive experiments over various unconditioned and conditional image generation tasks, ranging from text-to-image synthesis, layout-to-image, scene-graph-to-image, to label-to-image. More specifically, we achieved state-of-the-art FID scores on five benchmarks, namely layout-to-image on COCO and OpenImages, scene-graph-to-image on COCO and Visual Genome, and label-to-image on COCO",
    "checked": true,
    "id": "a888dd6d8dd0087fc7d74da8a005922d0923ad2b",
    "semantic_title": "frido: feature pyramid diffusion for complex scene image synthesis",
    "citation_count": 31
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25134": {
    "title": "Target-Free Text-Guided Image Manipulation",
    "volume": "main",
    "abstract": "We tackle the problem of target-free text-guided image manipulation, which requires one to modify the input reference image based on the given text instruction, while no ground truth target image is observed during training. To address this challenging task, we propose a Cyclic-Manipulation GAN (cManiGAN) in this paper, which is able to realize where and how to edit the image regions of interest. Specifically, the image editor in cManiGAN learns to identify and complete the input image, while cross-modal interpreter and reasoner are deployed to verify the semantic correctness of the output image based on the input instruction. While the former utilizes factual/counterfactual description learning for authenticating the image semantics, the latter predicts the \"undo\" instruction and provides pixel-level supervision for the training of cManiGAN. With the above operational cycle-consistency, our cManiGAN can be trained in the above weakly supervised setting. We conduct extensive experiments on the datasets of CLEVR and COCO datasets, and the effectiveness and generalizability of our proposed method can be successfully verified. Project page: sites.google.com/view/wancyuanfan/projects/cmanigan",
    "checked": true,
    "id": "4a426c8209b38ed63b224363f4dcf179692d51ac",
    "semantic_title": "target-free text-guided image manipulation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25135": {
    "title": "One Is All: Bridging the Gap between Neural Radiance Fields Architectures with Progressive Volume Distillation",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) methods have proved effective as compact, high-quality and versatile representations for 3D scenes, and enable downstream tasks such as editing, retrieval, navigation, etc. Various neural architectures are vying for the core structure of NeRF, including the plain Multi-Layer Perceptron (MLP), sparse tensors, low-rank tensors, hashtables and their compositions. Each of these representations has its particular set of trade-offs. For example, the hashtable-based representations admit faster training and rendering but their lack of clear geometric meaning hampers downstream tasks like spatial-relation-aware editing. In this paper, we propose Progressive Volume Distillation (PVD), a systematic distillation method that allows any-to-any conversions between different architectures, including MLP, sparse or low-rank tensors, hashtables and their compositions. PVD consequently empowers downstream applications to optimally adapt the neural representations for the task at hand in a post hoc fashion. The conversions are fast, as distillation is progressively performed on different levels of volume representations, from shallower to deeper. We also employ special treatment of density to deal with its specific numerical instability problem. Empirical evidence is presented to validate our method on the NeRF-Synthetic, LLFF and TanksAndTemples datasets. For example, with PVD, an MLP-based NeRF model can be distilled from a hashtable-based Instant-NGP model at a 10~20X faster speed than being trained the original NeRF from scratch, while achieving a superior level of synthesis quality. Code is available at https://github.com/megvii-research/AAAI2023-PVD",
    "checked": true,
    "id": "5e6f661b7ce2c457af3b9a4cee77101d93c2013c",
    "semantic_title": "one is all: bridging the gap between neural radiance fields architectures with progressive volume distillation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25136": {
    "title": "Weakly-Supervised Semantic Segmentation for Histopathology Images Based on Dataset Synthesis and Feature Consistency Constraint",
    "volume": "main",
    "abstract": "Tissue segmentation is a critical task in computational pathology due to its desirable ability to indicate the prognosis of cancer patients. Currently, numerous studies attempt to use image-level labels to achieve pixel-level segmentation to reduce the need for fine annotations. However, most of these methods are based on class activation map, which suffers from inaccurate segmentation boundaries. To address this problem, we propose a novel weakly-supervised tissue segmentation framework named PistoSeg, which is implemented under a fully-supervised manner by transferring tissue category labels to pixel-level masks. Firstly, a dataset synthesis method is proposed based on Mosaic transformation to generate synthesized images with pixel-level masks. Next, considering the difference between synthesized and real images, this paper devises an attention-based feature consistency, which directs the training process of a proposed pseudo-mask refining module. Finally, the refined pseudo-masks are used to train a precise segmentation model for testing. Experiments based on WSSS4LUAD and BCSS-WSSS validate that PistoSeg outperforms the state-of-the-art methods. The code is released at https://github.com/Vison307/PistoSeg",
    "checked": true,
    "id": "f27d61ba154ada6b375a4dc313b673be7dbfdd84",
    "semantic_title": "weakly-supervised semantic segmentation for histopathology images based on dataset synthesis and feature consistency constraint",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25137": {
    "title": "Uncertainty-Aware Image Captioning",
    "volume": "main",
    "abstract": "It is well believed that the higher uncertainty in a word of the caption, the more inter-correlated context information is required to determine it. However, current image captioning methods usually consider the generation of all words in a sentence sequentially and equally. In this paper, we propose an uncertainty-aware image captioning framework, which parallelly and iteratively operates insertion of discontinuous candidate words between existing words from easy to difficult until converged. We hypothesize that high-uncertainty words in a sentence need more prior information to make a correct decision and should be produced at a later stage. The resulting non-autoregressive hierarchy makes the caption generation explainable and intuitive. Specifically, we utilize an image-conditioned bag-of-word model to measure the word uncertainty and apply a dynamic programming algorithm to construct the training pairs. During inference, we devise an uncertainty-adaptive parallel beam search technique that yields an empirically logarithmic time complexity. Extensive experiments on the MS COCO benchmark reveal that our approach outperforms the strong baseline and related methods on both captioning quality as well as decoding speed",
    "checked": true,
    "id": "d13f52cbff1416d11986f996fa68ee9767844c60",
    "semantic_title": "uncertainty-aware image captioning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25138": {
    "title": "Unsupervised Domain Adaptation for Medical Image Segmentation by Selective Entropy Constraints and Adaptive Semantic Alignment",
    "volume": "main",
    "abstract": "Generalizing a deep learning model to new domains is crucial for computer-aided medical diagnosis systems. Most existing unsupervised domain adaptation methods have made significant progress in reducing the domain distribution gap through adversarial training. However, these methods may still produce overconfident but erroneous results on unseen target images. This paper proposes a new unsupervised domain adaptation framework for cross-modality medical image segmentation. Specifically, We first introduce two data augmentation approaches to generate two sets of semantics-preserving augmented images. Based on the model's predictive consistency on these two sets of augmented images, we identify reliable and unreliable pixels. We then perform a selective entropy constraint: we minimize the entropy of reliable pixels to increase their confidence while maximizing the entropy of unreliable pixels to reduce their confidence. Based on the identified reliable and unreliable pixels, we further propose an adaptive semantic alignment module which performs class-level distribution adaptation by minimizing the distance between same class prototypes between domains, where unreliable pixels are removed to derive more accurate prototypes. We have conducted extensive experiments on the cross-modality cardiac structure segmentation task. The experimental results show that the proposed method significantly outperforms the state-of-the-art comparison algorithms. Our code and data are available at https://github.com/fengweie/SE_ASA",
    "checked": true,
    "id": "b027cb327be4a5051f00f0b84d936e8b2d3b653c",
    "semantic_title": "unsupervised domain adaptation for medical image segmentation by selective entropy constraints and adaptive semantic alignment",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25139": {
    "title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
    "volume": "main",
    "abstract": "Effectively preserving and encoding structure features from objects in irregular and sparse LiDAR points is a crucial challenge to 3D object detection on the point cloud. Recently, Transformer has demonstrated promising performance on many 2D and even 3D vision tasks. Compared with the fixed and rigid convolution kernels, the self-attention mechanism in Transformer can adaptively exclude the unrelated or noisy points and is thus suitable for preserving the local spatial structure in the irregular LiDAR point cloud. However, Transformer only performs a simple sum on the point features, based on the self-attention mechanism, and all the points share the same transformation for value. A such isotropic operation cannot capture the direction-distance-oriented local structure, which is essential for 3D object detection. In this work, we propose a Structure-Embedding transFormer (SEFormer), which can not only preserve the local structure as a traditional Transformer but also have the ability to encode the local structure. Compared to the self-attention mechanism in traditional Transformer, SEFormer learns different feature transformations for value points based on the relative directions and distances to the query point. Then we propose a SEFormer-based network for high-performance 3D object detection. Extensive experiments show that the proposed architecture can achieve SOTA results on the Waymo Open Dataset, one of the most significant 3D detection benchmarks for autonomous driving. Specifically, SEFormer achieves 79.02% mAP, which is 1.2% higher than existing works. https://github.com/tdzdog/SEFormer",
    "checked": true,
    "id": "08e31f99bd0738c34100b24ffa0b059cdeebfc26",
    "semantic_title": "seformer: structure embedding transformer for 3d object detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25140": {
    "title": "Exploit Domain-Robust Optical Flow in Domain Adaptive Video Semantic Segmentation",
    "volume": "main",
    "abstract": "Domain adaptive semantic segmentation aims to exploit the pixel-level annotated samples on source domain to assist the segmentation of unlabeled samples on target domain. For such a task, the key is to construct reliable supervision signals on target domain. However, existing methods can only provide unreliable supervision signals constructed by segmentation model (SegNet) that are generally domain-sensitive. In this work, we try to find a domain-robust clue to construct more reliable supervision signals. Particularly, we experimentally observe the domain-robustness of optical flow in video tasks as it mainly represents the motion characteristics of scenes. However, optical flow cannot be directly used as supervision signals of semantic segmentation since both of them essentially represent different information. To tackle this issue, we first propose a novel Segmentation-to-Flow Module (SFM) that converts semantic segmentation maps to optical flows, named the segmentation-based flow (SF), and then propose a Segmentation-based Flow Consistency (SFC) method to impose consistency between SF and optical flow, which can implicitly supervise the training of segmentation model. The extensive experiments on two challenging benchmarks demonstrate the effectiveness of our method, and it outperforms previous state-of-the-art methods with considerable performance improvement. Our code is available at https://github.com/EdenHazardan/SFC",
    "checked": true,
    "id": "e4c180bdfe46f3d59791c7e970ccef667ab8481d",
    "semantic_title": "exploit domain-robust optical flow in domain adaptive video semantic segmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25141": {
    "title": "Scene-Level Sketch-Based Image Retrieval with Minimal Pairwise Supervision",
    "volume": "main",
    "abstract": "The sketch-based image retrieval (SBIR) task has long been researched at the instance level, where both query sketches and candidate images are assumed to contain only one dominant object. This strong assumption constrains its application, especially with the increasingly popular intelligent terminals and human-computer interaction technology. In this work, a more general scene-level SBIR task is explored, where sketches and images can both contain multiple object instances. The new general task is extremely challenging due to several factors: (i) scene-level SBIR inherently shares sketch-specific difficulties with instance-level SBIR (e.g., sparsity, abstractness, and diversity), (ii) the cross-modal similarity is measured between two partially aligned domains (i.e., not all objects in images are drawn in scene sketches), and (iii) besides instance-level visual similarity, a more complex multi-dimensional scene-level feature matching problem is imposed (including appearance, semantics, layout, etc.). Addressing these challenges, a novel Conditional Graph Autoencoder model is proposed to deal with scene-level sketch-images retrieval. More importantly, the model can be trained with only pairwise supervision, which distinguishes our study from others in that elaborate instance-level annotations (for example, bounding boxes) are no longer required. Extensive experiments confirm the ability of our model to robustly retrieve multiple related objects at the scene level and exhibit superior performance beyond strong competitors",
    "checked": true,
    "id": "85b696039df8e3a934f69c5e5d72a904d4856fa2",
    "semantic_title": "scene-level sketch-based image retrieval with minimal pairwise supervision",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25142": {
    "title": "Causal Intervention for Human Trajectory Prediction with Cross Attention Mechanism",
    "volume": "main",
    "abstract": "Human trajectory Prediction (HTP) in complex social environments plays a crucial and fundamental role in artificial intelligence systems. Conventional methods make use of both history behaviors and social interactions to forecast future trajectories. However, we demonstrate that the social environment is a confounder that misleads the model to learn spurious correlations between history and future trajectories. To end this, we first formulate the social environment, history and future trajectory variables into a structural causal model to analyze the causalities among them. Based on causal intervention rather than conventional likelihood, we propose a Social Environment ADjustment (SEAD) method, to remove the confounding effect of the social environment. The core of our method is implemented by a Social Cross Attention (SCA) module, which is universal, simple and effective. Our method has consistent improvements on ETH-UCY datasets with three baseline models and achieves competitive performances with existing methods",
    "checked": true,
    "id": "610924a1aa60494f01158fdb9cb680d0e6203148",
    "semantic_title": "causal intervention for human trajectory prediction with cross attention mechanism",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25143": {
    "title": "Point-Teaching: Weakly Semi-supervised Object Detection with Point Annotations",
    "volume": "main",
    "abstract": "Point annotations are considerably more time-efficient than bounding box annotations. However, how to use cheap point annotations to boost the performance of semi-supervised object detection is still an open question. In this work, we present Point-Teaching, a weakly- and semi-supervised object detection framework to fully utilize the point annotations. Specifically, we propose a Hungarian-based point-matching method to generate pseudo labels for point-annotated images. We further propose multiple instance learning (MIL) approaches at the level of images and points to supervise the object detector with point annotations. Finally, we propose a simple data augmentation, named Point-Guided Copy-Paste, to reduce the impact of those unmatched points. Experiments demonstrate the effectiveness of our method on a few datasets and various data regimes. In particular, Point-Teaching outperforms the previous best method Group R-CNN by 3.1 AP with 5% fully labeled data and 2.3 AP with 30% fully labeled data on the MS COCO dataset. We believe that our proposed framework can largely lower the bar of learning accurate object detectors and pave the way for its broader applications. The code is available at https://github.com/YongtaoGe/Point-Teaching",
    "checked": true,
    "id": "6acab3f2379de04d4a3449ed653add6db32a4af2",
    "semantic_title": "point-teaching: weakly semi-supervised object detection with point annotations",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25144": {
    "title": "Progressive Multi-View Human Mesh Recovery with Self-Supervision",
    "volume": "main",
    "abstract": "To date, little attention has been given to multi-view 3D human mesh estimation, despite real-life applicability (e.g., motion capture, sport analysis) and robustness to single-view ambiguities. Existing solutions typically suffer from poor generalization performance to new settings, largely due to the limited diversity of image/3D-mesh pairs in multi-view training data. To address this shortcoming, people have explored the use of synthetic images. But besides the usual impact of visual gap between rendered and target data, synthetic-data-driven multi-view estimators also suffer from overfitting to the camera viewpoint distribution sampled during training which usually differs from real-world distributions. Tackling both challenges, we propose a novel simulation-based training pipeline for multi-view human mesh recovery, which (a) relies on intermediate 2D representations which are more robust to synthetic-to-real domain gap; (b) leverages learnable calibration and triangulation to adapt to more diversified camera setups; and (c) progressively aggregates multi-view information in a canonical 3D space to remove ambiguities in 2D representations. Through extensive benchmarking, we demonstrate the superiority of the proposed solution especially for unseen in-the-wild scenarios",
    "checked": true,
    "id": "f63d71bfd0c363132865a36cd2169b3915888c94",
    "semantic_title": "progressive multi-view human mesh recovery with self-supervision",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25145": {
    "title": "Incremental Image De-raining via Associative Memory",
    "volume": "main",
    "abstract": "While deep learning models have achieved the state-of-the-art performance on single-image rain removal, most methods only consider learning fixed mapping rules on the single synthetic dataset for lifetime. This limits the real-life application as iterative optimization may change mapping rules and training samples. However, when models learn a sequence of datasets in multiple incremental steps, they are susceptible to catastrophic forgetting that adapts to new incremental episodes while failing to preserve previously acquired mapping rules. In this paper, we argue the importance of sample diversity in the episodes on the iterative optimization, and propose a novel memory management method, Associative Memory, to achieve incremental image de-raining. It bridges connections between current and past episodes for feature reconstruction by sampling domain mappings of past learning steps, and guides the learning to trace the current pathway back to the historical environment without storing extra data. Experiments demonstrate that our method can achieve better performance than existing approaches on both inhomogeneous and incremental datasets within the spectrum of highly compact systems",
    "checked": true,
    "id": "c7ff3f133e4290b266f5881a7108a681ffa72e45",
    "semantic_title": "incremental image de-raining via associative memory",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25146": {
    "title": "Flexible 3D Lane Detection by Hierarchical Shape Matching",
    "volume": "main",
    "abstract": "As one of the basic while vital technologies for HD map construction, 3D lane detection is still an open problem due to varying visual conditions, complex typologies, and strict demands for precision. In this paper, an end-to-end flexible and hierarchical lane detector is proposed to precisely predict 3D lane lines from point clouds. Specifically, we design a hierarchical network predicting flexible representations of lane shapes at different levels, simultaneously collecting global instance semantics and avoiding local errors. In the global scope, we propose to regress parametric curves w.r.t adaptive axes that help to make more robust predictions towards complex scenes, while in the local vision the structure of lane segment is detected in each of the dynamic anchor cells sampled along the global predicted curves. Moreover, corresponding global and local shape matching losses and anchor cell generation strategies are designed. Experiments on two datasets show that we overwhelm current top methods under high precision standards, and full ablation studies also verify each part of our method. Our codes will be released at https://github.com/Doo-do/FHLD",
    "checked": true,
    "id": "247dfec8ebff13808228e73d4ad3166b45cb4972",
    "semantic_title": "flexible 3d lane detection by hierarchical shape matching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25147": {
    "title": "Underwater Ranker: Learn Which Is Better and How to Be Better",
    "volume": "main",
    "abstract": "In this paper, we present a ranking-based underwater image quality assessment (UIQA) method, abbreviated as URanker. The URanker is built on the efficient conv-attentional image Transformer. In terms of underwater images, we specially devise (1) the histogram prior that embeds the color distribution of an underwater image as histogram token to attend global degradation and (2) the dynamic cross-scale correspondence to model local degradation. The final prediction depends on the class tokens from different scales, which comprehensively considers multi-scale dependencies. With the margin ranking loss, our URanker can accurately rank the order of underwater images of the same scene enhanced by different underwater image enhancement (UIE) algorithms according to their visual quality. To achieve that, we also contribute a dataset, URankerSet, containing sufficient results enhanced by different UIE algorithms and the corresponding perceptual rankings, to train our URanker. Apart from the good performance of URanker, we found that a simple U-shape UIE network can obtain promising performance when it is coupled with our pre-trained URanker as additional supervision. In addition, we also propose a normalization tail that can significantly improve the performance of UIE networks. Extensive experiments demonstrate the state-of-the-art performance of our method. The key designs of our method are discussed. Our code and dataset are available at https://li-chongyi.github.io/URanker_files/",
    "checked": true,
    "id": "f9a07858f88daf54e0ced43ddc4e1178f754458b",
    "semantic_title": "underwater ranker: learn which is better and how to be better",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25148": {
    "title": "ShadowFormer: Global Context Helps Shadow Removal",
    "volume": "main",
    "abstract": "Recent deep learning methods have achieved promising results in image shadow removal. However, most of the existing approaches focus on working locally within shadow and non-shadow regions, resulting in severe artifacts around the shadow boundaries as well as inconsistent illumination between shadow and non-shadow regions. It is still challenging for the deep shadow removal model to exploit the global contextual correlation between shadow and non-shadow regions. In this work, we first propose a Retinex-based shadow model, from which we derive a novel transformer-based network, dubbed ShandowFormer, to exploit non-shadow regions to help shadow region restoration. A multi-scale channel attention framework is employed to hierarchically capture the global information. Based on that, we propose a Shadow-Interaction Module (SIM) with Shadow-Interaction Attention (SIA) in the bottleneck stage to effectively model the context correlation between shadow and non-shadow regions. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to evaluate the proposed method. Our method achieves state-of-the-art performance by using up to 150X fewer model parameters",
    "checked": true,
    "id": "b7ffb3dee668255ccffc5970c795f0c1d79ad79b",
    "semantic_title": "shadowformer: global context helps shadow removal",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25149": {
    "title": "RAFaRe: Learning Robust and Accurate Non-parametric 3D Face Reconstruction from Pseudo 2D&3D Pairs",
    "volume": "main",
    "abstract": "We propose a robust and accurate non-parametric method for single-view 3D face reconstruction (SVFR). While tremendous efforts have been devoted to parametric SVFR, a visible gap still lies between the result 3D shape and the ground truth. We believe there are two major obstacles: 1) the representation of the parametric model is limited to a certain face database; 2) 2D images and 3D shapes in the fitted datasets are distinctly misaligned. To resolve these issues, a large-scale pseudo 2D&3D dataset is created by first rendering the detailed 3D faces, then swapping the face in the wild images with the rendered face. These pseudo 2D&3D pairs are created from publicly available datasets which eliminate the gaps between 2D and 3D data while covering diverse appearances, poses, scenes, and illumination. We further propose a non-parametric scheme to learn a well-generalized SVFR model from the created dataset, and the proposed hierarchical signed distance function turns out to be effective in predicting middle-scale and small-scale 3D facial geometry. Our model outperforms previous methods on FaceScape-wild/lab and MICC benchmarks and is well generalized to various appearances, poses, expressions, and in-the-wild environments. The code is released at https://github.com/zhuhao-nju/rafare",
    "checked": true,
    "id": "743cc26eb711e740724f5a94bb3bcc17a5201728",
    "semantic_title": "rafare: learning robust and accurate non-parametric 3d face reconstruction from pseudo 2d&3d pairs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25150": {
    "title": "RankDNN: Learning to Rank for Few-Shot Learning",
    "volume": "main",
    "abstract": "This paper introduces a new few-shot learning pipeline that casts relevance ranking for image retrieval as binary ranking relation classification. In comparison to image classification, ranking relation classification is sample efficient and domain agnostic. Besides, it provides a new perspective on few-shot learning and is complementary to state-of-the-art methods. The core component of our deep neural network is a simple MLP, which takes as input an image triplet encoded as the difference between two vector-Kronecker products, and outputs a binary relevance ranking order. The proposed RankMLP can be built on top of any state-of-the-art feature extractors, and our entire deep neural network is called the ranking deep neural network, or RankDNN. Meanwhile, RankDNN can be flexibly fused with other post-processing methods. During the meta test, RankDNN ranks support images according to their similarity with the query samples, and each query sample is assigned the class label of its nearest neighbor. Experiments demonstrate that RankDNN can effectively improve the performance of its baselines based on a variety of backbones and it outperforms previous state-of-the-art algorithms on multiple few-shot learning benchmarks, including miniImageNet, tieredImageNet, Caltech-UCSD Birds, and CIFAR-FS. Furthermore, experiments on the cross-domain challenge demonstrate the superior transferability of RankDNN.The code is available at: https://github.com/guoqianyu-alberta/RankDNN",
    "checked": true,
    "id": "f9c63c1de8de192c62d334eb97bdff30c2f6fc64",
    "semantic_title": "rankdnn: learning to rank for few-shot learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25151": {
    "title": "Social Relation Reasoning Based on Triangular Constraints",
    "volume": "main",
    "abstract": "Social networks are essentially in a graph structure where persons act as nodes and the edges connecting nodes denote social relations. The prediction of social relations, therefore, relies on the context in graphs to model the higher-order constraints among relations, which has not been exploited sufficiently by previous works, however. In this paper, we formulate the paradigm of the higher-order constraints in social relations into triangular relational closed-loop structures, i.e., triangular constraints, and further introduce the triangular reasoning graph attention network (TRGAT). Our TRGAT employs the attention mechanism to aggregate features with triangular constraints in the graph, thereby exploiting the higher-order context to reason social relations iteratively. Besides, to acquire better feature representations of persons, we introduce node contrastive learning into relation reasoning. Experimental results show that our method outperforms existing approaches significantly, with higher accuracy and better consistency in generating social relation graphs",
    "checked": true,
    "id": "71e0b80918978266b24e1d2df79f399885ba661d",
    "semantic_title": "social relation reasoning based on triangular constraints",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25152": {
    "title": "CALIP: Zero-Shot Enhancement of CLIP with Parameter-Free Attention",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has been shown to learn visual representations with promising zero-shot performance. To further improve its downstream accuracy, existing works propose additional learnable modules upon CLIP and fine-tune them by few-shot training sets. However, the resulting extra training cost and data requirement severely hinder the efficiency for model deployment and knowledge transfer. In this paper, we introduce a free-lunch enhancement method, CALIP, to boost CLIP's zero-shot performance via a parameter-free attention module. Specifically, we guide visual and textual representations to interact with each other and explore cross-modal informative features via attention. As the pre-training has largely reduced the embedding distances between two modalities, we discard all learnable parameters in the attention and bidirectionally update the multi-modal features, enabling the whole process to be parameter-free and training-free. In this way, the images are blended with textual-aware signals and the text representations become visual-guided for better adaptive zero-shot alignment. We evaluate CALIP on various benchmarks of 14 datasets for both 2D image and 3D point cloud few-shot classification, showing consistent zero-shot performance improvement over CLIP. Based on that, we further insert a small number of linear layers in CALIP's attention module and verify our robustness under the few-shot settings, which also achieves leading performance compared to existing methods. Those extensive experiments demonstrate the superiority of our approach for efficient enhancement of CLIP. Code is available at https://github.com/ZiyuGuo99/CALIP",
    "checked": true,
    "id": "ca26023c4dbde9a54145b68e1a6a40533fcc1a4a",
    "semantic_title": "calip: zero-shot enhancement of clip with parameter-free attention",
    "citation_count": 18
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25153": {
    "title": "Few-Shot Object Detection via Variational Feature Aggregation",
    "volume": "main",
    "abstract": "As few-shot object detectors are often trained with abundant base samples and fine-tuned on few-shot novel examples, the learned models are usually biased to base classes and sensitive to the variance of novel examples. To address this issue, we propose a meta-learning framework with two novel feature aggregation schemes. More precisely, we first present a Class-Agnostic Aggregation (CAA) method, where the query and support features can be aggregated regardless of their categories. The interactions between different classes encourage class-agnostic representations and reduce confusion between base and novel classes. Based on the CAA, we then propose a Variational Feature Aggregation (VFA) method, which encodes support examples into class-level support features for robust feature aggregation. We use a variational autoencoder to estimate class distributions and sample variational features from distributions that are more robust to the variance of support examples. Besides, we decouple classification and regression tasks so that VFA is performed on the classification branch without affecting object localization. Extensive experiments on PASCAL VOC and COCO demonstrate that our method significantly outperforms a strong baseline (up to 16%) and previous state-of-the-art methods (4% in average)",
    "checked": true,
    "id": "4d1e8a99b49e5ac7325e42c5958f76c22ac26e82",
    "semantic_title": "few-shot object detection via variational feature aggregation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25154": {
    "title": "Generating Transferable 3D Adversarial Point Cloud via Random Perturbation Factorization",
    "volume": "main",
    "abstract": "Recent studies have demonstrated that existing deep neural networks (DNNs) on 3D point clouds are vulnerable to adversarial examples, especially under the white-box settings where the adversaries have access to model parameters. However, adversarial 3D point clouds generated by existing white-box methods have limited transferability across different DNN architectures. They have only minor threats in real-world scenarios under the black-box settings where the adversaries can only query the deployed victim model. In this paper, we revisit the transferability of adversarial 3D point clouds. We observe that an adversarial perturbation can be randomly factorized into two sub-perturbations, which are also likely to be adversarial perturbations. It motivates us to consider the effects of the perturbation and its sub-perturbations simultaneously to increase the transferability for sub-perturbations also contain helpful information. In this paper, we propose a simple yet effective attack method to generate more transferable adversarial 3D point clouds. Specifically, rather than simply optimizing the loss of perturbation alone, we combine it with its random factorization. We conduct experiments on benchmark dataset, verifying our method's effectiveness in increasing transferability while preserving high efficiency",
    "checked": true,
    "id": "eca815987e1c6a51e00fdc202342d50a288599f0",
    "semantic_title": "generating transferable 3d adversarial point cloud via random perturbation factorization",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25155": {
    "title": "Target-Aware Tracking with Long-Term Context Attention",
    "volume": "main",
    "abstract": "Most deep trackers still follow the guidance of the siamese paradigms and use a template that contains only the target without any contextual information, which makes it difficult for the tracker to cope with large appearance changes, rapid target movement, and attraction from similar objects. To alleviate the above problem, we propose a long-term context attention (LCA) module that can perform extensive information fusion on the target and its context from long-term frames, and calculate the target correlation while enhancing target features. The complete contextual information contains the location of the target as well as the state around the target. LCA uses the target state from the previous frame to exclude the interference of similar objects and complex backgrounds, thus accurately locating the target and enabling the tracker to obtain higher robustness and regression accuracy. By embedding the LCA module in Transformer, we build a powerful online tracker with a target-aware backbone, termed as TATrack. In addition, we propose a dynamic online update algorithm based on the classification confidence of historical information without additional calculation burden. Our tracker achieves state-of-the-art performance on multiple benchmarks, with 71.1% AUC, 89.3% NP, and 73.0% AO on LaSOT, TrackingNet, and GOT-10k. The code and trained models are available on https://github.com/hekaijie123/TATrack",
    "checked": true,
    "id": "62fc770746f6a283fbc5cfc32275e23bfef82eb7",
    "semantic_title": "target-aware tracking with long-term context attention",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25156": {
    "title": "Weakly-Supervised Camouflaged Object Detection with Scribble Annotations",
    "volume": "main",
    "abstract": "Existing camouflaged object detection (COD) methods rely heavily on large-scale datasets with pixel-wise annotations. However, due to the ambiguous boundary, annotating camouflage objects pixel-wisely is very time-consuming and labor-intensive, taking ~60mins to label one image. In this paper, we propose the first weakly-supervised COD method, using scribble annotations as supervision. To achieve this, we first relabel 4,040 images in existing camouflaged object datasets with scribbles, which takes ~10s to label one image. As scribble annotations only describe the primary structure of objects without details, for the network to learn to localize the boundaries of camouflaged objects, we propose a novel consistency loss composed of two parts: a cross-view loss to attain reliable consistency over different images, and an inside-view loss to maintain consistency inside a single prediction map. Besides, we observe that humans use semantic information to segment regions near the boundaries of camouflaged objects. Hence, we further propose a feature-guided loss, which includes visual features directly extracted from images and semantically significant features captured by the model. Finally, we propose a novel network for COD via scribble learning on structural information and semantic relations. Our network has two novel modules: the local-context contrasted (LCC) module, which mimics visual inhibition to enhance image contrast/sharpness and expand the scribbles into potential camouflaged regions, and the logical semantic relation (LSR) module, which analyzes the semantic relation to determine the regions representing the camouflaged object. Experimental results show that our model outperforms relevant SOTA methods on three COD benchmarks with an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on E-measure, and 4.4% on weighted F-measure",
    "checked": true,
    "id": "613ac2c205cad421a28d7b3a357472a0883803c2",
    "semantic_title": "weakly-supervised camouflaged object detection with scribble annotations",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25157": {
    "title": "Efficient Mirror Detection via Multi-Level Heterogeneous Learning",
    "volume": "main",
    "abstract": "We present HetNet (Multi-level Heterogeneous Network), a highly efficient mirror detection network. Current mirror detection methods focus more on performance than efficiency, limiting the real-time applications (such as drones). Their lack of efficiency is aroused by the common design of adopting homogeneous modules at different levels, which ignores the difference between different levels of features. In contrast, HetNet detects potential mirror regions initially through low-level understandings (e.g., intensity contrasts) and then combines with high-level understandings (contextual discontinuity for instance) to finalize the predictions. To perform accurate yet efficient mirror detection, HetNet follows an effective architecture that obtains specific information at different stages to detect mirrors. We further propose a multi-orientation intensity-based contrasted module (MIC) and a reflection semantic logical module (RSL), equipped on HetNet, to predict potential mirror regions by low-level understandings and analyze semantic logic in scenarios by high-level understandings, respectively. Compared to the state-of-the-art method, HetNet runs 664% faster and draws an average performance gain of 8.9% on MAE, 3.1% on IoU, and 2.0% on F-measure on two mirror detection benchmarks. The code is available at https://github.com/Catherine-R-He/HetNet",
    "checked": true,
    "id": "eb30447cac68f1b22466da2f3f0d85e7a8e0d7c9",
    "semantic_title": "efficient mirror detection via multi-level heterogeneous learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25158": {
    "title": "TransVCL: Attention-Enhanced Video Copy Localization Network with Flexible Supervision",
    "volume": "main",
    "abstract": "Video copy localization aims to precisely localize all the copied segments within a pair of untrimmed videos in video retrieval applications. Previous methods typically start from frame-to-frame similarity matrix generated by cosine similarity between frame-level features of the input video pair, and then detect and refine the boundaries of copied segments on similarity matrix under temporal constraints. In this paper, we propose TransVCL: an attention-enhanced video copy localization network, which is optimized directly from initial frame-level features and trained end-to-end with three main components: a customized Transformer for feature enhancement, a correlation and softmax layer for similarity matrix generation, and a temporal alignment module for copied segments localization. In contrast to previous methods demanding the handcrafted similarity matrix, TransVCL incorporates long-range temporal information between feature sequence pair using self- and cross- attention layers. With the joint design and optimization of three components, the similarity matrix can be learned to present more discriminative copied patterns, leading to significant improvements over previous methods on segment-level labeled datasets (VCSL and VCDB). Besides the state-of-the-art performance in fully supervised setting, the attention architecture facilitates TransVCL to further exploit unlabeled or simply video-level labeled data. Additional experiments of supplementing video-level labeled datasets including SVD and FIVR reveal the high flexibility of TransVCL from full supervision to semi-supervision (with or without video-level annotation). Code is publicly available at https://github.com/transvcl/TransVCL",
    "checked": true,
    "id": "18c912802e3d6d84283d24655beb04f904d03675",
    "semantic_title": "transvcl: attention-enhanced video copy localization network with flexible supervision",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25159": {
    "title": "Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer",
    "volume": "main",
    "abstract": "Real-world recognition system often encounters the challenge of unseen labels. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL) focuses on transferring knowledge by a pre-trained textual label embedding (e.g., GloVe). However, such methods only exploit single-modal knowledge from a language model, while ignoring the rich semantic information inherent in image-text pairs. Instead, recently developed open-vocabulary (OV) based methods succeed in exploiting such information of image-text pairs in object detection, and achieve impressive performance. Inspired by the success of OV-based methods, we propose a novel open-vocabulary framework, named multi-modal knowledge transfer (MKT), for multi-label classification. Specifically, our method exploits multi-modal knowledge of image-text pairs based on a vision and language pre-training (VLP) model. To facilitate transferring the image-text matching ability of VLP model, knowledge distillation is employed to guarantee the consistency of image and label embeddings, along with prompt tuning to further update the label embeddings. To further enable the recognition of multiple objects, a simple but effective two-stream module is developed to capture both local and global features. Extensive experimental results show that our method significantly outperforms state-of-the-art methods on public benchmark datasets",
    "checked": true,
    "id": "ee301715607f618d22f21cb51c2c63ca85a4340c",
    "semantic_title": "open-vocabulary multi-label classification via multi-modal knowledge transfer",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25160": {
    "title": "Parameter-Efficient Model Adaptation for Vision Transformers",
    "volume": "main",
    "abstract": "In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation method. We analyze and compare our method with a diverse set of baseline model adaptation methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across 20 datasets under the few-shot setting and 7 image classification datasets under the full-shot setting",
    "checked": true,
    "id": "af593c53a9221bd12211f78d4f1ebd6b59cc4e7c",
    "semantic_title": "parameter-efficient model adaptation for vision transformers",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25161": {
    "title": "DarkFeat: Noise-Robust Feature Detector and Descriptor for Extremely Low-Light RAW Images",
    "volume": "main",
    "abstract": "Low-light visual perception, such as SLAM or SfM at night, has received increasing attention, in which keypoint detection and local feature description play an important role. Both handcraft designs and machine learning methods have been widely studied for local feature detection and description, however, the performance of existing methods degrades in the extreme low-light scenarios in a certain degree, due to the low signal-to-noise ratio in images. To address this challenge, images in RAW format that retain more raw sensing information have been considered in recent works with a denoise-then-detect scheme. However, existing denoising methods are still insufficient for RAW images and heavily time-consuming, which limits the practical applications of such scheme. In this paper, we propose DarkFeat, a deep learning model which directly detects and describes local features from extreme low-light RAW images in an end-to-end manner. A novel noise robustness map and selective suppression constraints are proposed to effectively mitigate the influence of noise and extract more reliable keypoints. Furthermore, a customized pipeline of synthesizing dataset containing low-light RAW image matching pairs is proposed to extend end-to-end training. Experimental results show that DarkFeat achieves state-of-the-art performance on both indoor and outdoor parts of the challenging MID benchmark, outperforms the denoise-then-detect methods and significantly reduces computational costs up to 70%. Code is available at https://github.com/THU-LYJ-Lab/DarkFeat",
    "checked": true,
    "id": "a988d11e4a2c3de692bc879cf8c3d5be5e8e3ead",
    "semantic_title": "darkfeat: noise-robust feature detector and descriptor for extremely low-light raw images",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25162": {
    "title": "GAM: Gradient Attention Module of Optimization for Point Clouds Analysis",
    "volume": "main",
    "abstract": "In the point cloud analysis task, the existing local feature aggregation descriptors (LFAD) do not fully utilize the neighborhood information of center points. Previous methods only use the distance information to constrain the local aggregation process, which is easy to be affected by abnormal points and cannot adequately fit the original geometry of the point cloud. This paper argues that fine-grained geometric information (FGGI) plays an important role in the aggregation of local features. Based on this, we propose a gradient-based local attention module to address the above problem, which is called Gradient Attention Module (GAM). GAM simplifies the process of extracting the gradient information in the neighborhood to explicit representation using the Zenith Angle matrix and Azimuth Angle matrix, which makes the module 35X faster. The comprehensive experiments on the ScanObjectNN dataset, ShapeNet dataset, S3DIS dataset, Modelnet40 dataset, and KITTI dataset demonstrate the effectiveness, efficientness, and generalization of our newly proposed GAM for 3D point cloud analysis. Especially in S3DIS, GAM achieves the highest index in the current point-based model with mIoU/OA/mAcc of 74.4%/90.6%/83.2%",
    "checked": false,
    "id": "a5820730290e81f76d843178af927591e669053d",
    "semantic_title": "gam : gradient attention module of optimization for point clouds analysis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25163": {
    "title": "Self-Supervised Learning for Multilevel Skeleton-Based Forgery Detection via Temporal-Causal Consistency of Actions",
    "volume": "main",
    "abstract": "Skeleton-based human action recognition and analysis have become increasingly attainable in many areas, such as security surveillance and anomaly detection. Given the prevalence of skeleton-based applications, tampering attacks on human skeletal features have emerged very recently. In particular, checking the temporal inconsistency and/or incoherence (TII) in the skeletal sequence of human action is a principle of forgery detection. To this end, we propose an approach to self-supervised learning of the temporal causality behind human action, which can effectively check TII in skeletal sequences. Especially, we design a multilevel skeleton-based forgery detection framework to recognize the forgery on frame level, clip level, and action level in terms of learning the corresponding temporal-causal skeleton representations for each level. Specifically, a hierarchical graph convolution network architecture is designed to learn low-level skeleton representations based on physical skeleton connections and high-level action representations based on temporal-causal dependencies for specific actions. Extensive experiments consistently show state-of-the-art results on multilevel forgery detection tasks and superior performance of our framework compared to current competing methods",
    "checked": true,
    "id": "84a150e189e34771752a5454a6f9e143a602dbb8",
    "semantic_title": "self-supervised learning for multilevel skeleton-based forgery detection via temporal-causal consistency of actions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25164": {
    "title": "Self-Emphasizing Network for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "Hand and face play an important role in expressing sign language. Their features are usually especially leveraged to improve system performance. However, to effectively extract visual representations and capture trajectories for hands and face, previous methods always come at high computations with increased training complexity. They usually employ extra heavy pose-estimation networks to locate human body keypoints or rely on additional pre-extracted heatmaps for supervision. To relieve this problem, we propose a self-emphasizing network (SEN) to emphasize informative spatial regions in a self-motivated way, with few extra computations and without additional expensive supervision. Specifically, SEN first employs a lightweight subnetwork to incorporate local spatial-temporal features to identify informative regions, and then dynamically augment original features via attention maps. It's also observed that not all frames contribute equally to recognition. We present a temporal self-emphasizing module to adaptively emphasize those discriminative frames and suppress redundant ones. A comprehensive comparison with previous methods equipped with hand and face features demonstrates the superiority of our method, even though they always require huge computations and rely on expensive extra supervision. Remarkably, with few extra computations, SEN achieves new state-of-the-art accuracy on four large-scale datasets, PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. Visualizations verify the effects of SEN on emphasizing informative spatial and temporal features. Code is available at https://github.com/hulianyuyy/SEN_CSLR",
    "checked": true,
    "id": "1f55d35e1d1a148898a756cb0380b22fa8878dcb",
    "semantic_title": "self-emphasizing network for continuous sign language recognition",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25165": {
    "title": "Store and Fetch Immediately: Everything Is All You Need for Space-Time Video Super-resolution",
    "volume": "main",
    "abstract": "Existing space-time video super-resolution (ST-VSR) methods fail to achieve high-quality reconstruction since they fail to fully explore the spatial-temporal correlations, long-range components in particular. Although the recurrent structure for ST-VSR adopts bidirectional propagation to aggregate information from the entire video, collecting the temporal information between the past and future via one-stage representations inevitably loses the long-range relations. To alleviate the limitation, this paper proposes an immediate storeand-fetch network to promote long-range correlation learning, where the stored information from the past and future can be refetched to help the representation of the current frame. Specifically, the proposed network consists of two modules: a backward recurrent module (BRM) and a forward recurrent module (FRM). The former first performs backward inference from future to past, while storing future super-resolution (SR) information for each frame. Following that, the latter performs forward inference from past to future to super-resolve all frames, while storing past SR information for each frame. Since FRM inherits SR information from BRM, therefore, spatial and temporal information from the entire video sequence is immediately stored and fetched, which allows drastic improvement for ST-VSR. Extensive experiments both on ST-VSR and space video super-resolution (S-VSR) as well as time video super-resolution (T-VSR) have demonstrated the effectiveness of our proposed method over other state-of-the-art methods on public datasets. Code is available https://github.com/hhhhhumengshun/SFI-STVR",
    "checked": true,
    "id": "e32b2e5f07434a4d6ba73ee7394829ef93260124",
    "semantic_title": "store and fetch immediately: everything is all you need for space-time video super-resolution",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25166": {
    "title": "PointCA: Evaluating the Robustness of 3D Point Cloud Completion Models against Adversarial Examples",
    "volume": "main",
    "abstract": "Point cloud completion, as the upstream procedure of 3D recognition and segmentation, has become an essential part of many tasks such as navigation and scene understanding. While various point cloud completion models have demonstrated their powerful capabilities, their robustness against adversarial attacks, which have been proven to be fatally malicious towards deep neural networks, remains unknown. In addition, existing attack approaches towards point cloud classifiers cannot be applied to the completion models due to different output forms and attack purposes. In order to evaluate the robustness of the completion models, we propose PointCA, the first adversarial attack against 3D point cloud completion models. PointCA can generate adversarial point clouds that maintain high similarity with the original ones, while being completed as another object with totally different semantic information. Specifically, we minimize the representation discrepancy between the adversarial example and the target point set to jointly explore the adversarial point clouds in the geometry space and the feature space. Furthermore, to launch a stealthier attack, we innovatively employ the neighbourhood density information to tailor the perturbation constraint, leading to geometry-aware and distribution-adaptive modifications for each point. Extensive experiments against different premier point cloud completion networks show that PointCA can cause the performance degradation from 77.9% to 16.7%, with the structure chamfer distance kept below 0.01. We conclude that existing completion models are severely vulnerable to adversarial examples, and state-of-the-art defenses for point cloud classification will be partially invalid when applied to incomplete and uneven point cloud data",
    "checked": true,
    "id": "aec240626448b3506860a577d5d9ea3a20bfe794",
    "semantic_title": "pointca: evaluating the robustness of 3d point cloud completion models against adversarial examples",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25167": {
    "title": "High-Resolution Iterative Feedback Network for Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Spotting camouflaged objects that are visually assimilated into the background is tricky for both object detection algorithms and humans who are usually confused or cheated by the perfectly intrinsic similarities between the foreground objects and the background surroundings. To tackle this challenge, we aim to extract the high-resolution texture details to avoid the detail degradation that causes blurred vision in edges and boundaries. We introduce a novel HitNet to refine the low-resolution representations by high-resolution features in an iterative feedback manner, essentially a global loop-based connection among the multi-scale resolutions. To design better feedback feature ﬂow and avoid the feature corruption caused by recurrent path, an iterative feedback strategy is proposed to impose more constraints on each feedback connection. Extensive experiments on four challenging datasets demonstrate that our HitNet breaks the performance bottleneck and achieves significant improvements compared with 29 state-of-the-art methods. In addition, to address the data scarcity in camouflaged scenarios, we provide an application example to convert the salient objects to camouflaged objects, thereby generating more camouflaged training samples from the diverse salient object datasets. Code will be made publicly available",
    "checked": true,
    "id": "453d36c646576b04241ca7b964698eef64ebbdc8",
    "semantic_title": "high-resolution iterative feedback network for camouflaged object detection",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25168": {
    "title": "Leveraging Sub-class Discimination for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims at identifying unseen compositions composed of previously seen attributes and objects during the test phase. In real images, the visual appearances of attributes and objects (primitive concepts) generally interact with each other. Namely, the visual appearances of an attribute may change when composed with different objects, and vice versa. But previous works overlook this important property. In this paper, we introduce a simple yet effective approach with leveraging sub-class discrimination. Specifically, we define the primitive concepts in different compositions as sub-classes, and then maintain the sub-class discrimination to address the above challenge. More specifically, inspired by the observation that the composed recognition models could account for the differences across sub-classes, we first propose to impose the embedding alignment between the composed and disentangled recognition to incorporate sub-class discrimination at the feature level. Then we develop the prototype modulator networks to adjust the class prototypes w.r.t. the composition information, which can enhance sub-class discrimination at the classifier level. We conduct extensive experiments on the challenging benchmark datasets, and the considerable performance improvement over state-of-the-art approaches is achieved, which indicates the effectiveness of our method. Our code is available at https://github.com/hxm97/SCD-CZSL",
    "checked": true,
    "id": "0cb450cc6ccdaa0d1914cbc4d715f0eeb2792aa7",
    "semantic_title": "leveraging sub-class discimination for compositional zero-shot learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25169": {
    "title": "GPTR: Gestalt-Perception Transformer for Diagram Object Detection",
    "volume": "main",
    "abstract": "Diagram object detection is the key basis of practical applications such as textbook question answering. Because the diagram mainly consists of simple lines and color blocks, its visual features are sparser than those of natural images. In addition, diagrams usually express diverse knowledge, in which there are many low-frequency object categories in diagrams. These lead to the fact that traditional data-driven detection model is not suitable for diagrams. In this work, we propose a gestalt-perception transformer model for diagram object detection, which is based on an encoder-decoder architecture. Gestalt perception contains a series of laws to explain human perception, that the human visual system tends to perceive patches in an image that are similar, close or connected without abrupt directional changes as a perceptual whole object. Inspired by these thoughts, we build a gestalt-perception graph in transformer encoder, which is composed of diagram patches as nodes and the relationships between patches as edges. This graph aims to group these patches into objects via laws of similarity, proximity, and smoothness implied in these edges, so that the meaningful objects can be effectively detected. The experimental results demonstrate that the proposed GPTR achieves the best results in the diagram object detection task. Our model also obtains comparable results over the competitors in natural image object detection",
    "checked": true,
    "id": "6329ee4cf6bbd30c5d76d6caa78959946c656c56",
    "semantic_title": "gptr: gestalt-perception transformer for diagram object detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25170": {
    "title": "Resolving Task Confusion in Dynamic Expansion Architectures for Class Incremental Learning",
    "volume": "main",
    "abstract": "The dynamic expansion architecture is becoming popular in class incremental learning, mainly due to its advantages in alleviating catastrophic forgetting. However, task confu- sion is not well assessed within this framework, e.g., the discrepancy between classes of different tasks is not well learned (i.e., inter-task confusion, ITC), and certain prior- ity is still given to the latest class batch (i.e., old-new con- fusion, ONC). We empirically validate the side effects of the two types of confusion. Meanwhile, a novel solution called Task Correlated Incremental Learning (TCIL) is pro- posed to encourage discriminative and fair feature utilization across tasks. TCIL performs a multi-level knowledge distil- lation to propagate knowledge learned from old tasks to the new one. It establishes information flow paths at both fea- ture and logit levels, enabling the learning to be aware of old classes. Besides, attention mechanism and classifier re- scoring are applied to generate more fair classification scores. We conduct extensive experiments on CIFAR100 and Ima- geNet100 datasets. The results demonstrate that TCIL con- sistently achieves state-of-the-art accuracy. It mitigates both ITC and ONC, while showing advantages in battle with catas- trophic forgetting even no rehearsal memory is reserved. Source code: https://github.com/YellowPancake/TCIL",
    "checked": true,
    "id": "a63eff00c20172847439826377e412934caad068",
    "semantic_title": "resolving task confusion in dynamic expansion architectures for class incremental learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25171": {
    "title": "ClassFormer: Exploring Class-Aware Dependency with Transformer for Medical Image Segmentation",
    "volume": "main",
    "abstract": "Vision Transformers have recently shown impressive performances on medical image segmentation. Despite their strong capability of modeling long-range dependencies, the current methods still give rise to two main concerns in a class-level perspective: (1) intra-class problem: the existing methods lacked in extracting class-specific correspondences of different pixels, which may lead to poor object coverage and/or boundary prediction; (2) inter-class problem: the existing methods failed to model explicit category-dependencies among various objects, which may result in inaccurate localization. In light of these two issues, we propose a novel transformer, called ClassFormer, powered by two appealing transformers, i.e., intra-class dynamic transformer and inter-class interactive transformer, to address the challenge of fully exploration on compactness and discrepancy. Technically, the intra-class dynamic transformer is first designed to decouple representations of different categories with an adaptive selection mechanism for compact learning, which optimally highlights the informative features to reflect the salient keys/values from multiple scales. We further introduce the inter-class interactive transformer to capture the category dependency among different objects, and model class tokens as the representative class centers to guide a global semantic reasoning. As a consequence, the feature consistency is ensured with the expense of intra-class penalization, while inter-class constraint strengthens the feature discriminability between different categories. Extensive empirical evidence shows that ClassFormer can be easily plugged into any architecture, and yields improvements over the state-of-the-art methods in three public benchmarks",
    "checked": true,
    "id": "eed051a90f635adb806c025c9b1fcc790f35ba0b",
    "semantic_title": "classformer: exploring class-aware dependency with transformer for medical image segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25172": {
    "title": "NLIP: Noise-Robust Language-Image Pre-training",
    "volume": "main",
    "abstract": "Large-scale cross-modal pre-training paradigms have recently shown ubiquitous success on a wide range of downstream tasks, e.g., zero-shot classification, retrieval and image captioning. However, their successes highly rely on the scale and quality of web-crawled data that naturally contain much incomplete and noisy information (e.g., wrong or irrelevant contents). Existing works either design manual rules to clean data or generate pseudo-targets as auxiliary signals for reducing noise impact, which do not explicitly tackle both the incorrect and incomplete challenges at the same time. In this paper, to automatically mitigate the impact of noise by solely mining over existing data, we propose a principled Noise-robust Language-Image Pre-training framework (NLIP) to stabilize pre-training via two schemes: noise-harmonization and noise-completion. First, in noise-harmonization scheme, NLIP estimates the noise probability of each pair according to the memorization effect of cross-modal transformers, then adopts noise-adaptive regularization to harmonize the cross-modal alignments with varying degrees. Second, in noise-completion scheme, to enrich the missing object information of text, NLIP injects a concept-conditioned cross-modal decoder to obtain semantic-consistent synthetic captions to complete noisy ones, which uses the retrieved visual concepts (i.e., objects' names) for the corresponding image to guide captioning generation. By collaboratively optimizing noise-harmonization and noise-completion schemes, our NLIP can alleviate the common noise effects during image-text pre-training in a more efficient way. Extensive experiments show the significant performance improvements of our NLIP using only 26M data over existing pre-trained models (e.g., CLIP, FILIP and BLIP) on 12 zero-shot classification datasets (e.g., +8.6% over CLIP on average accuracy), MSCOCO image captioning (e.g., +1.9 over BLIP trained with 129M data on CIDEr) and zero-shot image-text retrieval tasks",
    "checked": true,
    "id": "2dd4bdb71e8d1a1bb6f7f2cb500972f082aa91fd",
    "semantic_title": "nlip: noise-robust language-image pre-training",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25173": {
    "title": "Symmetry-Aware Transformer-Based Mirror Detection",
    "volume": "main",
    "abstract": "Mirror detection aims to identify the mirror regions in the given input image. Existing works mainly focus on integrating the semantic features and structural features to mine specific relations between mirror and non-mirror regions, or introducing mirror properties like depth or chirality to help analyze the existence of mirrors. In this work, we observe that a real object typically forms a loose symmetry relationship with its corresponding reflection in the mirror, which is beneficial in distinguishing mirrors from real objects. Based on this observation, we propose a dual-path Symmetry-Aware Transformer-based mirror detection Network (SATNet), which includes two novel modules: Symmetry-Aware Attention Module (SAAM) and Contrast and Fusion Decoder Module (CFDM). Specifically, we first adopt a transformer backbone to model global information aggregation in images, extracting multi-scale features in two paths. We then feed the high-level dual-path features to SAAMs to capture the symmetry relations. Finally, we fuse the dual-path features and refine our prediction maps progressively with CFDMs to obtain the final mirror mask. Experimental results show that SATNet outperforms both RGB and RGB-D mirror detection methods on all available mirror detection datasets",
    "checked": true,
    "id": "b819d724661b35b93aa9048062d988fc6fc868fc",
    "semantic_title": "symmetry-aware transformer-based mirror detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25174": {
    "title": "AudioEar: Single-View Ear Reconstruction for Personalized Spatial Audio",
    "volume": "main",
    "abstract": "Spatial audio, which focuses on immersive 3D sound rendering, is widely applied in the acoustic industry. One of the key problems of current spatial audio rendering methods is the lack of personalization based on different anatomies of individuals, which is essential to produce accurate sound source positions. In this work, we address this problem from an interdisciplinary perspective. The rendering of spatial audio is strongly correlated with the 3D shape of human bodies, particularly ears. To this end, we propose to achieve personalized spatial audio by reconstructing 3D human ears with single-view images. First, to benchmark the ear reconstruction task, we introduce AudioEar3D, a high-quality 3D ear dataset consisting of 112 point cloud ear scans with RGB images. To self-supervisedly train a reconstruction model, we further collect a 2D ear dataset composed of 2,000 images, each one with manual annotation of occlusion and 55 landmarks, named AudioEar2D. To our knowledge, both datasets have the largest scale and best quality of their kinds for public use. Further, we propose AudioEarM, a reconstruction method guided by a depth estimation network that is trained on synthetic data, with two loss functions tailored for ear data. Lastly, to fill the gap between the vision and acoustics community, we develop a pipeline to integrate the reconstructed ear mesh with an off-the-shelf 3D human body and simulate a personalized Head-Related Transfer Function (HRTF), which is the core of spatial audio rendering. Code and data are publicly available in https://github.com/seanywang0408/AudioEar",
    "checked": true,
    "id": "6b42f494bf45ec5e154a007844cc449a07279c4f",
    "semantic_title": "audioear: single-view ear reconstruction for personalized spatial audio",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25175": {
    "title": "Boosting Point Clouds Rendering via Radiance Mapping",
    "volume": "main",
    "abstract": "Recent years we have witnessed rapid development in NeRF-based image rendering due to its high quality. However, point clouds rendering is somehow less explored. Compared to NeRF-based rendering which suffers from dense spatial sampling, point clouds rendering is naturally less computation intensive, which enables its deployment in mobile computing device. In this work, we focus on boosting the image quality of point clouds rendering with a compact model design. We first analyze the adaption of the volume rendering formulation on point clouds. Based on the analysis, we simplify the NeRF representation to a spatial mapping function which only requires single evaluation per pixel. Further, motivated by ray marching, we rectify the the noisy raw point clouds to the estimated intersection between rays and surfaces as queried coordinates, which could avoid spatial frequency collapse and neighbor point disturbance. Composed of rasterization, spatial mapping and the refinement stages, our method achieves the state-of-the-art performance on point clouds rendering, outperforming prior works by notable margins, with a smaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 on ScanNet and 30.81 on DTU. Code and data are publicly available in https://github.com/seanywang0408/RadianceMapping",
    "checked": true,
    "id": "18289220a3f778a35d89b9fadeba25820c1ad9ad",
    "semantic_title": "boosting point clouds rendering via radiance mapping",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25176": {
    "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost",
    "volume": "main",
    "abstract": "Recent years have witnessed significant growth of face alignment. Though dense facial landmark is highly demanded in various scenarios, e.g., cosmetic medicine and facial beautification, most works only consider sparse face alignment. To address this problem, we present a framework that can enrich landmark density by existing sparse landmark datasets, e.g., 300W with 68 points and WFLW with 98 points. Firstly, we observe that the local patches along each semantic contour are highly similar in appearance. Then, we propose a weakly-supervised idea of learning the refinement ability on original sparse landmarks and adapting this ability to enriched dense landmarks. Meanwhile, several operators are devised and organized together to implement the idea. Finally, the trained model is applied as a plug-and-play module to the existing face alignment networks. To evaluate our method, we manually label the dense landmarks on 300W testset. Our method yields state-of-the-art accuracy not only in newly-constructed dense 300W testset but also in the original sparse 300W and WFLW testsets without additional cost",
    "checked": true,
    "id": "9889925126ad324f3f60b0978882f00e23541206",
    "semantic_title": "freeenricher: enriching face landmarks without additional cost",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25177": {
    "title": "PATRON: Perspective-Aware Multitask Model for Referring Expression Grounding Using Embodied Multimodal Cues",
    "volume": "main",
    "abstract": "Humans naturally use referring expressions with verbal utterances and nonverbal gestures to refer to objects and events. As these referring expressions can be interpreted differently from the speaker's or the observer's perspective, people effectively decide on the perspective in comprehending the expressions. However, existing models do not explicitly learn perspective grounding, which often causes the models to perform poorly in understanding embodied referring expressions. To make it exacerbate, these models are often trained on datasets collected in non-embodied settings without nonverbal gestures and curated from an exocentric perspective. To address these issues, in this paper, we present a perspective-aware multitask learning model, called PATRON, for relation and object grounding tasks in embodied settings by utilizing verbal utterances and nonverbal cues. In PATRON, we have developed a guided fusion approach, where a perspective grounding task guides the relation and object grounding task. Through this approach, PATRON learns disentangled task-specific and task-guidance representations, where task-guidance representations guide the extraction of salient multimodal features to ground the relation and object accurately. Furthermore, we have curated a synthetic dataset of embodied referring expressions with multimodal cues, called CAESAR-PRO. The experimental results suggest that PATRON outperforms the evaluated state-of-the-art visual-language models. Additionally, the results indicate that learning to ground perspective helps machine learning models to improve the performance of the relation and object grounding task. Furthermore, the insights from the extensive experimental results and the proposed dataset will enable researchers to evaluate visual-language models' effectiveness in understanding referring expressions in other embodied settings",
    "checked": true,
    "id": "a7fab5b0cc0bf96a1f987f9ebf18177e7915ba60",
    "semantic_title": "patron: perspective-aware multitask model for referring expression grounding using embodied multimodal cues",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25178": {
    "title": "Unifying Vision-Language Representation Space with Single-Tower Transformer",
    "volume": "main",
    "abstract": "Contrastive learning is a form of distance learning that aims to learn invariant features from two related representations. In this work, we explore the hypothesis that an image and caption can be regarded as two different views of the underlying mutual information, and train a model to learn a unified vision-language representation space that encodes both modalities at once in a modality-agnostic manner. We first identify difficulties in learning a one-tower model for vision-language pretraining (VLP), and propose One Representation (OneR) as a simple yet effective framework for our goal. We discover intriguing properties that distinguish OneR from the previous works that have modality-specific representation spaces such as zero-shot localization, text-guided visual reasoning and multi-modal retrieval, and present analyses to provide insights into this new form of multi-modal representation learning. Thorough evaluations demonstrate the potential of a unified modality-agnostic VLP framework",
    "checked": true,
    "id": "e978d2f7e2a04da803d1a224b3ad868ac919dbb8",
    "semantic_title": "unifying vision-language representation space with single-tower transformer",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25179": {
    "title": "Delving Deep into Pixel Alignment Feature for Accurate Multi-View Human Mesh Recovery",
    "volume": "main",
    "abstract": "Regression-based methods have shown high efficiency and effectiveness for multi-view human mesh recovery. The key components of a typical regressor lie in the feature extraction of input views and the fusion of multi-view features. In this paper, we present Pixel-aligned Feedback Fusion (PaFF) for accurate yet efficient human mesh recovery from multi-view images. PaFF is an iterative regression framework that performs feature extraction and fusion alternately. At each iteration, PaFF extracts pixel-aligned feedback features from each input view according to the reprojection of the current estimation and fuses them together with respect to each vertex of the downsampled mesh. In this way, our regressor can not only perceive the misalignment status of each view from the feedback features but also correct the mesh parameters more effectively based on the feature fusion on mesh vertices. Additionally, our regressor disentangles the global orientation and translation of the body mesh from the estimation of mesh parameters such that the camera parameters of input views can be better utilized in the regression process. The efficacy of our method is validated in the Human3.6M dataset via comprehensive ablation experiments, where PaFF achieves 33.02 MPJPE and brings significant improvements over the previous best solutions by more than 29%. The project page with code and video results can be found at https://kairobo.github.io/PaFF/",
    "checked": true,
    "id": "b5a878716aa4ab11e84ba0973d58ab35c68711a1",
    "semantic_title": "delving deep into pixel alignment feature for accurate multi-view human mesh recovery",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25180": {
    "title": "Semi-attention Partition for Occluded Person Re-identification",
    "volume": "main",
    "abstract": "This paper proposes a Semi-Attention Partition (SAP) method to learn well-aligned part features for occluded person re-identification (re-ID). Currently, the mainstream methods employ either external semantic partition or attention-based partition, and the latter manner is usually better than the former one. Under this background, this paper explores a potential that the weak semantic partition can be a good teacher for the strong attention-based partition. In other words, the attention-based student can substantially surpass its noisy semantic-based teacher, contradicting the common sense that the student usually achieves inferior (or comparable) accuracy. A key to this effect is: the proposed SAP encourages the attention-based partition of the (transformer) student to be partially consistent with the semantic-based teacher partition through knowledge distillation, yielding the so-called semi-attention. Such partial consistency allows the student to have both consistency and reasonable conflict with the noisy teacher. More specifically, on the one hand, the attention is guided by the semantic partition from the teacher. On the other hand, the attention mechanism itself still has some degree of freedom to comply with the inherent similarity between different patches, thus gaining resistance against noisy supervision. Moreover, we integrate a battery of well-engineered designs into SAP to reinforce their cooperation (e.g., multiple forms of teacher-student consistency), as well as to promote reasonable conflict (e.g., mutual absorbing partition refinement and a supervision signal dropout strategy). Experimental results confirm that the transformer student achieves substantial improvement after this semi-attention learning scheme, and produces new state-of-the-art accuracy on several standard re-ID benchmarks",
    "checked": true,
    "id": "810862a848aa164ad7e0611ee4b850d6d28f4a1a",
    "semantic_title": "semi-attention partition for occluded person re-identification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25181": {
    "title": "Fast Online Hashing with Multi-Label Projection",
    "volume": "main",
    "abstract": "Hashing has been widely researched to solve the large-scale approximate nearest neighbor search problem owing to its time and storage superiority. In recent years, a number of online hashing methods have emerged, which can update the hash functions to adapt to the new stream data and realize dynamic retrieval. However, existing online hashing methods are required to update the whole database with the latest hash functions when a query arrives, which leads to low retrieval efficiency with the continuous increase of the stream data. On the other hand, these methods ignore the supervision relationship among the examples, especially in the multi-label case. In this paper, we propose a novel Fast Online Hashing (FOH) method which only updates the binary codes of a small part of the database. To be specific, we first build a query pool in which the nearest neighbors of each central point are recorded. When a new query arrives, only the binary codes of the corresponding potential neighbors are updated. In addition, we create a similarity matrix which takes the multi-label supervision information into account and bring in the multi-label projection loss to further preserve the similarity among the multi-label data. The experimental results on two common benchmarks show that the proposed FOH can achieve dramatic superiority on query time up to 6.28 seconds less than state-of-the-art baselines with competitive retrieval accuracy",
    "checked": true,
    "id": "0f98d2bd7a791f0eaea7aab20496a2b5efa0ed44",
    "semantic_title": "fast online hashing with multi-label projection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25182": {
    "title": "Fourier-Net: Fast Image Registration with Band-Limited Deformation",
    "volume": "main",
    "abstract": "Unsupervised image registration commonly adopts U-Net style networks to predict dense displacement fields in the full-resolution spatial domain. For high-resolution volumetric image data, this process is however resource-intensive and time-consuming. To tackle this problem, we propose the Fourier-Net, replacing the expansive path in a U-Net style network with a parameter-free model-driven decoder. Specifically, instead of our Fourier-Net learning to output a full-resolution displacement field in the spatial domain, we learn its low-dimensional representation in a band-limited Fourier domain. This representation is then decoded by our devised model-driven decoder (consisting of a zero padding layer and an inverse discrete Fourier transform layer) to the dense, full-resolution displacement field in the spatial domain. These changes allow our unsupervised Fourier-Net to contain fewer parameters and computational operations, resulting in faster inference speeds. Fourier-Net is then evaluated on two public 3D brain datasets against various state-of-the-art approaches. For example, when compared to a recent transformer-based method, named TransMorph, our Fourier-Net, which only uses 2.2% of its parameters and 6.66% of the multiply-add operations, achieves a 0.5% higher Dice score and an 11.48 times faster inference speed. Code is available at https://github.com/xi-jia/Fourier-Net",
    "checked": true,
    "id": "743c723118ee61e2e5cc7161247c272522d8cdd5",
    "semantic_title": "fourier-net: fast image registration with band-limited deformation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25183": {
    "title": "Semi-supervised Deep Large-Baseline Homography Estimation with Progressive Equivalence Constraint",
    "volume": "main",
    "abstract": "Homography estimation is erroneous in the case of large-baseline due to the low image overlay and limited receptive field. To address it, we propose a progressive estimation strategy by converting large-baseline homography into multiple intermediate ones, cumulatively multiplying these intermediate items can reconstruct the initial homography. Meanwhile, a semi-supervised homography identity loss, which consists of two components: a supervised objective and an unsupervised objective, is introduced. The first supervised loss is acting to optimize intermediate homographies, while the second unsupervised one helps to estimate a large-baseline homography without photometric losses. To validate our method, we propose a large-scale dataset that covers regular and challenging scenes. Experiments show that our method achieves state-of-the-art performance in large-baseline scenes while keeping competitive performance in small-baseline scenes. Code and dataset are available at https://github.com/megvii-research/LBHomo",
    "checked": true,
    "id": "a96f76cc868630a74b758951f5206e9f7e670e83",
    "semantic_title": "semi-supervised deep large-baseline homography estimation with progressive equivalence constraint",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25184": {
    "title": "Multi-Modality Deep Network for Extreme Learned Image Compression",
    "volume": "main",
    "abstract": "Image-based single-modality compression learning approaches have demonstrated exceptionally powerful encoding and decoding capabilities in the past few years , but suffer from blur and severe semantics loss at extremely low bitrates. To address this issue, we propose a multimodal machine learning method for text-guided image compression, in which the semantic information of text is used as prior information to guide image compression for better compression performance. We fully study the role of text description in different components of the codec, and demonstrate its effectiveness. In addition, we adopt the image-text attention module and image-request complement module to better fuse image and text features, and propose an improved multimodal semantic-consistent loss to produce semantically complete reconstructions. Extensive experiments, including a user study, prove that our method can obtain visually pleasing results at extremely low bitrates, and achieves a comparable or even better performance than state-of-the-art methods, even though these methods are at 2x to 4x bitrates of ours",
    "checked": true,
    "id": "fcbe9b439f1f694d79a3e788a3a4e2ce4655219a",
    "semantic_title": "multi-modality deep network for extreme learned image compression",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25185": {
    "title": "PolarFormer: Multi-Camera 3D Object Detection with Polar Transformer",
    "volume": "main",
    "abstract": "3D object detection in autonomous driving aims to reason \"what\" and \"where\" the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car's perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird's-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross-attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar's distance dimension, we further introduce a multi-scale Polar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives",
    "checked": false,
    "id": "67a3e04199baad91ecf25b55f294f54b173c052b",
    "semantic_title": "polarformer: multi-camera 3d object detection with polar transformers",
    "citation_count": 61
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25186": {
    "title": "3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation",
    "volume": "main",
    "abstract": "Text-guided 3D object generation aims to generate 3D objects described by user-defined captions, which paves a flexible way to visualize what we imagined. Although some works have been devoted to solving this challenging task, these works either utilize some explicit 3D representations (e.g., mesh), which lack texture and require post-processing for rendering photo-realistic views; or require individual time-consuming optimization for every single case. Here, we make the first attempt to achieve generic text-guided cross-category 3D object generation via a new 3D-TOGO model, which integrates a text-to-views generation module and a views-to-3D generation module. The text-to-views generation module is designed to generate different views of the target 3D object given an input caption. prior-guidance, caption-guidance and view contrastive learning are proposed for achieving better view-consistency and caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D generation module to obtain the implicit 3D neural representation from the previously-generated views. Our 3D-TOGO model generates 3D objects in the form of the neural radiance field with good texture and requires no time-cost optimization for every single caption. Besides, 3D-TOGO can control the category, color and shape of generated 3D objects with the input caption. Extensive experiments on the largest 3D object dataset (i.e., ABO) are conducted to verify that 3D-TOGO can better generate high-quality 3D objects according to the input captions across 98 different categories, in terms of PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields",
    "checked": true,
    "id": "378eda76558011610d1f803f543a4c57cb5d6608",
    "semantic_title": "3d-togo: towards text-guided cross-category 3d object generation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25187": {
    "title": "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer",
    "volume": "main",
    "abstract": "Recent work has explored the potential to adapt a pre-trained vision transformer (ViT) by updating only a few parameters so as to improve storage efficiency, called parameter-efficient transfer learning (PETL). Current PETL methods have shown that by tuning only 0.5% of the parameters, ViT can be adapted to downstream tasks with even better performance than full fine-tuning. In this paper, we aim to further promote the efficiency of PETL to meet the extreme storage constraint in real-world applications. To this end, we propose a tensorization-decomposition framework to store the weight increments, in which the weights of each ViT are tensorized into a single 3D tensor, and their increments are then decomposed into lightweight factors. In the fine-tuning process, only the factors need to be updated and stored, termed Factor-Tuning (FacT). On VTAB-1K benchmark, our method performs on par with NOAH, the state-of-the-art PETL method, while being 5x more parameter-efficient. We also present a tiny version that only uses 8K (0.01% of ViT's parameters) trainable parameters but outperforms full fine-tuning and many other PETL methods such as VPT and BitFit. In few-shot settings, FacT also beats all PETL baselines using the fewest parameters, demonstrating its strong capability in the low-data regime",
    "checked": true,
    "id": "e835e50b4c067cec7457332ec119994fb1a26422",
    "semantic_title": "fact: factor-tuning for lightweight adaptation on vision transformer",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25188": {
    "title": "Estimating Reflectance Layer from a Single Image: Integrating Reflectance Guidance and Shadow/Specular Aware Learning",
    "volume": "main",
    "abstract": "Estimating the reflectance layer from a single image is a challenging task. It becomes more challenging when the input image contains shadows or specular highlights, which often render an inaccurate estimate of the reflectance layer. Therefore, we propose a two-stage learning method, including reflectance guidance and a Shadow/Specular-Aware (S-Aware) network to tackle the problem. In the first stage, an initial reflectance layer free from shadows and specularities is obtained with the constraint of novel losses that are guided by prior-based shadow-free and specular-free images. To further enforce the reflectance layer to be independent of shadows and specularities in the second-stage refinement, we introduce an S-Aware network that distinguishes the reflectance image from the input image. Our network employs a classifier to categorize shadow/shadow-free, specular/specular-free classes, enabling the activation features to function as attention maps that focus on shadow/specular regions. Our quantitative and qualitative evaluations show that our method outperforms the state-of-the-art methods in the reflectance layer estimation that is free from shadows and specularities",
    "checked": true,
    "id": "d3054551f9f9dfd8d49d029e16b0d27f46990826",
    "semantic_title": "estimating reflectance layer from a single image: integrating reflectance guidance and shadow/specular aware learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25189": {
    "title": "Weakly-Guided Self-Supervised Pretraining for Temporal Activity Detection",
    "volume": "main",
    "abstract": "Temporal Activity Detection aims to predict activity classes per frame, in contrast to video-level predictions in Activity Classification (i.e., Activity Recognition). Due to the expensive frame-level annotations required for detection, the scale of detection datasets is limited. Thus, commonly, previous work on temporal activity detection resorts to fine-tuning a classification model pretrained on large-scale classification datasets (e.g., Kinetics-400). However, such pretrained models are not ideal for downstream detection, due to the disparity between the pretraining and the downstream fine-tuning tasks. In this work, we propose a novel weakly-guided self-supervised pretraining method for detection. We leverage weak labels (classification) to introduce a self-supervised pretext task (detection) by generating frame-level pseudo labels, multi-action frames, and action segments. Simply put, we design a detection task similar to downstream, on large-scale classification data, without extra annotations. We show that the models pretrained with the proposed weakly-guided self-supervised detection task outperform prior work on multiple challenging activity detection benchmarks, including Charades and MultiTHUMOS. Our extensive ablations further provide insights on when and how to use the proposed models for activity detection. Code is available at github.com/kkahatapitiya/SSDet",
    "checked": true,
    "id": "cb7e9e6a68f64972cb42fc790c3a755175b80828",
    "semantic_title": "weakly-guided self-supervised pretraining for temporal activity detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25190": {
    "title": "Correlation Loss: Enforcing Correlation between Classification and Localization",
    "volume": "main",
    "abstract": "Object detectors are conventionally trained by a weighted sum of classification and localization losses. Recent studies (e.g., predicting IoU with an auxiliary head, Generalized Focal Loss, Rank & Sort Loss) have shown that forcing these two loss terms to interact with each other in non-conventional ways creates a useful inductive bias and improves performance. Inspired by these works, we focus on the correlation between classification and localization and make two main contributions: (i) We provide an analysis about the effects of correlation between classification and localization tasks in object detectors. We identify why correlation affects the performance of various NMS-based and NMS-free detectors, and we devise measures to evaluate the effect of correlation and use them to analyze common detectors. (ii) Motivated by our observations, e.g., that NMS-free detectors can also benefit from correlation, we propose Correlation Loss, a novel plug-in loss function that improves the performance of various object detectors by directly optimizing correlation coefficients: E.g., Correlation Loss on Sparse R-CNN, an NMS-free method, yields 1.6 AP gain on COCO and 1.8 AP gain on Cityscapes dataset. Our best model on Sparse R-CNN reaches 51.0 AP without test-time augmentation on COCO test-dev, reaching state-of-the-art. Code is available at: https://github.com/fehmikahraman/CorrLoss",
    "checked": true,
    "id": "448224d5629a2d3ab1571811a2d183189f424376",
    "semantic_title": "correlation loss: enforcing correlation between classification and localization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25191": {
    "title": "GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps",
    "volume": "main",
    "abstract": "Data augmentation is now an essential part of the image training process, as it effectively prevents overfitting and makes the model more robust against noisy datasets. Recent mixing augmentation strategies have advanced to generate the mixup mask that can enrich the saliency information, which is a supervisory signal. However, these methods incur a significant computational burden to optimize the mixup mask. From this motivation, we propose a novel saliency-aware mixup method, GuidedMixup, which aims to retain the salient regions in mixup images with low computational overhead. We develop an efficient pairing algorithm that pursues to minimize the conflict of salient regions of paired images and achieve rich saliency in mixup images. Moreover, GuidedMixup controls the mixup ratio for each pixel to better preserve the salient region by interpolating two paired images smoothly. The experiments on several datasets demonstrate that GuidedMixup provides a good trade-off between augmentation overhead and generalization performance on classification datasets. In addition, our method shows good performance in experiments with corrupted or reduced datasets",
    "checked": true,
    "id": "8ddb147c006cdeae6cd38f458e9c7c337d5b0965",
    "semantic_title": "guidedmixup: an efficient mixup strategy guided by saliency maps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25192": {
    "title": "3D Human Pose Lifting with Grid Convolution",
    "volume": "main",
    "abstract": "Existing lifting networks for regressing 3D human poses from 2D single-view poses are typically constructed with linear layers based on graph-structured representation learning. In sharp contrast to them, this paper presents Grid Convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is based on a novel Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map the irregular graph-structured human pose onto a regular weave-like grid pose representation joint by joint, enabling layer-wise feature learning with GridConv operations. We provide two ways to implement SGT, including handcrafted and learnable designs. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. We show that our fully convolutional grid lifting network outperforms state-of-the-art methods with noticeable margins under (1) conventional evaluation on Human3.6M and (2) cross-evaluation on MPI-INF-3DHP. Code is available at https://github.com/OSVAI/GridConv",
    "checked": true,
    "id": "eab5282e937a9b09b3af8c0956f1aa09ff20cfa9",
    "semantic_title": "3d human pose lifting with grid convolution",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25193": {
    "title": "Bidirectional Domain Mixup for Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "Mixup provides interpolated training samples and allows the model to obtain smoother decision boundaries for better generalization. The idea can be naturally applied to the domain adaptation task, where we can mix the source and target samples to obtain domain-mixed samples for better adaptation. However, the extension of the idea from classification to segmentation (i.e., structured output) is nontrivial. This paper systematically studies the impact of mixup under the domain adaptive semantic segmentation task and presents a simple yet effective mixup strategy called Bidirectional Domain Mixup (BDM). In specific, we achieve domain mixup in two-step: cut and paste. Given the warm-up model trained from any adaptation techniques, we forward the source and target samples and perform a simple threshold-based cut out of the unconfident regions (cut). After then, we fill-in the dropped regions with the other domain region patches (paste). In doing so, we jointly consider class distribution, spatial structure, and pseudo label confidence. Based on our analysis, we found that BDM leaves domain transferable regions by cutting, balances the dataset-level class distribution while preserving natural scene context by pasting. We coupled our proposal with various state-of-the-art adaptation models and observe significant improvement consistently. We also provide extensive ablation experiments to empirically verify our main components of the framework. Visit our project page with the code at https://sites.google.com/view/bidirectional-domain-mixup",
    "checked": true,
    "id": "a3577255e136a2c38198ac240cec2921bd739cde",
    "semantic_title": "bidirectional domain mixup for domain adaptive semantic segmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25194": {
    "title": "Frequency Selective Augmentation for Video Representation Learning",
    "volume": "main",
    "abstract": "Recent self-supervised video representation learning methods focus on maximizing the similarity between multiple augmented views from the same video and largely rely on the quality of generated views. However, most existing methods lack a mechanism to prevent representation learning from bias towards static information in the video. In this paper, we propose frequency augmentation (FreqAug), a spatio-temporal data augmentation method in the frequency domain for video representation learning. FreqAug stochastically removes specific frequency components from the video so that learned representation captures essential features more from the remaining information for various downstream tasks. Specifically, FreqAug pushes the model to focus more on dynamic features rather than static features in the video via dropping spatial or temporal low-frequency components. To verify the generality of the proposed method, we experiment with FreqAug on multiple self-supervised learning frameworks along with standard augmentations. Transferring the improved representation to five video action recognition and two temporal action localization downstream tasks shows consistent improvements over baselines",
    "checked": true,
    "id": "2de0034a889be933b6059f9cfebc4edb35e4ff29",
    "semantic_title": "frequency selective augmentation for video representation learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25195": {
    "title": "Pose-Guided 3D Human Generation in Indoor Scene",
    "volume": "main",
    "abstract": "In this work, we address the problem of scene-aware 3D human avatar generation based on human-scene interactions. In particular, we pay attention to the fact that physical contact between a 3D human and a scene (i.e., physical human-scene interactions) requires a geometrical alignment to generate natural 3D human avatar. Motivated by this fact, we present a new 3D human generation framework that considers geometric alignment on potential contact areas between 3D human avatars and their surroundings. In addition, we introduce a compact yet effective human pose classifier that classifies the human pose and provides potential contact areas of the 3D human avatar. It allows us to adaptively use geometric alignment loss according to the classified human pose. Compared to state-of-the-art method, our method can generate physically and semantically plausible 3D humans that interact naturally with 3D scenes without additional post-processing. In our evaluations, we achieve the improvements with more plausible interactions and more variety of poses than prior research in qualitative and quantitative analysis. Project page: https://bupyeonghealer.github.io/phin/",
    "checked": true,
    "id": "b60488cb4048b89121c0f50508c473bb8527d518",
    "semantic_title": "pose-guided 3d human generation in indoor scene",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25196": {
    "title": "Semantic-Aware Superpixel for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly-supervised semantic segmentation aims to train a semantic segmentation network using weak labels. Among weak labels, image-level label has been the most popular choice due to its simplicity. However, since image-level labels lack accurate object region information, additional modules such as saliency detector have been exploited in weakly supervised semantic segmentation, which requires pixel-level label for training. In this paper, we explore a self-supervised vision transformer to mitigate the heavy efforts on generation of pixel-level annotations. By exploiting the features obtained from self-supervised vision transformer, our superpixel discovery method finds out the semantic-aware superpixels based on the feature similarity in an unsupervised manner. Once we obtain the superpixels, we train the semantic segmentation network using superpixel-guided seeded region growing method. Despite its simplicity, our approach achieves the competitive result with the state-of-the-arts on PASCAL VOC 2012 and MS-COCO 2014 semantic segmentation datasets for weakly supervised semantic segmentation. Our code is available at https://github.com/st17kim/semantic-aware-superpixel",
    "checked": true,
    "id": "edf262d8983ca2ee80f6196365f8bf5e225603c5",
    "semantic_title": "semantic-aware superpixel for weakly supervised semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25197": {
    "title": "Multispectral Invisible Coating: Laminated Visible-Thermal Physical Attack against Multispectral Object Detectors Using Transparent Low-E Films",
    "volume": "main",
    "abstract": "Multispectral object detection plays a vital role in safety-critical vision systems that require an around-the-clock operation and encounter dynamic real-world situations(e.g., self-driving cars and autonomous surveillance systems). Despite its crucial competence in safety-related applications, its security against physical attacks is severely understudied. We investigate the vulnerability of multispectral detectors against physical attacks by proposing a new physical method: Multispectral Invisible Coating. Utilizing transparent Low-e films, we realize a laminated visible-thermal physical attack by attaching Low-e films over a visible attack printing. Moreover, we apply our physical method to manufacture a Multispectral Invisible Suit that hides persons from the multiple view angles of Multispectral detectors. To simulate our attack under various surveillance scenes, we constructed a large-scale multispectral pedestrian dataset which we will release in public. Extensive experiments show that our proposed method effectively attacks the state-of-the-art multispectral detector both in the digital space and the physical world",
    "checked": true,
    "id": "d6d1f02bba8aadc484edfc5becba4a8b91f66343",
    "semantic_title": "multispectral invisible coating: laminated visible-thermal physical attack against multispectral object detectors using transparent low-e films",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25198": {
    "title": "CRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer",
    "volume": "main",
    "abstract": "Camera and radar sensors have significant advantages in cost, reliability, and maintenance compared to LiDAR. Existing fusion methods often fuse the outputs of single modalities at the result-level, called the late fusion strategy. This can benefit from using off-the-shelf single sensor detection algorithms, but late fusion cannot fully exploit the complementary properties of sensors, thus having limited performance despite the huge potential of camera-radar fusion. Here we propose a novel proposal-level early fusion approach that effectively exploits both spatial and contextual properties of camera and radar for 3D object detection. Our fusion framework first associates image proposal with radar points in the polar coordinate system to efficiently handle the discrepancy between the coordinate system and spatial properties. Using this as a first stage, following consecutive cross-attention based feature fusion layers adaptively exchange spatio-contextual information between camera and radar, leading to a robust and attentive fusion. Our camera-radar fusion approach achieves the state-of-the-art 41.1% mAP and 52.3% NDS on the nuScenes test set, which is 8.7 and 10.8 points higher than the camera-only baseline, as well as yielding competitive performance on the LiDAR method",
    "checked": true,
    "id": "2a32aec5f06324bd0dabab8a41c97e29a954a21a",
    "semantic_title": "craft: camera-radar 3d object detection with spatio-contextual fusion transformer",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25199": {
    "title": "Simple and Effective Synthesis of Indoor 3D Scenes",
    "volume": "main",
    "abstract": "We study the problem of synthesizing immersive 3D indoor scenes from one or a few images. Our aim is to generate high-resolution images and videos from novel viewpoints, including viewpoints that extrapolate far beyond the input images while maintaining 3D consistency. Existing approaches are highly complex, with many separately trained stages and components. We propose a simple alternative: an image-to-image GAN that maps directly from reprojections of incomplete point clouds to full high-resolution RGB-D images. On the Matterport3D and RealEstate10K datasets, our approach significantly outperforms prior work when evaluated by humans, as well as on FID scores. Further, we show that our model is useful for generative data augmentation. A vision-and-language navigation (VLN) agent trained with trajectories spatially-perturbed by our model improves success rate by up to 1.5% over a state of the art baseline on the mature R2R benchmark. Our code will be made available to facilitate generative data augmentation and applications to downstream robotics and embodied AI tasks",
    "checked": true,
    "id": "749fc01c222e526dab89e9cb4cb280447d7d65fe",
    "semantic_title": "simple and effective synthesis of indoor 3d scenes",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25200": {
    "title": "MGTANet: Encoding Sequential LiDAR Points Using Long Short-Term Motion-Guided Temporal Attention for 3D Object Detection",
    "volume": "main",
    "abstract": "Most scanning LiDAR sensors generate a sequence of point clouds in real-time. While conventional 3D object detectors use a set of unordered LiDAR points acquired over a fixed time interval, recent studies have revealed that substantial performance improvement can be achieved by exploiting the spatio-temporal context present in a sequence of LiDAR point sets. In this paper, we propose a novel 3D object detection architecture, which can encode LiDAR point cloud sequences acquired by multiple successive scans. The encoding process of the point cloud sequence is performed on two different time scales. We first design a short-term motion-aware voxel encoding that captures the short-term temporal changes of point clouds driven by the motion of objects in each voxel. We also propose long-term motion-guided bird's eye view (BEV) feature enhancement that adaptively aligns and aggregates the BEV feature maps obtained by the short-term voxel encoding by utilizing the dynamic motion context inferred from the sequence of the feature maps. The experiments conducted on the public nuScenes benchmark demonstrate that the proposed 3D object detector offers significant improvements in performance compared to the baseline methods and that it sets a state-of-the-art performance for certain 3D object detection categories. Code is available at https://github.com/HYjhkoh/MGTANet.git",
    "checked": true,
    "id": "a600a57618e4f183ceec850fc9b441dda792728a",
    "semantic_title": "mgtanet: encoding sequential lidar points using long short-term motion-guided temporal attention for 3d object detection",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25201": {
    "title": "InstanceFormer: An Online Video Instance Segmentation Framework",
    "volume": "main",
    "abstract": "Recent transformer-based offline video instance segmentation (VIS) approaches achieve encouraging results and significantly outperform online approaches. However, their reliance on the whole video and the immense computational complexity caused by full Spatio-temporal attention limit them in real-life applications such as processing lengthy videos. In this paper, we propose a single-stage transformer-based efficient online VIS framework named InstanceFormer, which is especially suitable for long and challenging videos. We propose three novel components to model short-term and long-term dependency and temporal coherence. First, we propagate the representation, location, and semantic information of prior instances to model short-term changes. Second, we propose a novel memory cross-attention in the decoder, which allows the network to look into earlier instances within a certain temporal window. Finally, we employ a temporal contrastive loss to impose coherence in the representation of an instance across all frames. Memory attention and temporal coherence are particularly beneficial to long-range dependency modeling, including challenging scenarios like occlusion. The proposed InstanceFormer outperforms previous online benchmark methods by a large margin across multiple datasets. Most importantly, InstanceFormer surpasses offline approaches for challenging and long datasets such as YouTube-VIS-2021 and OVIS. Code is available at https://github.com/rajatkoner08/InstanceFormer",
    "checked": true,
    "id": "7eb8252d603b202c9cd06c338d132b6eecfd35e0",
    "semantic_title": "instanceformer: an online video instance segmentation framework",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25202": {
    "title": "Pixel-Wise Warping for Deep Image Stitching",
    "volume": "main",
    "abstract": "Existing image stitching approaches based on global or local homography estimation are not free from the parallax problem and suffer from undesired artifacts. In this paper, instead of relying on the homography-based warp, we propose a novel deep image stitching framework exploiting the pixel-wise warp field to handle the large-parallax problem. The proposed deep image stitching framework consists of a Pixel-wise Warping Module (PWM) and a Stitched Image Generating Module (SIGMo). For PWM, we obtain pixel-wise warp in a similar manner as estimating an optical flow (OF). In the stitching scenario, the input images usually include non-overlap (NOV) regions of which warp cannot be directly estimated, unlike the overlap (OV) regions. To help the PWM predict a reasonable warp on the NOV region, we impose two geometrical constraints: an epipolar loss and a line-preservation loss. With the obtained warp field, we relocate the pixels of the target image using forward warping. Finally, the SIGMo is trained by the proposed multi-branch training framework to generate a stitched image from a reference image and a warped target image. For training and evaluating the proposed framework, we build and publish a novel dataset including image pairs with corresponding pixel-wise ground truth warp and stitched result images. We show that the results of the proposed framework are quantitatively and qualitatively superior to those of the conventional methods",
    "checked": true,
    "id": "0cc6c9024ae3beb06dfefe7b1e7813ab9a38e954",
    "semantic_title": "pixel-wise warping for deep image stitching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25203": {
    "title": "Learning to Learn Better for Video Object Segmentation",
    "volume": "main",
    "abstract": "Recently, the joint learning framework (JOINT) integrates matching based transductive reasoning and online inductive learning to achieve accurate and robust semi-supervised video object segmentation (SVOS). However, using the mask embedding as the label to guide the generation of target features in the two branches may result in inadequate target representation and degrade the performance. Besides, how to reasonably fuse the target features in the two different branches rather than simply adding them together to avoid the adverse effect of one dominant branch has not been investigated. In this paper, we propose a novel framework that emphasizes Learning to Learn Better (LLB) target features for SVOS, termed LLB, where we design the discriminative label generation module (DLGM) and the adaptive fusion module to address these issues. Technically, the DLGM takes the background-filtered frame instead of the target mask as input and adopts a lightweight encoder to generate the target features, which serves as the label of the online few-shot learner and the value of the decoder in the transformer to guide the two branches to learn more discriminative target representation. The adaptive fusion module maintains a learnable gate for each branch, which reweighs the element-wise feature representation and allows an adaptive amount of target information in each branch flowing to the fused target feature, thus preventing one branch from being dominant and making the target feature more robust to distractor. Extensive experiments on public benchmarks show that our proposed LLB method achieves state-of-the-art performance",
    "checked": true,
    "id": "b0b7bf6eb268653a35a68e712516493a82b02d0f",
    "semantic_title": "learning to learn better for video object segmentation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25204": {
    "title": "Curriculum Multi-Negative Augmentation for Debiased Video Grounding",
    "volume": "main",
    "abstract": "Video Grounding (VG) aims to locate the desired segment from a video given a sentence query. Recent studies have found that current VG models are prone to over-rely the groundtruth moment annotation distribution biases in the training set. To discourage the standard VG model's behavior of exploiting such temporal annotation biases and improve the model generalization ability, we propose multiple negative augmentations in a hierarchical way, including cross-video augmentations from clip-/video-level, and self-shuffled augmentations with masks. These augmentations can effectively diversify the data distribution so that the model can make more reasonable predictions instead of merely fitting the temporal biases. However, directly adopting such data augmentation strategy may inevitably carry some noise shown in our cases, since not all of the handcrafted augmentations are semantically irrelevant to the groundtruth video. To further denoise and improve the grounding accuracy, we design a multi-stage curriculum strategy to adaptively train the standard VG model from easy to hard negative augmentations. Experiments on newly collected Charades-CD and ActivityNet-CD datasets demonstrate our proposed strategy can improve the performance of the base model on both i.i.d and o.o.d scenarios",
    "checked": true,
    "id": "235d6337eeb067042eb90c957a4380506a78c7b7",
    "semantic_title": "curriculum multi-negative augmentation for debiased video grounding",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25205": {
    "title": "Weakly Supervised 3D Segmentation via Receptive-Driven Pseudo Label Consistency and Structural Consistency",
    "volume": "main",
    "abstract": "As manual point-wise label is time and labor-intensive for fully supervised large-scale point cloud semantic segmentation, weakly supervised method is increasingly active. However, existing methods fail to generate high-quality pseudo labels effectively, leading to unsatisfactory results. In this paper, we propose a weakly supervised point cloud semantic segmentation framework via receptive-driven pseudo label consistency and structural consistency to mine potential knowledge. Specifically, we propose three consistency contrains: pseudo label consistency among different scales, semantic structure consistency between intra-class features and class-level relation structure consistency between pair-wise categories. Three consistency constraints are jointly used to effectively prepares and utilizes pseudo labels simultaneously for stable training. Finally, extensive experimental results on three challenging datasets demonstrate that our method significantly outperforms state-of-the-art weakly supervised methods and even achieves comparable performance to the fully supervised methods",
    "checked": true,
    "id": "e0d75d6529e6098461434f5f4b3dbdd28ef29691",
    "semantic_title": "weakly supervised 3d segmentation via receptive-driven pseudo label consistency and structural consistency",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25206": {
    "title": "MultiAct: Long-Term 3D Human Motion Generation from Multiple Action Labels",
    "volume": "main",
    "abstract": "We tackle the problem of generating long-term 3D human motion from multiple action labels. Two main previous approaches, such as action- and motion-conditioned methods, have limitations to solve this problem. The action-conditioned methods generate a sequence of motion from a single action. Hence, it cannot generate long-term motions composed of multiple actions and transitions between actions. Meanwhile, the motion-conditioned methods generate future motions from initial motion. The generated future motions only depend on the past, so they are not controllable by the user's desired actions. We present MultiAct, the first framework to generate long-term 3D human motion from multiple action labels. MultiAct takes account of both action and motion conditions with a unified recurrent generation system. It repetitively takes the previous motion and action label; then, it generates a smooth transition and the motion of the given action. As a result, MultiAct produces realistic long-term motion controlled by the given sequence of multiple action labels. The code is publicly available in https://github.com/TaeryungLee/MultiAct RELEASE",
    "checked": true,
    "id": "cc5df2954a38581b629ae0e86b8ec90b3af4b3bd",
    "semantic_title": "multiact: long-term 3d human motion generation from multiple action labels",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25207": {
    "title": "Not All Neighbors Matter: Point Distribution-Aware Pruning for 3D Point Cloud",
    "volume": "main",
    "abstract": "Applying deep neural networks to 3D point cloud processing has demonstrated a rapid pace of advancement in those domains where 3D geometry information can greatly boost task performance, such as AR/VR, robotics, and autonomous driving. However, as the size of both the neural network model and 3D point cloud continues to scale, reducing the entailed computation and memory access overhead is a primary challenge to meet strict latency and energy constraints of practical applications. This paper proposes a new weight pruning technique for 3D point cloud based on spatial point distribution. We identify that particular groups of neighborhood voxels in 3D point cloud contribute more frequently to actual output features than others. Based on this observation, we propose to selectively prune less contributing groups of neighborhood voxels first to reduce the computation overhead while minimizing the impact on model accuracy. We apply our proposal to three representative sparse 3D convolution libraries. Our proposal reduces the inference latency by 1.60× on average and energy consumption by 1.74× on NVIDIA GV100 GPU with no loss in accuracy metric",
    "checked": true,
    "id": "5f8a9671d33837ad90a9df091e03360c43fb5cc9",
    "semantic_title": "not all neighbors matter: point distribution-aware pruning for 3d point cloud",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25208": {
    "title": "Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task",
    "volume": "main",
    "abstract": "VQA is an ambitious task aiming to answer any image-related question. However, in reality, it is hard to build such a system once for all since the needs of users are continuously updated, and the system has to implement new functions. Thus, Continual Learning (CL) ability is a must in developing advanced VQA systems. Recently, a pioneer work split a VQA dataset into disjoint answer sets to study this topic. However, CL on VQA involves not only the expansion of label sets (new Answer sets). It is crucial to study how to answer questions when deploying VQA systems to new environments (new Visual scenes) and how to answer questions requiring new functions (new Question types). Thus, we propose CLOVE, a benchmark for Continual Learning On Visual quEstion answering, which contains scene- and function-incremental settings for the two aforementioned CL scenarios. In terms of methodology, the main difference between CL on VQA and classification is that the former additionally involves expanding and preventing forgetting of reasoning mechanisms, while the latter focusing on class representation. Thus, we propose a real-data-free replay-based method tailored for CL on VQA, named Scene Graph as Prompt for Symbolic Replay. Using a piece of scene graph as a prompt, it replays pseudo scene graphs to represent the past images, along with correlated QA pairs. A unified VQA model is also proposed to utilize the current and replayed data to enhance its QA ability. Finally, experimental results reveal challenges in CLOVE and demonstrate the effectiveness of our method. Code and data are available at https://github.com/showlab/CLVQA",
    "checked": true,
    "id": "fc1ed0c94657e9e340a038f02cab7b225704a4b9",
    "semantic_title": "symbolic replay: scene graph as prompt for continual learning on vqa task",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25209": {
    "title": "Linking People across Text and Images Based on Social Relation Reasoning",
    "volume": "main",
    "abstract": "As a sub-task of visual grounding, linking people across text and images aims to localize target people in images with corresponding sentences. Existing approaches tend to capture superficial features of people (e.g., dress and location) that suffer from the incompleteness information across text and images. We observe that humans are adept at exploring social relations to assist identifying people. Therefore, we propose a Social Relation Reasoning (SRR) model to address the aforementioned issues. Firstly, we design a Social Relation Extraction (SRE) module to extract social relations between people in the input sentence. Specially, the SRE module based on zero-shot learning is able to extract social relations even though they are not defined in the existing datasets. A Reasoning based Cross-modal Matching (RCM) module is further used to generate matching matrices by reasoning on the social relations and visual features. Experimental results show that the accuracy of our proposed SRR model outperforms the state-of-the-art models on the challenging datasets Who's Waldo and FL: MSRE, by more than 5\\% and 7\\%, respectively. Our source code is available at https://github.com/VILAN-Lab/SRR",
    "checked": true,
    "id": "5d0b66ca3fec08cf52d56a375aada4a1360af49e",
    "semantic_title": "linking people across text and images based on social relation reasoning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25210": {
    "title": "ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing",
    "volume": "main",
    "abstract": "The StyleGAN family succeed in high-fidelity image generation and allow for flexible and plausible editing of generated images by manipulating the semantic-rich latent style space. However, projecting a real image into its latent space encounters an inherent trade-off between inversion quality and editability. Existing encoder-based or optimization-based StyleGAN inversion methods attempt to mitigate the trade-off but see limited performance. To fundamentally resolve this problem, we propose a novel two-phase framework by designating two separate networks to tackle editing and reconstruction respectively, instead of balancing the two. Specifically, in Phase I, a W-space-oriented StyleGAN inversion network is trained and used to perform image inversion and edit- ing, which assures the editability but sacrifices reconstruction quality. In Phase II, a carefully designed rectifying network is utilized to rectify the inversion errors and perform ideal reconstruction. Experimental results show that our approach yields near-perfect reconstructions without sacrificing the editability, thus allowing accurate manipulation of real images. Further, we evaluate the performance of our rectifying net- work, and see great generalizability towards unseen manipulation types and out-of-domain images",
    "checked": true,
    "id": "ea5e945cde29b61254115d512e9d39fb57a7514c",
    "semantic_title": "reganie: rectifying gan inversion errors for accurate real image editing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25211": {
    "title": "SWBNet: A Stable White Balance Network for sRGB Images",
    "volume": "main",
    "abstract": "The white balance methods for sRGB images (sRGB-WB) aim to directly remove their color temperature shifts. Despite achieving promising white balance (WB) performance, the existing methods suffer from WB instability, i.e., their results are inconsistent for images with different color temperatures. We propose a stable white balance network (SWBNet) to alleviate this problem. It learns the color temperature-insensitive features to generate white-balanced images, resulting in consistent WB results. Specifically, the color temperatureinsensitive features are learned by implicitly suppressing lowfrequency information sensitive to color temperatures. Then, a color temperature contrastive loss is introduced to facilitate the most information shared among features of the same scene and different color temperatures. This way, features from the same scene are more insensitive to color temperatures regardless of the inputs. We also present a color temperature sensitivity-oriented transformer that globally perceives multiple color temperature shifts within an image and corrects them by different weights. It helps to improve the accuracy of stabilized SWBNet, especially for multiillumination sRGB images. Experiments indicate that our SWBNet achieves stable and remarkable WB performance",
    "checked": true,
    "id": "9ba6d7080c349fba2d29fe42357639b00d5cb2cb",
    "semantic_title": "swbnet: a stable white balance network for srgb images",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25212": {
    "title": "Frequency Domain Disentanglement for Arbitrary Neural Style Transfer",
    "volume": "main",
    "abstract": "Arbitrary neural style transfer has been a popular research topic due to its rich application scenarios. Effective disentanglement of content and style is the critical factor for synthesizing an image with arbitrary style. The existing methods focus on disentangling feature representations of content and style in the spatial domain where the content and style components are innately entangled and difficult to be disentangled clearly. Therefore, these methods always suffer from low-quality results because of the sub-optimal disentanglement. To address such a challenge, this paper proposes the frequency mixer (FreMixer) module that disentangles and re-entangles the frequency spectrum of content and style components in the frequency domain. Since content and style components have different frequency-domain characteristics (frequency bands and frequency patterns), the FreMixer could well disentangle these two components. Based on the FreMixer module, we design a novel Frequency Domain Disentanglement (FDD) framework for arbitrary neural style transfer. Qualitative and quantitative experiments verify that the proposed method can render better stylized results compared to the state-of-the-art methods",
    "checked": true,
    "id": "dc21b6d4fe70b99b74acb442e8cf711efabe644c",
    "semantic_title": "frequency domain disentanglement for arbitrary neural style transfer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25213": {
    "title": "Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation",
    "volume": "main",
    "abstract": "There has been a recent surge of interest in introducing transformers to 3D human pose estimation (HPE) due to their powerful capabilities in modeling long-term dependencies. However, existing transformer-based methods treat body joints as equally important inputs and ignore the prior knowledge of human skeleton topology in the self-attention mechanism. To tackle this issue, in this paper, we propose a Pose-Oriented Transformer (POT) with uncertainty guided refinement for 3D HPE. Specifically, we first develop novel pose-oriented self-attention mechanism and distance-related position embedding for POT to explicitly exploit the human skeleton topology. The pose-oriented self-attention mechanism explicitly models the topological interactions between body joints, whereas the distance-related position embedding encodes the distance of joints to the root joint to distinguish groups of joints with different difficulties in regression. Furthermore, we present an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art methods with reduced model parameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP",
    "checked": true,
    "id": "4cc1b3aecef868c9d56adc4e6d8a1116774faef9",
    "semantic_title": "pose-oriented transformer with uncertainty-guided refinement for 2d-to-3d human pose estimation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25214": {
    "title": "CEE-Net: Complementary End-to-End Network for 3D Human Pose Generation and Estimation",
    "volume": "main",
    "abstract": "The limited number of actors and actions in existing datasets make 3D pose estimators tend to overfit, which can be seen from the performance degradation of the algorithm on cross-datasets, especially for rare and complex poses. Although previous data augmentation works have increased the diversity of the training set, the changes in camera viewpoint and position play a dominant role in improving the accuracy of the estimator, while the generated 3D poses are limited and still heavily rely on the source dataset. In addition, these works do not consider the adaptability of the pose estimator to generated data, and complex poses will cause training collapse. In this paper, we propose the CEE-Net, a Complementary End-to-End Network for 3D human pose generation and estimation. The generator extremely expands the distribution of each joint-angle in the existing dataset and limits them to a reasonable range. By learning the correlations within and between the torso and limbs, the estimator can combine different body-parts more effectively and weaken the influence of specific joint-angle changes on the global pose, improving the generalization ability. Extensive ablation studies show that our pose generator greatly strengthens the joint-angle distribution, and our pose estimator can utilize these poses positively. Compared with the state-of-the-art methods, our method can achieve much better performance on various cross-datasets, rare and complex poses",
    "checked": true,
    "id": "09a03bcf681ef763cdbcf69869204230d205d07e",
    "semantic_title": "cee-net: complementary end-to-end network for 3d human pose generation and estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25215": {
    "title": "Real-World Deep Local Motion Deblurring",
    "volume": "main",
    "abstract": "Most existing deblurring methods focus on removing global blur caused by camera shake, while they cannot well handle local blur caused by object movements. To fill the vacancy of local deblurring in real scenes, we establish the first real local motion blur dataset (ReLoBlur), which is captured by a synchronized beam-splitting photographing system and corrected by a post-progressing pipeline. Based on ReLoBlur, we propose a Local Blur-Aware Gated network (LBAG) and several local blur-aware techniques to bridge the gap between global and local deblurring: 1) a blur detection approach based on background subtraction to localize blurred regions; 2) a gate mechanism to guide our network to focus on blurred regions; and 3) a blur-aware patch cropping strategy to address data imbalance problem. Extensive experiments prove the reliability of ReLoBlur dataset, and demonstrate that LBAG achieves better performance than state-of-the-art global deblurring methods and our proposed local blur-aware techniques are effective",
    "checked": true,
    "id": "a2c6c426b7f7921a41ba61fedcef8d12e05c32fe",
    "semantic_title": "real-world deep local motion deblurring",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25216": {
    "title": "Disentangle and Remerge: Interventional Knowledge Distillation for Few-Shot Object Detection from a Conditional Causal Perspective",
    "volume": "main",
    "abstract": "Few-shot learning models learn representations with limited human annotations, and such a learning paradigm demonstrates practicability in various tasks, e.g., image classification, object detection, etc. However, few-shot object detection methods suffer from an intrinsic defect that the limited training data makes the model cannot sufficiently explore semantic information. To tackle this, we introduce knowledge distillation to the few-shot object detection learning paradigm. We further run a motivating experiment, which demonstrates that in the process of knowledge distillation, the empirical error of the teacher model degenerates the prediction performance of the few-shot object detection model as the student. To understand the reasons behind this phenomenon, we revisit the learning paradigm of knowledge distillation on the few-shot object detection task from the causal theoretic standpoint, and accordingly, develop a Structural Causal Model. Following the theoretical guidance, we propose a backdoor adjustment-based knowledge distillation method for the few-shot object detection task, namely Disentangle and Remerge (D&R), to perform conditional causal intervention toward the corresponding Structural Causal Model. Empirically, the experiments on benchmarks demonstrate that D&R can yield significant performance boosts in few-shot object detection. Code is available at https://github.com/ZYN-1101/DandR.git",
    "checked": true,
    "id": "be2672ba4b68a5ebf69ce7c2f6024bd60f1d75c4",
    "semantic_title": "disentangle and remerge: interventional knowledge distillation for few-shot object detection from a conditional causal perspective",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25217": {
    "title": "Learning Motion-Robust Remote Photoplethysmography through Arbitrary Resolution Videos",
    "volume": "main",
    "abstract": "Remote photoplethysmography (rPPG) enables non-contact heart rate (HR) estimation from facial videos which gives significant convenience compared with traditional contact-based measurements. In the real-world long-term health monitoring scenario, the distance of the participants and their head movements usually vary by time, resulting in the inaccurate rPPG measurement due to the varying face resolution and complex motion artifacts. Different from the previous rPPG models designed for a constant distance between camera and participants, in this paper, we propose two plug-and-play blocks (i.e., physiological signal feature extraction block (PFE) and temporal face alignment block (TFA)) to alleviate the degradation of changing distance and head motion. On one side, guided with representative-area information, PFE adaptively encodes the arbitrary resolution facial frames to the fixed-resolution facial structure features. On the other side, leveraging the estimated optical flow, TFA is able to counteract the rPPG signal confusion caused by the head movement thus benefit the motion-robust rPPG signal recovery. Besides, we also train the model with a cross-resolution constraint using a two-stream dual-resolution framework, which further helps PFE learn resolution-robust facial rPPG features. Extensive experiments on three benchmark datasets (UBFC-rPPG, COHFACE and PURE) demonstrate the superior performance of the proposed method. One highlight is that with PFE and TFA, the off-the-shelf spatio-temporal rPPG models can predict more robust rPPG signals under both varying face resolution and severe head movement scenarios. The codes are available at https://github.com/LJWGIT/Arbitrary_Resolution_rPPG",
    "checked": true,
    "id": "b6ba44d7412305c1bab39b4b568e2e8492f9c9ed",
    "semantic_title": "learning motion-robust remote photoplethysmography through arbitrary resolution videos",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25218": {
    "title": "FSR: A General Frequency-Oriented Framework to Accelerate Image Super-resolution Networks",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have witnessed remarkable achievement in image super-resolution (SR), and plenty of DNN-based SR models with elaborated network designs have recently been proposed. However, existing methods usually require substantial computations by operating in spatial domain. To address this issue, we propose a general frequency-oriented framework (FSR) to accelerate SR networks by considering data characteristics in frequency domain. Our FSR mainly contains dual feature aggregation module (DFAM) to extract informative features in both spatial and transform domains, followed by a four-path SR-Module with different capacities to super-resolve in the frequency domain. Specifically, DFAM further consists of a transform attention block (TABlock) and a spatial context block (SCBlock) to extract global spectral information and local spatial information, respectively, while SR-Module is a parallel network container that contains four to-be-accelerated branches. Furthermore, we propose an adaptive weight strategy for a trade-off between image details recovery and visual quality. Extensive experiments show that our FSR can save FLOPs by almost 40% while reducing inference time by 50% for other SR methods (e.g., FSRCNN, CARN, SRResNet and RCAN). Code is available at https://github.com/THU-Kingmin/FSR",
    "checked": true,
    "id": "9fa82af8be2e392986ad01984ae74de495efffeb",
    "semantic_title": "fsr: a general frequency-oriented framework to accelerate image super-resolution networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25219": {
    "title": "Learning Polysemantic Spoof Trace: A Multi-Modal Disentanglement Network for Face Anti-spoofing",
    "volume": "main",
    "abstract": "Along with the widespread use of face recognition systems, their vulnerability has become highlighted. While existing face anti-spoofing methods can be generalized between attack types, generic solutions are still challenging due to the diversity of spoof characteristics. Recently, the spoof trace disentanglement framework has shown great potential for coping with both seen and unseen spoof scenarios, but the performance is largely restricted by the single-modal input. This paper focuses on this issue and presents a multi-modal disentanglement model which targetedly learns polysemantic spoof traces for more accurate and robust generic attack detection. In particular, based on the adversarial learning mechanism, a two-stream disentangling network is designed to estimate spoof patterns from the RGB and depth inputs, respectively. In this case, it captures complementary spoofing clues inhering in different attacks. Furthermore, a fusion module is exploited, which recalibrates both representations at multiple stages to promote the disentanglement in each individual modality. It then performs cross-modality aggregation to deliver a more comprehensive spoof trace representation for prediction. Extensive evaluations are conducted on multiple benchmarks, demonstrating that learning polysemantic spoof traces favorably contributes to anti-spoofing with more perceptible and interpretable results",
    "checked": true,
    "id": "20e4132660bf3e93e170c38dc970f5a59fba3d7e",
    "semantic_title": "learning polysemantic spoof trace: a multi-modal disentanglement network for face anti-spoofing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25220": {
    "title": "Stroke Extraction of Chinese Character Based on Deep Structure Deformable Image Registration",
    "volume": "main",
    "abstract": "Stroke extraction of Chinese characters plays an important role in the field of character recognition and generation. The most existing character stroke extraction methods focus on image morphological features. These methods usually lead to errors of cross strokes extraction and stroke matching due to rarely using stroke semantics and prior information. In this paper, we propose a deep learning-based character stroke extraction method that takes semantic features and prior information of strokes into consideration. This method consists of three parts: image registration-based stroke registration that establishes the rough registration of the reference strokes and the target as prior information; image semantic segmentation-based stroke segmentation that preliminarily separates target strokes into seven categories; and high-precision extraction of single strokes. In the stroke registration, we propose a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes for character images with complex structures. In order to verify the effectiveness of the method, we construct two datasets respectively for calligraphy characters and regular handwriting characters. The experimental results show that our method strongly outperforms the baselines. Code is available at https://github.com/MengLi-l1/StrokeExtraction",
    "checked": true,
    "id": "d98fd91ed23b3ab623a60cd1713382b2163d5ce2",
    "semantic_title": "stroke extraction of chinese character based on deep structure deformable image registration",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25221": {
    "title": "Spatial-Spectral Transformer for Hyperspectral Image Denoising",
    "volume": "main",
    "abstract": "Hyperspectral image (HSI) denoising is a crucial preprocessing procedure for the subsequent HSI applications. Unfortunately, though witnessing the development of deep learning in HSI denoising area, existing convolution-based methods face the trade-off between computational efficiency and capability to model non-local characteristics of HSI. In this paper, we propose a Spatial-Spectral Transformer (SST) to alleviate this problem. To fully explore intrinsic similarity characteristics in both spatial dimension and spectral dimension, we conduct non-local spatial self-attention and global spectral self-attention with Transformer architecture. The window-based spatial self-attention focuses on the spatial similarity beyond the neighboring region. While, the spectral self-attention exploits the long-range dependencies between highly correlative bands. Experimental results show that our proposed method outperforms the state-of-the-art HSI denoising methods in quantitative quality and visual results. The code is released at https://github.com/MyuLi/SST",
    "checked": true,
    "id": "efc12fd00542450f688bc4d8c9fc7db73309c723",
    "semantic_title": "spatial-spectral transformer for hyperspectral image denoising",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25222": {
    "title": "Learning Semantic Alignment with Global Modality Reconstruction for Video-Language Pre-training towards Retrieval",
    "volume": "main",
    "abstract": "Video-language pre-training for text-based video retrieval tasks is vitally important. Previous pre-training methods suffer from the semantic misalignments. The reason is that these methods ignore sequence alignments but focusing on critical token alignment. To alleviate the problem, we propose a video-language pre-training framework, termed videolanguage pre-training For lEarning sEmantic aLignments (FEEL), to learn semantic alignments at the sequence level. Specifically, the global modality reconstruction and the cross- modal self-contrasting method is utilized to learn the alignments at the sequence level better. Extensive experimental results demonstrate the effectiveness of FEEL on text-based video retrieval and text-based video corpus moment retrieval",
    "checked": true,
    "id": "4a17e108c9a99ba9a8db813fecc45b3e651e6a74",
    "semantic_title": "learning semantic alignment with global modality reconstruction for video-language pre-training towards retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25223": {
    "title": "Layout-Aware Dreamer for Embodied Visual Referring Expression Grounding",
    "volume": "main",
    "abstract": "In this work, we study the problem of Embodied Referring Expression Grounding, where an agent needs to navigate in a previously unseen environment and localize a remote object described by a concise high-level natural language instruction. When facing such a situation, a human tends to imagine what the destination may look like and to explore the environment based on prior knowledge of the environmental layout, such as the fact that a bathroom is more likely to be found near a bedroom than a kitchen. We have designed an autonomous agent called Layout-aware Dreamer (LAD), including two novel modules, that is, the Layout Learner and the Goal Dreamer to mimic this cognitive decision process. The Layout Learner learns to infer the room category distribution of neighboring unexplored areas along the path for coarse layout estimation, which effectively introduces layout common sense of room-to-room transitions to our agent. To learn an effective exploration of the environment, the Goal Dreamer imagines the destination beforehand. Our agent achieves new state-of-the-art performance on the public leaderboard of REVERIE dataset in challenging unseen test environments with improvement on navigation success rate (SR) by 4.02% and remote grounding success (RGS) by 3.43% comparing to previous previous state of the art. The code is released at https://github.com/zehao-wang/LAD",
    "checked": true,
    "id": "0c79bc6ce52539e42985a892e3745822590c2865",
    "semantic_title": "layout-aware dreamer for embodied visual referring expression grounding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25224": {
    "title": "NeAF: Learning Neural Angle Fields for Point Normal Estimation",
    "volume": "main",
    "abstract": "Normal estimation for unstructured point clouds is an important task in 3D computer vision. Current methods achieve encouraging results by mapping local patches to normal vectors or learning local surface fitting using neural networks. However, these methods are not generalized well to unseen scenarios and are sensitive to parameter settings. To resolve these issues, we propose an implicit function to learn an angle field around the normal of each point in the spherical coordinate system, which is dubbed as Neural Angle Fields (NeAF). Instead of directly predicting the normal of an input point, we predict the angle offset between the ground truth normal and a randomly sampled query normal. This strategy pushes the network to observe more diverse samples, which leads to higher prediction accuracy in a more robust manner. To predict normals from the learned angle fields at inference time, we randomly sample query vectors in a unit spherical space and take the vectors with minimal angle values as the predicted normals. To further leverage the prior learned by NeAF, we propose to refine the predicted normal vectors by minimizing the angle offsets. The experimental results with synthetic data and real scans show significant improvements over the state-of-the-art under widely used benchmarks. Project page: https://lisj575.github.io/NeAF/",
    "checked": true,
    "id": "a765befc5a9f7816a2ae6cf2a911a94ff24c3399",
    "semantic_title": "neaf: learning neural angle fields for point normal estimation",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25225": {
    "title": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels",
    "volume": "main",
    "abstract": "Pre-trained vision-language models like CLIP have recently shown superior performances on various downstream tasks, including image classification and segmentation. However, in fine-grained image re-identification (ReID), the labels are indexes, lacking concrete text descriptions. Therefore, it remains to be determined how such models could be applied to these tasks. This paper first finds out that simply fine-tuning the visual model initialized by the image encoder in CLIP, has already obtained competitive performances in various ReID tasks. Then we propose a two-stage strategy to facilitate a better visual representation. The key idea is to fully exploit the cross-modal description ability in CLIP through a set of learnable text tokens for each ID and give them to the text encoder to form ambiguous descriptions. In the first training stage, image and text encoders from CLIP keep fixed, and only the text tokens are optimized from scratch by the contrastive loss computed within a batch. In the second stage, the ID-specific text tokens and their encoder become static, providing constraints for fine-tuning the image encoder. With the help of the designed loss in the downstream task, the image encoder is able to represent data as vectors in the feature embedding accurately. The effectiveness of the proposed strategy is validated on several datasets for the person or vehicle ReID tasks. Code is available at https://github.com/Syliz517/CLIP-ReID",
    "checked": true,
    "id": "a8adfb137332a61893417609563897abe9307a11",
    "semantic_title": "clip-reid: exploiting vision-language model for image re-identification without concrete text labels",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25226": {
    "title": "DC-Former: Diverse and Compact Transformer for Person Re-identification",
    "volume": "main",
    "abstract": "In person re-identification (ReID) task, it is still challenging to learn discriminative representation by deep learning, due to limited data. Generally speaking, the model will get better performance when increasing the amount of data. The addition of similar classes strengthens the ability of the classifier to identify similar identities, thereby improving the discrimination of representation. In this paper, we propose a Diverse and Compact Transformer (DC-Former) that can achieve a similar effect by splitting embedding space into multiple diverse and compact subspaces. Compact embedding subspace helps model learn more robust and discriminative embedding to identify similar classes. And the fusion of these diverse embeddings containing more fine-grained information can further improve the effect of ReID. Specifically, multiple class tokens are used in vision transformer to represent multiple embedding spaces. Then, a self-diverse constraint (SDC) is applied to these spaces to push them away from each other, which makes each embedding space diverse and compact. Further, a dynamic weight controller (DWC) is further designed for balancing the relative importance among them during training. The experimental results of our method are promising, which surpass previous state-of-the-art methods on several commonly used person ReID benchmarks. Our code is available at https://github.com/ant-research/Diverse-and-Compact-Transformer",
    "checked": true,
    "id": "540dcdf4a4556facf03a6cffed4f23a584f64317",
    "semantic_title": "dc-former: diverse and compact transformer for person re-identification",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25227": {
    "title": "Panoramic Video Salient Object Detection with Ambisonic Audio Guidance",
    "volume": "main",
    "abstract": "Video salient object detection (VSOD), as a fundamental computer vision problem, has been extensively discussed in the last decade. However, all existing works focus on addressing the VSOD problem in 2D scenarios. With the rapid development of VR devices, panoramic videos have been a promising alternative to 2D videos to provide immersive feelings of the real world. In this paper, we aim to tackle the video salient object detection problem for panoramic videos, with their corresponding ambisonic audios. A multimodal fusion module equipped with two pseudo-siamese audio-visual context fusion (ACF) blocks is proposed to effectively conduct audio-visual interaction. The ACF block equipped with spherical positional encoding enables the fusion in the 3D context to capture the spatial correspondence between pixels and sound sources from the equirectangular frames and ambisonic audios. Experimental results verify the effectiveness of our proposed components and demonstrate that our method achieves state-of-the-art performance on the ASOD60K dataset",
    "checked": true,
    "id": "1ecce926a9877bb7512ce4d1101582877e07db12",
    "semantic_title": "panoramic video salient object detection with ambisonic audio guidance",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25228": {
    "title": "LWSIS: LiDAR-Guided Weakly Supervised Instance Segmentation for Autonomous Driving",
    "volume": "main",
    "abstract": "Image instance segmentation is a fundamental research topic in autonomous driving, which is crucial for scene understanding and road safety. Advanced learning-based approaches often rely on the costly 2D mask annotations for training. In this paper, we present a more artful framework, LiDAR-guided Weakly Supervised Instance Segmentation (LWSIS), which leverages the off-the-shelf 3D data, i.e., Point Cloud, together with the 3D boxes, as natural weak supervisions for training the 2D image instance segmentation models. Our LWSIS not only exploits the complementary information in multimodal data during training but also significantly reduces the annotation cost of the dense 2D masks. In detail, LWSIS consists of two crucial modules, Point Label Assignment (PLA) and Graph-based Consistency Regularization (GCR). The former module aims to automatically assign the 3D point cloud as 2D point-wise labels, while the atter further refines the predictions by enforcing geometry and appearance consistency of the multimodal data. Moreover, we conduct a secondary instance segmentation annotation on the nuScenes, named nuInsSeg, to encourage further research on multimodal perception tasks. Extensive experiments on the nuInsSeg, as well as the large-scale Waymo, show that LWSIS can substantially improve existing weakly supervised segmentation models by only involving 3D data during training. Additionally, LWSIS can also be incorporated into 3D object detectors like PointPainting to boost the 3D detection performance for free. The code and dataset are available at https://github.com/Serenos/LWSIS",
    "checked": true,
    "id": "e11ee3c730263c6b92c24f1d707ed35626b7e5a1",
    "semantic_title": "lwsis: lidar-guided weakly supervised instance segmentation for autonomous driving",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25229": {
    "title": "Adaptive Texture Filtering for Single-Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "Domain generalization in semantic segmentation aims to alleviate the performance degradation on unseen domains through learning domain-invariant features. Existing methods diversify images in the source domain by adding complex or even abnormal textures to reduce the sensitivity to domain-specific features. However, these approaches depends heavily on the richness of the texture bank and training them can be time-consuming. In contrast to importing textures arbitrarily or augmenting styles randomly, we focus on the single source domain itself to achieve the generalization. In this paper, we present a novel adaptive texture filtering mechanism to suppress the influence of texture without using augmentation, thus eliminating the interference of domain-specific features. Further, we design a hierarchical guidance generalization network equipped with structure-guided enhancement modules, which purpose to learn the domain-invariant generalized knowledge. Extensive experiments together with ablation studies on widely-used datasets are conducted to verify the effectiveness of the proposed model, and reveal its superiority over other state-of-the-art alternatives",
    "checked": true,
    "id": "8d475edd830f150b9a6aa3066524c77913510df8",
    "semantic_title": "adaptive texture filtering for single-domain generalized segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25230": {
    "title": "MEID: Mixture-of-Experts with Internal Distillation for Long-Tailed Video Recognition",
    "volume": "main",
    "abstract": "The long-tailed video recognition problem is especially challenging, as videos tend to be long and untrimmed, and each video may contain multiple classes, causing frame-level class imbalance. The previous method tackles the long-tailed video recognition only through frame-level sampling for class re-balance without distinguishing the frame-level feature representation between head and tail classes. To improve the frame-level feature representation of tail classes, we modulate the frame-level features with an auxiliary distillation loss to reduce the distribution distance between head and tail classes. Moreover, we design a mixture-of-experts framework with two different expert designs, i.e., the first expert with an attention-based classification network handling the original long-tailed distribution, and the second expert dealing with the re-balanced distribution from class-balanced sampling. Notably, in the second expert, we specifically focus on the frames unsolved by the first expert through designing a complementary frame selection module, which inherits the attention weights from the first expert and selects frames with low attention weights, and we also enhance the motion feature representation for these selected frames. To highlight the multi-label challenge in long-tailed video recognition, we create two additional benchmarks based on Charades and CharadesEgo videos with the multi-label property, called CharadesLT and CharadesEgoLT. Extensive experiments are conducted on the existing long-tailed video benchmark VideoLT and the two new benchmarks to verify the effectiveness of our proposed method with state-of-the-art performance. The code and proposed benchmarks are released at https://github.com/VisionLanguageLab/MEID",
    "checked": true,
    "id": "a96fdfacff96f3a4e944849f856752bc374f7bf1",
    "semantic_title": "meid: mixture-of-experts with internal distillation for long-tailed video recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25231": {
    "title": "Gradient Corner Pooling for Keypoint-Based Object Detection",
    "volume": "main",
    "abstract": "Detecting objects as multiple keypoints is an important approach in the anchor-free object detection methods while corner pooling is an effective feature encoding method for corner positioning. The corners of the bounding box are located by summing the feature maps which are max-pooled in the x and y directions respectively by corner pooling. In the unidirectional max pooling operation, the features of the densely arranged objects of the same class are prone to occlusion. To this end, we propose a method named Gradient Corner Pooling. The spatial distance information of objects on the feature map is encoded during the unidirectional pooling process, which effectively alleviates the occlusion of the homogeneous object features. Further, the computational complexity of gradient corner pooling is the same as traditional corner pooling and hence it can be implemented efficiently. Gradient corner pooling obtains consistent improvements for various keypoint-based methods by directly replacing corner pooling. We verify the gradient corner pooling algorithm on the dataset and in real scenarios, respectively. The networks with gradient corner pooling located the corner points earlier in the training process and achieve an average accuracy improvement of 0.2%-1.6% on the MS-COCO dataset. The detectors with gradient corner pooling show better angle adaptability for arrayed objects in the actual scene test",
    "checked": true,
    "id": "2812527625498e4dd256e4ddb0f4a49aeffd2d61",
    "semantic_title": "gradient corner pooling for keypoint-based object detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25232": {
    "title": "Towards Real-Time Segmentation on the Edge",
    "volume": "main",
    "abstract": "The research in real-time segmentation mainly focuses on desktop GPUs. However, autonomous driving and many other applications rely on real-time segmentation on the edge, and current arts are far from the goal. In addition, recent advances in vision transformers also inspire us to re-design the network architecture for dense prediction task. In this work, we propose to combine the self attention block with lightweight convolutions to form new building blocks, and employ latency constraints to search an efficient sub-network. We train an MLP latency model based on generated architecture configurations and their latency measured on mobile devices, so that we can predict the latency of subnets during search phase. To the best of our knowledge, we are the first to achieve over 74% mIoU on Cityscapes with semi-real-time inference (over 15 FPS) on mobile GPU from an off-the-shelf phone",
    "checked": true,
    "id": "98b67efc8ca78e7e254a1fa543996b69635aa080",
    "semantic_title": "towards real-time segmentation on the edge",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25233": {
    "title": "BEVDepth: Acquisition of Reliable Depth for Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "In this research, we propose a new 3D object detector with a trustworthy depth estimation, dubbed BEVDepth, for camera-based Bird's-Eye-View~(BEV) 3D object detection. Our work is based on a key observation -- depth estimation in recent approaches is surprisingly inadequate given the fact that depth is essential to camera 3D detection. Our BEVDepth resolves this by leveraging explicit depth supervision. A camera-awareness depth estimation module is also introduced to facilitate the depth predicting capability. Besides, we design a novel Depth Refinement Module to counter the side effects carried by imprecise feature unprojection. Aided by customized Efficient Voxel Pooling and multi-frame mechanism, BEVDepth achieves the new state-of-the-art 60.9% NDS on the challenging nuScenes test set while maintaining high efficiency. For the first time, the NDS score of a camera model reaches 60%. Codes have been released",
    "checked": true,
    "id": "234f0122e0edccba5c91763e800c2f02fe8ae4fe",
    "semantic_title": "bevdepth: acquisition of reliable depth for multi-view 3d object detection",
    "citation_count": 133
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25234": {
    "title": "BEVStereo: Enhancing Depth Estimation in Multi-View 3D Object Detection with Temporal Stereo",
    "volume": "main",
    "abstract": "Restricted by the ability of depth perception, all Multi-view 3D object detection methods fall into the bottleneck of depth accuracy. By constructing temporal stereo, depth estimation is quite reliable in indoor scenarios. However, there are two difficulties in directly integrating temporal stereo into outdoor multi-view 3D object detectors: 1) The construction of temporal stereos for all views results in high computing costs. 2) Unable to adapt to challenging outdoor scenarios. In this study, we propose an effective method for creating temporal stereo by dynamically determining the center and range of the temporal stereo. The most confident center is found using the EM algorithm. Numerous experiments on nuScenes have shown the BEVStereo's ability to deal with complex outdoor scenarios that other stereo-based methods are unable to handle. For the first time, a stereo-based approach shows superiority in scenarios like a static ego vehicle and moving objects. BEVStereo achieves the new state-of-the-art in the camera-only track of nuScenes dataset while maintaining memory efficiency. Codes have been released",
    "checked": true,
    "id": "1285d14bffedcc4362fdd05213fd6ee4ec5ca885",
    "semantic_title": "bevstereo: enhancing depth estimation in multi-view 3d object detection with temporal stereo",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25235": {
    "title": "Learning Single Image Defocus Deblurring with Misaligned Training Pairs",
    "volume": "main",
    "abstract": "By adopting popular pixel-wise loss, existing methods for defocus deblurring heavily rely on well aligned training image pairs. Although training pairs of ground-truth and blurry images are carefully collected, e.g., DPDD dataset, misalignment is inevitable between training pairs, making existing methods possibly suffer from deformation artifacts. In this paper, we propose a joint deblurring and reblurring learning (JDRL) framework for single image defocus deblurring with misaligned training pairs. Generally, JDRL consists of a deblurring module and a spatially invariant reblurring module, by which deblurred result can be adaptively supervised by ground-truth image to recover sharp textures while maintaining spatial consistency with the blurry image. First, in the deblurring module, a bi-directional optical flow-based deformation is introduced to tolerate spatial misalignment between deblurred and ground-truth images. Second, in the reblurring module, deblurred result is reblurred to be spatially aligned with blurry image, by predicting a set of isotropic blur kernels and weighting maps. Moreover, we establish a new single image defocus deblurring (SDD) dataset, further validating our JDRL and also benefiting future research. Our JDRL can be applied to boost defocus deblurring networks in terms of both quantitative metrics and visual quality on DPDD, RealDOF and our SDD datasets",
    "checked": true,
    "id": "b31de933dc82b38a598c1d274fc9ade2d987c0ba",
    "semantic_title": "learning single image defocus deblurring with misaligned training pairs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25236": {
    "title": "Curriculum Temperature for Knowledge Distillation",
    "volume": "main",
    "abstract": "Most existing distillation methods ignore the flexible role of the temperature in the loss function and fix it as a hyper-parameter that can be decided by an inefficient grid search. In general, the temperature controls the discrepancy between two distributions and can faithfully determine the difficulty level of the distillation task. Keeping a constant temperature, i.e., a fixed level of task difficulty, is usually sub-optimal for a growing student during its progressive learning stages. In this paper, we propose a simple curriculum-based technique, termed Curriculum Temperature for Knowledge Distillation (CTKD), which controls the task difficulty level during the student's learning career through a dynamic and learnable temperature. Specifically, following an easy-to-hard curriculum, we gradually increase the distillation loss w.r.t. the temperature, leading to increased distillation difficulty in an adversarial manner. As an easy-to-use plug-in technique, CTKD can be seamlessly integrated into existing knowledge distillation frameworks and brings general improvements at a negligible additional computation cost. Extensive experiments on CIFAR-100, ImageNet-2012, and MS-COCO demonstrate the effectiveness of our method",
    "checked": true,
    "id": "6ee96ee0d32816658daa507e2228b037768f148b",
    "semantic_title": "curriculum temperature for knowledge distillation",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25237": {
    "title": "Actionness Inconsistency-Guided Contrastive Learning for Weakly-Supervised Temporal Action Localization",
    "volume": "main",
    "abstract": "Weakly-supervised temporal action localization (WTAL) aims to detect action instances given only video-level labels. To address the challenge, recent methods commonly employ a two-branch framework, consisting of a class-aware branch and a class-agnostic branch. In principle, the two branches are supposed to produce the same actionness activation. However, we observe that there are actually many inconsistent activation regions. These inconsistent regions usually contain some challenging segments whose semantic information (action or background) is ambiguous. In this work, we propose a novel Actionness Inconsistency-guided Contrastive Learning (AICL) method which utilizes the consistent segments to boost the representation learning of the inconsistent segments. Specifically, we first define the consistent and inconsistent segments by comparing the predictions of two branches and then construct positive and negative pairs between consistent segments and inconsistent segments for contrastive learning. In addition, to avoid the trivial case where there is no consistent sample, we introduce an action consistency constraint to control the difference between the two branches. We conduct extensive experiments on THUMOS14, ActivityNet v1.2, and ActivityNet v1.3 datasets, and the results show the effectiveness of AICL with state-of-the-art performance. Our code is available at https://github.com/lizhilin-ustc/AAAI2023-AICL",
    "checked": true,
    "id": "fce2e326ec543ef51f6d6e0294ae21d5074320bc",
    "semantic_title": "actionness inconsistency-guided contrastive learning for weakly-supervised temporal action localization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25238": {
    "title": "READ: Large-Scale Neural Scene Rendering for Autonomous Driving",
    "volume": "main",
    "abstract": "With the development of advanced driver assistance systems~(ADAS) and autonomous vehicles, conducting experiments in various scenarios becomes an urgent need. Although having been capable of synthesizing photo-realistic street scenes, conventional image-to-image translation methods cannot produce coherent scenes due to the lack of 3D information. In this paper, a large-scale neural rendering method is proposed to synthesize the autonomous driving scene~(READ), which makes it possible to generate large-scale driving scenes in real time on a PC through a variety of sampling schemes. In order to effectively represent driving scenarios, we propose an ω-net rendering network to learn neural descriptors from sparse point clouds. Our model can not only synthesize photo-realistic driving scenes but also stitch and edit them. The promising experimental results show that our model performs well in large-scale driving scenarios",
    "checked": true,
    "id": "982236b056edc17962853c7344e5cf43513b474c",
    "semantic_title": "read: large-scale neural scene rendering for autonomous driving",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25239": {
    "title": "CDTA: A Cross-Domain Transfer-Based Attack with Contrastive Learning",
    "volume": "main",
    "abstract": "Despite the excellent performance, deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Besides, these examples are often transferable among different models. In other words, the same adversarial example can fool multiple models with different architectures at the same time. Based on this property, many black-box transfer-based attack techniques have been developed. However, current transfer-based attacks generally focus on the cross-architecture setting, where the attacker has access to the training data of the target model, which is not guaranteed in realistic situations. In this paper, we design a Cross-Domain Transfer-Based Attack (CDTA), which works in the cross-domain scenario. In this setting, attackers have no information about the target model, such as its architecture and training data. Specifically, we propose a contrastive spectral training method to train a feature extractor on a source domain (e.g., ImageNet) and use it to craft adversarial examples on target domains (e.g., Oxford 102 Flower). Our method corrupts the semantic information of the benign image by scrambling the outputs of both the intermediate feature layers and the final layer of the feature extractor. We evaluate CDTA with 16 target deep models on four datasets with widely varying styles. The results confirm that, in terms of the attack success rate, our approach can consistently outperform the state-of-the-art baselines by an average of 11.45% across all target models. Our code is available at https://github.com/LiulietLee/CDTA",
    "checked": true,
    "id": "5206aa1f39cdbc90245bab8761375f9159d913a0",
    "semantic_title": "cdta: a cross-domain transfer-based attack with contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25240": {
    "title": "HybridCap: Inertia-Aid Monocular Capture of Challenging Human Motions",
    "volume": "main",
    "abstract": "Monocular 3D motion capture (mocap) is beneficial to many applications. The use of a single camera, however, often fails to handle occlusions of different body parts and hence it is limited to capture relatively simple movements. We present a light-weight, hybrid mocap technique called HybridCap that augments the camera with only 4 Inertial Measurement Units (IMUs) in a novel learning-and-optimization framework. We first employ a weakly-supervised and hierarchical motion inference module based on cooperative pure residual recurrent blocks that serve as limb, body and root trackers as well as an inverse kinematics solver. Our network effectively narrows the search space of plausible motions via coarse-to-fine pose estimation and manages to tackle challenging movements with high efficiency. We further develop a hybrid optimization scheme that combines inertial feedback and visual cues to improve tracking accuracy. Extensive experiments on various datasets demonstrate HybridCap can robustly handle challenging movements ranging from fitness actions to Latin dance. It also achieves real-time performance up to 60 fps with state-of-the-art accuracy",
    "checked": true,
    "id": "15453b482f58c9ef4a2f55f1cfb8872ef7cb7426",
    "semantic_title": "hybridcap: inertia-aid monocular capture of challenging human motions",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25241": {
    "title": "Global Dilated Attention and Target Focusing Network for Robust Tracking",
    "volume": "main",
    "abstract": "Self Attention has shown the excellent performance in tracking due to its global modeling capability. However, it brings two challenges: First, its global receptive field has less attention on local structure and inter-channel associations, which limits the semantics to distinguish objects and backgrounds; Second, its feature fusion with linear process cannot avoid the interference of non-target semantic objects. To solve the above issues, this paper proposes a robust tracking method named GdaTFT by defining the Global Dilated Attention (GDA) and Target Focusing Network (TFN). The GDA provides a new global semantics modeling approach to enhance the semantic objects while eliminating the background. It is defined via the local focusing module, dilated attention and channel adaption module. Thus, it promotes semantics by focusing local key information, building long-range dependencies and enhancing the semantics of channels. Subsequently, to distinguish the target and non-target objects both with rich semantics, the TFN is proposed to accurately focus the target region. Different from the present feature fusion, it uses the template as the query to build a point-to-point correlation between the template and search region, and finally achieves part-level augmentation of target feature in the search region. Thus, the TFN efficiently augments the target embedding while weakening the non-target objects. Experiments on challenging benchmarks (LaSOT, TrackingNet, GOT-10k, OTB-100) demonstrate that the GdaTFT outperforms many state-of-the-art trackers and achieves leading performance. Code will be available",
    "checked": true,
    "id": "d2e10ada5a6ac8442d38117e92508de98f165526",
    "semantic_title": "global dilated attention and target focusing network for robust tracking",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25242": {
    "title": "Only a Few Classes Confusing: Pixel-Wise Candidate Labels Disambiguation for Foggy Scene Understanding",
    "volume": "main",
    "abstract": "Not all semantics become confusing when deploying a semantic segmentation model for real-world scene understanding of adverse weather. The true semantics of most pixels have a high likelihood of appearing in the few top classes according to confidence ranking. In this paper, we replace the one-hot pseudo label with a candidate label set (CLS) that consists of only a few ambiguous classes and exploit its effects on self-training-based unsupervised domain adaptation. Specifically, we formulate the problem as a coarse-to-fine process. In the coarse-level process, adaptive CLS selection is proposed to pick a minimal set of confusing candidate labels based on the reliability of label predictions. Then, representation learning and label rectification are iteratively performed to facilitate feature clustering in an embedding space and to disambiguate the confusing semantics. Experimentally, our method outperforms the state-of-the-art methods on three realistic foggy benchmarks",
    "checked": true,
    "id": "2c4f3efd0e7c9c0e1ce649fea31d7f8836f01293",
    "semantic_title": "only a few classes confusing: pixel-wise candidate labels disambiguation for foggy scene understanding",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25243": {
    "title": "Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation",
    "volume": "main",
    "abstract": "Vision-Language Navigation (VLN) is a challenging task which requires an agent to align complex visual observations to language instructions to reach the goal position. Most existing VLN agents directly learn to align the raw directional features and visual features trained using one-hot labels to linguistic instruction features. However, the big semantic gap among these multi-modal inputs makes the alignment difficult and therefore limits the navigation performance. In this paper, we propose Actional Atomic-Concept Learning (AACL), which maps visual observations to actional atomic concepts for facilitating the alignment. Specifically, an actional atomic concept is a natural language phrase containing an atomic action and an object, e.g., ``go up stairs''. These actional atomic concepts, which serve as the bridge between observations and instructions, can effectively mitigate the semantic gap and simplify the alignment. AACL contains three core components: 1) a concept mapping module to map the observations to the actional atomic concept representations through the VLN environment and the recently proposed Contrastive Language-Image Pretraining (CLIP) model, 2) a concept refining adapter to encourage more instruction-oriented object concept extraction by re-ranking the predicted object concepts by CLIP, and 3) an observation co-embedding module which utilizes concept representations to regularize the observation representations. Our AACL establishes new state-of-the-art results on both fine-grained (R2R) and high-level (REVERIE and R2R-Last) VLN benchmarks. Moreover, the visualization shows that AACL significantly improves the interpretability in action decision. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/VLN-AACL",
    "checked": true,
    "id": "137330d3030f75b01c88c14e1164d9b0d8c2dc70",
    "semantic_title": "actional atomic-concept learning for demystifying vision-language navigation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25244": {
    "title": "Probability Guided Loss for Long-Tailed Multi-Label Image Classification",
    "volume": "main",
    "abstract": "Long-tailed learning has attracted increasing attention in very recent years. Long-tailed multi-label image classification is one subtask and remains challenging and poorly researched. In this paper, we provide a fresh perspective from probability to tackle this problem. More specifically, we find that existing cost-sensitive learning methods for long-tailed multi-label classification will affect the predicted probability of positive and negative labels in varying degrees during training, and different processes of probability will affect the final performance in turn. We thus propose a probability guided loss which contains two components to control this process. One is the probability re-balancing which can flexibly adjust the process of training probability. And the other is the adaptive probability-aware focal which can further reduce the probability gap between positive and negative labels. We conduct extensive experiments on two long-tailed multi-label image classification datasets: VOC-LT and COCO-LT. The results demonstrate the rationality and superiority of our strategy",
    "checked": true,
    "id": "aa528fcfa48fe9333220012501e224cde12040fa",
    "semantic_title": "probability guided loss for long-tailed multi-label image classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25245": {
    "title": "Self-Supervised Image Denoising Using Implicit Deep Denoiser Prior",
    "volume": "main",
    "abstract": "We devise a new regularization for denoising with self-supervised learning. The regularization uses a deep image prior learned by the network, rather than a traditional predefined prior. Specifically, we treat the output of the network as a ``prior'' that we again denoise after ``re-noising.'' The network is updated to minimize the discrepancy between the twice-denoised image and its prior. We demonstrate that this regularization enables the network to learn to denoise even if it has not seen any clean images. The effectiveness of our method is based on the fact that CNNs naturally tend to capture low-level image statistics. Since our method utilizes the image prior implicitly captured by the deep denoising CNN to guide denoising, we refer to this training strategy as an Implicit Deep Denoiser Prior (IDDP). IDDP can be seen as a mixture of learning-based methods and traditional model-based denoising methods, in which regularization is adaptively formulated using the output of the network. We apply IDDP to various denoising tasks using only observed corrupted data and show that it achieves better denoising results than other self-supervised denoising methods",
    "checked": true,
    "id": "0753df5be5fabf893ae9604c34c12a0a6233a91c",
    "semantic_title": "self-supervised image denoising using implicit deep denoiser prior",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25246": {
    "title": "Accelerating the Training of Video Super-resolution Models",
    "volume": "main",
    "abstract": "Despite that convolution neural networks (CNN) have recently demonstrated high-quality reconstruction for video super-resolution (VSR), efficiently training competitive VSR models remains a challenging problem. It usually takes an order of magnitude more time than training their counterpart image models, leading to long research cycles. Existing VSR methods typically train models with fixed spatial and temporal sizes from beginning to end. The fixed sizes are usually set to large values for good performance, resulting to slow training. However, is such a rigid training strategy necessary for VSR? In this work, we show that it is possible to gradually train video models from small to large spatial/temporal sizes, \\ie, in an easy-to-hard manner. In particular, the whole training is divided into several stages and the earlier stage has smaller training spatial shape. Inside each stage, the temporal size also varies from short to long while the spatial size remains unchanged. Training is accelerated by such a multigrid training strategy, as most of computation is performed on smaller spatial and shorter temporal shapes. For further acceleration with GPU parallelization, we also investigate the large minibatch training without the loss in accuracy. Extensive experiments demonstrate that our method is capable of largely speeding up training (up to $6.2\\times$ speedup in wall-clock training time) without performance drop for various VSR models",
    "checked": true,
    "id": "060bac73d0c6cab65b91f77364645a0afe142d38",
    "semantic_title": "accelerating the training of video super-resolution models",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25247": {
    "title": "SelectAugment: Hierarchical Deterministic Sample Selection for Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation (DA) has been extensively studied to facilitate model optimization in many tasks. Prior DA works focus on designing augmentation operations themselves, while leaving selecting suitable samples for augmentation out of consideration. This might incur visual ambiguities and further induce training biases. In this paper, we propose an effective approach, dubbed SelectAugment, to select samples for augmentation in a deterministic and online manner based on the sample contents and the network training status. To facilitate the policy learning, in each batch, we exploit the hierarchy of this task by first determining the augmentation ratio and then deciding whether to augment each training sample under this ratio. We model this process as two-step decision-making and adopt Hierarchical Reinforcement Learning (HRL) to learn the selection policy. In this way, the negative effects of the randomness in selecting samples to augment can be effectively alleviated and the effectiveness of DA is improved. Extensive experiments demonstrate that our proposed SelectAugment significantly improves various off-the-shelf DA methods on image classification and fine-grained image recognition",
    "checked": true,
    "id": "44f4975b026c7c9ab934ead385afae080b20d66b",
    "semantic_title": "selectaugment: hierarchical deterministic sample selection for data augmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25248": {
    "title": "AdaCM: Adaptive ColorMLP for Real-Time Universal Photo-Realistic Style Transfer",
    "volume": "main",
    "abstract": "Photo-realistic style transfer aims at migrating the artistic style from an exemplar style image to a content image, producing a result image without spatial distortions or unrealistic artifacts. Impressive results have been achieved by recent deep models. However, deep neural network based methods are too expensive to run in real-time. Meanwhile, bilateral grid based methods are much faster but still contain artifacts like overexposure. In this work, we propose the Adaptive ColorMLP (AdaCM), an effective and efficient framework for universal photo-realistic style transfer. First, we find the complex non-linear color mapping between input and target domain can be efficiently modeled by a small multi-layer perceptron (ColorMLP) model. Then, in AdaCM, we adopt a CNN encoder to adaptively predict all parameters for the ColorMLP conditioned on each input content and style image pair. Experimental results demonstrate that AdaCM can generate vivid and high-quality stylization results. Meanwhile, our AdaCM is ultrafast and can process a 4K resolution image in 6ms on one V100 GPU",
    "checked": true,
    "id": "0b19b1569f783fc4252deb5128889b5b466afefc",
    "semantic_title": "adacm: adaptive colormlp for real-time universal photo-realistic style transfer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25249": {
    "title": "SEPT: Towards Scalable and Efficient Visual Pre-training",
    "volume": "main",
    "abstract": "Recently, the self-supervised pre-training paradigm has shown great potential in leveraging large-scale unlabeled data to improve downstream task performance. However, increasing the scale of unlabeled pre-training data in real-world scenarios requires prohibitive computational costs and faces the challenge of uncurated samples. To address these issues, we build a task-specific self-supervised pre-training framework from a data selection perspective based on a simple hypothesis that pre-training on the unlabeled samples with similar distribution to the target task can bring substantial performance gains. Buttressed by the hypothesis, we propose the first yet novel framework for Scalable and Efficient visual Pre-Training (SEPT) by introducing a retrieval pipeline for data selection. SEPT first leverage a self-supervised pre-trained model to extract the features of the entire unlabeled dataset for retrieval pipeline initialization. Then, for a specific target task, SEPT retrievals the most similar samples from the unlabeled dataset based on feature similarity for each target instance for pre-training. Finally, SEPT pre-trains the target model with the selected unlabeled samples in a self-supervised manner for target data finetuning. By decoupling the scale of pre-training and available upstream data for a target task, SEPT achieves high scalability of the upstream dataset and high efficiency of pre-training, resulting in high model architecture flexibility. Results on various downstream tasks demonstrate that SEPT can achieve competitive or even better performance compared with ImageNet pre-training while reducing the size of training samples by one magnitude without resorting to any extra annotations",
    "checked": true,
    "id": "53169794d50f7541ea4a93417443751ed76a2009",
    "semantic_title": "sept: towards scalable and efficient visual pre-training",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25250": {
    "title": "Cross-Modality Earth Mover's Distance for Visible Thermal Person Re-identification",
    "volume": "main",
    "abstract": "Visible thermal person re-identification (VT-ReID) suffers from inter-modality discrepancy and intra-identity variations. Distribution alignment is a popular solution for VT-ReID, however, it is usually restricted to the influence of the intra-identity variations. In this paper, we propose the Cross-Modality Earth Mover's Distance (CM-EMD) that can alleviate the impact of the intra-identity variations during modality alignment. CM-EMD selects an optimal transport strategy and assigns high weights to pairs that have a smaller intra-identity variation. In this manner, the model will focus on reducing the inter-modality discrepancy while paying less attention to intra-identity variations, leading to a more effective modality alignment. Moreover, we introduce two techniques to improve the advantage of CM-EMD. First, Cross-Modality Discrimination Learning (CM-DL) is designed to overcome the discrimination degradation problem caused by modality alignment. By reducing the ratio between intra-identity and inter-identity variances, CM-DL leads the model to learn more discriminative representations. Second, we construct the Multi-Granularity Structure (MGS), enabling us to align modalities from both coarse- and fine-grained levels with the proposed CM-EMD. Extensive experiments show the benefits of the proposed CM-EMD and its auxiliary techniques (CM-DL and MGS). Our method achieves state-of-the-art performance on two VT-ReID benchmarks",
    "checked": true,
    "id": "e511d05b03a4d6d3ba9c02ec1e1dbf00794ff81e",
    "semantic_title": "cross-modality earth mover's distance for visible thermal person re-identification",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25251": {
    "title": "Hypotheses Tree Building for One-Shot Temporal Sentence Localization",
    "volume": "main",
    "abstract": "Given an untrimmed video, temporal sentence localization (TSL) aims to localize a specific segment according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on dense video frame annotations, which require a tremendous amount of human effort to collect. In this paper, we target another more practical and challenging setting: one-shot temporal sentence localization (one-shot TSL), which learns to retrieve the query information among the entire video with only one annotated frame. Particularly, we propose an effective and novel tree-structure baseline for one-shot TSL, called Multiple Hypotheses Segment Tree (MHST), to capture the query-aware discriminative frame-wise information under the insufficient annotations. Each video frame is taken as the leaf-node, and the adjacent frames sharing the same visual-linguistic semantics will be merged into the upper non-leaf node for tree building. At last, each root node is an individual segment hypothesis containing the consecutive frames of its leaf-nodes. During the tree construction, we also introduce a pruning strategy to eliminate the interference of query-irrelevant nodes. With our designed self-supervised loss functions, our MHST is able to generate high-quality segment hypotheses for ranking and selection with the query. Experiments on two challenging datasets demonstrate that MHST achieves competitive performance compared to existing methods",
    "checked": true,
    "id": "d0bfac78dbab5f4318e1a065d6de413ba8ce24ed",
    "semantic_title": "hypotheses tree building for one-shot temporal sentence localization",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25252": {
    "title": "The Devil Is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-training",
    "volume": "main",
    "abstract": "The self-supervised Masked Image Modeling (MIM) schema, following \"mask-and-reconstruct\" pipeline of recovering contents from masked image, has recently captured the increasing interest in the community, owing to the excellent ability of learning visual representation from unlabeled data. Aiming at learning representations with high semantics abstracted, a group of works attempts to reconstruct non-semantic pixels with large-ratio masking strategy, which may suffer from \"over-smoothing\" problem, while others directly infuse semantics into targets in off-line way requiring extra data. Different from them, we shift the perspective to the Fourier domain which naturally has global perspective and present a new Masked Image Modeling (MIM), termed Geminated Gestalt Autoencoder (Ge^2-AE) for visual pre-training. Specifically, we equip our model with geminated decoders in charge of reconstructing image contents from both pixel and frequency space, where each other serves as not only the complementation but also the reciprocal constraints. Through this way, more robust representations can be learned in the pre-trained encoders, of which the effectiveness is confirmed by the juxtaposing experimental results on downstream recognition tasks. We also conduct several quantitative and qualitative experiments to investigate the learning behavior of our method. To our best knowledge, this is the first MIM work to solve the visual pre-training through the lens of frequency domain",
    "checked": true,
    "id": "3ab64c8e9e05fd2426d6cc51f16d931279b862df",
    "semantic_title": "the devil is in the frequency: geminated gestalt autoencoder for self-supervised visual pre-training",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25253": {
    "title": "M3AE: Multimodal Representation Learning for Brain Tumor Segmentation with Missing Modalities",
    "volume": "main",
    "abstract": "Multimodal magnetic resonance imaging (MRI) provides complementary information for sub-region analysis of brain tumors. Plenty of methods have been proposed for automatic brain tumor segmentation using four common MRI modalities and achieved remarkable performance. In practice, however, it is common to have one or more modalities missing due to image corruption, artifacts, acquisition protocols, allergy to contrast agents, or simply cost. In this work, we propose a novel two-stage framework for brain tumor segmentation with missing modalities. In the first stage, a multimodal masked autoencoder (M3AE) is proposed, where both random modalities (i.e., modality dropout) and random patches of the remaining modalities are masked for a reconstruction task, for self-supervised learning of robust multimodal representations against missing modalities. To this end, we name our framework M3AE. Meanwhile, we employ model inversion to optimize a representative full-modal image at marginal extra cost, which will be used to substitute for the missing modalities and boost performance during inference. Then in the second stage, a memory-efficient self distillation is proposed to distill knowledge between heterogenous missing-modal situations while fine-tuning the model for supervised segmentation. Our M3AE belongs to the ‘catch-all' genre where a single model can be applied to all possible subsets of modalities, thus is economic for both training and deployment. Extensive experiments on BraTS 2018 and 2020 datasets demonstrate its superior performance to existing state-of-the-art methods with missing modalities, as well as the efficacy of its components. Our code is available at: https://github.com/ccarliu/m3ae",
    "checked": true,
    "id": "0f61187d734d9dfc7cdbb5e0c8ecb6e0a2f70c85",
    "semantic_title": "m3ae: multimodal representation learning for brain tumor segmentation with missing modalities",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25254": {
    "title": "From Coarse to Fine: Hierarchical Pixel Integration for Lightweight Image Super-resolution",
    "volume": "main",
    "abstract": "Image super-resolution (SR) serves as a fundamental tool for the processing and transmission of multimedia data. Recently, Transformer-based models have achieved competitive performances in image SR. They divide images into fixed-size patches and apply self-attention on these patches to model long-range dependencies among pixels. However, this architecture design is originated for high-level vision tasks, which lacks design guideline from SR knowledge. In this paper, we aim to design a new attention block whose insights are from the interpretation of Local Attribution Map (LAM) for SR networks. Specifically, LAM presents a hierarchical importance map where the most important pixels are located in a fine area of a patch and some less important pixels are spread in a coarse area of the whole image. To access pixels in the coarse area, instead of using a very large patch size, we propose a lightweight Global Pixel Access (GPA) module that applies cross-attention with the most similar patch in an image. In the fine area, we use an Intra-Patch Self-Attention (IPSA) module to model long-range pixel dependencies in a local patch, and then a spatial convolution is applied to process the finest details. In addition, a Cascaded Patch Division (CPD) strategy is proposed to enhance perceptual quality of recovered images. Extensive experiments suggest that our method outperforms state-of-the-art lightweight SR methods by a large margin. Code is available at https://github.com/passerer/HPINet",
    "checked": true,
    "id": "f56ed9f1ca57d3fb73dd6a08c101119e278c3f65",
    "semantic_title": "from coarse to fine: hierarchical pixel integration for lightweight image super-resolution",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25255": {
    "title": "Fast Fluid Simulation via Dynamic Multi-Scale Gridding",
    "volume": "main",
    "abstract": "Recent works on learning-based frameworks for Lagrangian (i.e., particle-based) fluid simulation, though bypassing iterative pressure projection via efficient convolution operators, are still time-consuming due to excessive amount of particles. To address this challenge, we propose a dynamic multi-scale gridding method to reduce the magnitude of elements that have to be processed, by observing repeated particle motion patterns within certain consistent regions. Specifically, we hierarchically generate multi-scale micelles in Euclidean space by grouping particles that share similar motion patterns/characteristics based on super-light motion and scale estimation modules. With little internal motion variation, each micelle is modeled as a single rigid body with convolution only applied to a single representative particle. In addition, a distance-based interpolation is conducted to propagate relative motion message among micelles. With our efficient design, the network produces high visual fidelity fluid simulations with the inference time to be only 4.24 ms/frame (with 6K fluid particles), hence enables real-time human-computer interaction and animation. Experimental results on multiple datasets show that our work achieves great simulation acceleration with negligible prediction error increase",
    "checked": true,
    "id": "91c3eea164d6c32dd00647594f3ca3be21030a0d",
    "semantic_title": "fast fluid simulation via dynamic multi-scale gridding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25256": {
    "title": "TransLO: A Window-Based Masked Point Transformer Framework for Large-Scale LiDAR Odometry",
    "volume": "main",
    "abstract": "Recently, transformer architecture has gained great success in the computer vision community, such as image classification, object detection, etc. Nonetheless, its application for 3D vision remains to be explored, given that point cloud is inherently sparse, irregular, and unordered. Furthermore, existing point transformer frameworks usually feed raw point cloud of N×3 dimension into transformers, which limits the point processing scale because of their quadratic computational costs to the input size N. In this paper, we rethink the structure of point transformer. Instead of directly applying transformer to points, our network (TransLO) can process tens of thousands of points simultaneously by projecting points onto a 2D surface and then feeding them into a local transformer with linear complexity. Specifically, it is mainly composed of two components: Window-based Masked transformer with Self Attention (WMSA) to capture long-range dependencies; Masked Cross-Frame Attention (MCFA) to associate two frames and predict pose estimation. To deal with the sparsity issue of point cloud, we propose a binary mask to remove invalid and dynamic points. To our knowledge, this is the first transformer-based LiDAR odometry network. The experiment results on the KITTI odometry dataset show that our average rotation and translation RMSE achieves 0.500°/100m and 0.993% respectively. The performance of our network surpasses all recent learning-based methods and even outperforms LOAM on most evaluation sequences.Codes will be released on https://github.com/IRMVLab/TransLO",
    "checked": true,
    "id": "792798d7cfde416beedcae70cd5b5a92c4ab2737",
    "semantic_title": "translo: a window-based masked point transformer framework for large-scale lidar odometry",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25257": {
    "title": "Low-Light Video Enhancement with Synthetic Event Guidance",
    "volume": "main",
    "abstract": "Low-light video enhancement (LLVE) is an important yet challenging task with many applications such as photographing and autonomous driving. Unlike single image low-light enhancement, most LLVE methods utilize temporal information from adjacent frames to restore the color and remove the noise of the target frame. However, these algorithms, based on the framework of multi-frame alignment and enhancement, may produce multi-frame fusion artifacts when encountering extreme low light or fast motion. In this paper, inspired by the low latency and high dynamic range of events, we use synthetic events from multiple frames to guide the enhancement and restoration of low-light videos. Our method contains three stages: 1) event synthesis and enhancement, 2) event and image fusion, and 3) low-light enhancement. In this framework, we design two novel modules (event-image fusion transform and event-guided dual branch) for the second and third stages, respectively. Extensive experiments show that our method outperforms existing low-light video or single image enhancement approaches on both synthetic and real LLVE datasets. Our code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/LLVE-SEG",
    "checked": true,
    "id": "15837a406a4f6859ca89b1cbbd7b391464164195",
    "semantic_title": "low-light video enhancement with synthetic event guidance",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25258": {
    "title": "Novel Motion Patterns Matter for Practical Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "Most skeleton-based action recognition methods assume that the same type of action samples in the training set and the test set share similar motion patterns. However, action samples in real scenarios usually contain novel motion patterns which are not involved in the training set. As it is laborious to collect sufficient training samples to enumerate various types of novel motion patterns, this paper presents a practical skeleton-based action recognition task where the training set contains common motion patterns of action samples and the test set contains action samples that suffer from novel motion patterns. For this task, we present a Mask Graph Convolutional Network (Mask-GCN) to focus on learning action-specific skeleton joints that mainly convey action information meanwhile masking action-agnostic skeleton joints that convey rare action information and suffer more from novel motion patterns. Specifically, we design a policy network to learn layer-wise body masks to construct masked adjacency matrices, which guide a GCN-based backbone to learn stable yet informative action features from dynamic graph structure. Extensive experiments on our newly collected dataset verify that Mask-GCN outperforms most GCN-based methods when testing with various novel motion patterns",
    "checked": true,
    "id": "6a841c4254d8d95ef69abb365087ef3544ad0f82",
    "semantic_title": "novel motion patterns matter for practical skeleton-based action recognition",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25259": {
    "title": "EMEF: Ensemble Multi-Exposure Image Fusion",
    "volume": "main",
    "abstract": "Although remarkable progress has been made in recent years, current multi-exposure image fusion (MEF) research is still bounded by the lack of real ground truth, objective evaluation function, and robust fusion strategy. In this paper, we study the MEF problem from a new perspective. We don't utilize any synthesized ground truth, design any loss function, or develop any fusion strategy. Our proposed method EMEF takes advantage of the wisdom of multiple imperfect MEF contributors including both conventional and deep learning-based methods. Specifically, EMEF consists of two main stages: pre-train an imitator network and tune the imitator in the runtime. In the first stage, we make a unified network imitate different MEF targets in a style modulation way. In the second stage, we tune the imitator network by optimizing the style code, in order to find an optimal fusion result for each input pair. In the experiment, we construct EMEF from four state-of-the-art MEF methods and then make comparisons with the individuals and several other competitive methods on the latest released MEF benchmark dataset. The promising experimental results demonstrate that our ensemble framework can \"get the best of all worlds\". The code is available at https://github.com/medalwill/EMEF",
    "checked": true,
    "id": "7e8af97164df4a203f39f6d19c17d9b58952d93d",
    "semantic_title": "emef: ensemble multi-exposure image fusion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25260": {
    "title": "Reducing Domain Gap in Frequency and Spatial Domain for Cross-Modality Domain Adaptation on Medical Image Segmentation",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation (UDA) aims to learn a model trained on source domain and performs well on unlabeled target domain. In medical image segmentation field, most existing UDA methods depend on adversarial learning to address the domain gap between different image modalities, which is ineffective due to its complicated training process. In this paper, we propose a simple yet effective UDA method based on frequency and spatial domain transfer under multi-teacher distillation framework. In the frequency domain, we first introduce non-subsampled contourlet transform for identifying domain-invariant and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs unchanged while replacing the DVFs of the source domain images with that of the target domain images to narrow the domain gap. In the spatial domain, we propose a batch momentum update-based histogram matching strategy to reduce the domain-variant image style bias. Experiments on two commonly used cross-modality medical image segmentation datasets show that our proposed method achieves superior performance compared to state-of-the-art methods",
    "checked": true,
    "id": "ea10d0a86d0989d97088964d9dfb3f3dbc34daa2",
    "semantic_title": "reducing domain gap in frequency and spatial domain for cross-modality domain adaptation on medical image segmentation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25261": {
    "title": "DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding",
    "volume": "main",
    "abstract": "In this paper, we study the problem of visual grounding by considering both phrase extraction and grounding (PEG). In contrast to the previous phrase-known-at-test setting, PEG requires a model to extract phrases from text and locate objects from image simultaneously, which is a more practical setting in real applications. As phrase extraction can be regarded as a 1D text segmentation problem, we formulate PEG as a dual detection problem and propose a novel DQ-DETR model, which introduces dual queries to probe different features from image and text for object prediction and phrase mask prediction. Each pair of dual queries are designed to have shared positional parts but different content parts. Such a design effectively alleviates the difficulty of modality alignment between image and text (in contrast to a single query design) and empowers Transformer decoder to leverage phrase mask-guided attention to improve the performance. To evaluate the performance of PEG, we also propose a new metric CMAP (cross-modal average precision), analogous to the AP metric in object detection. The new metric overcomes the ambiguity of Recall@1 in many-box-to-one-phrase cases in phrase grounding. As a result, our PEG pre-trained DQ-DETR establishes new state-of-the-art results on all visual grounding benchmarks with a ResNet-101 backbone. For example, it achieves 91.04% and 83.51% in terms of recall rate on RefCOCO testA and testB with a ResNet-101 backbone",
    "checked": true,
    "id": "19ddeaf042fa00814a459e12d03f15801551e423",
    "semantic_title": "dq-detr: dual query detection transformer for phrase extraction and grounding",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25262": {
    "title": "Progressive Neighborhood Aggregation for Semantic Segmentation Refinement",
    "volume": "main",
    "abstract": "Multi-scale features from backbone networks have been widely applied to recover object details in segmentation tasks. Generally, the multi-level features are fused in a certain manner for further pixel-level dense prediction. Whereas, the spatial structure information is not fully explored, that is similar nearby pixels can be used to complement each other. In this paper, we investigate a progressive neighborhood aggregation (PNA) framework to refine the semantic segmentation prediction, resulting in an end-to-end solution that can perform the coarse prediction and refinement in a unified network. Specifically, we first present a neighborhood aggregation module, the neighborhood similarity matrices for each pixel are estimated on multi-scale features, which are further used to progressively aggregate the high-level feature for recovering the spatial structure. In addition, to further integrate the high-resolution details into the aggregated feature, we apply a self-aggregation module on the low-level features to emphasize important semantic information for complementing losing spatial details. Extensive experiments on five segmentation datasets, including Pascal VOC 2012, CityScapes, COCO-Stuff 10k, DeepGlobe, and Trans10k, demonstrate that the proposed framework can be cascaded into existing segmentation models providing consistent improvements. In particular, our method achieves new state-of-the-art performances on two challenging datasets, DeepGlobe and Trans10k. The code is available at https://github.com/liutinglt/PNA",
    "checked": true,
    "id": "2be38731ed22dbb6ee0ad425fe09f3dc89c856d3",
    "semantic_title": "progressive neighborhood aggregation for semantic segmentation refinement",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25263": {
    "title": "CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying",
    "volume": "main",
    "abstract": "Image inpainting aims to fill the missing hole of the input. It is hard to solve this task efficiently when facing high-resolution images due to two reasons: (1) Large reception field needs to be handled for high-resolution image inpainting. (2) The general encoder and decoder network synthesizes many background pixels synchronously due to the form of the image matrix. In this paper, we try to break the above limitations for the first time thanks to the recent development of continuous implicit representation. In detail, we down-sample and encode the degraded image to produce the spatial-adaptive parameters for each spatial patch via an attentional Fast Fourier Convolution (FFC)-based parameter generation network. Then, we take these parameters as the weights and biases of a series of multi-layer perceptron (MLP), where the input is the encoded continuous coordinates and the output is the synthesized color value. Thanks to the proposed structure, we only encode the high-resolution image in a relatively low resolution for larger reception field capturing. Then, the continuous position encoding will be helpful to synthesize the photo-realistic high-frequency textures by re-sampling the coordinate in a higher resolution. Also, our framework enables us to query the coordinates of missing pixels only in parallel, yielding a more efficient solution than the previous methods. Experiments show that the proposed method achieves real-time performance on the 2048X2048 images using a single GTX 2080 Ti GPU and can handle 4096X4096 images, with much better performance than existing state-of-the-art methods visually and numerically. The code is available at: https://github.com/NiFangBaAGe/CoordFill",
    "checked": true,
    "id": "bbefdde6de6e100794448eadd2de8fc01f147d27",
    "semantic_title": "coordfill: efficient high-resolution image inpainting via parameterized coordinate querying",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25264": {
    "title": "CCQ: Cross-Class Query Network for Partially Labeled Organ Segmentation",
    "volume": "main",
    "abstract": "Learning multi-organ segmentation from multiple partially-labeled datasets attracts increasing attention. It can be a promising solution for the scarcity of large-scale, fully labeled 3D medical image segmentation datasets. However, existing algorithms of multi-organ segmentation on partially-labeled datasets neglect the semantic relations and anatomical priors between different categories of organs, which is crucial for partially-labeled multi-organ segmentation. In this paper, we tackle the limitations above by proposing the Cross-Class Query Network (CCQ). CCQ consists of an image encoder, a cross-class query learning module, and an attentive refinement segmentation module. More specifically, the image encoder captures the long-range dependency of a single image via the transformer encoder. Cross-class query learning module first generates query vectors that represent semantic concepts of different categories and then utilizes these query vectors to find the class-relevant features of image representation for segmentation. The attentive refinement segmentation module with an attentive skip connection incorporates the high-resolution image details and eliminates the class-irrelevant noise. Extensive experiment results demonstrate that CCQ outperforms all the state-of-the-art models on the MOTS dataset, which consists of seven organ and tumor segmentation tasks. Code is available at https://github.com/Yang-007/CCQ.git",
    "checked": true,
    "id": "1f27fa335b4be16de2e43b765f9dcc97ace8dfff",
    "semantic_title": "ccq: cross-class query network for partially labeled organ segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25265": {
    "title": "Counterfactual Dynamics Forecasting – a New Setting of Quantitative Reasoning",
    "volume": "main",
    "abstract": "Rethinking and introspection are important elements of human intelligence. To mimic these capabilities, counterfactual reasoning has attracted attention of AI researchers recently, which aims to forecast the alternative outcomes for hypothetical scenarios (\"what-if\"). However, most existing approaches focused on qualitative reasoning (e.g., casual-effect relationship). It lacks a well-defined description of the differences between counterfactuals and facts, as well as how these differences evolve over time. This paper defines a new problem formulation - counterfactual dynamics forecasting - which is described in middle-level abstraction under the structural causal models (SCM) framework and derived as ordinary differential equations (ODEs) as low-level quantitative computation. Based on it, we propose a method to infer counterfactual dynamics considering the factual dynamics as demonstration. Moreover, the evolution of differences between facts and counterfactuals are modelled by an explicit temporal component. The experimental results on two dynamical systems demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": "6cb1a49f248413f851eb6c345a2006aca8e96192",
    "semantic_title": "counterfactual dynamics forecasting - a new setting of quantitative reasoning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25266": {
    "title": "Self-Decoupling and Ensemble Distillation for Efficient Segmentation",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) is a promising teacher-student learning paradigm that transfers information from a cumbersome teacher to a student network. To avoid the training cost of a large teacher network, the recent studies propose to distill knowledge from the student itself, called Self-KD. However, due to the limitations of the performance and capacity of the student, the soft-labels or features distilled by the student barely provide reliable guidance. Moreover, most of the Self-KD algorithms are specific to classification tasks based on soft-labels, and not suitable for semantic segmentation. To alleviate these contradictions, we revisit the label and feature distillation problem in segmentation, and propose Self-Decoupling and Ensemble Distillation for Efficient Segmentation (SDES). Specifically, we design a decoupled prediction ensemble distillation (DPED) algorithm that generates reliable soft-labels with multiple expert decoders, and a decoupled feature ensemble distillation (DFED) mechanism to utilize more important channel-wise feature maps for encoder learning. The extensive experiments on three public segmentation datasets demonstrate the superiority of our approach and the efficacy of each component in the framework through the ablation study",
    "checked": true,
    "id": "6b192a0023f769edb31dc191fcc9210318c558df",
    "semantic_title": "self-decoupling and ensemble distillation for efficient segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25267": {
    "title": "Token Mixing: Parameter-Efficient Transfer Learning from Image-Language to Video-Language",
    "volume": "main",
    "abstract": "Applying large scale pre-trained image-language model to video-language tasks has recently become a trend, which brings two challenges. One is how to effectively transfer knowledge from static images to dynamic videos, and the other is how to deal with the prohibitive cost of fully fine-tuning due to growing model size. Existing works that attempt to realize parameter-efficient image-language to video-language transfer learning can be categorized into two types: 1) appending a sequence of temporal transformer blocks after the 2D Vision Transformer (ViT), and 2) inserting a temporal block into the ViT architecture. While these two types of methods only require fine-tuning the newly added components, there are still many parameters to update, and they are only validated on a single video-language task. In this work, based on our analysis of the core ideas of different temporal modeling components in existing approaches, we propose a token mixing strategy to enable cross-frame interactions, which enables transferring from the pre-trained image-language model to video-language tasks through selecting and mixing a key set and a value set from the input video samples. As token mixing does not require the addition of any components or modules, we can directly partially fine-tune the pre-trained image-language model to achieve parameter-efficiency. We carry out extensive experiments to compare our proposed token mixing method with other parameter-efficient transfer learning methods. Our token mixing method outperforms other methods on both understanding tasks and generation tasks. Besides, our method achieves new records on multiple video-language tasks. The code is available at https://github.com/yuqi657/video_language_model",
    "checked": true,
    "id": "5fcb5889e7c716b2493da67b63179cb0034c0c66",
    "semantic_title": "token mixing: parameter-efficient transfer learning from image-language to video-language",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25268": {
    "title": "StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-Based 3D Object Detection",
    "volume": "main",
    "abstract": "In this paper, we propose a cross-modal distillation method named StereoDistill to narrow the gap between the stereo and LiDAR-based approaches via distilling the stereo detectors from the superior LiDAR model at the response level, which is usually overlooked in 3D object detection distillation. The key designs of StereoDistill are: the X-component Guided Distillation~(XGD) for regression and the Cross-anchor Logit Distillation~(CLD) for classification. In XGD, instead of empirically adopting a threshold to select the high-quality teacher predictions as soft targets, we decompose the predicted 3D box into sub-components and retain the corresponding part for distillation if the teacher component pilot is consistent with ground truth to largely boost the number of positive predictions and alleviate the mimicking difficulty of the student model. For CLD, we aggregate the probability distribution of all anchors at the same position to encourage the highest probability anchor rather than individually distill the distribution at the anchor level. Finally, our StereoDistill achieves state-of-the-art results for stereo-based 3D detection on the KITTI test benchmark and extensive experiments on KITTI and Argoverse Dataset validate the effectiveness",
    "checked": true,
    "id": "628766d52767f8c872047708a21cc0c320366715",
    "semantic_title": "stereodistill: pick the cream from lidar for distilling stereo-based 3d object detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25269": {
    "title": "Good Helper Is around You: Attention-Driven Masked Image Modeling",
    "volume": "main",
    "abstract": "It has been witnessed that masked image modeling (MIM) has shown a huge potential in self-supervised learning in the past year. Benefiting from the universal backbone vision transformer, MIM learns self-supervised visual representations through masking a part of patches of the image while attempting to recover the missing pixels. Most previous works mask patches of the image randomly, which underutilizes the semantic information that is beneficial to visual representation learning. On the other hand, due to the large size of the backbone, most previous works have to spend much time on pre-training. In this paper, we propose Attention-driven Masking and Throwing Strategy (AMT), which could solve both problems above. We first leverage the self-attention mechanism to obtain the semantic information of the image during the training process automatically without using any supervised methods. Masking strategy can be guided by that information to mask areas selectively, which is helpful for representation learning. Moreover, a redundant patch throwing strategy is proposed, which makes learning more efficient. As a plug-and-play module for masked image modeling, AMT improves the linear probing accuracy of MAE by 2.9% ~ 5.9% on CIFAR-10/100, STL-10, Tiny ImageNet, and ImageNet-1K, and obtains an improved performance with respect to fine-tuning accuracy of MAE and SimMIM. Moreover, this design also achieves superior performance on downstream detection and segmentation tasks",
    "checked": true,
    "id": "c58af37caedda1af6833209ef6b384d350743ffd",
    "semantic_title": "good helper is around you: attention-driven masked image modeling",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25270": {
    "title": "RADIANT: Radar-Image Association Network for 3D Object Detection",
    "volume": "main",
    "abstract": "As a direct depth sensor, radar holds promise as a tool to improve monocular 3D object detection, which suffers from depth errors, due in part to the depth-scale ambiguity. On the other hand, leveraging radar depths is hampered by difficulties in precisely associating radar returns with 3D estimates from monocular methods, effectively erasing its benefits. This paper proposes a fusion network that addresses this radar-camera association challenge. We train our network to predict the 3D offsets between radar returns and object centers, enabling radar depths to enhance the accuracy of 3D monocular detection. By using parallel radar and camera backbones, our network fuses information at both the feature level and detection level, while at the same time leveraging a state-of-the-art monocular detection technique without retraining it. Experimental results show significant improvement in mean average precision and translation error on the nuScenes dataset over monocular counterparts. Our source code is available at https://github.com/longyunf/radiant",
    "checked": true,
    "id": "9968d3c8e3b099e02f7e57ab8aeee0f95a781fdc",
    "semantic_title": "radiant: radar-image association network for 3d object detection",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25271": {
    "title": "CRIN: Rotation-Invariant Point Cloud Analysis and Rotation Estimation via Centrifugal Reference Frame",
    "volume": "main",
    "abstract": "Various recent methods attempt to implement rotation-invariant 3D deep learning by replacing the input coordinates of points with relative distances and angles. Due to the incompleteness of these low-level features, they have to undertake the expense of losing global information. In this paper, we propose the CRIN, namely Centrifugal Rotation-Invariant Network. CRIN directly takes the coordinates of points as input and transforms local points into rotation-invariant representations via centrifugal reference frames. Aided by centrifugal reference frames, each point corresponds to a discrete rotation so that the information of rotations can be implicitly stored in point features. Unfortunately, discrete points are far from describing the whole rotation space. We further introduce a continuous distribution for 3D rotations based on points. Furthermore, we propose an attention-based down-sampling strategy to sample points invariant to rotations. A relation module is adopted at last for reinforcing the long-range dependencies between sampled points and predicts the anchor point for unsupervised rotation estimation. Extensive experiments show that our method achieves rotation invariance, accurately estimates the object rotation, and obtains state-of-the-art results on rotation-augmented classification and part segmentation. Ablation studies validate the effectiveness of the network design",
    "checked": true,
    "id": "45ea3b4f2e53eaa4fb57ea679646848c48d489eb",
    "semantic_title": "crin: rotation-invariant point cloud analysis and rotation estimation via centrifugal reference frame",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25272": {
    "title": "See Your Emotion from Gait Using Unlabeled Skeleton Data",
    "volume": "main",
    "abstract": "This paper focuses on contrastive learning for gait-based emotion recognition. The existing contrastive learning approaches are rarely suitable for learning skeleton-based gait representations, which suffer from limited gait diversity and inconsistent semantics. In this paper, we propose a Cross-coordinate contrastive learning framework utilizing Ambiguity samples for self-supervised Gait-based Emotion representation (CAGE). First, we propose ambiguity transform to push positive samples into ambiguous semantic space. By learning similarities between ambiguity samples and positive samples, our model can learn higher-level semantics of the gait sequences and maintain semantic diversity. Second, to encourage learning the semantic invariance, we uniquely propose cross-coordinate contrastive learning between the Cartesian coordinate and the Spherical coordinate, which brings rich supervisory signals to learn the intrinsic semantic consistency information. Exhaustive experiments show that CAGE improves existing self-supervised methods by 5%–10% accuracy, and it achieves comparable or even superior performance to supervised methods",
    "checked": true,
    "id": "42683c65f41540177c08642aa635bce8d567537f",
    "semantic_title": "see your emotion from gait using unlabeled skeleton data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25273": {
    "title": "Learning Progressive Modality-Shared Transformers for Effective Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "Visible-Infrared Person Re-Identification (VI-ReID) is a challenging retrieval task under complex modality changes. Existing methods usually focus on extracting discriminative visual features while ignoring the reliability and commonality of visual features between different modalities. In this paper, we propose a novel deep learning framework named Progressive Modality-shared Transformer (PMT) for effective VI-ReID. To reduce the negative effect of modality gaps, we first take the gray-scale images as an auxiliary modality and propose a progressive learning strategy. Then, we propose a Modality-Shared Enhancement Loss (MSEL) to guide the model to explore more reliable identity information from modality-shared features. Finally, to cope with the problem of large intra-class differences and small inter-class differences, we propose a Discriminative Center Loss (DCL) combined with the MSEL to further improve the discrimination of reliable features. Extensive experiments on SYSU-MM01 and RegDB datasets show that our proposed framework performs better than most state-of-the-art methods. For model reproduction, we release the source code at https://github.com/hulu88/PMT",
    "checked": true,
    "id": "ee04b2b113700b71de4fe91bddc9cd10348486b7",
    "semantic_title": "learning progressive modality-shared transformers for effective visible-infrared person re-identification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25274": {
    "title": "Breaking Immutable: Information-Coupled Prototype Elaboration for Few-Shot Object Detection",
    "volume": "main",
    "abstract": "Few-shot object detection, expecting detectors to detect novel classes with a few instances, has made conspicuous progress. However, the prototypes extracted by existing meta-learning based methods still suffer from insufficient representative information and lack awareness of query images, which cannot be adaptively tailored to different query images. Firstly, only the support images are involved for extracting prototypes, resulting in scarce perceptual information of query images. Secondly, all pixels of all support images are treated equally when aggregating features into prototype vectors, thus the salient objects are overwhelmed by the cluttered background. In this paper, we propose an Information-Coupled Prototype Elaboration (ICPE) method to generate specific and representative prototypes for each query image. Concretely, a conditional information coupling module is introduced to couple information from the query branch to the support branch, strengthening the query-perceptual information in support features. Besides, we design a prototype dynamic aggregation module that dynamically adjusts intra-image and inter-image aggregation weights to highlight the salient information useful for detecting query images. Experimental results on both Pascal VOC and MS COCO demonstrate that our method achieves state-of-the-art performance in almost all settings. Code will be available at: https://github.com/lxn96/ICPE",
    "checked": true,
    "id": "b09025b0b145ad07346fd75e7b5c0429e422ec7d",
    "semantic_title": "breaking immutable: information-coupled prototype elaboration for few-shot object detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25275": {
    "title": "ParaFormer: Parallel Attention Transformer for Efficient Feature Matching",
    "volume": "main",
    "abstract": "Heavy computation is a bottleneck limiting deep-learning-based feature matching algorithms to be applied in many real-time applications. However, existing lightweight networks optimized for Euclidean data cannot address classical feature matching tasks, since sparse keypoint based descriptors are expected to be matched. This paper tackles this problem and proposes two concepts: 1) a novel parallel attention model entitled ParaFormer and 2) a graph based U-Net architecture with attentional pooling. First, ParaFormer fuses features and keypoint positions through the concept of amplitude and phase, and integrates self- and cross-attention in a parallel manner which achieves a win-win performance in terms of accuracy and efficiency. Second, with U-Net architecture and proposed attentional pooling, the ParaFormer-U variant significantly reduces computational complexity, and minimize performance loss caused by downsampling. Sufficient experiments on various applications, including homography estimation, pose estimation, and image matching, demonstrate that ParaFormer achieves state-of-the-art performance while maintaining high efficiency. The efficient ParaFormer-U variant achieves comparable performance with less than 50% FLOPs of the existing attention-based models",
    "checked": true,
    "id": "467268d70982cceb82206f526485951b0cff3e80",
    "semantic_title": "paraformer: parallel attention transformer for efficient feature matching",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25276": {
    "title": "Robust One-Shot Segmentation of Brain Tissues via Image-Aligned Style Transformation",
    "volume": "main",
    "abstract": "One-shot segmentation of brain tissues is typically a dual-model iterative learning: a registration model (reg-model) warps a carefully-labeled atlas onto unlabeled images to initialize their pseudo masks for training a segmentation model (seg-model); the seg-model revises the pseudo masks to enhance the reg-model for a better warping in the next iteration. However, there is a key weakness in such dual-model iteration that the spatial misalignment inevitably caused by the reg-model could misguide the seg-model, which makes it converge on an inferior segmentation performance eventually. In this paper, we propose a novel image-aligned style transformation to reinforce the dual-model iterative learning for robust one-shot segmentation of brain tissues. Specifically, we first utilize the reg-model to warp the atlas onto an unlabeled image, and then employ the Fourier-based amplitude exchange with perturbation to transplant the style of the unlabeled image into the aligned atlas. This allows the subsequent seg-model to learn on the aligned and style-transferred copies of the atlas instead of unlabeled images, which naturally guarantees the correct spatial correspondence of an image-mask training pair, without sacrificing the diversity of intensity patterns carried by the unlabeled images. Furthermore, we introduce a feature-aware content consistency in addition to the image-level similarity to constrain the reg-model for a promising initialization, which avoids the collapse of image-aligned style transformation in the first iteration. Experimental results on two public datasets demonstrate 1) a competitive segmentation performance of our method compared to the fully-supervised method, and 2) a superior performance over other state-of-the-art with an increase of average Dice by up to 4.67%. The source code is available at: https://github.com/JinxLv/One-shot-segmentation-via-IST",
    "checked": true,
    "id": "2ddc061eec9873bd2b38a284e21561eda1ef0756",
    "semantic_title": "robust one-shot segmentation of brain tissues via image-aligned style transformation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25277": {
    "title": "HRDoc: Dataset and Baseline Method toward Hierarchical Reconstruction of Document Structures",
    "volume": "main",
    "abstract": "The problem of document structure reconstruction refers to converting digital or scanned documents into corresponding semantic structures. Most existing works mainly focus on splitting the boundary of each element in a single document page, neglecting the reconstruction of semantic structure in multi-page documents. This paper introduces hierarchical reconstruction of document structures as a novel task suitable for NLP and CV fields. To better evaluate the system performance on the new task, we built a large-scale dataset named HRDoc, which consists of 2,500 multi-page documents with nearly 2 million semantic units. Every document in HRDoc has line-level annotations including categories and relations obtained from rule-based extractors and human annotators. Moreover, we proposed an encoder-decoder-based hierarchical document structure parsing system (DSPS) to tackle this problem. By adopting a multi-modal bidirectional encoder and a structure-aware GRU decoder with soft-mask operation, the DSPS model surpass the baseline method by a large margin. All scripts and datasets will be made publicly available at https://github.com/jfma-USTC/HRDoc",
    "checked": true,
    "id": "cef1779a38aeb0aefbc347318b22e7b5cf890c03",
    "semantic_title": "hrdoc: dataset and baseline method toward hierarchical reconstruction of document structures",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25278": {
    "title": "Semantic 3D-Aware Portrait Synthesis and Manipulation Based on Compositional Neural Radiance Field",
    "volume": "main",
    "abstract": "Recently 3D-aware GAN methods with neural radiance field have developed rapidly. However, current methods model the whole image as an overall neural radiance field, which limits the partial semantic editability of synthetic results. Since NeRF renders an image pixel by pixel, it is possible to split NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field (CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF divides the image by semantic regions and learns an independent neural radiance field for each region, and finally fuses them and renders the complete image. Thus we can manipulate the synthesized semantic regions independently, while fixing the other parts unchanged. Furthermore, CNeRF is also designed to decouple shape and texture within each semantic region. Compared to state-of-the-art 3D-aware GAN methods, our approach enables fine-grained semantic region manipulation, while maintaining high-quality 3D-consistent synthesis. The ablation studies show the effectiveness of the structure and loss function used by our method. In addition real image inversion and cartoon portrait 3D editing experiments demonstrate the application potential of our method",
    "checked": true,
    "id": "01df332f62508f976fea91cdad8010669efc5701",
    "semantic_title": "semantic 3d-aware portrait synthesis and manipulation based on compositional neural radiance field",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25279": {
    "title": "CFFT-GAN: Cross-Domain Feature Fusion Transformer for Exemplar-Based Image Translation",
    "volume": "main",
    "abstract": "Exemplar-based image translation refers to the task of generating images with the desired style, while conditioning on certain input image. Most of the current methods learn the correspondence between two input domains and lack the mining of information within the domain. In this paper, we propose a more general learning approach by considering two domain features as a whole and learning both inter-domain correspondence and intra-domain potential information interactions. Specifically, we propose a Cross-domain Feature Fusion Transformer (CFFT) to learn inter- and intra-domain feature fusion. Based on CFFT, the proposed CFFT-GAN works well on exemplar-based image translation. Moreover, CFFT-GAN is able to decouple and fuse features from multiple domains by cascading CFFT modules. We conduct rich quantitative and qualitative experiments on several image translation tasks, and the results demonstrate the superiority of our approach compared to state-of-the-art methods. Ablation studies show the importance of our proposed CFFT. Application experimental results reflect the potential of our method",
    "checked": true,
    "id": "29d35ab84788ca14e2b65a457c7fed5ee6f150a4",
    "semantic_title": "cfft-gan: cross-domain feature fusion transformer for exemplar-based image translation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25280": {
    "title": "StyleTalk: One-Shot Talking Head Generation with Controllable Speaking Styles",
    "volume": "main",
    "abstract": "Different people speak with diverse personalized speaking styles. Although existing one-shot talking head methods have made significant progress in lip sync, natural facial expressions, and stable head motions, they still cannot generate diverse speaking styles in the final talking head videos. To tackle this problem, we propose a one-shot style-controllable talking face generation framework. In a nutshell, we aim to attain a speaking style from an arbitrary reference speaking video and then drive the one-shot portrait to speak with the reference speaking style and another piece of audio. Specifically, we first develop a style encoder to extract dynamic facial motion patterns of a style reference video and then encode them into a style code. Afterward, we introduce a style-controllable decoder to synthesize stylized facial animations from the speech content and style code. In order to integrate the reference speaking style into generated videos, we design a style-aware adaptive transformer, which enables the encoded style code to adjust the weights of the feed-forward layers accordingly. Thanks to the style-aware adaptation mechanism, the reference speaking style can be better embedded into synthesized videos during decoding. Extensive experiments demonstrate that our method is capable of generating talking head videos with diverse speaking styles from only one portrait image and an audio clip while achieving authentic visual effects. Project Page: https://github.com/FuxiVirtualHuman/styletalk",
    "checked": true,
    "id": "9ef7df2ace0723583ba22bf165188cd7d2044e93",
    "semantic_title": "styletalk: one-shot talking head generation with controllable speaking styles",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25281": {
    "title": "Intriguing Findings of Frequency Selection for Image Deblurring",
    "volume": "main",
    "abstract": "Blur was naturally analyzed in the frequency domain, by estimating the latent sharp image and the blur kernel given a blurry image. Recent progress on image deblurring always designs end-to-end architectures and aims at learning the difference between blurry and sharp image pairs from pixel-level, which inevitably overlooks the importance of blur kernels. This paper reveals an intriguing phenomenon that simply applying ReLU operation on the frequency domain of a blur image followed by inverse Fourier transform, i.e., frequency selection, provides faithful information about the blur pattern (e.g., the blur direction and blur level, implicitly shows the kernel pattern). Based on this observation, we attempt to leverage kernel-level information for image deblurring networks by inserting Fourier transform, ReLU operation, and inverse Fourier transform to the standard ResBlock. 1 × 1 convolution is further added to let the network modulate flexible thresholds for frequency selection. We term our newly built block as Res FFT-ReLU Block, which takes advantages of both kernel-level and pixel-level features via learning frequency-spatial dual-domain representations. Extensive experiments are conducted to acquire a thorough analysis on the insights of the method. Moreover, after plugging the proposed block into NAFNet, we can achieve 33.85 dB in PSNR on GoPro dataset. Our method noticeably improves backbone architectures without introducing many parameters, while maintaining low computational complexity. Code is available at https://github.com/DeepMed-Lab/DeepRFT-AAAI2023",
    "checked": true,
    "id": "a6f039e6f7b0ef12afc01f0992f477ecc7bfdb31",
    "semantic_title": "intriguing findings of frequency selection for image deblurring",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25282": {
    "title": "DocEdit: Language-Guided Document Editing",
    "volume": "main",
    "abstract": "Professional document editing tools require a certain level of expertise to perform complex edit operations. To make editing tools accessible to increasingly novice users, we investigate intelligent document assistant systems that can make or suggest edits based on a user's natural language request. Such a system should be able to understand the user's ambiguous requests and contextualize them to the visual cues and textual content found in a document image to edit localized unstructured text and structured layouts. To this end, we propose a new task of language-guided localized document editing, where the user provides a document and an open vocabulary editing request, and the intelligent system produces a command that can be used to automate edits in real-world document editing software. In support of this task, we curate the DocEdit dataset, a collection of approximately 28K instances of user edit requests over PDF and design templates along with their corresponding ground truth software executable commands. To our knowledge, this is the first dataset that provides a diverse mix of edit operations with direct and indirect references to the embedded text and visual objects such as paragraphs, lists, tables, etc. We also propose DocEditor, a Transformer-based localization-aware multimodal (textual, spatial, and visual) model that performs the new task. The model attends to both document objects and related text contents which may be referred to in a user edit request, generating a multimodal embedding that is used to predict an edit command and associated bounding box localizing it. Our proposed model empirically outperforms other baseline deep learning approaches by 15-18%, providing a strong starting point for future work",
    "checked": true,
    "id": "383db93b039c2f7d743a06dc62a8db2ff1ea33f7",
    "semantic_title": "docedit: language-guided document editing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25283": {
    "title": "Progressive Few-Shot Adaptation of Generative Model with Align-Free Spatial Correlation",
    "volume": "main",
    "abstract": "In few-shot generative model adaptation, the model for target domain is prone to the mode-collapse. Recent studies attempted to mitigate the problem by matching the relationship among samples generated from the same latent codes in source and target domains. The objective is further extended to image patch-level to transfer the spatial correlation within an instance. However, the patch-level approach assumes the consistency of spatial structure between source and target domains. For example, the positions of eyes in two domains are almost identical. Thus, it can bring visual artifacts if source and target domain images are not nicely aligned. In this paper, we propose a few-shot generative model adaptation method free from such assumption, based on a motivation that generative models are progressively adapting from the source domain to the target domain. Such progressive changes allow us to identify semantically coherent image regions between instances generated by models at a neighboring training iteration to consider the spatial correlation. We also propose an importance-based patch selection strategy to reduce the complexity of patch-level correlation matching. Our method shows the state-of-the-art few-shot domain adaptation performance in the qualitative and quantitative evaluations",
    "checked": true,
    "id": "e5bed4c8dbc00e975c9d585e7592eaaf2d9d28e5",
    "semantic_title": "progressive few-shot adaptation of generative model with align-free spatial correlation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25284": {
    "title": "Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition",
    "volume": "main",
    "abstract": "A dramatic increase in real-world video volume with extremely diverse and emerging topics naturally forms a long-tailed video distribution in terms of their categories, and it spotlights the need for Video Long-Tailed Recognition (VLTR). In this work, we summarize the challenges in VLTR and explore how to overcome them. The challenges are: (1) it is impractical to re-train the whole model for high-quality features, (2) acquiring frame-wise labels requires extensive cost, and (3) long-tailed data triggers biased training. Yet, most existing works for VLTR unavoidably utilize image-level features extracted from pretrained models which are task-irrelevant, and learn by video-level labels. Therefore, to deal with such (1) task-irrelevant features and (2) video-level labels, we introduce two complementary learnable feature aggregators. Learnable layers in each aggregator are to produce task-relevant representations, and each aggregator is to assemble the snippet-wise knowledge into a video representative. Then, we propose Minority-Oriented Vicinity Expansion (MOVE) that explicitly leverages the class frequency into approximating the vicinity distributions to alleviate (3) biased training. By combining these solutions, our approach achieves state-of-the-art results on large-scale VideoLT and synthetically induced Imbalanced-MiniKinetics200. With VideoLT features from ResNet-50, it attains 18% and 58% relative improvements on head and tail classes over the previous state-of-the-art method, respectively. Code and dataset are available at https://github.com/wjun0830/MOVE",
    "checked": true,
    "id": "5020cc02130c7625836f28969aa632eb87d83f28",
    "semantic_title": "minority-oriented vicinity expansion with attentive aggregation for video long-tailed recognition",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25285": {
    "title": "Show, Interpret and Tell: Entity-Aware Contextualised Image Captioning in Wikipedia",
    "volume": "main",
    "abstract": "Humans exploit prior knowledge to describe images, and are able to adapt their explanation to specific contextual information given, even to the extent of inventing plausible explanations when contextual information and images do not match. In this work, we propose the novel task of captioning Wikipedia images by integrating contextual knowledge. Specifically, we produce models that jointly reason over Wikipedia articles, Wikimedia images and their associated descriptions to produce contextualized captions. The same Wikimedia image can be used to illustrate different articles, and the produced caption needs to be adapted to the specific context allowing us to explore the limits of the model to adjust captions to different contextual information. Dealing with out-of-dictionary words and Named Entities is a challenging task in this domain. To address this, we propose a pre-training objective, Masked Named Entity Modeling (MNEM), and show that this pretext task results to significantly improved models. Furthermore, we verify that a model pre-trained in Wikipedia generalizes well to News Captioning datasets. We further define two different test splits according to the difficulty of the captioning task. We offer insights on the role and the importance of each modality and highlight the limitations of our model",
    "checked": true,
    "id": "c7405b595f266e02f5cac24a11d83b7e341662f6",
    "semantic_title": "show, interpret and tell: entity-aware contextualised image captioning in wikipedia",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25286": {
    "title": "TaCo: Textual Attribute Recognition via Contrastive Learning",
    "volume": "main",
    "abstract": "As textual attributes like font are core design elements of document format and page style, automatic attributes recognition favor comprehensive practical applications. Existing approaches already yield satisfactory performance in differentiating disparate attributes, but they still suffer in distinguishing similar attributes with only subtle difference. Moreover, their performance drop severely in real-world scenarios where unexpected and obvious imaging distortions appear. In this paper, we aim to tackle these problems by proposing TaCo, a contrastive framework for textual attribute recognition tailored toward the most common document scenes. Specifically, TaCo leverages contrastive learning to dispel the ambiguity trap arising from vague and open-ended attributes. To realize this goal, we design the learning paradigm from three perspectives: 1) generating attribute views, 2) extracting subtle but crucial details, and 3) exploiting valued view pairs for learning, to fully unlock the pre-training potential. Extensive experiments show that TaCo surpasses the supervised counterparts and advances the state-of-the-art remarkably on multiple attribute recognition tasks. Online services of TaCo will be made available",
    "checked": true,
    "id": "69df56f5b6134ab85fbf878593eb17a968453538",
    "semantic_title": "taco: textual attribute recognition via contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25287": {
    "title": "GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds",
    "volume": "main",
    "abstract": "Current 3D single object tracking methods are typically based on VoteNet, a 3D region proposal network. Despite the success, using a single seed point feature as the cue for offset learning in VoteNet prevents high-quality 3D proposals from being generated. Moreover, seed points with different importance are treated equally in the voting process, aggravating this defect. To address these issues, we propose a novel global-local transformer voting scheme to provide more informative cues and guide the model pay more attention on potential seed points, promoting the generation of high-quality 3D proposals. Technically, a global-local transformer (GLT) module is employed to integrate object- and patch-aware prior into seed point features to effectively form strong feature representation for geometric positions of the seed points, thus providing more robust and accurate cues for offset learning. Subsequently, a simple yet effective training strategy is designed to train the GLT module. We develop an importance prediction branch to learn the potential importance of the seed points and treat the output weights vector as a training constraint term. By incorporating the above components together, we exhibit a superior tracking method GLT-T. Extensive experiments on challenging KITTI and NuScenes benchmarks demonstrate that GLT-T achieves state-of-the-art performance in the 3D single object tracking task. Besides, further ablation studies show the advantages of the proposed global-local transformer voting scheme over the original VoteNet. Code and models will be available at https://github.com/haooozi/GLT-T",
    "checked": true,
    "id": "6df6c71be7a04e3e5c2a0dae037fe38458fe1ef6",
    "semantic_title": "glt-t: global-local transformer voting for 3d single object tracking in point clouds",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25288": {
    "title": "Adapting Object Size Variance and Class Imbalance for Semi-supervised Object Detection",
    "volume": "main",
    "abstract": "Semi-supervised object detection (SSOD) attracts extensive research interest due to its great significance in reducing the data annotation effort. Collecting high-quality and category-balanced pseudo labels for unlabeled images is critical to addressing the SSOD problem. However, most of the existing pseudo-labeling-based methods depend on a large and fixed threshold to select high-quality pseudo labels from the predictions of a teacher model. Considering different object classes usually have different detection difficulty levels due to scale variance and data distribution imbalance, conventional pseudo-labeling-based methods are arduous to explore the value of unlabeled data sufficiently. To address these issues, we propose an adaptive pseudo labeling strategy, which can assign thresholds to classes with respect to their \"hardness\". This is beneficial for ensuring the high quality of easier classes and increasing the quantity of harder classes simultaneously. Besides, label refinement modules are set up based on box jittering for guaranteeing the localization quality of pseudo labels. To further improve the algorithm's robustness against scale variance and make the most of pseudo labels, we devise a joint feature-level and prediction-level consistency learning pipeline for transferring the information of the teacher model to the student model. Extensive experiments on COCO and VOC datasets indicate that our method achieves state-of-the-art performance. Especially, it brings mean average precision gains of 2.08 and 1.28 on MS-COCO dataset with 5% and 10% labeled images, respectively",
    "checked": true,
    "id": "7a30c72de1c89f3c1c38f8e4390699c86bb15626",
    "semantic_title": "adapting object size variance and class imbalance for semi-supervised object detection",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25289": {
    "title": "MIMO Is All You Need：A Strong Multi-in-Multi-Out Baseline for Video Prediction",
    "volume": "main",
    "abstract": "The mainstream of the existing approaches for video prediction builds up their models based on a Single-In-Single-Out (SISO) architecture, which takes the current frame as input to predict the next frame in a recursive manner. This way often leads to severe performance degradation when they try to extrapolate a longer period of future, thus limiting the practical use of the prediction model. Alternatively, a Multi-In-Multi-Out (MIMO) architecture that outputs all the future frames at one shot naturally breaks the recursive manner and therefore prevents error accumulation. However, only a few MIMO models for video prediction are proposed and they only achieve inferior performance due to the date. The real strength of the MIMO model in this area is not well noticed and is largely under-explored. Motivated by that, we conduct a comprehensive investigation in this paper to thoroughly exploit how far a simple MIMO architecture can go. Surprisingly, our empirical studies reveal that a simple MIMO model can outperform the state-of-the-art work with a large margin much more than expected, especially in dealing with long-term error accumulation. After exploring a number of ways and designs, we propose a new MIMO architecture based on extending the pure Transformer with local spatio-temporal blocks and a new multi-output decoder, namely MIMO-VP, to establish a new standard in video prediction. We evaluate our model in four highly competitive benchmarks. Extensive experiments show that our model wins 1st place on all the benchmarks with remarkable performance gains and surpasses the best SISO model in all aspects including efficiency, quantity, and quality. A dramatic error reduction is achieved when predicting 10 frames on Moving MNIST and Weather datasets respectively. We believe our model can serve as a new baseline to facilitate the future research of video prediction tasks. The code will be released",
    "checked": false,
    "id": "6ec6176e06ba9918ce563dc89a96303408fd97cf",
    "semantic_title": "mimo is all you need : a strong multi-in-multi-out baseline for video prediction",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25290": {
    "title": "Universe Points Representation Learning for Partial Multi-Graph Matching",
    "volume": "main",
    "abstract": "Many challenges from natural world can be formulated as a graph matching problem. Previous deep learning-based methods mainly consider a full two-graph matching setting. In this work, we study the more general partial matching problem with multi-graph cycle consistency guarantees. Building on a recent progress in deep learning on graphs, we propose a novel data-driven method (URL) for partial multi-graph matching, which uses an object-to-universe formulation and learns latent representations of abstract universe points. The proposed approach advances the state of the art in semantic keypoint matching problem, evaluated on Pascal VOC, CUB, and Willow datasets. Moreover, the set of controlled experiments on a synthetic graph matching dataset demonstrates the scalability of our method to graphs with large number of nodes and its robustness to high partiality",
    "checked": true,
    "id": "61feea09030ace88eb0ddcdcf8942b002b5adfd3",
    "semantic_title": "universe points representation learning for partial multi-graph matching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25291": {
    "title": "Robust Image Denoising of No-Flash Images Guided by Consistent Flash Images",
    "volume": "main",
    "abstract": "Images taken in low light conditions typically contain distracting noise, and eliminating such noise is a crucial computer vision problem. Additional photos captured with a camera flash can guide an image denoiser to preserve edges since the flash images often contain fine details with reduced noise. Nonetheless, a denoiser can be misled by inconsistent flash images, which have image structures (e.g., edges) that do not exist in no-flash images. Unfortunately, this disparity frequently occurs as the flash/no-flash pairs are taken in different light conditions. We propose a learning-based technique that robustly fuses the image pairs while considering their inconsistency. Our framework infers consistent flash image patches locally, which have similar image structures with the ground truth, and denoises no-flash images using the inferred ones via a combination model. We demonstrate that our technique can produce more robust results than state-of-the-art methods, given various flash/no-flash pairs with inconsistent image structures. The source code is available at https://github.com/CGLab-GIST/RIDFnF",
    "checked": true,
    "id": "81eb06c149775e0de9ec7b04bb4d4b53abf04d43",
    "semantic_title": "robust image denoising of no-flash images guided by consistent flash images",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25292": {
    "title": "Coarse2Fine: Local Consistency Aware Re-prediction for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "Weakly supervised object localization aims to localize objects of interest by using only image-level labels. Existing methods generally segment activation map by threshold to obtain mask and generate bounding box. However, the activation map is locally inconsistent, i.e., similar neighboring pixels of the same object are not equally activated, which leads to the blurred boundary issue: the localization result is sensitive to the threshold, and the mask obtained directly from the activation map loses the fine contours of the object, making it difficult to obtain a tight bounding box. In this paper, we introduce the Local Consistency Aware Re-prediction (LCAR) framework, which aims to recover the complete fine object mask from locally inconsistent activation map and hence obtain a tight bounding box. To this end, we propose the self-guided re-prediction module (SGRM), which employs a novel superpixel aggregation network to replace the post-processing of threshold segmentation. In order to derive more reliable pseudo label from the activation map to supervise the SGRM, we further design an affinity refinement module (ARM) that utilizes the original image feature to better align the activation map with the image appearance, and design a self-distillation CAM (SD-CAM) to alleviate the locator dependence on saliency. Experiments demonstrate that our LCAR outperforms the state-of-the-art on both the CUB-200-2011 and ILSVRC datasets, achieving 95.89% and 70.72% of GT-Know localization accuracy, respectively",
    "checked": true,
    "id": "9070385c89cf2385629b70e1ed7267a9391554d8",
    "semantic_title": "coarse2fine: local consistency aware re-prediction for weakly supervised object localization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25293": {
    "title": "Find Beauty in the Rare: Contrastive Composition Feature Clustering for Nontrivial Cropping Box Regression",
    "volume": "main",
    "abstract": "Automatic image cropping algorithms aim to recompose images like human-being photographers by generating the cropping boxes with improved composition quality. Cropping box regression approaches learn the beauty of composition from annotated cropping boxes. However, the bias of annotations leads to quasi-trivial recomposing results, which has an obvious tendency to the average location of training samples. The crux of this predicament is that the task is naively treated as a box regression problem, where rare samples might be dominated by normal samples, and the composition patterns of rare samples are not well exploited. Observing that similar composition patterns tend to be shared by the cropping boundaries annotated nearly, we argue to find the beauty of composition from the rare samples by clustering the samples with similar cropping boundary annotations, i.e., similar composition patterns. We propose a novel Contrastive Composition Clustering (C2C) to regularize the composition features by contrasting dynamically established similar and dissimilar pairs. In this way, common composition patterns of multiple images can be better summarized, which especially benefits the rare samples and endows our model with better generalizability to render nontrivial results. Extensive experimental results show the superiority of our model compared with prior arts. We also illustrate the philosophy of our design with an interesting analytical visualization",
    "checked": true,
    "id": "82fa44d3b927745ed1e2397eacc02596da50d0f6",
    "semantic_title": "find beauty in the rare: contrastive composition feature clustering for nontrivial cropping box regression",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25294": {
    "title": "Domain Decorrelation with Potential Energy Ranking",
    "volume": "main",
    "abstract": "Machine learning systems, especially the methods based on deep learning, enjoy great success in modern computer vision tasks under ideal experimental settings. Generally, these classic deep learning methods are built on the i.i.d. assumption, supposing the training and test data are drawn from the same distribution independently and identically. However, the aforementioned i.i.d. assumption is, in general, unavailable in the real-world scenarios, and as a result, leads to sharp performance decay of deep learning algorithms. Behind this, domain shift is one of the primary factors to be blamed. In order to tackle this problem, we propose using Potential Energy Ranking (PoER) to decouple the object feature and the domain feature in given images, promoting the learning of label-discriminative representations while filtering out the irrelevant correlations between the objects and the background. PoER employs the ranking loss in shallow layers to make features with identical category and domain labels close to each other and vice versa. This makes the neural networks aware of both objects and background characteristics, which is vital for generating domain-invariant features. Subsequently, with the stacked convolutional blocks, PoER further uses the contrastive loss to make features within the same categories distribute densely no matter domains, filtering out the domain information progressively for feature alignment. PoER reports superior performance on domain generalization benchmarks, improving the average top-1 accuracy by at least 1.20% compared to the existing methods. Moreover, we use PoER in the ECCV 2022 NICO Challenge, achieving top place with only a vanilla ResNet-18 and winning the jury award. The code has been made publicly available at: https://github.com/ForeverPs/PoER",
    "checked": true,
    "id": "55b51b993600aa23d84e1e15fb53641100e9c772",
    "semantic_title": "domain decorrelation with potential energy ranking",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25295": {
    "title": "PDRF: Progressively Deblurring Radiance Field for Fast Scene Reconstruction from Blurry Images",
    "volume": "main",
    "abstract": "We present Progressively Deblurring Radiance Field (PDRF), a novel approach to efficiently reconstruct high quality radiance fields from blurry images. While current State-of-The-Art (SoTA) scene reconstruction methods achieve photo-realistic renderings from clean source views, their performances suffer when the source views are affected by blur, which is commonly observed in the wild. Previous deblurring methods either do not account for 3D geometry, or are computationally intense. To addresses these issues, PDRF uses a progressively deblurring scheme for radiance field modeling, which can accurately model blur with 3D scene context. PDRF further uses an efficient importance sampling scheme that results in fast scene optimization. We perform extensive experiments and show that PDRF is 15X faster than previous SoTA while achieving better performance on both synthetic and real scenes",
    "checked": true,
    "id": "559f7202200ef3c187780301590e2f2b4665f047",
    "semantic_title": "pdrf: progressively deblurring radiance field for fast scene reconstruction from blurry images",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25296": {
    "title": "Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer",
    "volume": "main",
    "abstract": "This paper presents a new method for end-to-end Video Question Answering (VideoQA), aside from the current popularity of using large-scale pre-training with huge feature extractors. We achieve this with a pyramidal multimodal transformer (PMT) model, which simply incorporates a learnable word embedding layer, a few convolutional and transformer layers. We use the anisotropic pyramid to fulfill video-language interactions across different spatio-temporal scales. In addition to the canonical pyramid, which includes both bottom-up and top-down pathways with lateral connections, novel strategies are proposed to decompose the visual feature stream into spatial and temporal sub-streams at different scales and implement their interactions with the linguistic semantics while preserving the integrity of local and global semantics. We demonstrate better or on-par performances with high computational efficiency against state-of-the-art methods on five VideoQA benchmarks. Our ablation study shows the scalability of our model that achieves competitive results for text-to-video retrieval by leveraging feature extractors with reusable pre-trained weights, and also the effectiveness of the pyramid. Code available at: https://github.com/Trunpm/PMT-AAAI23",
    "checked": true,
    "id": "2837e0cab1d600fa9ca29ebdbfab239701065071",
    "semantic_title": "efficient end-to-end video question answering with pyramidal multimodal transformer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25297": {
    "title": "CL3D: Unsupervised Domain Adaptation for Cross-LiDAR 3D Detection",
    "volume": "main",
    "abstract": "Domain adaptation for Cross-LiDAR 3D detection is challenging due to the large gap on the raw data representation with disparate point densities and point arrangements. By exploring domain-invariant 3D geometric characteristics and motion patterns, we present an unsupervised domain adaptation method that overcomes above difficulties. First, we propose the Spatial Geometry Alignment module to extract similar 3D shape geometric features of the same object class to align two domains, while eliminating the effect of distinct point distributions. Second, we present Temporal Motion Alignment module to utilize motion features in sequential frames of data to match two domains. Prototypes generated from two modules are incorporated into the pseudo-label reweighting procedure and contribute to our effective self-training framework for the target domain. Extensive experiments show that our method achieves state-of-the-art performance on cross-device datasets, especially for the datasets with large gaps captured by mechanical scanning LiDARs and solid-state LiDARs in various scenes. Project homepage is at https://github.com/4DVLab/CL3D.git",
    "checked": true,
    "id": "b57872957f8f70573b62a27c4b91e0636b218983",
    "semantic_title": "cl3d: unsupervised domain adaptation for cross-lidar 3d detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25298": {
    "title": "Better and Faster: Adaptive Event Conversion for Event-Based Object Detection",
    "volume": "main",
    "abstract": "Event cameras are a kind of bio-inspired imaging sensor, which asynchronously collect sparse event streams with many advantages. In this paper, we focus on building better and faster event-based object detectors. To this end, we first propose a computationally efficient event representation Hyper Histogram, which adequately preserves both the polarity and temporal information of events. Then we devise an Adaptive Event Conversion module, which converts events into Hyper Histograms according to event density via an adaptive queue. Moreover, we introduce a novel event-based augmentation method Shadow Mosaic, which significantly improves the event sample diversity and enhances the generalization ability of detection models. We equip our proposed modules on three representative object detection models: YOLOv5, Deformable-DETR, and RetinaNet. Experimental results on three event-based detection datasets (1Mpx, Gen1, and MVSEC-NIGHTL21) demonstrate that our proposed approach outperforms other state-of-the-art methods by a large margin, while achieving a much faster running speed (< 14 ms and < 4 ms for 50 ms event data on the 1Mpx and Gen1 datasets)",
    "checked": true,
    "id": "bd82d465f64ef748e3de025a2ad0edf05f300fdd",
    "semantic_title": "better and faster: adaptive event conversion for event-based object detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25299": {
    "title": "CSTAR: Towards Compact and Structured Deep Neural Networks with Adversarial Robustness",
    "volume": "main",
    "abstract": "Model compression and model defense for deep neural networks (DNNs) have been extensively and individually studied. Considering the co-importance of model compactness and robustness in practical applications, several prior works have explored to improve the adversarial robustness of the sparse neural networks. However, the structured sparse models obtained by the existing works suffer severe performance degradation for both benign and robust accuracy, thereby causing a challenging dilemma between robustness and structuredness of compact DNNs. To address this problem, in this paper, we propose CSTAR, an efficient solution that simultaneously impose Compactness, high STructuredness and high Adversarial Robustness on the target DNN models. By formulating the structuredness and robustness requirement within the same framework, the compressed DNNs can simultaneously achieve high compression performance and strong adversarial robustness. Evaluations for various DNN models on different datasets demonstrate the effectiveness of CSTAR. Compared with the state-of-the-art robust structured pruning, CSTAR shows consistently better performance. For instance, when compressing ResNet-18 on CIFAR-10, CSTAR achieves up to 20.07% and 11.91% improvement for benign accuracy and robust accuracy, respectively. For compressing ResNet-18 with 16x compression ratio on Imagenet, CSTAR obtains 8.58% benign accuracy gain and 4.27% robust accuracy gain compared to the existing robust structured pruning",
    "checked": true,
    "id": "29bb2597be1a499d30d23844bf19d5c43f6afc12",
    "semantic_title": "cstar: towards compact and structured deep neural networks with adversarial robustness",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25300": {
    "title": "Exploring Stochastic Autoregressive Image Modeling for Visual Representation",
    "volume": "main",
    "abstract": "Autoregressive language modeling (ALM) has been successfully used in self-supervised pre-training in Natural language processing (NLP). However, this paradigm has not achieved comparable results with other self-supervised approaches in computer vision (e.g., contrastive learning, masked image modeling). In this paper, we try to find the reason why autoregressive modeling does not work well on vision tasks. To tackle this problem, we fully analyze the limitation of visual autoregressive methods and proposed a novel stochastic autoregressive image modeling (named SAIM) by the two simple designs. First, we serialize the image into patches. Second, we employ the stochastic permutation strategy to generate an effective and robust image context which is critical for vision tasks. To realize this task, we create a parallel encoder-decoder training process in which the encoder serves a similar role to the standard vision transformer focusing on learning the whole contextual information, and meanwhile the decoder predicts the content of the current position so that the encoder and decoder can reinforce each other. Our method significantly improves the performance of autoregressive image modeling and achieves the best accuracy (83.9%) on the vanilla ViT-Base model among methods using only ImageNet-1K data. Transfer performance in downstream tasks also shows that our model achieves competitive performance. Code is available at https://github.com/qiy20/SAIM",
    "checked": true,
    "id": "fa0872e45bf6c0fee528c86120f52490142efba6",
    "semantic_title": "exploring stochastic autoregressive image modeling for visual representation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25301": {
    "title": "Context-Aware Transformer for 3D Point Cloud Automatic Annotation",
    "volume": "main",
    "abstract": "3D automatic annotation has received increased attention since manually annotating 3D point clouds is laborious. However, existing methods are usually complicated, e.g., pipelined training for 3D foreground/background segmentation, cylindrical object proposals, and point completion. Furthermore, they often overlook the inter-object feature correlation that is particularly informative to hard samples for 3D annotation. To this end, we propose a simple yet effective end-to-end Context-Aware Transformer (CAT) as an automated 3D-box labeler to generate precise 3D box annotations from 2D boxes, trained with a small number of human annotations. We adopt the general encoder-decoder architecture, where the CAT encoder consists of an intra-object encoder (local) and an inter-object encoder (global), performing self-attention along the sequence and batch dimensions, respectively. The former models intra-object interactions among points and the latter extracts feature relations among different objects, thus boosting scene-level understanding. Via local and global encoders, CAT can generate high-quality 3D box annotations with a streamlined workflow, allowing it to outperform existing state-of-the-arts by up to 1.79% 3D AP on the hard task of the KITTI test set",
    "checked": true,
    "id": "ec629633af4d16bbc8db0cc3fb26964cfa33eb6c",
    "semantic_title": "context-aware transformer for 3d point cloud automatic annotation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25302": {
    "title": "Data-Efficient Image Quality Assessment with Attention-Panel Decoder",
    "volume": "main",
    "abstract": "Blind Image Quality Assessment (BIQA) is a fundamental task in computer vision, which however remains unresolved due to the complex distortion conditions and diversified image contents. To confront this challenge, we in this paper propose a novel BIQA pipeline based on the Transformer architecture, which achieves an efficient quality-aware feature representation with much fewer data. More specifically, we consider the traditional fine-tuning in BIQA as an interpretation of the pre-trained model. In this way, we further introduce a Transformer decoder to refine the perceptual information of the CLS token from different perspectives. This enables our model to establish the quality-aware feature manifold efficiently while attaining a strong generalization capability. Meanwhile, inspired by the subjective evaluation behaviors of human, we introduce a novel attention panel mechanism, which improves the model performance and reduces the prediction uncertainty simultaneously. The proposed BIQA method maintains a light-weight design with only one layer of the decoder, yet extensive experiments on eight standard BIQA datasets (both synthetic and authentic) demonstrate its superior performance to the state-of-the-art BIQA methods, i.e., achieving the SRCC values of 0.875 (vs. 0.859 in LIVEC) and 0.980 (vs. 0.969 in LIVE). Checkpoints, logs and code will be available at https://github.com/narthchin/DEIQT",
    "checked": true,
    "id": "959eac73869bb8ec9096871957c938840e0fdcd0",
    "semantic_title": "data-efficient image quality assessment with attention-panel decoder",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25303": {
    "title": "FoPro: Few-Shot Guided Robust Webly-Supervised Prototypical Learning",
    "volume": "main",
    "abstract": "Recently, webly supervised learning (WSL) has been studied to leverage numerous and accessible data from the Internet. Most existing methods focus on learning noise-robust models from web images while neglecting the performance drop caused by the differences between web domain and real-world domain. However, only by tackling the performance gap above can we fully exploit the practical value of web datasets. To this end, we propose a Few-shot guided Prototypical (FoPro) representation learning method, which only needs a few labeled examples from reality and can significantly improve the performance in the real-world domain. Specifically, we initialize each class center with few-shot real-world data as the ``realistic\" prototype. Then, the intra-class distance between web instances and ``realistic\" prototypes is narrowed by contrastive learning. Finally, we measure image-prototype distance with a learnable metric. Prototypes are polished by adjacent high-quality web images and involved in removing distant out-of-distribution samples. In experiments, FoPro is trained on web datasets with a few real-world examples guided and evaluated on real-world datasets. Our method achieves the state-of-the-art performance on three fine-grained datasets and two large-scale datasets. Compared with existing WSL methods under the same few-shot settings, FoPro still excels in real-world generalization. Code is available at https://github.com/yuleiqin/fopro",
    "checked": true,
    "id": "29a2c4d44117e51e0d2fa6ffac516222ecab254f",
    "semantic_title": "fopro: few-shot guided robust webly-supervised prototypical learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25304": {
    "title": "Exposing the Self-Supervised Space-Time Correspondence Learning via Graph Kernels",
    "volume": "main",
    "abstract": "Self-supervised space-time correspondence learning is emerging as a promising way of leveraging unlabeled video. Currently, most methods adapt contrastive learning with mining negative samples or reconstruction adapted from the image domain, which requires dense affinity across multiple frames or optical flow constraints. Moreover, video correspondence predictive models require mining more inherent properties in videos, such as structural information. In this work, we propose the VideoHiGraph, a space-time correspondence framework based on a learnable graph kernel. Concerning the video as the spatial-temporal graph, the learning objectives of VideoHiGraph are emanated in a self-supervised manner for predicting unobserved hidden graphs via graph kernel manner. We learn a representation of the temporal coherence across frames in which pairwise similarity defines the structured hidden graph, such that a biased random walk graph kernel along the sub-graph can predict long-range correspondence. Then, we learn a refined representation across frames on the node-level via a dense graph kernel. The self-supervision of the model training is formed by the structural and temporal consistency of the graph. VideoHiGraph achieves superior performance and demonstrates its robustness across the benchmark of label propagation tasks involving objects, semantic parts, keypoints, and instances. Our algorithm implementations have been made publicly available at https://github.com/zyqin19/VideoHiGraph",
    "checked": true,
    "id": "1b137feaa09a7660c26107d7cf0411e2e99058c3",
    "semantic_title": "exposing the self-supervised space-time correspondence learning via graph kernels",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25305": {
    "title": "Exploring Stroke-Level Modifications for Scene Text Editing",
    "volume": "main",
    "abstract": "Scene text editing (STE) aims to replace text with the desired one while preserving background and styles of the original text. However, due to the complicated background textures and various text styles, existing methods fall short in generating clear and legible edited text images. In this study, we attribute the poor editing performance to two problems: 1) Implicit decoupling structure. Previous methods of editing the whole image have to learn different translation rules of background and text regions simultaneously. 2) Domain gap. Due to the lack of edited real scene text images, the network can only be well trained on synthetic pairs and performs poorly on real-world images. To handle the above problems, we propose a novel network by MOdifying Scene Text image at strokE Level (MOSTEL). Firstly, we generate stroke guidance maps to explicitly indicate regions to be edited. Different from the implicit one by directly modifying all the pixels at image level, such explicit instructions filter out the distractions from background and guide the network to focus on editing rules of text regions. Secondly, we propose a Semi-supervised Hybrid Learning to train the network with both labeled synthetic images and unpaired real scene text images. Thus, the STE model is adapted to real-world datasets distributions. Moreover, two new datasets (Tamper-Syn2k and Tamper-Scene) are proposed to fill the blank of public evaluation datasets. Extensive experiments demonstrate that our MOSTEL outperforms previous methods both qualitatively and quantitatively. Datasets and code will be available at https://github.com/qqqyd/MOSTEL",
    "checked": true,
    "id": "b8fe2b02776208670906c89f8b8d361074fc87d5",
    "semantic_title": "exploring stroke-level modifications for scene text editing",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25306": {
    "title": "Unsupervised Deep Learning for Phase Retrieval via Teacher-Student Distillation",
    "volume": "main",
    "abstract": "Phase retrieval (PR) is a challenging nonlinear inverse problem in scientific imaging that involves reconstructing the phase of a signal from its intensity measurements. Recently, there has been an increasing interest in deep learning-based PR. Motivated by the challenge of collecting ground-truth (GT) images in many domains, this paper proposes a fully-unsupervised learning approach for PR, which trains an end-to-end deep model via a GT-free teacher-student online distillation framework. Specifically, a teacher model is trained using a self-expressive loss with noise resistance, while a student model is trained with a consistency loss on augmented data to exploit the teacher's dark knowledge. Additionally, we develop an enhanced unfolding network for both the teacher and student models. Extensive experiments show that our proposed approach outperforms existing unsupervised PR methods with higher computational efficiency and performs competitively against supervised methods",
    "checked": true,
    "id": "c02b92e4ffb3edd22280154b535017de8a1938c9",
    "semantic_title": "unsupervised deep learning for phase retrieval via teacher-student distillation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25307": {
    "title": "A Learnable Radial Basis Positional Embedding for Coordinate-MLPs",
    "volume": "main",
    "abstract": "We propose a novel method to enhance the performance of coordinate-MLPs (also referred to as neural fields) by learning instance-specific positional embeddings. End-to-end optimization of positional embedding parameters along with network weights leads to poor generalization performance. Instead, we develop a generic framework to learn the positional embedding based on the classic graph-Laplacian regularization, which can implicitly balance the trade-off between memorization and generalization. This framework is then used to propose a novel positional embedding scheme, where the hyperparameters are learned per coordinate (i.e instance) to deliver optimal performance. We show that the proposed embedding achieves better performance with higher stability compared to the well-established random Fourier features (RFF). Further, we demonstrate that the proposed embedding scheme yields stable gradients, enabling seamless integration into deep architectures as intermediate layers",
    "checked": true,
    "id": "05450d77727a1c58854e0b8550ca9f82e05d1cd8",
    "semantic_title": "a learnable radial basis positional embedding for coordinate-mlps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25308": {
    "title": "Action-Conditioned Generation of Bimanual Object Manipulation Sequences",
    "volume": "main",
    "abstract": "The generation of bimanual object manipulation sequences given a semantic action label has broad applications in collaborative robots or augmented reality. This relatively new problem differs from existing works that generate whole-body motions without any object interaction as it now requires the model to additionally learn the spatio-temporal relationship that exists between the human joints and object motion given said label. To tackle this task, we leverage the varying degree each muscle or joint is involved during object manipulation. For instance, the wrists act as the prime movers for the objects while the finger joints are angled to provide a firm grip. The remaining body joints are the least involved in that they are positioned as naturally and comfortably as possible. We thus design an architecture that comprises 3 main components: (i) a graph recurrent network that generates the wrist and object motion, (ii) an attention-based recurrent network that estimates the required finger joint angles given the graph configuration, and (iii) a recurrent network that reconstructs the body pose given the locations of the wrist. We evaluate our approach on the KIT Motion Capture and KIT RGBD Bimanual Manipulation datasets and show improvements over a simplified approach that treats the entire body as a single entity, and existing whole-body-only methods",
    "checked": true,
    "id": "44d343bfa354e4afa7a3bed82a7831d677e3e210",
    "semantic_title": "action-conditioned generation of bimanual object manipulation sequences",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25309": {
    "title": "Mean-Shifted Contrastive Loss for Anomaly Detection",
    "volume": "main",
    "abstract": "Deep anomaly detection methods learn representations that separate between normal and anomalous images. Although self-supervised representation learning is commonly used, small dataset sizes limit its effectiveness. It was previously shown that utilizing external, generic datasets (e.g. ImageNet classification) can significantly improve anomaly detection performance. One approach is outlier exposure, which fails when the external datasets do not resemble the anomalies. We take the approach of transferring representations pre-trained on external datasets for anomaly detection. Anomaly detection performance can be significantly improved by fine-tuning the pre-trained representations on the normal training images. In this paper, we first demonstrate and analyze that contrastive learning, the most popular self-supervised learning paradigm cannot be naively applied to pre-trained features. The reason is that pre-trained feature initialization causes poor conditioning for standard contrastive objectives, resulting in bad optimization dynamics. Based on our analysis, we provide a modified contrastive objective, the Mean-Shifted Contrastive Loss. Our method is highly effective and achieves a new state-of-the-art anomaly detection performance including 98.6% ROC-AUC on the CIFAR-10 dataset",
    "checked": true,
    "id": "7d90243c5a46430a36c5ba88627b5d254450a1e1",
    "semantic_title": "mean-shifted contrastive loss for anomaly detection",
    "citation_count": 52
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25310": {
    "title": "Two Heads Are Better than One: Image-Point Cloud Network for Depth-Based 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "Depth images and point clouds are the two most commonly used data representations for depth-based 3D hand pose estimation. Benefiting from the structuring of image data and the inherent inductive biases of the 2D Convolutional Neural Network (CNN), image-based methods are highly efficient and effective. However, treating the depth data as a 2D image inevitably ignores the 3D nature of depth data. Point cloud-based methods can better mine the 3D geometric structure of depth data. However, these methods suffer from the disorder and non-structure of point cloud data, which is computationally inefficient. In this paper, we propose an Image-Point cloud Network (IPNet) for accurate and robust 3D hand pose estimation. IPNet utilizes 2D CNN to extract visual representations in 2D image space and performs iterative correction in 3D point cloud space to exploit the 3D geometry information of depth data. In particular, we propose a sparse anchor-based \"aggregation-interaction-propagation'' paradigm to enhance point cloud features and refine the hand pose, which reduces irregular data access. Furthermore, we introduce a 3D hand model to the iterative correction process, which significantly improves the robustness of IPNet to occlusion and depth holes. Experiments show that IPNet outperforms state-of-the-art methods on three challenging hand datasets",
    "checked": true,
    "id": "0a09509004fb374d794fb4065b54e9a5dda630e6",
    "semantic_title": "two heads are better than one: image-point cloud network for depth-based 3d hand pose estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25311": {
    "title": "MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-robust Classifier",
    "volume": "main",
    "abstract": "We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findings are supported by various qualitative comparisons with the state-of-the-art on the same images sampled from ImageNet and quantitative analysis using machine perception along with a user survey of 100+ participants that endorse our synthesis quality",
    "checked": true,
    "id": "790ba79fa1fcc7446fd81047c9e86b2b4c862d7a",
    "semantic_title": "magic: mask-guided image synthesis by inverting a quasi-robust classifier",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25312": {
    "title": "Domain Generalised Faster R-CNN",
    "volume": "main",
    "abstract": "Domain generalisation (i.e. out-of-distribution generalisation) is an open problem in machine learning, where the goal is to train a model via one or more source domains, that will generalise well to unknown target domains. While the topic is attracting increasing interest, it has not been studied in detail in the context of object detection. The established approaches all operate under the covariate shift assumption, where the conditional distributions are assumed to be approximately equal across source domains. This is the first paper to address domain generalisation in the context of object detection, with a rigorous mathematical analysis of domain shift, without the covariate shift assumption. We focus on improving the generalisation ability of object detection by proposing new regularisation terms to address the domain shift that arises due to both classification and bounding box regression. Also, we include an additional consistency regularisation term to align the local and global level predictions. The proposed approach is implemented as a Domain Generalised Faster R-CNN and evaluated using four object detection datasets which provide domain metadata (GWHD, Cityscapes, BDD100K, Sim10K) where it exhibits a consistent performance improvement over the baselines. All the codes for replicating the results in this paper can be found at https://github.com/karthikiitm87/domain-generalisation.git",
    "checked": true,
    "id": "6b22618848e56abbf7dcf1d7f419b72215cefcdb",
    "semantic_title": "domain generalised faster r-cnn",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25313": {
    "title": "MIDMs: Matching Interleaved Diffusion Models for Exemplar-Based Image Translation",
    "volume": "main",
    "abstract": "We present a novel method for exemplar-based image translation, called matching interleaved diffusion models (MIDMs). Most existing methods for this task were formulated as GAN-based matching-then-generation framework. However, in this framework, matching errors induced by the difficulty of semantic matching across cross-domain, e.g., sketch and photo, can be easily propagated to the generation step, which in turn leads to the degenerated results. Motivated by the recent success of diffusion models, overcoming the shortcomings of GANs, we incorporate the diffusion models to overcome these limitations. Specifically, we formulate a diffusion-based matching-and-generation framework that interleaves cross-domain matching and diffusion steps in the latent space by iteratively feeding the intermediate warp into the noising process and denoising it to generate a translated image. In addition, to improve the reliability of diffusion process, we design confidence-aware process using cycle-consistency to consider only confident regions during translation. Experimental results show that our MIDMs generate more plausible images than state-of-the-art methods",
    "checked": true,
    "id": "1ed3b73719016f3500c5976234111b87c21837bf",
    "semantic_title": "midms: matching interleaved diffusion models for exemplar-based image translation",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25314": {
    "title": "JR2Net: Joint Monocular 3D Face Reconstruction and Reenactment",
    "volume": "main",
    "abstract": "Face reenactment and reconstruction benefit various applications in self-media, VR, etc. Recent face reenactment methods use 2D facial landmarks to implicitly retarget facial expressions and poses from driving videos to source images, while they suffer from pose and expression preservation issues for cross-identity scenarios, i.e., when the source and the driving subjects are different. Current self-supervised face reconstruction methods also demonstrate impressive results. However, these methods do not handle large expressions well, since their training data lacks samples of large expressions, and 2D facial attributes are inaccurate on such samples. To mitigate the above problems, we propose to explore the inner connection between the two tasks, i.e., using face reconstruction to provide sufficient 3D information for reenactment, and synthesizing videos paired with captured face model parameters through face reenactment to enhance the expression module of face reconstruction. In particular, we propose a novel cascade framework named JR2Net for Joint Face Reconstruction and Reenactment, which begins with the training of a coarse reconstruction network, followed by a 3D-aware face reenactment network based on the coarse reconstruction results. In the end, we train an expression tracking network based on our synthesized videos composed by image-face model parameter pairs. Such an expression tracking network can further enhance the coarse face reconstruction. Extensive experiments show that our JR2Net outperforms the state-of-the-art methods on several face reconstruction and reenactment benchmarks",
    "checked": true,
    "id": "2e51e322712413d3e6d5b8abef8d74cbfdaa2ae4",
    "semantic_title": "jr2net: joint monocular 3d face reconstruction and reenactment",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25315": {
    "title": "HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image",
    "volume": "main",
    "abstract": "Survival prediction based on whole slide images (WSIs) is a challenging task for patient-level multiple instance learning (MIL). Due to the vast amount of data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped property of WSI, it is difficult to fully explore spatial, contextual, and hierarchical interaction in the patient-level bag. Many studies adopt random sampling pre-processing strategy and WSI-level aggregation models, which inevitably lose critical prognostic information in the patient-level bag. In this work, we propose a hierarchical vision Transformer framework named HVTSurv, which can encode the local-level relative spatial information, strengthen WSI-level context-aware communication, and establish patient-level hierarchical interaction. Firstly, we design a feature pre-processing strategy, including feature rearrangement and random window masking. Then, we devise three layers to progressively obtain patient-level representation, including a local-level interaction layer adopting Manhattan distance, a WSI-level interaction layer employing spatial shuffle, and a patient-level interaction layer using attention pooling. Moreover, the design of hierarchical network helps the model become more computationally efficient. Finally, we validate HVTSurv with 3,104 patients and 3,752 WSIs across 6 cancer types from The Cancer Genome Atlas (TCGA). The average C-Index is 2.50-11.30% higher than all the prior weakly supervised methods over 6 TCGA datasets. Ablation study and attention visualization further verify the superiority of the proposed HVTSurv. Implementation is available at: https://github.com/szc19990412/HVTSurv",
    "checked": true,
    "id": "643626048cc70b1ed4ed3a0fc94d58b02b7945f7",
    "semantic_title": "hvtsurv: hierarchical vision transformer for patient-level survival prediction from whole slide image",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25316": {
    "title": "Channel Regeneration: Improving Channel Utilization for Compact DNNs",
    "volume": "main",
    "abstract": "Overparameterized deep neural networks have redundant neurons that do not contribute to the network's accuracy. In this paper, we introduce a novel channel regeneration technique that reinvigorates these redundant channels by re-initializing its batch normalization scaling factor gamma. This re-initialization of BN gamma promotes regular weight updates during training. Furthermore, we show that channel regeneration encourages the channels to contribute equally to the learned representation and further boosts the generalization accuracy. We apply our technique at regular intervals of the training cycle to improve channel utilization. The solutions proposed in previous works either raise the total computational cost or increase the model complexity. Integrating the proposed channel regeneration technique into the training methodology of efficient architectures requires minimal effort and comes at no additional cost in size or memory. Extensive experiments on several image classification and semantic segmentation benchmarks demonstrate the effectiveness of applying the channel regeneration technique to compact architectures",
    "checked": true,
    "id": "e32b11399247648b4d9f50bd54f8a383280489fb",
    "semantic_title": "channel regeneration: improving channel utilization for compact dnns",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25317": {
    "title": "Adaptive Dynamic Filtering Network for Image Denoising",
    "volume": "main",
    "abstract": "In image denoising networks, feature scaling is widely used to enlarge the receptive field size and reduce computational costs. This practice, however, also leads to the loss of high-frequency information and fails to consider within-scale characteristics. Recently, dynamic convolution has exhibited powerful capabilities in processing high-frequency information (e.g., edges, corners, textures), but previous works lack sufficient spatial contextual information in filter generation. To alleviate these issues, we propose to employ dynamic convolution to improve the learning of high-frequency and multi-scale features. Specifically, we design a spatially enhanced kernel generation (SEKG) module to improve dynamic convolution, enabling the learning of spatial context information with a very low computational complexity. Based on the SEKG module, we propose a dynamic convolution block (DCB) and a multi-scale dynamic convolution block (MDCB). The former enhances the high-frequency information via dynamic convolution and preserves low-frequency information via skip connections. The latter utilizes shared adaptive dynamic kernels and the idea of dilated convolution to achieve efficient multi-scale feature extraction. The proposed multi-dimension feature integration (MFI) mechanism further fuses the multi-scale features, providing precise and contextually enriched feature representations. Finally, we build an efficient denoising network with the proposed DCB and MDCB, named ADFNet. It achieves better performance with low computational complexity on real-world and synthetic Gaussian noisy datasets. The source code is available at https://github.com/it-hao/ADFNet",
    "checked": true,
    "id": "6f14a642033b72454afbecdb90c82872d8085dd2",
    "semantic_title": "adaptive dynamic filtering network for image denoising",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25318": {
    "title": "Edge Structure Learning via Low Rank Residuals for Robust Image Classification",
    "volume": "main",
    "abstract": "Traditional low-rank methods overlook residuals as corruptions, but we discovered that low-rank residuals actually keep image edges together with corrupt components. Therefore, filtering out such structural information could hamper the discriminative details in images, especially in heavy corruptions. In order to address this limitation, this paper proposes a novel method named ESL-LRR, which preserves image edges by finding image projections from low-rank residuals. Specifically, our approach is built in a manifold learning framework where residuals are regarded as another view of image data. Edge preserved image projections are then pursued using a dynamic affinity graph regularization to capture the more accurate similarity between residuals while suppressing the influence of corrupt ones. With this adaptive approach, the proposed method can also find image intrinsic low-rank representation, and much discriminative edge preserved projections. As a result, a new classification strategy is introduced, aligning both modalities to enhance accuracy. Experiments are conducted on several benchmark image datasets, including MNIST, LFW, and COIL100. The results show that the proposed method has clear advantages over compared state-of-the-art (SOTA) methods, such as Low-Rank Embedding (LRE), Low-Rank Preserving Projection via Graph Regularized Reconstruction (LRPP_GRR), and Feature Selective Projection (FSP) with more than 2% improvement, particularly in corrupted cases",
    "checked": true,
    "id": "925794910710207106ee192ff97475a2617bef3c",
    "semantic_title": "edge structure learning via low rank residuals for robust image classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25319": {
    "title": "Memory-Oriented Structural Pruning for Efficient Image Restoration",
    "volume": "main",
    "abstract": "Deep learning (DL) based methods have significantly pushed forward the state-of-the-art for image restoration (IR) task. Nevertheless, DL-based IR models are highly computation- and memory-intensive. The surging demands for processing higher-resolution images and multi-task paralleling in practical mobile usage further add to their computation and memory burdens. In this paper, we reveal the overlooked memory redundancy of the IR models and propose a Memory-Oriented Structural Pruning (MOSP) method. To properly compress the long-range skip connections (a major source of the memory burden), we introduce a compactor module onto each skip connection to decouple the pruning of the skip connections and the main branch. MOSP progressively prunes the original model layers and the compactors to cut down the peak memory while maintaining high IR quality. Experiments on real image denoising, image super-resolution and low-light image enhancement show that MOSP can yield models with higher memory efficiency while better preserving performance compared with baseline pruning methods",
    "checked": true,
    "id": "9f5408d8c08a2f7beb35cb16c6ccdcf67b3c7d3d",
    "semantic_title": "memory-oriented structural pruning for efficient image restoration",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25320": {
    "title": "YOLOV: Making Still Image Object Detectors Great at Video Object Detection",
    "volume": "main",
    "abstract": "Video object detection (VID) is challenging because of the high variation of object appearance as well as the diverse deterioration in some frames. On the positive side, the detection in a certain frame of a video, compared with that in a still image, can draw support from other frames. Hence, how to aggregate features across different frames is pivotal to VID problem. Most of existing aggregation algorithms are customized for two-stage detectors. However, these detectors are usually computationally expensive due to their two-stage nature. This work proposes a simple yet effective strategy to address the above concerns, which costs marginal overheads with significant gains in accuracy. Concretely, different from traditional two-stage pipeline, we select important regions after the one-stage detection to avoid processing massive low-quality candidates. Besides, we evaluate the relationship between a target frame and reference frames to guide the aggregation. We conduct extensive experiments and ablation studies to verify the efficacy of our design, and reveal its superiority over other state-of-the-art VID approaches in both effectiveness and efficiency. Our YOLOX-based model can achieve promising performance (e.g., 87.5% AP50 at over 30 FPS on the ImageNet VID dataset on a single 2080Ti GPU), making it attractive for large-scale or real-time applications. The implementation is simple, we have made the demo codes and models available at https://github.com/YuHengsss/YOLOV",
    "checked": true,
    "id": "aa289e06d4a11f50ce1f5287415a7ff752c136f6",
    "semantic_title": "yolov: making still image object detectors great at video object detection",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25321": {
    "title": "FeedFormer: Revisiting Transformer Decoder for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "With the success of Vision Transformer (ViT) in image classification, its variants have yielded great success in many downstream vision tasks. Among those, the semantic segmentation task has also benefited greatly from the advance of ViT variants. However, most studies of the transformer for semantic segmentation only focus on designing efficient transformer encoders, rarely giving attention to designing the decoder. Several studies make attempts in using the transformer decoder as the segmentation decoder with class-wise learnable query. Instead, we aim to directly use the encoder features as the queries. This paper proposes the Feature Enhancing Decoder transFormer (FeedFormer) that enhances structural information using the transformer decoder. Our goal is to decode the high-level encoder features using the lowest-level encoder feature. We do this by formulating high-level features as queries, and the lowest-level feature as the key and value. This enhances the high-level features by collecting the structural information from the lowest-level feature. Additionally, we use a simple reformation trick of pushing the encoder blocks to take the place of the existing self-attention module of the decoder to improve efficiency. We show the superiority of our decoder with various light-weight transformer-based decoders on popular semantic segmentation datasets. Despite the minute computation, our model has achieved state-of-the-art performance in the performance computation trade-off. Our model FeedFormer-B0 surpasses SegFormer-B0 with 1.8% higher mIoU and 7.1% less computation on ADE20K, and 1.7% higher mIoU and 14.4% less computation on Cityscapes, respectively. Code will be released at: https://github.com/jhshim1995/FeedFormer",
    "checked": true,
    "id": "de8d3b6ef17f924fc8d01d09bcc128c068dc1469",
    "semantic_title": "feedformer: revisiting transformer decoder for efficient semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25322": {
    "title": "Task-Specific Scene Structure Representations",
    "volume": "main",
    "abstract": "Understanding the informative structures of scenes is essential for low-level vision tasks. Unfortunately, it is difficult to obtain a concrete visual definition of the informative structures because influences of visual features are task-specific. In this paper, we propose a single general neural network architecture for extracting task-specific structure guidance for scenes. To do this, we first analyze traditional spectral clustering methods, which computes a set of eigenvectors to model a segmented graph forming small compact structures on image domains. We then unfold the traditional graph-partitioning problem into a learnable network, named Scene Structure Guidance Network (SSGNet), to represent the task-specific informative structures. The SSGNet yields a set of coefficients of eigenvectors that produces explicit feature representations of image structures. In addition, our SSGNet is light-weight (56K parameters), and can be used as a plug-and-play module for off-the-shelf architectures. We optimize the SSGNet without any supervision by proposing two novel training losses that enforce task-specific scene structure generation during training. Our main contribution is to show that such a simple network can achieve state-of-the-art results for several low-level vision applications including joint upsampling and image denoising. We also demonstrate that our SSGNet generalizes well on unseen datasets, compared to existing methods which use structural embedding frameworks. Our source codes are available at https://github.com/jsshin98/SSGNet",
    "checked": true,
    "id": "3194258a0374f03f1ba1b968fa181fc1e1456b49",
    "semantic_title": "task-specific scene structure representations",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25323": {
    "title": "Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion",
    "volume": "main",
    "abstract": "In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Finally, proper self-occlusion at each local object level and external-occlusion at the global frame level are applied using the Hidden Point Removal (HPR) algorithm that is computationally efficient. HPR is also used for adaptively controlling the point density of each object according to the object's distance from the LiDAR. Experiment results show that the proposed DR.CPO algorithm is data-efficient and model-agnostic without incurring any computational overhead. Also, DR.CPO can improve mAP performance by 2.08% when compared to the best 3D detection result known for KITTI dataset",
    "checked": false,
    "id": "41e11c36e6fd2377fe5ecb9d0d0422c635ee4be0",
    "semantic_title": "dr.cpo: diversified and realistic 3d augmentation via iterative construction, random placement, and hpr occlusion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25324": {
    "title": "SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation",
    "volume": "main",
    "abstract": "We propose a novel solution for unpaired image-to-image (I2I) translation. To translate complex images with a wide range of objects to a different domain, recent approaches often use the object annotations to perform per-class source-to-target style mapping. However, there remains a point for us to exploit in the I2I. An object in each class consists of multiple components, and all the sub-object components have different characteristics. For example, a car in CAR class consists of a car body, tires, windows and head and tail lamps, etc., and they should be handled separately for realistic I2I translation. The simplest solution to the problem will be to use more detailed annotations with sub-object component annotations than the simple object annotations, but it is not possible. The key idea of this paper is to bypass the sub-object component annotations by leveraging the original style of the input image because the original style will include the information about the characteristics of the sub-object components. Specifically, for each pixel, we use not only the per-class style gap between the source and target domains but also the pixel's original style to determine the target style of a pixel. To this end, we present Style Harmonization for unpaired I2I translation (SHUNIT). Our SHUNIT generates a new style by harmonizing the target domain style retrieved from a class memory and an original source image style. Instead of direct source-to-target style mapping, we aim for source and target styles harmonization. We validate our method with extensive experiments and achieve state-of-the-art performance on the latest benchmark sets. The source code is available online: https://github.com/bluejangbaljang/SHUNIT",
    "checked": true,
    "id": "8fe203feefec99d20ed68003e5389a46a0d7f285",
    "semantic_title": "shunit: style harmonization for unpaired image-to-image translation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25325": {
    "title": "Siamese-Discriminant Deep Reinforcement Learning for Solving Jigsaw Puzzles with Large Eroded Gaps",
    "volume": "main",
    "abstract": "Jigsaw puzzle solving has recently become an emerging research area. The developed techniques have been widely used in applications beyond puzzle solving. This paper focuses on solving Jigsaw Puzzles with Large Eroded Gaps (JPwLEG). We formulate the puzzle reassembly as a combinatorial optimization problem and propose a Siamese-Discriminant Deep Reinforcement Learning (SD2RL) to solve it. A Deep Q-network (DQN) is designed to visually understand the puzzles, which consists of two sets of Siamese Discriminant Networks, one set to perceive the pairwise relations between vertical neighbors and another set for horizontal neighbors. The proposed DQN considers not only the evidence from the incumbent fragment but also the support from its four neighbors. The DQN is trained using replay experience with carefully designed rewards to guide the search for a sequence of fragment swaps to reach the correct puzzle solution. Two JPwLEG datasets are constructed to evaluate the proposed method, and the experimental results show that the proposed SD2RL significantly outperforms state-of-the-art methods",
    "checked": true,
    "id": "14ef45b70e11c8222baf19f30cc045f54d574c23",
    "semantic_title": "siamese-discriminant deep reinforcement learning for solving jigsaw puzzles with large eroded gaps",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25326": {
    "title": "CLIPVG: Text-Guided Image Manipulation Using Differentiable Vector Graphics",
    "volume": "main",
    "abstract": "Considerable progress has recently been made in leveraging CLIP (Contrastive Language-Image Pre-Training) models for text-guided image manipulation. However, all existing works rely on additional generative models to ensure the quality of results, because CLIP alone cannot provide enough guidance information for fine-scale pixel-level changes. In this paper, we introduce CLIPVG, a text-guided image manipulation framework using differentiable vector graphics, which is also the first CLIP-based general image manipulation framework that does not require any additional generative models. We demonstrate that CLIPVG can not only achieve state-of-art performance in both semantic correctness and synthesis quality, but also is flexible enough to support various applications far beyond the capability of all existing methods",
    "checked": true,
    "id": "cda631065b5300aa3d3d3226a4da9d3a36939494",
    "semantic_title": "clipvg: text-guided image manipulation using differentiable vector graphics",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25327": {
    "title": "Compact Transformer Tracker with Correlative Masked Modeling",
    "volume": "main",
    "abstract": "Transformer framework has been showing superior performances in visual object tracking for its great strength in information aggregation across the template and search image with the well-known attention mechanism. Most recent advances focus on exploring attention mechanism variants for better information aggregation. We find these schemes are equivalent to or even just a subset of the basic self-attention mechanism. In this paper, we prove that the vanilla self-attention structure is sufficient for information aggregation, and structural adaption is unnecessary. The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image. Based on this finding, we adopt the basic vision transformer (ViT) architecture as our main tracker and concatenate the template and search image for feature embedding. To guide the encoder to capture the invariant feature for tracking, we attach a lightweight correlative masked decoder which reconstructs the original template and search image from the corresponding masked tokens. The correlative masked decoder serves as a plugin for the compact transformer tracker and is skipped in inference. Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps. Extensive experiments show the proposed compact transform tracker outperforms existing approaches, including advanced attention variants, and demonstrates the sufficiency of self-attention in tracking tasks. Our method achieves state-of-the-art performance on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/HUSTDML/CTTrack",
    "checked": true,
    "id": "b8e6e17ca2288c26b79cedff0a666e2549441ac1",
    "semantic_title": "compact transformer tracker with correlative masked modeling",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25328": {
    "title": "Text-DIAE: A Self-Supervised Degradation Invariant Autoencoder for Text Recognition and Document Enhancement",
    "volume": "main",
    "abstract": "In this paper, we propose a Text-Degradation Invariant Auto Encoder (Text-DIAE), a self-supervised model designed to tackle two tasks, text recognition (handwritten or scene-text) and document image enhancement. We start by employing a transformer-based architecture that incorporates three pretext tasks as learning objectives to be optimized during pre-training without the usage of labelled data. Each of the pretext objectives is specifically tailored for the final downstream tasks. We conduct several ablation experiments that confirm the design choice of the selected pretext tasks. Importantly, the proposed model does not exhibit limitations of previous state-of-the-art methods based on contrastive losses, while at the same time requiring substantially fewer data samples to converge. Finally, we demonstrate that our method surpasses the state-of-the-art in existing supervised and self-supervised settings in handwritten and scene text recognition and document image enhancement. Our code and trained models will be made publicly available at https://github.com/dali92002/SSL-OCR",
    "checked": true,
    "id": "0e0714e9dc5dc5ce5b4eda032d166fb07bb26ca1",
    "semantic_title": "text-diae: a self-supervised degradation invariant autoencoder for text recognition and document enhancement",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25329": {
    "title": "PUPS: Point Cloud Unified Panoptic Segmentation",
    "volume": "main",
    "abstract": "Point cloud panoptic segmentation is a challenging task that seeks a holistic solution for both semantic and instance segmentation to predict groupings of coherent points. Previous approaches treat semantic and instance segmentation as surrogate tasks, and they either use clustering methods or bounding boxes to gather instance groupings with costly computation and hand-craft designs in the instance segmentation task. In this paper, we propose a simple but effective point cloud unified panoptic segmentation (PUPS) framework, which use a set of point-level classifiers to directly predict semantic and instance groupings in an end-to-end manner. To realize PUPS, we introduce bipartite matching to our training pipeline so that our classifiers are able to exclusively predict groupings of instances, getting rid of hand-crafted designs, e.g. anchors and Non-Maximum Suppression (NMS). In order to achieve better grouping results, we utilize a transformer decoder to iteratively refine the point classifiers and develop a context-aware CutMix augmentation to overcome the class imbalance problem. As a result, PUPS achieves 1st place on the leader board of SemanticKITTI panoptic segmentation task and state-of-the-art results on nuScenes",
    "checked": true,
    "id": "72e8986a912f812b71f02e4083d349104c0db158",
    "semantic_title": "pups: point cloud unified panoptic segmentation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25330": {
    "title": "Efficient Edge-Preserving Multi-View Stereo Network for Depth Estimation",
    "volume": "main",
    "abstract": "Over the years, learning-based multi-view stereo methods have achieved great success based on their coarse-to-fine depth estimation frameworks. However, 3D CNN-based cost volume regularization inevitably leads to over-smoothing problems at object boundaries due to its smooth properties. Moreover, discrete and sparse depth hypothesis sampling exacerbates the difficulty in recovering the depth of thin structures and object boundaries. To this end, we present an Efficient edge-Preserving multi-view stereo Network (EPNet) for practical depth estimation. To keep delicate estimation at details, a Hierarchical Edge-Preserving Residual learning (HEPR) module is proposed to progressively rectify the upsampling errors and help refine multi-scale depth estimation. After that, a Cross-view Photometric Consistency (CPC) is proposed to enhance the gradient flow for detailed structures, which further boosts the estimation accuracy. Last, we design a lightweight cascade framework and inject the above two strategies into it to achieve better efficiency and performance trade-offs. Extensive experiments show that our method achieves state-of-the-art performance with fast inference speed and low memory usage. Notably, our method tops the first place on challenging Tanks and Temples advanced dataset and ETH3D high-res benchmark among all published learning-based methods. Code will be available at https://github.com/susuwj/EPNet",
    "checked": true,
    "id": "20475d345d152fa4a78b9eb37f3ffe7f2d3cfcf6",
    "semantic_title": "efficient edge-preserving multi-view stereo network for depth estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25331": {
    "title": "Referring Expression Comprehension Using Language Adaptive Inference",
    "volume": "main",
    "abstract": "Different from universal object detection, referring expression comprehension (REC) aims to locate specific objects referred to by natural language expressions. The expression provides high-level concepts of relevant visual and contextual patterns, which vary significantly with different expressions and account for only a few of those encoded in the REC model. This leads us to a question: do we really need the entire network with a fixed structure for various referring expressions? Ideally, given an expression, only expression-relevant components of the REC model are required. These components should be small in number as each expression only contains very few visual and contextual clues. This paper explores the adaptation between expressions and REC models for dynamic inference. Concretely, we propose a neat yet efficient framework named Language Adaptive Dynamic Subnets (LADS), which can extract language-adaptive subnets from the REC model conditioned on the referring expressions. By using the compact subnet, the inference can be more economical and efficient. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and Referit show that the proposed method achieves faster inference speed and higher accuracy against state-of-the-art approaches",
    "checked": true,
    "id": "d0b4d116cb264894b47e0c25cb89344c924ba9cc",
    "semantic_title": "referring expression comprehension using language adaptive inference",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25332": {
    "title": "Rethinking Data Augmentation for Single-Source Domain Generalization in Medical Image Segmentation",
    "volume": "main",
    "abstract": "Single-source domain generalization (SDG) in medical image segmentation is a challenging yet essential task as domain shifts are quite common among clinical image datasets. Previous attempts most conduct global-only/random augmentation. Their augmented samples are usually insufficient in diversity and informativeness, thus failing to cover the possible target domain distribution. In this paper, we rethink the data augmentation strategy for SDG in medical image segmentation. Motivated by the class-level representation invariance and style mutability of medical images, we hypothesize that unseen target data can be sampled from a linear combination of C (the class number) random variables, where each variable follows a location-scale distribution at the class level. Accordingly, data augmented can be readily made by sampling the random variables through a general form. On the empirical front, we implement such strategy with constrained Bezier transformation on both global and local (i.e. class-level) regions, which can largely increase the augmentation diversity. A Saliency-balancing Fusion mechanism is further proposed to enrich the informativeness by engaging the gradient information, guiding augmentation with proper orientation and magnitude. As an important contribution, we prove theoretically that our proposed augmentation can lead to an upper bound of the generalization risk on the unseen target domain, thus confirming our hypothesis. Combining the two strategies, our Saliency-balancing Location-scale Augmentation (SLAug) exceeds the state-of-the-art works by a large margin in two challenging SDG tasks. Code is available at https://github.com/Kaiseem/SLAug",
    "checked": true,
    "id": "c8368b16b978c36bcb368e673c292254d8a4cf01",
    "semantic_title": "rethinking data augmentation for single-source domain generalization in medical image segmentation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25333": {
    "title": "Hybrid Pixel-Unshuffled Network for Lightweight Image Super-resolution",
    "volume": "main",
    "abstract": "Convolutional neural network (CNN) has achieved great success on image super-resolution (SR). However, most deep CNN-based SR models take massive computations to obtain high performance. Downsampling features for multi-resolution fusion is an efficient and effective way to improve the performance of visual recognition. Still, it is counter-intuitive in the SR task, which needs to project a low-resolution input to high-resolution. In this paper, we propose a novel Hybrid Pixel-Unshuffled Network (HPUN) by introducing an efficient and effective downsampling module into the SR task. The network contains pixel-unshuffled downsampling and Self-Residual Depthwise Separable Convolutions. Specifically, we utilize pixel-unshuffle operation to downsample the input features and use grouped convolution to reduce the channels. Besides, we enhance the depthwise convolution's performance by adding the input feature to its output. The comparison findings demonstrate that, with fewer parameters and computational costs, our HPUN achieves and surpasses the state-of-the-art performance on SISR. All results are provided in the github https://github.com/Sun1992/HPUN",
    "checked": true,
    "id": "0e4aa84de53f8d6f8245ff6bc3c535ed260edfe2",
    "semantic_title": "hybrid pixel-unshuffled network for lightweight image super-resolution",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25334": {
    "title": "Learning Event-Relevant Factors for Video Anomaly Detection",
    "volume": "main",
    "abstract": "Most video anomaly detection methods discriminate events that deviate from normal patterns as anomalies. However, these methods are prone to interferences from event-irrelevant factors, such as background textures and object scale variations, incurring an increased false detection rate. In this paper, we propose to explicitly learn event-relevant factors to eliminate the interferences from event-irrelevant factors on anomaly predictions. To this end, we introduce a causal generative model to separate the event-relevant factors and event-irrelevant ones in videos, and learn the prototypes of event-relevant factors in a memory augmentation module. We design a causal objective function to optimize the causal generative model and develop a counterfactual learning strategy to guide anomaly predictions, which increases the influence of the event-relevant factors. The extensive experiments show the effectiveness of our method for video anomaly detection",
    "checked": true,
    "id": "1f1b6f5dcd5f802f2782ef15629be57202e3b629",
    "semantic_title": "learning event-relevant factors for video anomaly detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25335": {
    "title": "Superpoint Transformer for 3D Scene Instance Segmentation",
    "volume": "main",
    "abstract": "Most existing methods realize 3D instance segmentation by extending those models used for 3D object detection or 3D semantic segmentation. However, these non-straightforward methods suffer from two drawbacks: 1) Imprecise bounding boxes or unsatisfactory semantic predictions limit the performance of the overall 3D instance segmentation framework. 2) Existing method requires a time-consuming intermediate step of aggregation. To address these issues, this paper proposes a novel end-to-end 3D instance segmentation method based on Superpoint Transformer, named as SPFormer. It groups potential features from point clouds into superpoints, and directly predicts instances through query vectors without relying on the results of object detection or semantic segmentation. The key step in this framework is a novel query decoder with transformers that can capture the instance information through the superpoint cross-attention mechanism and generate the superpoint masks of the instances. Through bipartite matching based on superpoint masks, SPFormer can implement the network training without the intermediate aggregation step, which accelerates the network. Extensive experiments on ScanNetv2 and S3DIS benchmarks verify that our method is concise yet efficient. Notably, SPFormer exceeds compared state-of-the-art methods by 4.3% on ScanNetv2 hidden test set in terms of mAP and keeps fast inference speed (247ms per frame) simultaneously. Code is available at https://github.com/sunjiahao1999/SPFormer",
    "checked": true,
    "id": "e015130b27aaa95674e7c4b5dcbb5a7a7fe7ed04",
    "semantic_title": "superpoint transformer for 3d scene instance segmentation",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25336": {
    "title": "Asynchronous Event Processing with Local-Shift Graph Convolutional Network",
    "volume": "main",
    "abstract": "Event cameras are bio-inspired sensors that produce sparse and asynchronous event streams instead of frame-based images at a high-rate. Recent works utilizing graph convolutional networks (GCNs) have achieved remarkable performance in recognition tasks, which model event stream as spatio-temporal graph. However, the computational mechanism of graph convolution introduces redundant computation when aggregating neighbor features, which limits the low-latency nature of the events. And they perform a synchronous inference process, which can not achieve a fast response to the asynchronous event signals. This paper proposes a local-shift graph convolutional network (LSNet), which utilizes a novel local-shift operation equipped with a local spatio-temporal attention component to achieve efficient and adaptive aggregation of neighbor features. To improve the efficiency of pooling operation in feature extraction, we design a node-importance based parallel pooling method (NIPooling) for sparse and low-latency event data. Based on the calculated importance of each node, NIPooling can efficiently obtain uniform sampling results in parallel, which retains the diversity of event streams. Furthermore, for achieving a fast response to asynchronous event signals, an asynchronous event processing procedure is proposed to restrict the network nodes which need to recompute activations only to those affected by the new arrival event. Experimental results show that the computational cost can be reduced by nearly 9 times through using local-shift operation and the proposed asynchronous procedure can further improve the inference efficiency, while achieving state-of-the-art performance on gesture recognition and object recognition",
    "checked": true,
    "id": "29f12988ea953725e791e7467b479f7161a487ac",
    "semantic_title": "asynchronous event processing with local-shift graph convolutional network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25337": {
    "title": "DENet: Disentangled Embedding Network for Visible Watermark Removal",
    "volume": "main",
    "abstract": "Adding visible watermark into image is a common copyright protection method of medias. Meanwhile, public research on watermark removal can be utilized as an adversarial technology to help the further development of watermarking. Existing watermark removal methods mainly adopt multi-task learning networks, which locate the watermark and restore the background simultaneously. However, these approaches view the task as an image-to-image reconstruction problem, where they only impose supervision after the final output, making the high-level semantic features shared between different tasks. To this end, inspired by the two-stage coarse-refinement network, we propose a novel contrastive learning mechanism to disentangle the high-level embedding semantic information of the images and watermarks, driving the respective network branch more oriented. Specifically, the proposed mechanism is leveraged for watermark image decomposition, which aims to decouple the clean image and watermark hints in the high-level embedding space. This can guarantee the learning representation of the restored image enjoy more task-specific cues. In addition, we introduce a self-attention-based enhancement module, which promotes the network's ability to capture semantic information among different regions, leading to further improvement on the contrastive learning mechanism. To validate the effectiveness of our proposed method, extensive experiments are conducted on different challenging benchmarks. Experimental evaluations show that our approach can achieve state-of-the-art performance and yield high-quality images. The code is available at: https://github.com/lianchengmingjue/DENet",
    "checked": true,
    "id": "5720ed9d22f2f83a75e7f8efb7b466691f631138",
    "semantic_title": "denet: disentangled embedding network for visible watermark removal",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25338": {
    "title": "Deep Manifold Attack on Point Clouds via Parameter Plane Stretching",
    "volume": "main",
    "abstract": "Adversarial attack on point clouds plays a vital role in evaluating and improving the adversarial robustness of 3D deep learning models. Current attack methods are mainly applied by point perturbation in a non-manifold manner. In this paper, we formulate a novel manifold attack, which deforms the underlying 2-manifold surfaces via parameter plane stretching to generate adversarial point clouds. First, we represent the mapping between the parameter plane and underlying surface using generative-based networks. Second, the stretching is learned in the 2D parameter domain such that the generated 3D point cloud fools a pretrained classifier with minimal geometric distortion. Extensive experiments show that adversarial point clouds generated by manifold attack are smooth, undefendable and transferable, and outperform those samples generated by the state-of-the-art non-manifold ones",
    "checked": true,
    "id": "38a454f66e8fca40a342f5edb3b1d433eb091971",
    "semantic_title": "deep manifold attack on point clouds via parameter plane stretching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25339": {
    "title": "Fair Generative Models via Transfer Learning",
    "volume": "main",
    "abstract": "This work addresses fair generative models. Dataset biases have been a major cause of unfairness in deep generative models. Previous work had proposed to augment large, biased datasets with small, unbiased reference datasets. Under this setup, a weakly-supervised approach has been proposed, which achieves state-of-the-art quality and fairness in generated samples. In our work, based on this setup, we propose a simple yet effective approach. Specifically, first, we propose fairTL, a transfer learning approach to learn fair generative models. Under fairTL, we pre-train the generative model with the available large, biased datasets and subsequently adapt the model using the small, unbiased reference dataset. We find that our fairTL can learn expressive sample generation during pre-training, thanks to the large (biased) dataset. This knowledge is then transferred to the target model during adaptation, which also learns to capture the underlying fair distribution of the small reference dataset. Second, we propose fairTL++, where we introduce two additional innovations to improve upon fairTL: (i) multiple feedback and (ii) Linear-Probing followed by Fine-Tuning (LP-FT). Taking one step further, we consider an alternative, challenging setup when only a pre-trained (potentially biased) model is available but the dataset that was used to pre-train the model is inaccessible. We demonstrate that our proposed fairTL and fairTL++ remain very effective under this setup. We note that previous work requires access to the large, biased datasets and is incapable of handling this more challenging setup. Extensive experiments show that fairTL and fairTL++ achieve state-of-the-art in both quality and fairness of generated samples. The code and additional resources can be found at bearwithchris.github.io/fairTL/",
    "checked": true,
    "id": "c77f32cb3a030c04de992be40fdf882690afeef8",
    "semantic_title": "fair generative models via transfer learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25340": {
    "title": "Learning Context-Aware Classifier for Semantic Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation is still a challenging task for parsing diverse contexts in different scenes, thus the fixed classifier might not be able to well address varying feature distributions during testing. Different from the mainstream literature where the efficacy of strong backbones and effective decoder heads has been well studied, in this paper, additional contextual hints are instead exploited via learning a context-aware classifier whose content is data-conditioned, decently adapting to different latent distributions. Since only the classifier is dynamically altered, our method is model-agnostic and can be easily applied to generic segmentation models. Notably, with only negligible additional parameters and +2\\% inference time, decent performance gain has been achieved on both small and large models with challenging benchmarks, manifesting substantial practical merits brought by our simple yet effective method. The implementation is available at https://github.com/tianzhuotao/CAC",
    "checked": true,
    "id": "4ca374400fc1dd1078ce39c942ff8df562fb163b",
    "semantic_title": "learning context-aware classifier for semantic segmentation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25341": {
    "title": "TopicFM: Robust and Interpretable Topic-Assisted Feature Matching",
    "volume": "main",
    "abstract": "This study addresses an image-matching problem in challenging cases, such as large scene variations or textureless scenes. To gain robustness to such situations, most previous studies have attempted to encode the global contexts of a scene via graph neural networks or transformers. However, these contexts do not explicitly represent high-level contextual information, such as structural shapes or semantic instances; therefore, the encoded features are still not sufficiently discriminative in challenging scenes. We propose a novel image-matching method that applies a topic-modeling strategy to encode high-level contexts in images. The proposed method trains latent semantic instances called topics. It explicitly models an image as a multinomial distribution of topics, and then performs probabilistic feature matching. This approach improves the robustness of matching by focusing on the same semantic areas between the images. In addition, the inferred topics provide interpretability for matching the results, making our method explainable. Extensive experiments on outdoor and indoor datasets show that our method outperforms other state-of-the-art methods, particularly in challenging cases",
    "checked": true,
    "id": "1df8ce9e21c544a8ba0911e3e7825abc752236eb",
    "semantic_title": "topicfm: robust and interpretable topic-assisted feature matching",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25342": {
    "title": "Learning Fractals by Gradient Descent",
    "volume": "main",
    "abstract": "Fractals are geometric shapes that can display complex and self-similar patterns found in nature (e.g., clouds and plants). Recent works in visual recognition have leveraged this property to create random fractal images for model pre-training. In this paper, we study the inverse problem --- given a target image (not necessarily a fractal), we aim to generate a fractal image that looks like it. We propose a novel approach that learns the parameters underlying a fractal image via gradient descent. We show that our approach can find fractal parameters of high visual quality and be compatible with different loss functions, opening up several potentials, e.g., learning fractals for downstream tasks, scientific understanding, etc",
    "checked": true,
    "id": "8715975820cbacdc844a196e2f2f471cca982ccb",
    "semantic_title": "learning fractals by gradient descent",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25343": {
    "title": "Leveraging Weighted Cross-Graph Attention for Visual and Semantic Enhanced Video Captioning Network",
    "volume": "main",
    "abstract": "Video captioning has become a broad and interesting research area. Attention-based encoder-decoder methods are extensively used for caption generation. However, these methods mostly utilize the visual attentive feature to highlight the video regions while overlooked the semantic features of the available captions. These semantic features contain significant information that helps to generate highly informative human description-like captions. Therefore, we propose a novel visual and semantic enhanced video captioning network, named as VSVCap, that efficiently utilizes multiple ground truth captions. We aim to generate captions that are visually and semantically enhanced by exploiting both video and text modalities. To achieve this, we propose a fine-grained cross-graph attention mechanism that captures detailed graph embedding correspondence between visual graphs and textual knowledge graphs. We have performed node-level matching and structure-level reasoning between the weighted regional graph and knowledge graph. The proposed network achieves promising results on three benchmark datasets, i.e., YouTube2Text, MSR-VTT, and VATEX. The experimental results show that our network accurately captures all key objects, relationships, and semantically enhanced events of a video to generate human annotation-like captions",
    "checked": true,
    "id": "a5e3cf08174095cbe4cec418d19b8fc260af49cb",
    "semantic_title": "leveraging weighted cross-graph attention for visual and semantic enhanced video captioning network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25344": {
    "title": "Doodle to Object: Practical Zero-Shot Sketch-Based 3D Shape Retrieval",
    "volume": "main",
    "abstract": "Zero-shot (ZS) sketch-based three-dimensional (3D) shape retrieval (SBSR) is challenging due to the abstraction of sketches, cross-domain discrepancies between two-dimensional sketches and 3D shapes, and ZS-driven semantic knowledge transference from seen to unseen categories. Extant SBSR datasets suffer from lack of data, and no current SBSR methods consider ZS scenarios. In this paper, we contribute a new Doodle2Object (D2O) dataset consisting of 8,992 3D shapes and over 7M sketches spanning 50 categories. Then, we propose a novel prototype contrastive learning (PCL) method that effectively extracts features from different domains and adapts them to unseen categories. Specifically, our PCL method combines the ideas of contrastive and cluster-based prototype learning, and several randomly selected prototypes of different classes are assigned to each sample. By comparing these prototypes, a given sample can be moved closer to the same semantic class of samples while moving away from negative ones. Extensive experiments on two common SBSR benchmarks and our D2O dataset demonstrate the efficacy of the proposed PCL method for ZS-SBSR. Resource is available at https://github.com/yigohw/doodle2object",
    "checked": true,
    "id": "d658819f2aad7f8fe5ec951c595d432f0d8db33f",
    "semantic_title": "doodle to object: practical zero-shot sketch-based 3d shape retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25345": {
    "title": "Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning",
    "volume": "main",
    "abstract": "For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordinal layout). The soft layout constraint is realized by constraining that the proxy layout should always produce unimodal proxy-to-proxies similarity distribution for each proxy (i.e., to be a relaxed ordinal layout). Experiments show that the proposed CPL method outperforms previous deep ordinal classification methods under the same setting of feature extractor",
    "checked": true,
    "id": "920ba18779c5d90146c4ce9511253429b858219e",
    "semantic_title": "controlling class layout for deep ordinal classification via constrained proxies learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25346": {
    "title": "Dual Memory Aggregation Network for Event-Based Object Detection with Learnable Representation",
    "volume": "main",
    "abstract": "Event-based cameras are bio-inspired sensors that capture brightness change of every pixel in an asynchronous manner. Compared with frame-based sensors, event cameras have microsecond-level latency and high dynamic range, hence showing great potential for object detection under high-speed motion and poor illumination conditions. Due to sparsity and asynchronism nature with event streams, most of existing approaches resort to hand-crafted methods to convert event data into 2D grid representation. However, they are sub-optimal in aggregating information from event stream for object detection. In this work, we propose to learn an event representation optimized for event-based object detection. Specifically, event streams are divided into grids in the x-y-t coordinates for both positive and negative polarity, producing a set of pillars as 3D tensor representation. To fully exploit information with event streams to detect objects, a dual-memory aggregation network (DMANet) is proposed to leverage both long and short memory along event streams to aggregate effective information for object detection. Long memory is encoded in the hidden state of adaptive convLSTMs while short memory is modeled by computing spatial-temporal correlation between event pillars at neighboring time intervals. Extensive experiments on the recently released event-based automotive detection dataset demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "d902c86c7a1cbdac013523488adf85942aa11169",
    "semantic_title": "dual memory aggregation network for event-based object detection with learnable representation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25347": {
    "title": "Text to Point Cloud Localization with Relation-Enhanced Transformer",
    "volume": "main",
    "abstract": "Automatically localizing a position based on a few natural language instructions is essential for future robots to communicate and collaborate with humans. To approach this goal, we focus on a text-to-point-cloud cross-modal localization problem. Given a textual query, it aims to identify the described location from city-scale point clouds. The task involves two challenges. 1) In city-scale point clouds, similar ambient instances may exist in several locations. Searching each location in a huge point cloud with only instances as guidance may lead to less discriminative signals and incorrect results. 2) In textual descriptions, the hints are provided separately. In this case, the relations among those hints are not explicitly described, leaving the difficulties of learning relations to the agent itself. To alleviate the two challenges, we propose a unified Relation-Enhanced Transformer (RET) to improve representation discriminability for both point cloud and nature language queries. The core of the proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism, which explicitly encodes instance (hint)-wise relations for the two modalities. Moreover, we propose a fine-grained cross-modal matching method to further refine the location predictions in a subsequent instance-hint matching stage. Experimental results on the KITTI360Pose dataset demonstrate that our approach surpasses the previous state-of-the-art method by large margins",
    "checked": true,
    "id": "23a24e1740bf1b5bd5f39b18114b65ceabed1db0",
    "semantic_title": "text to point cloud localization with relation-enhanced transformer",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25348": {
    "title": "UCoL: Unsupervised Learning of Discriminative Facial Representations via Uncertainty-Aware Contrast",
    "volume": "main",
    "abstract": "This paper presents Uncertainty-aware Contrastive Learning (UCoL): a fully unsupervised framework for discriminative facial representation learning. Our UCoL is built upon a momentum contrastive network, referred to as Dual-path Momentum Network. Specifically, two flows of pairwise contrastive training are conducted simultaneously: one is formed with intra-instance self augmentation, and the other is to identify positive pairs collected by online pairwise prediction. We introduce a novel uncertainty-aware consistency K-nearest neighbors algorithm to generate predicted positive pairs, which enables efficient discriminative learning from large-scale open-world unlabeled data. Experiments show that UCoL significantly improves the baselines of unsupervised models and performs on par with the semi-supervised and supervised face representation learning methods",
    "checked": true,
    "id": "77f6e57c4c5d73fe7805c662381ebad5ed643789",
    "semantic_title": "ucol: unsupervised learning of discriminative facial representations via uncertainty-aware contrast",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25349": {
    "title": "Calibrated Teacher for Sparsely Annotated Object Detection",
    "volume": "main",
    "abstract": "Fully supervised object detection requires training images in which all instances are annotated. This is actually impractical due to the high labor and time costs and the unavoidable missing annotations. As a result, the incomplete annotation in each image could provide misleading supervision and harm the training. Recent works on sparsely annotated object detection alleviate this problem by generating pseudo labels for the missing annotations. Such a mechanism is sensitive to the threshold of the pseudo label score. However, the effective threshold is different in different training stages and among different object detectors. Therefore, the current methods with fixed thresholds have sub-optimal performance, and are difficult to be applied to other detectors. In order to resolve this obstacle, we propose a Calibrated Teacher, of which the confidence estimation of the prediction is well calibrated to match its real precision. In this way, different detectors in different training stages would share a similar distribution of the output confidence, so that multiple detectors could share the same fixed threshold and achieve better performance. Furthermore, we present a simple but effective Focal IoU Weight (FIoU) for the classification loss. FIoU aims at reducing the loss weight of false negative samples caused by the missing annotation, and thus works as the complement of the teacher-student paradigm. Extensive experiments show that our methods set new state-of-the-art under all different sparse settings in COCO. Code will be available at https://github.com/Whileherham/CalibratedTeacher",
    "checked": true,
    "id": "045465a04985227f05a437ecc459fbc8b1cb0478",
    "semantic_title": "calibrated teacher for sparsely annotated object detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25350": {
    "title": "Towards Real-Time Panoptic Narrative Grounding by an End-to-End Grounding Network",
    "volume": "main",
    "abstract": "Panoptic Narrative Grounding (PNG) is an emerging cross-modal grounding task, which locates the target regions of an image corresponding to the text description. Existing approaches for PNG are mainly based on a two-stage paradigm, which is computationally expensive. In this paper, we propose a one-stage network for real-time PNG, termed End-to-End Panoptic Narrative Grounding network (EPNG), which directly generates masks for referents. Specifically, we propose two innovative designs, i.e., Locality-Perceptive Attention (LPA) and a bidirectional Semantic Alignment Loss (SAL), to properly handle the many-to-many relationship between textual expressions and visual objects. LPA embeds the local spatial priors into attention modeling, i.e., a pixel may belong to multiple masks at different scales, thereby improving segmentation. To help understand the complex semantic relationships, SAL proposes a bidirectional contrastive objective to regularize the semantic consistency inter modalities. Extensive experiments on the PNG benchmark dataset demonstrate the effectiveness and efficiency of our method. Compared to the single-stage baseline, our method achieves a significant improvement of up to 9.4% accuracy. More importantly, our EPNG is 10 times faster than the two-stage model. Meanwhile, the generalization ability of EPNG is also validated by zero-shot experiments on other grounding tasks. The source codes and trained models for all our experiments are publicly available at https://github.com/Mr-Neko/EPNG.git",
    "checked": true,
    "id": "436f2d681c3448b6d10cea53238974987d71c0f2",
    "semantic_title": "towards real-time panoptic narrative grounding by an end-to-end grounding network",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25351": {
    "title": "LeNo: Adversarial Robust Salient Object Detection Networks with Learnable Noise",
    "volume": "main",
    "abstract": "Pixel-wise prediction with deep neural network has become an effective paradigm for salient object detection (SOD) and achieved remarkable performance. However, very few SOD models are robust against adversarial attacks which are visually imperceptible for human visual attention. The previous work robust saliency (ROSA) shuffles the pre-segmented superpixels and then refines the coarse saliency map by the densely connected conditional random field (CRF). Different from ROSA that rely on various pre- and post-processings, this paper proposes a light-weight Learnable Noise (LeNo) to defend adversarial attacks for SOD models. LeNo preserves accuracy of SOD models on both adversarial and clean images, as well as inference speed. In general, LeNo consists of a simple shallow noise and noise estimation that embedded in the encoder and decoder of arbitrary SOD networks respectively. Inspired by the center prior of human visual attention mechanism, we initialize the shallow noise with a cross-shaped gaussian distribution for better defense against adversarial attacks. Instead of adding additional network components for post-processing, the proposed noise estimation modifies only one channel of the decoder. With the deeply-supervised noise-decoupled training on state-of-the-art RGB and RGB-D SOD networks, LeNo outperforms previous works not only on adversarial images but also on clean images, which contributes stronger robustness for SOD. Our code is available at https://github.com/ssecv/LeNo",
    "checked": true,
    "id": "36b5d21ee97844ec915f5740e37e95c41992409c",
    "semantic_title": "leno: adversarial robust salient object detection networks with learnable noise",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25352": {
    "title": "Defending Black-Box Skeleton-Based Human Activity Classifiers",
    "volume": "main",
    "abstract": "Skeletal motions have been heavily relied upon for human activity recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR has been identified across a variety of classifiers and data, calling for mitigation. To this end, we propose the first black-box defense method for skeleton-based HAR to our best knowledge. Our method is featured by full Bayesian treatments of the clean data, the adversaries and the classifier, leading to (1) a new Bayesian Energy-based formulation of robust discriminative classifiers, (2) a new adversary sampling scheme based on natural motion manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is straightforward but elegant, which turns vulnerable black-box classifiers into robust ones without sacrificing accuracy. It demonstrates surprising and universal effectiveness across a wide range of skeletal HAR classifiers and datasets, under various attacks. Appendix and code are available",
    "checked": true,
    "id": "24d47781fb60fd3a427d426d763fc544bee8175b",
    "semantic_title": "defending black-box skeleton-based human activity classifiers",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25353": {
    "title": "Exploring CLIP for Assessing the Look and Feel of Images",
    "volume": "main",
    "abstract": "Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images without explicit task-specific training. In particular, we discuss effective prompt designs and show an effective prompt pairing strategy to harness the prior. We also provide extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks. Our results show that CLIP captures meaningful priors that generalize well to different perceptual assessments",
    "checked": true,
    "id": "03ae89e796b1a8b566ae1554fab65c8c88b3a55f",
    "semantic_title": "exploring clip for assessing the look and feel of images",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25354": {
    "title": "Robust Video Portrait Reenactment via Personalized Representation Quantization",
    "volume": "main",
    "abstract": "While progress has been made in the field of portrait reenactment, the problem of how to produce high-fidelity and robust videos remains. Recent studies normally find it challenging to handle rarely seen target poses due to the limitation of source data. This paper proposes the Video Portrait via Non-local Quantization Modeling (VPNQ) framework, which produces pose- and disturbance-robust reenactable video portraits. Our key insight is to learn position-invariant quantized local patch representations and build a mapping between simple driving signals and local textures with non-local spatial-temporal modeling. Specifically, instead of learning a universal quantized codebook, we identify that a personalized one can be trained to preserve desired position-invariant local details better. Then, a simple representation of projected landmarks can be used as sufficient driving signals to avoid 3D rendering. Following, we employ a carefully designed Spatio-Temporal Transformer to predict reasonable and temporally consistent quantized tokens from the driving signal. The predicted codes can be decoded back to robust and high-quality videos. Comprehensive experiments have been conducted to validate the effectiveness of our approach",
    "checked": true,
    "id": "f54e8f1d3af326ccbd372fe1cf7f548f7fd6a483",
    "semantic_title": "robust video portrait reenactment via personalized representation quantization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25355": {
    "title": "De-biased Teacher: Rethinking IoU Matching for Semi-supervised Object Detection",
    "volume": "main",
    "abstract": "Most of the recent research in semi-supervised object detection follows the pseudo-labeling paradigm evolved from the semi-supervised image classification task. However, the training paradigm of the two-stage object detector inevitably makes the pseudo-label learning process for unlabeled images full of bias. Specifically, the IoU matching scheme used for selecting and labeling candidate boxes is based on the assumption that the matching source~(ground truth) is accurate enough in terms of the number of objects, object position and object category. Obviously, pseudo-labels generated for unlabeled images cannot satisfy such a strong assumption, which makes the produced training proposals extremely unreliable and thus severely spoil the follow-up training. To de-bias the training proposals generated by the pseudo-label-based IoU matching, we propose a general framework -- De-biased Teacher, which abandons both the IoU matching and pseudo labeling processes by directly generating favorable training proposals for consistency regularization between the weak/strong augmented image pairs. Moreover, a distribution-based refinement scheme is designed to eliminate the scattered class predictions of significantly low values for higher efficiency. Extensive experiments demonstrate that the proposed De-biased Teacher consistently outperforms other state-of-the-art methods on the MS-COCO and PASCAL VOC benchmarks. Source codes are available at https://github.com/wkfdb/De-biased-Teracher",
    "checked": true,
    "id": "656d87d314009a9f1925348d30652f39e0cc7ed5",
    "semantic_title": "de-biased teacher: rethinking iou matching for semi-supervised object detection",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25356": {
    "title": "Learning to Generate an Unbiased Scene Graph by Using Attribute-Guided Predicate Features",
    "volume": "main",
    "abstract": "Scene Graph Generation (SGG) aims to capture the semantic information in an image and build a structured representation, which facilitates downstream tasks. The current challenge in SGG is to tackle the biased predictions caused by the long-tailed distribution of predicates. Since multiple predicates in SGG are coupled in an image, existing data re-balancing methods cannot completely balance the head and tail predicates. In this work, a decoupled learning framework is proposed for unbiased scene graph generation by using attribute-guided predicate features to construct a balanced training set. Specifically, the predicate recognition is decoupled into Predicate Feature Representation Learning (PFRL) and predicate classifier training with a class-balanced predicate feature set, which is constructed by our proposed Attribute-guided Predicate Feature Generation (A-PFG) model. In the A-PFG model, we first define the class labels of and corresponding visual feature as attributes to describe a predicate. Then the predicate feature and the attribute embedding are mapped into a shared hidden space by a dual Variational Auto-encoder (VAE), and finally the synthetic predicate features are forced to learn the contextual information in the attributes via cross reconstruction and distribution alignment. To demonstrate the effectiveness of our proposed method, our decoupled learning framework and A-PFG model are applied to various SGG models. The empirical results show that our method is substantially improved on all benchmarks and achieves new state-of-the-art performance for unbiased scene graph generation. Our code is available at https://github.com/wanglei0618/A-PFG",
    "checked": true,
    "id": "da31afdbc962f0323d013a88a0c308c61b4d247c",
    "semantic_title": "learning to generate an unbiased scene graph by using attribute-guided predicate features",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25357": {
    "title": "Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models",
    "volume": "main",
    "abstract": "Alignment between image and text has shown promising improvements on patch-level pre-trained document image models. However, investigating more effective or finer-grained alignment techniques during pre-training requires a large amount of computation cost and time. Thus, a question naturally arises: Could we fine-tune the pre-trained models adaptive to downstream tasks with alignment objectives and achieve comparable or better performance? In this paper, we propose a new model architecture with alignment-enriched tuning (dubbed AETNet) upon pre-trained document image models, to adapt downstream tasks with the joint task-specific supervised and alignment-aware contrastive objective. Specifically, we introduce an extra visual transformer as the alignment-ware image encoder and an extra text transformer as the alignment-ware text encoder before multimodal fusion. We consider alignment in the following three aspects: 1) document-level alignment by leveraging the cross-modal and intra-modal contrastive loss; 2) global-local alignment for modeling localized and structural information in document images; and 3) local-level alignment for more accurate patch-level information. Experiments on various downstream tasks show that AETNet can achieve state-of-the-art performance on various downstream tasks. Notably, AETNet consistently outperforms state-of-the-art pre-trained models, such as LayoutLMv3 with fine-tuning techniques, on three different downstream tasks. Code is available at https://github.com/MAEHCM/AET",
    "checked": true,
    "id": "1ff504e8d0ed00d8e9d6dd317d3b7572d1f3ecaa",
    "semantic_title": "alignment-enriched tuning for patch-level pre-trained document image models",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25358": {
    "title": "Flora: Dual-Frequency LOss-Compensated ReAl-Time Monocular 3D Video Reconstruction",
    "volume": "main",
    "abstract": "In this work, we propose a real-time monocular 3D video reconstruction approach named Flora for reconstructing delicate and complete 3D scenes from RGB video sequences in an end-to-end manner. Specifically, we introduce a novel method with two main contributions. Firstly, the proposed feature aggregation module retains both color and reliability in a dual-frequency form. Secondly, the loss compensation module solves missing structure by correcting losses for falsely pruned voxels. The dual-frequency feature aggregation module enhances reconstruction quality in both precision and recall, and the loss compensation module benefits the recall. Notably, both proposed contributions achieve great results with negligible inferencing overhead. Our state-of-the-art experimental results on real-world datasets demonstrate Flora's leading performance in both effectiveness and efficiency. The code is available at https://github.com/NoOneUST/Flora",
    "checked": true,
    "id": "de0d19110a0601c0741df7d26982a0c44288ce90",
    "semantic_title": "flora: dual-frequency loss-compensated real-time monocular 3d video reconstruction",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25359": {
    "title": "Efficient Image Captioning for Edge Devices",
    "volume": "main",
    "abstract": "Recent years have witnessed the rapid progress of image captioning. However, the demands for large memory storage and heavy computational burden prevent these captioning models from being deployed on mobile devices. The main obstacles lie in the heavyweight visual feature extractors (i.e., object detectors) and complicated cross-modal fusion networks. To this end, we propose LightCap, a lightweight image captioner for resource-limited devices. The core design is built on the recent CLIP model for efficient image captioning. To be specific, on the one hand, we leverage the CLIP model to extract the compact grid features without relying on the time-consuming object detectors. On the other hand, we transfer the image-text retrieval design of CLIP to image captioning scenarios by devising a novel visual concept extractor and a cross-modal modulator. We further optimize the cross-modal fusion model and parallel prediction heads via sequential and ensemble distillations. With the carefully designed architecture, our model merely contains 40M parameters, saving the model size by more than 75% and the FLOPs by more than 98% in comparison with the current state-of-the-art methods. In spite of the low capacity, our model still exhibits state-of-the-art performance on prevalent datasets, e.g., 136.6 CIDEr on COCO Karpathy test split. Testing on the smartphone with only a single CPU, the proposed LightCap exhibits a fast inference speed of 188ms per image, which is ready for practical applications",
    "checked": true,
    "id": "d6332c9a4607ce9823123e6407e40ab6ac33ac08",
    "semantic_title": "efficient image captioning for edge devices",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25360": {
    "title": "Controllable Image Captioning via Prompting",
    "volume": "main",
    "abstract": "Despite the remarkable progress of image captioning, existing captioners typically lack the controllable capability to generate desired image captions, e.g., describing the image in a rough or detailed manner, in a factual or emotional view, etc. In this paper, we show that a unified model is qualified to perform well in diverse domains and freely switch among multiple styles. Such a controllable capability is achieved by embedding the prompt learning into the image captioning framework. To be specific, we design a set of prompts to fine-tune the pre-trained image captioner. These prompts allow the model to absorb stylized data from different domains for joint training, without performance degradation in each domain. Furthermore, we optimize the prompts with learnable vectors in the continuous word embedding space, avoiding the heuristic prompt engineering and meanwhile exhibiting superior performance. In the inference stage, our model is able to generate desired stylized captions by choosing the corresponding prompts. Extensive experiments verify the controllable capability of the proposed method. Notably, we achieve outstanding performance on two diverse image captioning benchmarks including COCO Karpathy split and TextCaps using a unified model",
    "checked": true,
    "id": "f66aeec98816c3a52685e570a04fa8f2bd53dfb4",
    "semantic_title": "controllable image captioning via prompting",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25361": {
    "title": "ECO-3D: Equivariant Contrastive Learning for Pre-training on Perturbed 3D Point Cloud",
    "volume": "main",
    "abstract": "In this work, we investigate contrastive learning on perturbed point clouds and find that the contrasting process may widen the domain gap caused by random perturbations, making the pre-trained network fail to generalize on testing data. To this end, we propose the Equivariant COntrastive framework which closes the domain gap before contrasting, further introduces the equivariance property, and enables pre-training networks under more perturbation types to obtain meaningful features. Specifically, to close the domain gap, a pre-trained VAE is adopted to convert perturbed point clouds into less perturbed point embedding of similar domains and separated perturbation embedding. The contrastive pairs can then be generated by mixing the point embedding with different perturbation embedding. Moreover, to pursue the equivariance property, a Vector Quantizer is adopted during VAE training, discretizing the perturbation embedding into one-hot tokens which indicate the perturbation labels. By correctly predicting the perturbation labels from the perturbed point cloud, the property of equivariance can be encouraged in the learned features. Experiments on synthesized and real-world perturbed datasets show that ECO-3D outperforms most existing pre-training strategies under various downstream tasks, achieving SOTA performance for lots of perturbations",
    "checked": true,
    "id": "8c1cef162d3c6276c84deb22c08a2b6d7e2d38a1",
    "semantic_title": "eco-3d: equivariant contrastive learning for pre-training on perturbed 3d point cloud",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25362": {
    "title": "Global-Local Characteristic Excited Cross-Modal Attacks from Images to Videos",
    "volume": "main",
    "abstract": "The transferability of adversarial examples is the key property in practical black-box scenarios. Currently, numerous methods improve the transferability across different models trained on the same modality of data. The investigation of generating video adversarial examples with imagebased substitute models to attack the target video models, i.e., cross-modal transferability of adversarial examples, is rarely explored. A few works on cross-modal transferability directly apply image attack methods for each frame and no factors especial for video data are considered, which limits the cross-modal transferability of adversarial examples. In this paper, we propose an effective cross-modal attack method which considers both the global and local characteristics of video data. Firstly, from the global perspective, we introduce inter-frame interaction into attack process to induce more diverse and stronger gradients rather than perturb each frame separately. Secondly, from the local perspective, we disrupt the inherently local correlation of frames within a video, which prevents black-box video model from capturing valuable temporal clues. Extensive experiments on the UCF-101 and Kinetics-400 validate the proposed method significantly improves cross-modal transferability and even surpasses strong baseline using video models as substitute model. Our source codes are available at https://github.com/lwmming/Cross-Modal-Attack",
    "checked": true,
    "id": "af77300381156898024ea3a10f6d00cc31b8ee86",
    "semantic_title": "global-local characteristic excited cross-modal attacks from images to videos",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25363": {
    "title": "Fine-Grained Retrieval Prompt Tuning",
    "volume": "main",
    "abstract": "Fine-grained object retrieval aims to learn discriminative representation to retrieve visually similar objects. However, existing top-performing works usually impose pairwise similarities on the semantic embedding spaces or design a localization sub-network to continually fine-tune the entire model in limited data scenarios, thus resulting in convergence to suboptimal solutions. In this paper, we develop Fine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained model to perform the fine-grained retrieval task from the perspectives of sample prompting and feature adaptation. Specifically, FRPT only needs to learn fewer parameters in the prompt and adaptation instead of fine-tuning the entire model, thus solving the issue of convergence to suboptimal solutions caused by fine-tuning the entire model. Technically, a discriminative perturbation prompt (DPP) is introduced and deemed as a sample prompting process, which amplifies and even exaggerates some discriminative elements contributing to category prediction via a content-aware inhomogeneous sampling operation. In this way, DPP can make the fine-grained retrieval task aided by the perturbation prompts close to the solved task during the original pre-training. Thereby, it preserves the generalization and discrimination of representation extracted from input samples. Besides, a category-specific awareness head is proposed and regarded as feature adaptation, which removes the species discrepancies in features extracted by the pre-trained model using category-guided instance normalization. And thus, it makes the optimized features only include the discrepancies among subcategories. Extensive experiments demonstrate that our FRPT with fewer learnable parameters achieves the state-of-the-art performance on three widely-used fine-grained datasets",
    "checked": true,
    "id": "247be1257a1c5811ff48331e902771c7b00a56c5",
    "semantic_title": "fine-grained retrieval prompt tuning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25364": {
    "title": "Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method",
    "volume": "main",
    "abstract": "As the quality of optical sensors improves, there is a need for processing large-scale images. In particular, the ability of devices to capture ultra-high definition (UHD) images and video places new demands on the image processing pipeline. In this paper, we consider the task of low-light image enhancement (LLIE) and introduce a large-scale database consisting of images at 4K and 8K resolution. We conduct systematic benchmarking studies and provide a comparison of current LLIE algorithms. As a second contribution, we introduce LLFormer, a transformer-based low-light enhancement method. The core components of LLFormer are the axis-based multi-head self-attention and cross-layer attention fusion block, which significantly reduces the linear complexity. Extensive experiments on the new dataset and existing public datasets show that LLFormer outperforms state-of-the-art methods. We also show that employing existing LLIE methods trained on our benchmark as a pre-processing step significantly improves the performance of downstream tasks, e.g., face detection in low-light conditions. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLFormer",
    "checked": true,
    "id": "baf3d202261f1eb9122a157fc6480d93e2c3d03c",
    "semantic_title": "ultra-high-definition low-light image enhancement: a benchmark and transformer-based method",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25365": {
    "title": "3D Assembly Completion",
    "volume": "main",
    "abstract": "Automatic assembly is a promising research topic in 3D computer vision and robotics. Existing works focus on generating assembly (e.g., IKEA furniture) from scratch with a set of parts, namely 3D part assembly. In practice, there are higher demands for the robot to take over and finish an incomplete assembly (e.g., a half-assembled IKEA furniture) with an off-the-shelf toolkit, especially in human-robot and multi-agent collaborations. Compared to 3D part assembly, it is more complicated in nature and remains unexplored yet. The robot must understand the incomplete structure, infer what parts are missing, single out the correct parts from the toolkit and finally, assemble them with appropriate poses to finish the incomplete assembly. Geometrically similar parts in the toolkit can interfere, and this problem will be exacerbated with more missing parts. To tackle this issue, we propose a novel task called 3D assembly completion. Given an incomplete assembly, it aims to find its missing parts from a toolkit and predict the 6-DoF poses to make the assembly complete. To this end, we propose FiT, a framework for Finishing the incomplete 3D assembly with Transformer. We employ the encoder to model the incomplete assembly into memories. Candidate parts interact with memories in a memory-query paradigm for final candidate classification and pose prediction. Bipartite part matching and symmetric transformation consistency are embedded to refine the completion. For reasonable evaluation and further reference, we design two standard toolkits of different difficulty, containing different compositions of candidate parts. We conduct extensive comparisons with several baseline methods and ablation studies, demonstrating the effectiveness of the proposed method",
    "checked": true,
    "id": "eb2256efc25ad00d4a18a9901cf83b17dbc038a4",
    "semantic_title": "3d assembly completion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25366": {
    "title": "A Benchmark and Asymmetrical-Similarity Learning for Practical Image Copy Detection",
    "volume": "main",
    "abstract": "Image copy detection (ICD) aims to determine whether a query image is an edited copy of any image from a reference set. Currently, there are very limited public benchmarks for ICD, while all overlook a critical challenge in real-world applications, i.e., the distraction from hard negative queries. Specifically, some queries are not edited copies but are inherently similar to some reference images. These hard negative queries are easily false recognized as edited copies, significantly compromising the ICD accuracy. This observation motivates us to build the first ICD benchmark featuring this characteristic. Based on existing ICD datasets, this paper constructs a new dataset by additionally adding 100,000 and 24, 252 hard negative pairs into the training and test set, respectively. Moreover, this paper further reveals a unique difficulty for solving the hard negative problem in ICD, i.e., there is a fundamental conflict between current metric learning and ICD. This conflict is: the metric learning adopts symmetric distance while the edited copy is an asymmetric (unidirectional) process, e.g., a partial crop is close to its holistic reference image and is an edited copy, while the latter cannot be the edited copy of the former (in spite the distance is equally small). This insight results in an Asymmetrical-Similarity Learning (ASL) method, which allows the similarity in two directions (the query ↔ the reference image) to be different from each other. Experimental results show that ASL outperforms state-of-the-art methods by a clear margin, confirming that solving the symmetric-asymmetric conflict is critical for ICD. The NDEC dataset and code are available at https://github.com/WangWenhao0716/ASL",
    "checked": true,
    "id": "974e24ce5ca6a23b8841f3b1049edff0aeaf42ce",
    "semantic_title": "a benchmark and asymmetrical-similarity learning for practical image copy detection",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25367": {
    "title": "Revisiting Unsupervised Local Descriptor Learning",
    "volume": "main",
    "abstract": "Constructing accurate training tuples is crucial for unsupervised local descriptor learning, yet challenging due to the absence of patch labels. The state-of-the-art approach constructs tuples with heuristic rules, which struggle to precisely depict real-world patch transformations, in spite of enabling fast model convergence. A possible solution to alleviate the problem is the clustering-based approach, which can capture realistic patch variations and learn more accurate class decision boundaries, but suffers from slow model convergence. This paper presents HybridDesc, an unsupervised approach that learns powerful local descriptor models with fast convergence speed by combining the rule-based and clustering-based approaches to construct training tuples. In addition, HybridDesc also contributes two concrete enhancing mechanisms: (1) a Differentiable Hyperparameter Search (DHS) strategy to find the optimal hyperparameter setting of the rule-based approach so as to provide accurate prior for the clustering-based approach, (2) an On-Demand Clustering (ODC) method to reduce the clustering overhead of the clustering-based approach without eroding its advantage. Extensive experimental results show that HybridDesc can efficiently learn local descriptors that surpass existing unsupervised local descriptors and even rival competitive supervised ones",
    "checked": true,
    "id": "b72f022c06dbfa52af4f31a766ca57e7758b5990",
    "semantic_title": "revisiting unsupervised local descriptor learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25368": {
    "title": "Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning",
    "volume": "main",
    "abstract": "Self-supervised monocular methods can efficiently learn depth information of weakly textured surfaces or reflective objects. However, the depth accuracy is limited due to the inherent ambiguity in monocular geometric modeling. In contrast, multi-frame depth estimation methods improve depth accuracy thanks to the success of Multi-View Stereo (MVS), which directly makes use of geometric constraints. Unfortunately, MVS often suffers from texture-less regions, non-Lambertian surfaces, and moving objects, especially in real-world video sequences without known camera motion and depth supervision. Therefore, we propose MOVEDepth, which exploits the MOnocular cues and VElocity guidance to improve multi-frame Depth learning. Unlike existing methods that enforce consistency between MVS depth and monocular depth, MOVEDepth boosts multi-frame depth learning by directly addressing the inherent problems of MVS. The key of our approach is to utilize monocular depth as a geometric priority to construct MVS cost volume, and adjust depth candidates of cost volume under the guidance of predicted camera velocity. We further fuse monocular depth and MVS depth by learning uncertainty in the cost volume, which results in a robust depth estimation against ambiguity in multi-view geometry. Extensive experiments show MOVEDepth achieves state-of-the-art performance: Compared with Monodepth2 and PackNet, our method relatively improves the depth accuracy by 20% and 19.8% on the KITTI benchmark. MOVEDepth also generalizes to the more challenging DDAD benchmark, relatively outperforming ManyDepth by 7.2%. The code is available at https://github.com/JeffWang987/MOVEDepth",
    "checked": true,
    "id": "588042275a2e338df56ace42453c727137da36fc",
    "semantic_title": "crafting monocular cues and velocity guidance for self-supervised multi-frame depth learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25369": {
    "title": "Learning Continuous Depth Representation via Geometric Spatial Aggregator",
    "volume": "main",
    "abstract": "Depth map super-resolution (DSR) has been a fundamental task for 3D computer vision. While arbitrary scale DSR is a more realistic setting in this scenario, previous approaches predominantly suffer from the issue of inefficient real-numbered scale upsampling. To explicitly address this issue, we propose a novel continuous depth representation for DSR. The heart of this representation is our proposed Geometric Spatial Aggregator (GSA), which exploits a distance field modulated by arbitrarily upsampled target gridding, through which the geometric information is explicitly introduced into feature aggregation and target generation. Furthermore, bricking with GSA, we present a transformer-style backbone named GeoDSR, which possesses a principled way to construct the functional mapping between local coordinates and the high-resolution output results, empowering our model with the advantage of arbitrary shape transformation ready to help diverse zooming demand. Extensive experimental results on standard depth map benchmarks, e.g., NYU v2, have demonstrated that the proposed framework achieves significant restoration gain in arbitrary scale depth map super-resolution compared with the prior art. Our codes are available at https://github.com/nana01219/GeoDSR",
    "checked": true,
    "id": "9dc77eb07e11d3fd09f2dd5d2225ded4291f6d3b",
    "semantic_title": "learning continuous depth representation via geometric spatial aggregator",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25370": {
    "title": "SSDA3D: Semi-supervised Domain Adaptation for 3D Object Detection from Point Cloud",
    "volume": "main",
    "abstract": "LiDAR-based 3D object detection is an indispensable task in advanced autonomous driving systems. Though impressive detection results have been achieved by superior 3D detectors, they suffer from significant performance degeneration when facing unseen domains, such as different LiDAR configurations, different cities, and weather conditions. The mainstream approaches tend to solve these challenges by leveraging unsupervised domain adaptation (UDA) techniques. However, these UDA solutions just yield unsatisfactory 3D detection results when there is a severe domain shift, e.g., from Waymo (64-beam) to nuScenes (32-beam). To address this, we present a novel Semi-Supervised Domain Adaptation method for 3D object detection (SSDA3D), where only a few labeled target data is available, yet can significantly improve the adaptation performance. In particular, our SSDA3D includes an Inter-domain Adaptation stage and an Intra-domain Generalization stage. In the first stage, an Inter-domain Point-CutMix module is presented to efficiently align the point cloud distribution across domains. The Point-CutMix generates mixed samples of an intermediate domain, thus encouraging to learn domain-invariant knowledge. Then, in the second stage, we further enhance the model for better generalization on the unlabeled target set. This is achieved by exploring Intra-domain Point-MixUp in semi-supervised learning, which essentially regularizes the pseudo label distribution. Experiments from Waymo to nuScenes show that, with only 10% labeled target data, our SSDA3D can surpass the fully-supervised oracle model with 100% target label. Our code is available at https://github.com/yinjunbo/SSDA3D",
    "checked": true,
    "id": "bfc07ed6e6d4aef2674d0e412215b3253b95336c",
    "semantic_title": "ssda3d: semi-supervised domain adaptation for 3d object detection from point cloud",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25371": {
    "title": "High-Resolution GAN Inversion for Degraded Images in Large Diverse Datasets",
    "volume": "main",
    "abstract": "The last decades are marked by massive and diverse image data, which shows increasingly high resolution and quality. However, some images we obtained may be corrupted, affecting the perception and the application of downstream tasks. A generic method for generating a high-quality image from the degraded one is in demand. In this paper, we present a novel GAN inversion framework that utilizes the powerful generative ability of StyleGAN-XL for this problem. To ease the inversion challenge with StyleGAN-XL, Clustering \\& Regularize Inversion (CRI) is proposed. Specifically, the latent space is firstly divided into finer-grained sub-spaces by clustering. Instead of initializing the inversion with the average latent vector, we approximate a centroid latent vector from the clusters, which generates an image close to the input image. Then, an offset with a regularization term is introduced to keep the inverted latent vector within a certain range. We validate our CRI scheme on multiple restoration tasks (i.e., inpainting, colorization, and super-resolution) of complex natural images, and show preferable quantitative and qualitative results. We further demonstrate our technique is robust in terms of data and different GAN models. To our best knowledge, we are the first to adopt StyleGAN-XL for generating high-quality natural images from diverse degraded inputs. Code is available at https://github.com/Booooooooooo/CRI",
    "checked": true,
    "id": "36da639b3cba35311314a19e5bfd987c5865a061",
    "semantic_title": "high-resolution gan inversion for degraded images in large diverse datasets",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25372": {
    "title": "GAN Prior Based Null-Space Learning for Consistent Super-resolution",
    "volume": "main",
    "abstract": "Consistency and realness have always been the two critical issues of image super-resolution. While the realness has been dramatically improved with the use of GAN prior, the state-of-the-art methods still suffer inconsistencies in local structures and colors (e.g., tooth and eyes). In this paper, we show that these inconsistencies can be analytically eliminated by learning only the null-space component while fixing the range-space part. Further, we design a pooling-based decomposition (PD), a universal range-null space decomposition for super-resolution tasks, which is concise, fast, and parameter-free. PD can be easily applied to state-of-the-art GAN Prior based SR methods to eliminate their inconsistencies, neither compromise the realness nor bring extra parameters or computational costs. Besides, our ablation studies reveal that PD can replace pixel-wise losses for training and achieve better generalization performance when facing unseen downsamplings or even real-world degradation. Experiments show that the use of PD refreshes state-of-the-art SR performance and speeds up the convergence of training up to 2~10 times",
    "checked": true,
    "id": "7c12d091ea5c16938943559fb0f3d430ba040c08",
    "semantic_title": "gan prior based null-space learning for consistent super-resolution",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25373": {
    "title": "Contrastive Masked Autoencoders for Self-Supervised Video Hashing",
    "volume": "main",
    "abstract": "Self-Supervised Video Hashing (SSVH) models learn to generate short binary representations for videos without ground-truth supervision, facilitating large-scale video retrieval efficiency and attracting increasing research attention. The success of SSVH lies in the understanding of video content and the ability to capture the semantic relation among unlabeled videos. Typically, state-of-the-art SSVH methods consider these two points in a two-stage training pipeline, where they firstly train an auxiliary network by instance-wise mask-and-predict tasks and secondly train a hashing model to preserve the pseudo-neighborhood structure transferred from the auxiliary network. This consecutive training strategy is inflexible and also unnecessary. In this paper, we propose a simple yet effective one-stage SSVH method called ConMH, which incorporates video semantic information and video similarity relationship understanding in a single stage. To capture video semantic information for better hashing learning, we adopt an encoder-decoder structure to reconstruct the video from its temporal-masked frames. Particularly, we find that a higher masking ratio helps video understanding. Besides, we fully exploit the similarity relationship between videos by maximizing agreement between two augmented views of a video, which contributes to more discriminative and robust hash codes. Extensive experiments on three large-scale video datasets (i.e., FCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art results. Code is available at https://github.com/huangmozhi9527/ConMH",
    "checked": true,
    "id": "65be7f0b01cf6c2a1cded708b809182fbcb43548",
    "semantic_title": "contrastive masked autoencoders for self-supervised video hashing",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25374": {
    "title": "MicroAST: Towards Super-fast Ultra-Resolution Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "Arbitrary style transfer (AST) transfers arbitrary artistic styles onto content images. Despite the recent rapid progress, existing AST methods are either incapable or too slow to run at ultra-resolutions (e.g., 4K) with limited resources, which heavily hinders their further applications. In this paper, we tackle this dilemma by learning a straightforward and lightweight model, dubbed MicroAST. The key insight is to completely abandon the use of cumbersome pre-trained Deep Convolutional Neural Networks (e.g., VGG) at inference. Instead, we design two micro encoders (content and style encoders) and one micro decoder for style transfer. The content encoder aims at extracting the main structure of the content image. The style encoder, coupled with a modulator, encodes the style image into learnable dual-modulation signals that modulate both intermediate features and convolutional filters of the decoder, thus injecting more sophisticated and flexible style signals to guide the stylizations. In addition, to boost the ability of the style encoder to extract more distinct and representative style signals, we also introduce a new style signal contrastive loss in our model. Compared to the state of the art, our MicroAST not only produces visually superior results but also is 5-73 times smaller and 6-18 times faster, for the first time enabling super-fast (about 0.5 seconds) AST at 4K ultra-resolutions",
    "checked": true,
    "id": "7b0183c518ebc2100569f1086fd6fedab8659d96",
    "semantic_title": "microast: towards super-fast ultra-resolution arbitrary style transfer",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25375": {
    "title": "Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos",
    "volume": "main",
    "abstract": "Learning with noisy label is a classic problem that has been extensively studied for image tasks, but much less for video in the literature. A straightforward migration from images to videos without considering temporal semantics and computational cost is not a sound choice. In this paper, we propose two new strategies for video analysis with noisy labels: 1) a lightweight channel selection method dubbed as Channel Truncation for feature-based label noise detection. This method selects the most discriminative channels to split clean and noisy instances in each category. 2) A novel contrastive strategy dubbed as Noise Contrastive Learning, which constructs the relationship between clean and noisy instances to regularize model training. Experiments on three well-known benchmark datasets for video classification show that our proposed truNcatE-split-contrAsT (NEAT) significantly outperforms the existing baselines. By reducing the dimension to 10% of it, our method achieves over 0.4 noise detection F1-score and 5% classification accuracy improvement on Mini-Kinetics dataset under severe noise (symmetric-80%). Thanks to Noise Contrastive Learning, the average classification accuracy improvement on Mini-Kinetics and Sth-Sth-V1 is over 1.6%",
    "checked": true,
    "id": "bb62dd71543f4644cec09b00bdf7b8b2795063fd",
    "semantic_title": "truncate-split-contrast: a framework for learning from mislabeled videos",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25376": {
    "title": "Active Token Mixer",
    "volume": "main",
    "abstract": "The three existing dominant network families, i.e., CNNs, Transformers and MLPs, differ from each other mainly in the ways of fusing spatial contextual information, leaving designing more effective token-mixing mechanisms at the core of backbone architecture development. In this work, we propose an innovative token-mixer, dubbed Active Token Mixer (ATM), to actively incorporate contextual information from other tokens in the global scope into the given query token. This fundamental operator actively predicts where to capture useful contexts and learns how to fuse the captured contexts with the query token at channel level. In this way, the spatial range of token-mixing can be expanded to a global scope with limited computational complexity, where the way of token-mixing is reformed. We take ATMs as the primary operators and assemble them into a cascade architecture, dubbed ATMNet. Extensive experiments demonstrate that ATMNet is generally applicable and comprehensively surpasses different families of SOTA vision backbones by a clear margin on a broad range of vision tasks, including visual recognition and dense prediction tasks. Code is available at https://github.com/microsoft/ActiveMLP",
    "checked": true,
    "id": "391fecb4d4468f46d0fb19e60948830139ddefec",
    "semantic_title": "active token mixer",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25377": {
    "title": "Exploring Non-target Knowledge for Improving Ensemble Universal Adversarial Attacks",
    "volume": "main",
    "abstract": "The ensemble attack with average weights can be leveraged for increasing the transferability of universal adversarial perturbation (UAP) by training with multiple Convolutional Neural Networks (CNNs). However, after analyzing the Pearson Correlation Coefficients (PCCs) between the ensemble logits and individual logits of the crafted UAP trained by the ensemble attack, we find that one CNN plays a dominant role during the optimization. Consequently, this average weighted strategy will weaken the contributions of other CNNs and thus limit the transferability for other black-box CNNs. To deal with this bias issue, the primary attempt is to leverage the Kullback–Leibler (KL) divergence loss to encourage the joint contribution from different CNNs, which is still insufficient. After decoupling the KL loss into a target-class part and a non-target-class part, the main issue lies in that the non-target knowledge will be significantly suppressed due to the increasing logit of the target class. In this study, we simply adopt a KL loss that only considers the non-target classes for addressing the dominant bias issue. Besides, to further boost the transferability, we incorporate the min-max learning framework to self-adjust the ensemble weights for each CNN. Experiments results validate that considering the non-target KL loss can achieve superior transferability than the original KL loss by a large margin, and the min-max training can provide a mutual benefit in adversarial ensemble attacks. The source code is available at: https://github.com/WJJLL/ND-MM",
    "checked": true,
    "id": "2cf7b3b2fb58431d96e934effbb0af3a3ba60eb9",
    "semantic_title": "exploring non-target knowledge for improving ensemble universal adversarial attacks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25378": {
    "title": "Towards Good Practices for Missing Modality Robust Action Recognition",
    "volume": "main",
    "abstract": "Standard multi-modal models assume the use of the same modalities in training and inference stages. However, in practice, the environment in which multi-modal models operate may not satisfy such assumption. As such, their performances degrade drastically if any modality is missing in the inference stage. We ask: how can we train a model that is robust to missing modalities? This paper seeks a set of good practices for multi-modal action recognition, with a particular interest in circumstances where some modalities are not available at an inference time. First, we show how to effectively regularize the model during training (e.g., data augmentation). Second, we investigate on fusion methods for robustness to missing modalities: we find that transformer-based fusion shows better robustness for missing modality than summation or concatenation. Third, we propose a simple modular network, ActionMAE, which learns missing modality predictive coding by randomly dropping modality features and tries to reconstruct them with the remaining modality features. Coupling these good practices, we build a model that is not only effective in multi-modal action recognition but also robust to modality missing. Our model achieves the state-of-the-arts on multiple benchmarks and maintains competitive performances even in missing modality scenarios",
    "checked": true,
    "id": "12a43c120c9e7615535237bbee2f6375d07fdd7a",
    "semantic_title": "towards good practices for missing modality robust action recognition",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25379": {
    "title": "Reject Decoding via Language-Vision Models for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "Transformer-based text-to-image synthesis generates images from abstractive textual conditions and achieves prompt results. Since transformer-based models predict visual tokens step by step in testing, where the early error is hard to be corrected and would be propagated. To alleviate this issue, the common practice is drawing multi-paths from the transformer-based models and re-ranking the multi-images decoded from multi-paths to find the best one and filter out others. Therefore, the computing procedure of excluding images may be inefficient. To improve the effectiveness and efficiency of decoding, we exploit a reject decoding algorithm with tiny multi-modal models to enlarge the searching space and exclude the useless paths as early as possible. Specifically, we build tiny multi-modal models to evaluate the similarities between the partial paths and the caption at multi scales. Then, we propose a reject decoding algorithm to exclude some lowest quality partial paths at the inner steps. Thus, under the same computing load as the original decoding, we could search across more multi-paths to improve the decoding efficiency and synthesizing quality. The experiments conducted on the MS-COCO dataset and large-scale datasets show that the proposed reject decoding algorithm can exclude the useless paths and enlarge the searching paths to improve the synthesizing quality by consuming less time",
    "checked": true,
    "id": "1f9fff9bf668264b454ee45e67135ea2debd7b8f",
    "semantic_title": "reject decoding via language-vision models for text-to-image synthesis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25380": {
    "title": "Transformation-Equivariant 3D Object Detection for Autonomous Driving",
    "volume": "main",
    "abstract": "3D object detection received increasing attention in autonomous driving recently. Objects in 3D scenes are distributed with diverse orientations. Ordinary detectors do not explicitly model the variations of rotation and reflection transformations. Consequently, large networks and extensive data augmentation are required for robust detection. Recent equivariant networks explicitly model the transformation variations by applying shared networks on multiple transformed point clouds, showing great potential in object geometry modeling. However, it is difficult to apply such networks to 3D object detection in autonomous driving due to its large computation cost and slow reasoning speed. In this work, we present TED, an efficient Transformation-Equivariant 3D Detector to overcome the computation cost and speed issues. TED first applies a sparse convolution backbone to extract multi-channel transformation-equivariant voxel features; and then aligns and aggregates these equivariant features into lightweight and compact representations for high-performance 3D object detection. On the highly competitive KITTI 3D car detection leaderboard, TED ranked 1st among all submissions with competitive efficiency. Code is available at https://github.com/hailanyi/TED",
    "checked": true,
    "id": "8599f26dd40313a092dc05fe370b934beb6a29ef",
    "semantic_title": "transformation-equivariant 3d object detection for autonomous driving",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25381": {
    "title": "Super-efficient Echocardiography Video Segmentation via Proxy- and Kernel-Based Semi-supervised Learning",
    "volume": "main",
    "abstract": "Automatic segmentation of left ventricular endocardium in echocardiography videos is critical for assessing various cardiac functions and improving the diagnosis of cardiac diseases. It is yet a challenging task due to heavy speckle noise, significant shape variability of cardiac structure, and limited labeled data. Particularly, the real-time demand in clinical practice makes this task even harder. In this paper, we propose a novel proxy- and kernel-based semi-supervised segmentation network (PKEcho-Net) to comprehensively address these challenges. We first propose a multi-scale region proxy (MRP) mechanism to model the region-wise contexts, in which a learnable region proxy with an arbitrary shape is developed in each layer of the encoder, allowing the network to identify homogeneous semantics and hence alleviate the influence of speckle noise on segmentation. To sufficiently and efficiently exploit temporal consistency, different from traditional methods which only utilize the temporal contexts of two neighboring frames via feature warping or self-attention mechanism, we formulate the semi-supervised segmentation with a group of learnable kernels, which can naturally and uniformly encode the appearances of left ventricular endocardium, as well as extracting the inter-frame contexts across the whole video to resist the fast shape variability of cardiac structures. Extensive experiments have been conducted on two famous public echocardiography video datasets, EchoNet-Dynamic and CAMUS. Our model achieves the best performance-efficiency trade-off when compared with other state-of-the-art approaches, attaining comparative accuracy with a much faster speed. The code is available at https://github.com/JingyinLin/PKEcho-Net",
    "checked": true,
    "id": "b6f991a9b4da4d520d11684d42cdf403d0be5917",
    "semantic_title": "super-efficient echocardiography video segmentation via proxy- and kernel-based semi-supervised learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25382": {
    "title": "ACL-Net: Semi-supervised Polyp Segmentation via Affinity Contrastive Learning",
    "volume": "main",
    "abstract": "Automatic polyp segmentation from colonoscopy images is an essential prerequisite for the development of computer-assisted therapy. However, the complex semantic information and the blurred edges of polyps make segmentation extremely difficult. In this paper, we propose a novel semi-supervised polyp segmentation framework using affinity contrastive learning (ACL-Net), which is implemented between student and teacher networks to consistently refine the pseudo-labels for semi-supervised polyp segmentation. By aligning the affinity maps between the two branches, a better polyp region activation can be obtained to fully exploit the appearance-level context encoded in the feature maps, thereby improving the capability of capturing not only global localization and shape context, but also the local textural and boundary details. By utilizing the rich inter-image affinity context and establishing a global affinity context based on the memory bank, a cross-image affinity aggregation (CAA) module is also implemented to further refine the affinity aggregation between the two branches. By continuously and adaptively refining pseudo-labels with optimized affinity, we can improve the semi-supervised polyp segmentation based on the mutually reinforced knowledge interaction among contrastive learning and consistency learning iterations. Extensive experiments on five benchmark datasets, including Kvasir-SEG, CVC-ClinicDB, CVC-300, CVC-ColonDB and ETIS, demonstrate the effectiveness and superiority of our method. Codes are available at https://github.com/xiewende/ACL-Net",
    "checked": true,
    "id": "b7fe355f1db5dbf47e4ac223c7a48e8dbaa7cbe1",
    "semantic_title": "acl-net: semi-supervised polyp segmentation via affinity contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25383": {
    "title": "Bi-directional Feature Reconstruction Network for Fine-Grained Few-Shot Image Classification",
    "volume": "main",
    "abstract": "The main challenge for fine-grained few-shot image classification is to learn feature representations with higher inter-class and lower intra-class variations, with a mere few labelled samples. Conventional few-shot learning methods however cannot be naively adopted for this fine-grained setting -- a quick pilot study reveals that they in fact push for the opposite (i.e., lower inter-class variations and higher intra-class variations). To alleviate this problem, prior works predominately use a support set to reconstruct the query image and then utilize metric learning to determine its category. Upon careful inspection, we further reveal that such unidirectional reconstruction methods only help to increase inter-class variations and are not effective in tackling intra-class variations. In this paper, we for the first time introduce a bi-reconstruction mechanism that can simultaneously accommodate for inter-class and intra-class variations. In addition to using the support set to reconstruct the query set for increasing inter-class variations, we further use the query set to reconstruct the support set for reducing intra-class variations. This design effectively helps the model to explore more subtle and discriminative features which is key for the fine-grained problem in hand. Furthermore, we also construct a self-reconstruction module to work alongside the bi-directional module to make the features even more discriminative. Experimental results on three widely used fine-grained image classification datasets consistently show considerable improvements compared with other methods. Codes are available at: https://github.com/PRIS-CV/Bi-FRN",
    "checked": true,
    "id": "b8d3436667b7863759a5d2228366e357f35a6e8d",
    "semantic_title": "bi-directional feature reconstruction network for fine-grained few-shot image classification",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25384": {
    "title": "Preserving Structural Consistency in Arbitrary Artist and Artwork Style Transfer",
    "volume": "main",
    "abstract": "Deep generative models are effective in style transfer. Previous methods learn one or several specific artist-style from a collection of artworks. These methods not only homogenize the artist-style of different artworks of the same artist but also lack generalization for the unseen artists. To solve these challenges, we propose a double-style transferring module (DSTM). It extracts different artist-style and artwork-style from different artworks (even untrained) and preserves the intrinsic diversity between different artworks of the same artist. DSTM swaps the two styles in the adversarial training and encourages realistic image generation given arbitrary style combinations. However, learning style from single artwork can often cause over-adaption to it, resulting in the introduction of structural features of style image. We further propose an edge enhancing module (EEM) which derives edge information from multi-scale and multi-level features to enhance structural consistency. We broadly evaluate our method across six large-scale benchmark datasets. Empirical results show that our method achieves arbitrary artist-style and artwork-style extraction from a single artwork, and effectively avoids introducing the style image's structural features. Our method improves the state-of-the-art deception rate from 58.9% to 67.2% and the average FID from 48.74 to 42.83",
    "checked": true,
    "id": "e808f718e4f195527e8ea3ecb252e25e24c98bae",
    "semantic_title": "preserving structural consistency in arbitrary artist and artwork style transfer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25385": {
    "title": "End-to-End Zero-Shot HOI Detection via Vision and Language Knowledge Distillation",
    "volume": "main",
    "abstract": "Most existing Human-Object Interaction (HOI) Detection methods rely heavily on full annotations with predefined HOI categories, which is limited in diversity and costly to scale further. We aim at advancing zero-shot HOI detection to detect both seen and unseen HOIs simultaneously. The fundamental challenges are to discover potential human-object pairs and identify novel HOI categories. To overcome the above challenges, we propose a novel End-to-end zero-shot HOI Detection (EoID) framework via vision-language knowledge distillation. We first design an Interactive Score module combined with a Two-stage Bipartite Matching algorithm to achieve interaction distinguishment for human-object pairs in an action-agnostic manner. Then we transfer the distribution of action probability from the pretrained vision-language teacher as well as the seen ground truth to the HOI model to attain zero-shot HOI classification. Extensive experiments on HICO-Det dataset demonstrate that our model discovers potential interactive pairs and enables the recognition of unseen HOIs. Finally, our method outperforms the previous SOTA under various zero-shot settings. Moreover, our method is generalizable to large-scale object detection data to further scale up the action sets. The source code is available at: https://github.com/mrwu-mac/EoID",
    "checked": true,
    "id": "2b2a7f713d8efe2696df85ef22dac7ef35be7e10",
    "semantic_title": "end-to-end zero-shot hoi detection via vision and language knowledge distillation",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25386": {
    "title": "Revisiting Classifier: Transferring Vision-Language Models for Video Recognition",
    "volume": "main",
    "abstract": "Transferring knowledge from task-agnostic pre-trained deep models for downstream tasks is an important topic in computer vision research. Along with the growth of computational capacity, we now have open-source vision-language pre-trained models in large scales of the model architecture and amount of data. In this study, we focus on transferring knowledge for video classification tasks. Conventional methods randomly initialize the linear classifier head for vision classification, but they leave the usage of the text encoder for downstream visual recognition tasks undiscovered. In this paper, we revise the role of the linear classifier and replace the classifier with the different knowledge from pre-trained model. We utilize the well-pretrained language model to generate good semantic target for efficient transferring learning. The empirical study shows that our method improves both the performance and the training speed of video classification, with a negligible change in the model. Our simple yet effective tuning paradigm achieves state-of-the-art performance and efficient training on various video recognition scenarios, i.e., zero-shot, few-shot, general recognition. In particular, our paradigm achieves the state-of-the-art accuracy of 87.8% on Kinetics-400, and also surpasses previous methods by 20~50% absolute top-1 accuracy under zero-shot, few-shot settings on five video datasets. Code and models are available at https://github.com/whwu95/Text4Vis",
    "checked": true,
    "id": "c68b9483bcd91850e27cc6d667c783edf335a3e4",
    "semantic_title": "revisiting classifier: transferring vision-language models for video recognition",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25387": {
    "title": "Scene Graph to Image Synthesis via Knowledge Consensus",
    "volume": "main",
    "abstract": "In this paper, we study graph-to-image generation conditioned exclusively on scene graphs, in which we seek to disentangle the veiled semantics between knowledge graphs and images. While most existing research resorts to laborious auxiliary information such as object layouts or segmentation masks, it is also of interest to unveil the generality of the model with limited supervision, moreover, avoiding extra cross-modal alignments. To tackle this challenge, we delve into the causality of the adversarial generation process, and reason out a new principle to realize a simultaneous semantic disentanglement with an alignment on target and model distributions. This principle is named knowledge consensus, which explicitly describes a triangle causal dependency among observed images, graph semantics and hidden visual representations. The consensus also determines a new graph-to-image generation framework, carried on several adversarial optimization objectives. Extensive experimental results demonstrate that, even conditioned only on scene graphs, our model surprisingly achieves superior performance on semantics-aware image generation, without losing the competence on manipulating the generation through knowledge graphs",
    "checked": true,
    "id": "feb5504685a92ea0b2283959a1b508f54bab1627",
    "semantic_title": "scene graph to image synthesis via knowledge consensus",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25388": {
    "title": "Synthetic Data Can Also Teach: Synthesizing Effective Data for Unsupervised Visual Representation Learning",
    "volume": "main",
    "abstract": "Contrastive learning (CL), a self-supervised learning approach, can effectively learn visual representations from unlabeled data. Given the CL training data, generative models can be trained to generate synthetic data to supplement the real data. Using both synthetic and real data for CL training has the potential to improve the quality of learned representations. However, synthetic data usually has lower quality than real data, and using synthetic data may not improve CL compared with using real data. To tackle this problem, we propose a data generation framework with two methods to improve CL training by joint sample generation and contrastive learning. The first approach generates hard samples for the main model. The generator is jointly learned with the main model to dynamically customize hard samples based on the training state of the main model. Besides, a pair of data generators are proposed to generate similar but distinct samples as positive pairs. In joint learning, the hardness of a positive pair is progressively increased by decreasing their similarity. Experimental results on multiple datasets show superior accuracy and data efficiency of the proposed data generation methods applied to CL. For example, about 4.0%, 3.5%, and 2.6% accuracy improvements for linear classification are observed on ImageNet-100, CIFAR-100, and CIFAR-10, respectively. Besides, up to 2× data efficiency for linear classification and up to 5× data efficiency for transfer learning are achieved",
    "checked": true,
    "id": "fe6ee1f2bf6fa3093798723d3edab686fd927d1d",
    "semantic_title": "synthetic data can also teach: synthesizing effective data for unsupervised visual representation learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25389": {
    "title": "Multi-Stream Representation Learning for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Forecasting the future trajectory of pedestrians is an important task in computer vision with a range of applications, from security cameras to autonomous driving. It is very challenging because pedestrians not only move individually across time but also interact spatially, and the spatial and temporal information is deeply coupled with one another in a multi-agent scenario. Learning such complex spatio-temporal correlation is a fundamental issue in pedestrian trajectory prediction. Inspired by the procedure that the hippocampus processes and integrates spatio-temporal information to form memories, we propose a novel multi-stream representation learning module to learn complex spatio-temporal features of pedestrian trajectory. Specifically, we learn temporal, spatial and cross spatio-temporal correlation features in three respective pathways and then adaptively integrate these features with learnable weights by a gated network. Besides, we leverage the sparse attention gate to select informative interactions and correlations brought by complex spatio-temporal modeling and reduce complexity of our model. We evaluate our proposed method on two commonly used datasets, i.e. ETH-UCY and SDD, and the experimental results demonstrate our method achieves the state-of-the-art performance. Code: https://github.com/YuxuanIAIR/MSRL-master",
    "checked": true,
    "id": "b1552a37506344714092defaf1a4bc9488f9bd67",
    "semantic_title": "multi-stream representation learning for pedestrian trajectory prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25390": {
    "title": "Pixel Is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection",
    "volume": "main",
    "abstract": "Although weakly-supervised techniques can reduce the labeling effort, it is unclear whether a saliency model trained with weakly-supervised data (e.g., point annotation) can achieve the equivalent performance of its fully-supervised version. This paper attempts to answer this unexplored question by proving a hypothesis: there is a point-labeled dataset where saliency models trained on it can achieve equivalent performance when trained on the densely annotated dataset. To prove this conjecture, we proposed a novel yet effective adversarial trajectory-ensemble active learning (ATAL). Our contributions are three-fold: 1) Our proposed adversarial attack triggering uncertainty can conquer the overconfidence of existing active learning methods and accurately locate these uncertain pixels. 2) Our proposed trajectory-ensemble uncertainty estimation method maintains the advantages of the ensemble networks while significantly reducing the computational cost. 3) Our proposed relationship-aware diversity sampling algorithm can conquer oversampling while boosting performance. Experimental results show that our ATAL can find such a point-labeled dataset, where a saliency model trained on it obtained 97%-99% performance of its fully-supervised version with only 10 annotated points per image",
    "checked": true,
    "id": "088a89c3ef90e80b050ac717400209f564099654",
    "semantic_title": "pixel is all you need: adversarial trajectory-ensemble active learning for salient object detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25391": {
    "title": "Attention-Based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection is a low-cost but challenging task, as it requires generating accurate 3D localization solely from a single image input. Recent developed depth-assisted methods show promising results by using explicit depth maps as intermediate features, which are either precomputed by monocular depth estimation networks or jointly evaluated with 3D object detection. However, inevitable errors from estimated depth priors may lead to misaligned semantic information and 3D localization, hence resulting in feature smearing and suboptimal predictions. To mitigate this issue, we propose ADD, an Attention-based Depth knowledge Distillation framework with 3D-aware positional encoding. Unlike previous knowledge distillation frameworks that adopt stereo- or LiDAR-based teachers, we build up our teacher with identical architecture as the student but with extra ground-truth depth as input. Credit to our teacher design, our framework is seamless, domain-gap free, easily implementable, and is compatible with object-wise ground-truth depth. Specifically, we leverage intermediate features and responses for knowledge distillation. Considering long-range 3D dependencies, we propose 3D-aware self-attention and target-aware cross-attention modules for student adaptation. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. We implement our framework on three representative monocular detectors, and we achieve state-of-the-art performance with no additional inference computational cost relative to baseline models. Our code is available at https://github.com/rockywind/ADD",
    "checked": true,
    "id": "6c475af6a695d494b8790ca878f146e795eeb27b",
    "semantic_title": "attention-based depth distillation with 3d-aware positional encoding for monocular 3d object detection",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25392": {
    "title": "Skating-Mixer: Long-Term Sport Audio-Visual Modeling with MLPs",
    "volume": "main",
    "abstract": "Figure skating scoring is challenging because it requires judging players' technical moves as well as coordination with the background music. Most learning-based methods struggle for two reasons: 1) each move in figure skating changes quickly, hence simply applying traditional frame sampling will lose a lot of valuable information, especially in 3 to 5 minutes lasting videos; 2) prior methods rarely considered the critical audio-visual relationship in their models. Due to these reasons, we introduce a novel architecture, named Skating-Mixer. It extends the MLP framework into a multimodal fashion and effectively learns long-term representations through our designed memory recurrent unit (MRU). Aside from the model, we collected a high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8 types of programs with 7 different rating metrics, overtaking other datasets in both quantity and diversity. Experiments show the proposed method achieves SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In addition, we include an analysis applying our method to the recent competitions in Beijing 2022 Winter Olympic Games, proving our method has strong applicability",
    "checked": true,
    "id": "14165df6bd6ebe598d65d740ee7bcefee2e178f4",
    "semantic_title": "skating-mixer: long-term sport audio-visual modeling with mlps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25393": {
    "title": "SVFI: Spiking-Based Video Frame Interpolation for High-Speed Motion",
    "volume": "main",
    "abstract": "Occlusion and motion blur make it challenging to interpolate video frame, since estimating complex motions between two frames is hard and unreliable, especially in highly dynamic scenes. This paper aims to address these issues by exploiting spike stream as auxiliary visual information between frames to synthesize target frames. Instead of estimating motions by optical flow from RGB frames, we present a new dual-modal pipeline adopting both RGB frames and the corresponding spike stream as inputs (SVFI). It extracts the scene structure and objects' outline feature maps of the target frames from spike stream. Those feature maps are fused with the color and texture feature maps extracted from RGB frames to synthesize target frames. Benefited by the spike stream that contains consecutive information between two frames, SVFI can directly extract the information in occlusion and motion blur areas of target frames from spike stream, thus it is more robust than previous optical flow-based methods. Experiments show SVFI outperforms the SOTA methods on wide variety of datasets. For instance, in 7 and 15 frame skip evaluations, it shows up to 5.58 dB and 6.56 dB improvements in terms of PSNR over the corresponding second best methods BMBC and DAIN. SVFI also shows visually impressive performance in real-world scenes",
    "checked": true,
    "id": "a30499c63169d649d9104fa05de10dce6fef3e7b",
    "semantic_title": "svfi: spiking-based video frame interpolation for high-speed motion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25394": {
    "title": "FEditNet: Few-Shot Editing of Latent Semantics in GAN Spaces",
    "volume": "main",
    "abstract": "Generative Adversarial networks (GANs) have demonstrated their powerful capability of synthesizing high-resolution images, and great efforts have been made to interpret the semantics in the latent spaces of GANs. However, existing works still have the following limitations: (1) the majority of works rely on either pretrained attribute predictors or large-scale labeled datasets, which are difficult to collect in most cases, and (2) some other methods are only suitable for restricted cases, such as focusing on interpretation of human facial images using prior facial semantics. In this paper, we propose a GAN-based method called FEditNet, aiming to discover latent semantics using very few labeled data without any pretrained predictors or prior knowledge. Specifically, we reuse the knowledge from the pretrained GANs, and by doing so, avoid overfitting during the few-shot training of FEditNet. Moreover, our layer-wise objectives which take content consistency into account also ensure the disentanglement between attributes. Qualitative and quantitative results demonstrate that our method outperforms the state-of-the-art methods on various datasets. The code is available at https://github.com/THU-LYJ-Lab/FEditNet",
    "checked": true,
    "id": "5cde780c2bad086fa5267a11068bb40d320616b4",
    "semantic_title": "feditnet: few-shot editing of latent semantics in gan spaces",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25395": {
    "title": "Toward Robust Diagnosis: A Contour Attention Preserving Adversarial Defense for COVID-19 Detection",
    "volume": "main",
    "abstract": "As the COVID-19 pandemic puts pressure on healthcare systems worldwide, the computed tomography image based AI diagnostic system has become a sustainable solution for early diagnosis. However, the model-wise vulnerability under adversarial perturbation hinders its deployment in practical situation. The existing adversarial training strategies are difficult to generalized into medical imaging field challenged by complex medical texture features. To overcome this challenge, we propose a Contour Attention Preserving (CAP) method based on lung cavity edge extraction. The contour prior features are injected to attention layer via a parameter regularization and we optimize the robust empirical risk with hybrid distance metric. We then introduce a new cross-nation CT scan dataset to evaluate the generalization capability of the adversarial robustness under distribution shift. Experimental results indicate that the proposed method achieves state-of-the-art performance in multiple adversarial defense and generalization tasks. The code and dataset are available at https://github.com/Quinn777/CAP",
    "checked": true,
    "id": "a68b491f64daf9eee7551469e5d7a39fa62515ad",
    "semantic_title": "toward robust diagnosis: a contour attention preserving adversarial defense for covid-19 detection",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25396": {
    "title": "Boosting Semi-Supervised Semantic Segmentation with Probabilistic Representations",
    "volume": "main",
    "abstract": "Recent breakthroughs in semi-supervised semantic segmentation have been developed through contrastive learning. In prevalent pixel-wise contrastive learning solutions, the model maps pixels to deterministic representations and regularizes them in the latent space. However, there exist inaccurate pseudo-labels which map the ambiguous representations of pixels to the wrong classes due to the limited cognitive ability of the model. In this paper, we define pixel-wise representations from a new perspective of probability theory and propose a Probabilistic Representation Contrastive Learning (PRCL) framework that improves representation quality by taking its probability into consideration. Through modelling the mapping from pixels to representations as the probability via multivariate Gaussian distributions, we can tune the contribution of the ambiguous representations to tolerate the risk of inaccurate pseudo-labels. Furthermore, we define prototypes in the form of distributions, which indicates the confidence of a class, while the point prototype cannot. More- over, we propose to regularize the distribution variance to enhance the reliability of representations. Taking advantage of these benefits, high-quality feature representations can be derived in the latent space, thereby the performance of se- mantic segmentation can be further improved. We conduct sufficient experiment to evaluate PRCL on Pascal VOC and CityScapes to demonstrate its superiority. The code is available at https://github.com/Haoyu-Xie/PRCL",
    "checked": true,
    "id": "12e668ad61fbcfb1e4752f1243082dc8c0f75f4f",
    "semantic_title": "boosting semi-supervised semantic segmentation with probabilistic representations",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25397": {
    "title": "Less Is More Important: An Attention Module Guided by Probability Density Function for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Attention modules, which adaptively weight and refine features according to the importance of the input, have become a critical technique to boost the capability of convolutional neural networks. However, most existing attention modules are heuristic without a sound interpretation, and thus, require empirical engineering to design structure and operators within the modules. To handle the above issue, based on our 'less is more important' observation, we propose an Attention Module guided by Probability Density Function (PDF), dubbed PdfAM, which enjoys a rational motivation and requires few empirical structure designs. Concretely, we observe that pixels with less occurrence are prone to be textural details or foreground objects with much importance to aid vision tasks. Thus, with PDF values adopted as a smooth and anti-noise alternative to the pixel occurrence frequency, we design our PdfAM by first estimating the PDF based on some distribution assumption, and then predicting a 3D attention map via applying a negative correlation between the attention weights and the estimated PDF values. Furthermore, we develop learnable PDF-rescale parameters so as to adaptively transform the estimated PDF and predict a customized negative correlation. Experiments show that our PdfAM consistently boosts various networks under both high- and low-level vision tasks, and also performs favorably against other attention modules in terms of accuracy and convergence",
    "checked": true,
    "id": "7624dc2e2c610040b778ce11c3261523e7b27c93",
    "semantic_title": "less is more important: an attention module guided by probability density function for convolutional neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25398": {
    "title": "Mitigating Artifacts in Real-World Video Super-resolution Models",
    "volume": "main",
    "abstract": "The recurrent structure is a prevalent framework for the task of video super-resolution, which models the temporal dependency between frames via hidden states. When applied to real-world scenarios with unknown and complex degradations, hidden states tend to contain unpleasant artifacts and propagate them to restored frames. In this circumstance, our analyses show that such artifacts can be largely alleviated when the hidden state is replaced with a cleaner counterpart. Based on the observations, we propose a Hidden State Attention (HSA) module to mitigate artifacts in real-world video super-resolution. Specifically, we first adopt various cheap filters to produce a hidden state pool. For example, Gaussian blur filters are for smoothing artifacts while sharpening filters are for enhancing details. To aggregate a new hidden state that contains fewer artifacts from the hidden state pool, we devise a Selective Cross Attention (SCA) module, in which the attention between input features and each hidden state is calculated. Equipped with HSA, our proposed method, namely FastRealVSR, is able to achieve 2x speedup while obtaining better performance than Real-BasicVSR. Codes will be available at https://github.com/TencentARC/FastRealVSR",
    "checked": true,
    "id": "442f3bde58bd8f736f8643a5fb2feec0854bf368",
    "semantic_title": "mitigating artifacts in real-world video super-resolution models",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25399": {
    "title": "Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-Driven Approach",
    "volume": "main",
    "abstract": "Just noticeable difference (JND) refers to the maximum visual change that human eyes cannot perceive, and it has a wide range of applications in multimedia systems. However, most existing JND approaches only focus on a single modality, and rarely consider the complementary effects of multimodal information. In this article, we investigate the JND modeling from an end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically, we explore three important visually sensitive modalities, including saliency, depth, and segmentation. To better utilize homologous multimodal information, we establish an effective fusion method via summation enhancement and subtractive offset, and align homologous multimodal features based on a self-attention driven encoder-decoder paradigm. Extensive experimental results on eight different benchmark datasets validate the superiority of our hmJND-Net over eight representative methods",
    "checked": true,
    "id": "4dc3f4d5a05f9af6b5c78f67b8cb6704738ad7e5",
    "semantic_title": "just noticeable visual redundancy forecasting: a deep multimodal-driven approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25400": {
    "title": "Cross-Modal Contrastive Learning for Domain Adaptation in 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "Domain adaptation for 3D point cloud has attracted a lot of interest since it can avoid the time-consuming labeling process of 3D data to some extent. A recent work named xMUDA leveraged multi-modal data to domain adaptation task of 3D semantic segmentation by mimicking the predictions between 2D and 3D modalities, and outperformed the previous single modality methods only using point clouds. Based on it, in this paper, we propose a novel cross-modal contrastive learning scheme to further improve the adaptation effects. By employing constraints from the correspondences between 2D pixel features and 3D point features, our method not only facilitates interaction between the two different modalities, but also boosts feature representations in both labeled source domain and unlabeled target domain. Meanwhile, to sufficiently utilize 2D context information for domain adaptation through cross-modal learning, we introduce a neighborhood feature aggregation module to enhance pixel features. The module employs neighborhood attention to aggregate nearby pixels in the 2D image, which relieves the mismatching between the two different modalities, arising from projecting relative sparse point cloud to dense image pixels. We evaluate our method on three unsupervised domain adaptation scenarios, including country-to-country, day-to-night, and dataset-to-dataset. Experimental results show that our approach outperforms existing methods, which demonstrates the effectiveness of the proposed method",
    "checked": true,
    "id": "ff5d423b6dc934d58c7541df31b507fdda059be5",
    "semantic_title": "cross-modal contrastive learning for domain adaptation in 3d semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25401": {
    "title": "ROIFormer: Semantic-Aware Region of Interest Transformer for Efficient Self-Supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "The exploration of mutual-benefit cross-domains has shown great potential toward accurate self-supervised depth estimation. In this work, we revisit feature fusion between depth and semantic information and propose an efficient local adaptive attention method for geometric aware representation enhancement. Instead of building global connections or deforming attention across the feature space without restraint, we bound the spatial interaction within a learnable region of interest. In particular, we leverage geometric cues from semantic information to learn local adaptive bounding boxes to guide unsupervised feature aggregation. The local areas preclude most irrelevant reference points from attention space, yielding more selective feature learning and faster convergence. We naturally extend the paradigm into a multi-head and hierarchic way to enable the information distillation in different semantic levels and improve the feature discriminative ability for fine-grained depth estimation. Extensive experiments on the KITTI dataset show that our proposed method establishes a new state-of-the-art in self-supervised monocular depth estimation task, demonstrating the effectiveness of our approach over former Transformer variants",
    "checked": true,
    "id": "373959536e023e451b46e6e3d60228b59568a5ac",
    "semantic_title": "roiformer: semantic-aware region of interest transformer for efficient self-supervised monocular depth estimation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25402": {
    "title": "LORE: Logical Location Regression Network for Table Structure Recognition",
    "volume": "main",
    "abstract": "Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Recent methods solve this problem by predicting the adjacency relations of detected cell boxes, or learning to generate the corresponding markup sequences from the table images. However, they either count on additional heuristic rules to recover the table structures, or require a huge amount of training data and time-consuming sequential decoders. In this paper, we propose an alternative paradigm. We model TSR as a logical location regression problem and propose a new TSR framework called LORE, standing for LOgical location REgression network, which for the first time combines logical location regression together with spatial location regression of table cells. Our proposed LORE is conceptually simpler, easier to train and more accurate than previous TSR models of other paradigms. Experiments on standard benchmarks demonstrate that LORE consistently outperforms prior arts. Code is available at https:// github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LORE-TSR",
    "checked": true,
    "id": "a90a77893708ae5070e2ed4572ca9e20837e69c6",
    "semantic_title": "lore: logical location regression network for table structure recognition",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25403": {
    "title": "Revisiting the Spatial and Temporal Modeling for Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "Spatial and temporal modeling is one of the most core aspects of few-shot action recognition. Most previous works mainly focus on long-term temporal relation modeling based on high-level spatial representations, without considering the crucial low-level spatial features and short-term temporal relations. Actually, the former feature could bring rich local semantic information, and the latter feature could represent motion characteristics of adjacent frames, respectively. In this paper, we propose SloshNet, a new framework that revisits the spatial and temporal modeling for few-shot action recognition in a finer manner. First, to exploit the low-level spatial features, we design a feature fusion architecture search module to automatically search for the best combination of the low-level and high-level spatial features. Next, inspired by the recent transformer, we introduce a long-term temporal modeling module to model the global temporal relations based on the extracted spatial appearance features. Meanwhile, we design another short-term temporal modeling module to encode the motion characteristics between adjacent frame representations. After that, the final predictions can be obtained by feeding the embedded rich spatial-temporal features to a common frame-level class prototype matcher. We extensively validate the proposed SloshNet on four few-shot action recognition datasets, including Something-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorable results against state-of-the-art methods in all datasets",
    "checked": true,
    "id": "7723859aa67aa5324155dfafa1f078b3bc034178",
    "semantic_title": "revisiting the spatial and temporal modeling for few-shot action recognition",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25404": {
    "title": "Unsupervised Multi-Exposure Image Fusion Breaking Exposure Limits via Contrastive Learning",
    "volume": "main",
    "abstract": "This paper proposes an unsupervised multi-exposure image fusion (MEF) method via contrastive learning, termed as MEF-CL. It breaks exposure limits and performance bottleneck faced by existing methods. MEF-CL firstly designs similarity constraints to preserve contents in source images. It eliminates the need for ground truth (actually not exist and created artificially) and thus avoids negative impacts of inappropriate ground truth on performance and generalization. Moreover, we explore a latent feature space and apply contrastive learning in this space to guide fused image to approximate normal-light samples and stay away from inappropriately exposed ones. In this way, characteristics of fused images (e.g., illumination, colors) can be further improved without being subject to source images. Therefore, MEF-CL is applicable to image pairs of any multiple exposures rather than a pair of under-exposed and over-exposed images mandated by existing methods. By alleviating dependence on source images, MEF-CL shows better generalization for various scenes. Consequently, our results exhibit appropriate illumination, detailed textures, and saturated colors. Qualitative, quantitative, and ablation experiments validate the superiority and generalization of MEF-CL. Our code is publicly available at https://github.com/hanna-xu/MEF-CL",
    "checked": true,
    "id": "fc8f02331ed5baed6f5c0adb9c9cb2fca5e7c1a8",
    "semantic_title": "unsupervised multi-exposure image fusion breaking exposure limits via contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25405": {
    "title": "CasFusionNet: A Cascaded Network for Point Cloud Semantic Scene Completion by Dense Feature Fusion",
    "volume": "main",
    "abstract": "Semantic scene completion (SSC) aims to complete a partial 3D scene and predict its semantics simultaneously. Most existing works adopt the voxel representations, thus suffering from the growth of memory and computation cost as the voxel resolution increases. Though a few works attempt to solve SSC from the perspective of 3D point clouds, they have not fully exploited the correlation and complementarity between the two tasks of scene completion and semantic segmentation. In our work, we present CasFusionNet, a novel cascaded network for point cloud semantic scene completion by dense feature fusion. Specifically, we design (i) a global completion module (GCM) to produce an upsampled and completed but coarse point set, (ii) a semantic segmentation module (SSM) to predict the per-point semantic labels of the completed points generated by GCM, and (iii) a local refinement module (LRM) to further refine the coarse completed points and the associated labels from a local perspective. We organize the above three modules via dense feature fusion in each level, and cascade a total of four levels, where we also employ feature fusion between each level for sufficient information usage. Both quantitative and qualitative results on our compiled two point-based datasets validate the effectiveness and superiority of our CasFusionNet compared to state-of-the-art methods in terms of both scene completion and semantic segmentation. The codes and datasets are available at: https://github.com/JinfengX/CasFusionNet",
    "checked": true,
    "id": "ede618392b52947f4103415c97e444c20697e550",
    "semantic_title": "casfusionnet: a cascaded network for point cloud semantic scene completion by dense feature fusion",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25406": {
    "title": "Learning a Generalized Gaze Estimator from Gaze-Consistent Feature",
    "volume": "main",
    "abstract": "Gaze estimator computes the gaze direction based on face images. Most existing gaze estimation methods perform well under within-dataset settings, but can not generalize to unseen domains. In particular, the ground-truth labels in unseen domain are often unavailable. In this paper, we propose a new domain generalization method based on gaze-consistent features. Our idea is to consider the gaze-irrelevant factors as unfavorable interference and disturb the training data against them, so that the model cannot fit to these gaze-irrelevant factors, instead, only fits to the gaze-consistent features. To this end, we first disturb the training data via adversarial attack or data augmentation based on the gaze-irrelevant factors, i.e., identity, expression, illumination and tone. Then we extract the gaze-consistent features by aligning the gaze features from disturbed data with non-disturbed gaze features. Experimental results show that our proposed method achieves state-of-the-art performance on gaze domain generalization task. Furthermore, our proposed method also improves domain adaption performance on gaze estimation. Our work provides new insight on gaze domain generalization task",
    "checked": true,
    "id": "7a79ce59268ff9b07f61ba730bdfb8f0eaa54ab0",
    "semantic_title": "learning a generalized gaze estimator from gaze-consistent feature",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25407": {
    "title": "Class Overwhelms: Mutual Conditional Blended-Target Domain Adaptation",
    "volume": "main",
    "abstract": "Current methods of blended targets domain adaptation (BTDA) usually infer or consider domain label information but underemphasize hybrid categorical feature structures of targets, which yields limited performance, especially under the label distribution shift. We demonstrate that domain labels are not directly necessary for BTDA if categorical distributions of various domains are sufficiently aligned even facing the imbalance of domains and the label distribution shift of classes. However, we observe that the cluster assumption in BTDA does not comprehensively hold. The hybrid categorical feature space hinders the modeling of categorical distributions and the generation of reliable pseudo labels for categorical alignment. To address these, we propose a categorical domain discriminator guided by uncertainty to explicitly model and directly align categorical distributions P(Z|Y). Simultaneously, we utilize the low-level features to augment the single source features with diverse target styles to rectify the biased classifier P(Y|Z) among diverse targets. Such a mutual conditional alignment of P(Z|Y) and P(Y|Z) forms a mutual reinforced mechanism. Our approach outperforms the state-of-the-art in BTDA even compared with methods utilizing domain labels, especially under the label distribution shift, and in single target DA on DomainNet",
    "checked": true,
    "id": "44891165b360ccf98d7f6fd7f5361a6d5f3b4a3b",
    "semantic_title": "class overwhelms: mutual conditional blended-target domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25408": {
    "title": "Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Efficiently training accurate deep models for weakly supervised semantic segmentation (WSSS) with image-level labels is challenging and important. Recently, end-to-end WSSS methods have become the focus of research due to their high training efficiency. However, current methods suffer from insufficient extraction of comprehensive semantic information, resulting in low-quality pseudo-labels and sub-optimal solutions for end-to-end WSSS. To this end, we propose a simple and novel Self Correspondence Distillation (SCD) method to refine pseudo-labels without introducing external supervision. Our SCD enables the network to utilize feature correspondence derived from itself as a distillation target, which can enhance the network's feature learning process by complementing semantic information. In addition, to further improve the segmentation accuracy, we design a Variation-aware Refine Module to enhance the local consistency of pseudo-labels by computing pixel-level variation. Finally, we present an efficient end-to-end Transformer-based framework (TSCD) via SCD and Variation-aware Refine Module for the accurate WSSS task. Extensive experiments on the PASCAL VOC 2012 and MS COCO 2014 datasets demonstrate that our method significantly outperforms other state-of-the-art methods. Our code is available at https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/SCD-AAAI2023",
    "checked": true,
    "id": "17b71c39617d654ff345d7e48491068f7d519b0c",
    "semantic_title": "self correspondence distillation for end-to-end weakly-supervised semantic segmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25409": {
    "title": "Deep Parametric 3D Filters for Joint Video Denoising and Illumination Enhancement in Video Super Resolution",
    "volume": "main",
    "abstract": "Despite the quality improvement brought by the recent methods, video super-resolution (SR) is still very challenging, especially for videos that are low-light and noisy. The current best solution is to subsequently employ best models of video SR, denoising, and illumination enhancement, but doing so often lowers the image quality, due to the inconsistency between the models. This paper presents a new parametric representation called the Deep Parametric 3D Filters (DP3DF), which incorporates local spatiotemporal information to enable simultaneous denoising, illumination enhancement, and SR efficiently in a single encoder-and-decoder network. Also, a dynamic residual frame is jointly learned with the DP3DF via a shared backbone to further boost the SR quality. We performed extensive experiments, including a large-scale user study, to show our method's effectiveness. Our method consistently surpasses the best state-of-the-art methods on all the challenging real datasets with top PSNR and user ratings, yet having a very fast run time. The code is available at https://github.com/xiaogang00/DP3DF",
    "checked": true,
    "id": "e64440b53317fa8b5b800f238d500d586ae5e17f",
    "semantic_title": "deep parametric 3d filters for joint video denoising and illumination enhancement in video super resolution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25410": {
    "title": "Inter-image Contrastive Consistency for Multi-Person Pose Estimation",
    "volume": "main",
    "abstract": "Multi-person pose estimation (MPPE) has achieved impressive progress in recent years. However, due to the large variance of appearances among images or occlusions, the model can hardly learn consistent patterns enough, which leads to severe location jitter and missing issues. In this study, we propose a novel framework, termed Inter-image Contrastive consistency (ICON), to strengthen the keypoint consistency among images for MPPE. Concretely, we consider two-fold consistency constraints, which include single keypoint contrastive consistency (SKCC) and pair relation contrastive consistency (PRCC). The SKCC learns to strengthen the consistency of individual keypoints across images in the same category to improve the category-specific robustness. Only with SKCC, the model can effectively reduce location errors caused by large appearance variations, but remains challenging with extreme postures (e.g., occlusions) due to lack of relational guidance. Therefore, PRCC is proposed to strengthen the consistency of pair-wise joint relation between images to preserve the instructive relation. Cooperating with SKCC, PRCC further improves structure aware robustness in handling extreme postures. Extensive experiments on kinds of architectures across three datasets (i.e., MS-COCO, MPII, CrowdPose) show the proposed ICON achieves substantial improvements over baselines. Furthermore, ICON under the semi-supervised setup can obtain comparable results with the fully-supervised methods using only 30% labeled data",
    "checked": true,
    "id": "7e5b923a3498a977c0758a1578cb0af1fa180420",
    "semantic_title": "inter-image contrastive consistency for multi-person pose estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25411": {
    "title": "DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction",
    "volume": "main",
    "abstract": "Convolution neural networks (CNNs) and Transformers have their own advantages and both have been widely used for dense prediction in multi-task learning (MTL). Most of the current studies on MTL solely rely on CNN or Transformer. In this work, we present a novel MTL model by combining both merits of deformable CNN and query-based Transformer for multi-task learning of dense prediction. Our method, named DeMT, is based on a simple and effective encoder-decoder architecture (i.e., deformable mixer encoder and task-aware transformer decoder). First, the deformable mixer encoder contains two types of operators: the channel-aware mixing operator leveraged to allow communication among different channels (i.e., efficient channel location mixing), and the spatial-aware deformable operator with deformable convolution applied to efficiently sample more informative spatial locations (i.e., deformed features). Second, the task-aware transformer decoder consists of the task interaction block and task query block. The former is applied to capture task interaction features via self-attention. The latter leverages the deformed features and task-interacted features to generate the corresponding task-specific feature through a query-based Transformer for corresponding task predictions. Extensive experiments on two dense image prediction datasets, NYUD-v2 and PASCAL-Context, demonstrate that our model uses fewer GFLOPs and significantly outperforms current Transformer- and CNN-based competitive models on a variety of metrics. The code is available at https://github.com/yangyangxu0/DeMT",
    "checked": true,
    "id": "e3d3c1321554d7d14eec309e61ba70102b0629e1",
    "semantic_title": "demt: deformable mixer transformer for multi-task learning of dense prediction",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25412": {
    "title": "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning",
    "volume": "main",
    "abstract": "Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. Following the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github.com/UARK-AICV/VLTinT",
    "checked": true,
    "id": "c35fc28cb4e9336f4077cfc9cc559f55950b5996",
    "semantic_title": "vltint: visual-linguistic transformer-in-transformer for coherent video paragraph captioning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25413": {
    "title": "Rethinking Disparity: A Depth Range Free Multi-View Stereo Based on Disparity",
    "volume": "main",
    "abstract": "Existing learning-based multi-view stereo (MVS) methods rely on the depth range to build the 3D cost volume and may fail when the range is too large or unreliable. To address this problem, we propose a disparity-based MVS method based on the epipolar disparity flow (E-flow), called DispMVS, which infers the depth information from the pixel movement between two views. The core of DispMVS is to construct a 2D cost volume on the image plane along the epipolar line between each pair (between the reference image and several source images) for pixel matching and fuse uncountable depths triangulated from each pair by multi-view geometry to ensure multi-view consistency. To be robust, DispMVS starts from a randomly initialized depth map and iteratively refines the depth map with the help of the coarse-to-fine strategy. Experiments on DTUMVS and Tanks\\&Temple datasets show that DispMVS is not sensitive to the depth range and achieves state-of-the-art results with lower GPU memory",
    "checked": true,
    "id": "f1b14b7ed6601b4e1fd847a3043e749ca0bf02aa",
    "semantic_title": "rethinking disparity: a depth range free multi-view stereo based on disparity",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25414": {
    "title": "Video-Text Pre-training with Learned Regions for Retrieval",
    "volume": "main",
    "abstract": "Video-Text pre-training aims at learning transferable representations from large-scale video-text pairs via aligning the semantics between visual and textual information. State-of-the-art approaches extract visual features from raw pixels in an end-to-end fashion. However, these methods operate at frame-level directly and thus overlook the spatio-temporal structure of objects in video, which yet has a strong synergy with nouns in textual descriptions. In this work, we propose a simple yet effective module for video-text representation learning, namely RegionLearner, which can take into account the structure of objects during pre-training on large-scale video-text pairs. Given a video, our module (1) first quantizes continuous visual features via clustering patch-features into the same cluster according to content similarity, then (2) generates learnable masks to aggregate fragmentary features into regions with complete semantics, and finally (3) models the spatio-temporal dependencies between different semantic regions. In contrast to using off-the-shelf object detectors, our proposed module does not require explicit supervision and is much more computationally efficient. We pre-train the proposed approach on the public WebVid2M and CC3M datasets. Extensive evaluations on four downstream video-text retrieval benchmarks clearly demonstrate the effectiveness of our RegionLearner",
    "checked": true,
    "id": "615d077899e7c57c7073d427035499749fa7b355",
    "semantic_title": "video-text pre-training with learned regions for retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25415": {
    "title": "DesNet: Decomposed Scale-Consistent Network for Unsupervised Depth Completion",
    "volume": "main",
    "abstract": "Unsupervised depth completion aims to recover dense depth from the sparse one without using the ground-truth annotation. Although depth measurement obtained from LiDAR is usually sparse, it contains valid and real distance information, i.e., scale-consistent absolute depth values. Meanwhile, scale-agnostic counterparts seek to estimate relative depth and have achieved impressive performance. To leverage both the inherent characteristics, we thus suggest to model scale-consistent depth upon unsupervised scale-agnostic frameworks. Specifically, we propose the decomposed scale-consistent learning (DSCL) strategy, which disintegrates the absolute depth into relative depth prediction and global scale estimation, contributing to individual learning benefits. But unfortunately, most existing unsupervised scale-agnostic frameworks heavily suffer from depth holes due to the extremely sparse depth input and weak supervisory signal. To tackle this issue, we introduce the global depth guidance (GDG) module, which attentively propagates dense depth reference into the sparse target via novel dense-to-sparse attention. Extensive experiments show the superiority of our method on outdoor KITTI, ranking 1st and outperforming the best KBNet more than 12% in RMSE. Additionally, our approach achieves state-of-the-art performance on indoor NYUv2 benchmark as well",
    "checked": true,
    "id": "15d9392d12635221d6dba08a33344f6fc97060ce",
    "semantic_title": "desnet: decomposed scale-consistent network for unsupervised depth completion",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25416": {
    "title": "Self-Supervised Video Representation Learning via Latent Time Navigation",
    "volume": "main",
    "abstract": "Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as `enter' and `leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time parameterized contrastive learning strategy that is streamlined to capture fine-grained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In addition, we demonstrate that our proposed model, when pre-trained on Kinetics-400, generalizes well onto the unseen real world video benchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance in action recognition",
    "checked": true,
    "id": "71a971a7a86459eb7a568985fe398a7c79b3dd35",
    "semantic_title": "self-supervised video representation learning via latent time navigation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25417": {
    "title": "One-Shot Replay: Boosting Incremental Object Detection via Retrospecting One Object",
    "volume": "main",
    "abstract": "Modern object detectors are ill-equipped to incrementally learn new emerging object classes over time due to the well-known phenomenon of catastrophic forgetting. Due to data privacy or limited storage, few or no images of the old data can be stored for replay. In this paper, we design a novel One-Shot Replay (OSR) method for incremental object detection, which is an augmentation-based method. Rather than storing original images, only one object-level sample for each old class is stored to reduce memory usage significantly, and we find that copy-paste is a harmonious way to replay for incremental object detection. In the incremental learning procedure, diverse augmented samples with co-occurrence of old and new objects to existing training data are generated. To introduce more variants for objects of old classes, we propose two augmentation modules. The object augmentation module aims to enhance the ability of the detector to perceive potential unknown objects. The feature augmentation module explores the relations between old and new classes and augments the feature space via analogy. Extensive experimental results on VOC2007 and COCO demonstrate that OSR can outperform the state-of-the-art incremental object detection methods without using extra wild data",
    "checked": true,
    "id": "9bc81d69b990c814770096f3aa41ded4f639df6d",
    "semantic_title": "one-shot replay: boosting incremental object detection via retrospecting one object",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25418": {
    "title": "Video Event Extraction via Tracking Visual States of Arguments",
    "volume": "main",
    "abstract": "Video event extraction aims to detect salient events from a video and identify the arguments for each event as well as their semantic roles. Existing methods focus on capturing the overall visual scene of each frame, ignoring fine-grained argument-level information. Inspired by the definition of events as changes of states, we propose a novel framework to detect video events by tracking the changes in the visual states of all involved arguments, which are expected to provide the most informative evidence for the extraction of video events. In order to capture the visual state changes of arguments, we decompose them into changes in pixels within objects, displacements of objects, and interactions among multiple arguments. We further propose Object State Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to encode and track these changes respectively. Experiments on various video event extraction tasks demonstrate significant improvements compared to state-of-the-art models. In particular, on verb classification, we achieve 3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation Recognition. Our Code is publicly available at https://github.com/Shinetism/VStates for research purposes",
    "checked": true,
    "id": "2e3516142b2d7cf7fb67805a8b45fb077c9c6bb8",
    "semantic_title": "video event extraction via tracking visual states of arguments",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25419": {
    "title": "CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets",
    "volume": "main",
    "abstract": "Current RGB-D scene recognition approaches often train two standalone backbones for RGB and depth modalities with the same Places or ImageNet pre-training. However, the pre-trained depth network is still biased by RGB-based models which may result in a suboptimal solution. In this paper, we present a single-model self-supervised hybrid pre-training framework for RGB and depth modalities, termed as CoMAE. Our CoMAE presents a curriculum learning strategy to unify the two popular self-supervised representation learning algorithms: contrastive learning and masked image modeling. Specifically, we first build a patch-level alignment task to pre-train a single encoder shared by two modalities via cross-modal contrastive learning. Then, the pre-trained contrastive encoder is passed to a multi-modal masked autoencoder to capture the finer context features from a generative perspective. In addition, our single-model design without requirement of fusion module is very flexible and robust to generalize to unimodal scenario in both training and testing phases. Extensive experiments on SUN RGB-D and NYUDv2 datasets demonstrate the effectiveness of our CoMAE for RGB and depth representation learning. In addition, our experiment results reveal that CoMAE is a data-efficient representation learner. Although we only use the small-scale and unlabeled training set for pre-training, our CoMAE pre-trained models are still competitive to the state-of-the-art methods with extra large-scale and supervised RGB dataset pre-training. Code will be released at https://github.com/MCG-NJU/CoMAE",
    "checked": true,
    "id": "2feabd1e149da3aa82cc6d1d684cac836a0c01e7",
    "semantic_title": "comae: single model hybrid pre-training on small-scale rgb-d datasets",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25420": {
    "title": "Self-Asymmetric Invertible Network for Compression-Aware Image Rescaling",
    "volume": "main",
    "abstract": "High-resolution (HR) images are usually downscaled to low-resolution (LR) ones for better display and afterward upscaled back to the original size to recover details. Recent work in image rescaling formulates downscaling and upscaling as a unified task and learns a bijective mapping between HR and LR via invertible networks. However, in real-world applications (e.g., social media), most images are compressed for transmission. Lossy compression will lead to irreversible information loss on LR images, hence damaging the inverse upscaling procedure and degrading the reconstruction accuracy. In this paper, we propose the Self-Asymmetric Invertible Network (SAIN) for compression-aware image rescaling. To tackle the distribution shift, we first develop an end-to-end asymmetric framework with two separate bijective mappings for high-quality and compressed LR images, respectively. Then, based on empirical analysis of this framework, we model the distribution of the lost information (including downscaling and compression) using isotropic Gaussian mixtures and propose the Enhanced Invertible Block to derive high-quality/compressed LR images in one forward pass. Besides, we design a set of losses to regularize the learned LR images and enhance the invertibility. Extensive experiments demonstrate the consistent improvements of SAIN across various image rescaling datasets in terms of both quantitative and qualitative evaluation under standard image compression formats (i.e., JPEG and WebP). Code is available at https://github.com/yang-jin-hai/SAIN",
    "checked": true,
    "id": "ae0c998d3efe583b19fd4d1274eec3520c8f813a",
    "semantic_title": "self-asymmetric invertible network for compression-aware image rescaling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25421": {
    "title": "Stop-Gradient Softmax Loss for Deep Metric Learning",
    "volume": "main",
    "abstract": "Deep metric learning aims to learn a feature space that models the similarity between images, and feature normalization is a critical step for boosting performance. However directly optimizing L2-normalized softmax loss cause the network to fail to converge. Therefore some SOTA approaches appends a scale layer after the inner product to relieve the convergence problem, but it incurs a new problem that it's difficult to learn the best scaling parameters. In this letter, we look into the characteristic of softmax-based approaches and propose a novel learning objective function Stop-Gradient Softmax Loss (SGSL) to solve the convergence problem in softmax-based deep metric learning with L2-normalization. In addition, we found a useful trick named Remove the last BN-ReLU (RBR). It removes the last BN-ReLU in the backbone to reduce the learning burden of the model. Experimental results on four fine-grained image retrieval benchmarks show that our proposed approach outperforms most existing approaches, i.e., our approach achieves 75.9% on CUB-200-2011, 94.7% on CARS196 and 83.1% on SOP which outperforms other approaches at least 1.7%, 2.9% and 1.7% on Recall@1",
    "checked": true,
    "id": "a792e32749ebb98686791d4524f4e8d4ba575433",
    "semantic_title": "stop-gradient softmax loss for deep metric learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25422": {
    "title": "Local Path Integration for Attribution",
    "volume": "main",
    "abstract": "Path attribution methods are a popular tool to interpret a visual model's prediction on an input. They integrate model gradients for the input features over a path defined between the input and a reference, thereby satisfying certain desirable theoretical properties. However, their reliability hinges on the choice of the reference. Moreover, they do not exhibit weak dependence on the input, which leads to counter-intuitive feature attribution mapping. We show that path-based attribution can account for the weak dependence property by choosing the reference from the local distribution of the input. We devise a method to identify the local input distribution and propose a technique to stochastically integrate the model gradients over the paths defined by the references sampled from that distribution. Our local path integration (LPI) method is found to consistently outperform existing path attribution techniques when evaluated on deep visual models. Contributing to the ongoing search of reliable evaluation metrics for the interpretation methods, we also introduce DiffID metric that uses the relative difference between insertion and deletion games to alleviate the distribution shift problem faced by existing metrics. Our code is available at https://github.com/ypeiyu/LPI",
    "checked": true,
    "id": "87d4cf1dd6507e7c989b25a4a1eeea1a467c3a54",
    "semantic_title": "local path integration for attribution",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25423": {
    "title": "Spatiotemporal Deformation Perception for Fisheye Video Rectification",
    "volume": "main",
    "abstract": "Although the distortion correction of fisheye images has been extensively studied, the correction of fisheye videos is still an elusive challenge. For different frames of the fisheye video, the existing image correction methods ignore the correlation of sequences, resulting in temporal jitter in the corrected video. To solve this problem, we propose a temporal weighting scheme to get a plausible global optical flow, which mitigates the jitter effect by progressively reducing the weight of frames. Subsequently, we observe that the inter-frame optical flow of the video is facilitated to perceive the local spatial deformation of the fisheye video. Therefore, we derive the spatial deformation through the flows of fisheye and distorted-free videos, thereby enhancing the local accuracy of the predicted result. However, the independent correction for each frame disrupts the temporal correlation. Due to the property of fisheye video, a distorted moving object may be able to find its distorted-free pattern at another moment. To this end, a temporal deformation aggregator is designed to reconstruct the deformation correlation between frames and provide a reliable global feature. Our method achieves an end-to-end correction and demonstrates superiority in correction quality and stability compared with the SOTA correction methods",
    "checked": true,
    "id": "1fd89aa30d4917bde9a2343d5590456ce2d76ac8",
    "semantic_title": "spatiotemporal deformation perception for fisheye video rectification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25424": {
    "title": "Contrastive Multi-Task Dense Prediction",
    "volume": "main",
    "abstract": "This paper targets the problem of multi-task dense prediction which aims to achieve simultaneous learning and inference on a bunch of multiple dense prediction tasks in a single framework. A core objective in design is how to effectively model cross-task interactions to achieve a comprehensive improvement on different tasks based on their inherent complementarity and consistency. Existing works typically design extra expensive distillation modules to perform explicit interaction computations among different task-specific features in both training and inference, bringing difficulty in adaptation for different task sets, and reducing efficiency due to clearly increased size of multi-task models. In contrast, we introduce feature-wise contrastive consistency into modeling the cross-task interactions for multi-task dense prediction. We propose a novel multi-task contrastive regularization method based on the consistency to effectively boost the representation learning of the different sub-tasks, which can also be easily generalized to different multi-task dense prediction frameworks, and costs no additional computation in the inference. Extensive experiments on two challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the superiority of the proposed multi-task contrastive learning approach for dense predictions, establishing new state-of-the-art performances",
    "checked": true,
    "id": "59c681b58d85411ba5a70284947ad728f468c54c",
    "semantic_title": "contrastive multi-task dense prediction",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25425": {
    "title": "AutoStegaFont: Synthesizing Vector Fonts for Hiding Information in Documents",
    "volume": "main",
    "abstract": "Hiding information in text documents has been a hot topic recently, with the most typical schemes of utilizing fonts. By constructing several fonts with similar appearances, information can be effectively represented and embedded in documents. However, due to the unstructured characteristic, font vectors are more difficult to synthesize than font images. Existing methods mainly use handcrafted features to design the fonts manually, which is time-consuming and labor-intensive. Moreover, due to the diversity of fonts, handcrafted features are not generalizable to different fonts. Besides, in practice, since documents might be distorted through transmission, ensuring extractability under distortions is also an important requirement. Therefore, three requirements are imposed on vector font generation in this domain: automaticity, generalizability, and robustness. However, none of the existing methods can satisfy these requirements well and simultaneously. To satisfy the above requirements, we propose AutoStegaFont, an automatic vector font synthesis scheme for hiding information in documents. Specifically, we design a two-stage and dual-modality learning framework. In the first stage, we jointly train an encoder and a decoder to invisibly encode the font images with different information. To ensure robustness, we target designing a noise layer to work with the encoder and decoder during training. In the second stage, we employ a differentiable rasterizer to establish a connection between the image and the vector modality. Then, we design an optimization algorithm to convey the information from the encoded image to the corresponding vector. Thus the encoded font vectors can be automatically generated. Extensive experiments demonstrate the superior performance of our scheme in automatically synthesizing vector fonts for hiding information in documents, with robustness to distortions caused by low-resolution screenshots, printing, and photography. Besides, the proposed framework has better generalizability to fonts with diverse styles and languages",
    "checked": true,
    "id": "6225139fe937b85a1a4a7d03e10354745f764b5c",
    "semantic_title": "autostegafont: synthesizing vector fonts for hiding information in documents",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25426": {
    "title": "Towards Global Video Scene Segmentation with Context-Aware Transformer",
    "volume": "main",
    "abstract": "Videos such as movies or TV episodes usually need to divide the long storyline into cohesive units, i.e., scenes, to facilitate the understanding of video semantics. The key challenge lies in finding the boundaries of scenes by comprehensively considering the complex temporal structure and semantic information. To this end, we introduce a novel Context-Aware Transformer (CAT) with a self-supervised learning framework to learn high-quality shot representations, for generating well-bounded scenes. More specifically, we design the CAT with local-global self-attentions, which can effectively consider both the long-term and short-term context to improve the shot encoding. For training the CAT, we adopt the self-supervised learning schema. Firstly, we leverage shot-to-scene level pretext tasks to facilitate the pre-training with pseudo boundary, which guides CAT to learn the discriminative shot representations that maximize intra-scene similarity and inter-scene discrimination in an unsupervised manner. Then, we transfer contextual representations for fine-tuning the CAT with supervised data, which encourages CAT to accurately detect the boundary for scene segmentation. As a result, CAT is able to learn the context-aware shot representations and provides global guidance for scene segmentation. Our empirical analyses show that CAT can achieve state-of-the-art performance when conducting the scene segmentation task on the MovieNet dataset, e.g., offering 2.15 improvements on AP",
    "checked": true,
    "id": "807937ad69b41b6a320d9d180fa9fe0205d56804",
    "semantic_title": "towards global video scene segmentation with context-aware transformer",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25427": {
    "title": "Low-Light Image Enhancement Network Based on Multi-Scale Feature Complementation",
    "volume": "main",
    "abstract": "Images captured in low-light environments have problems of insufficient brightness and low contrast, which will affect subsequent image processing tasks. Although most current enhancement methods can obtain high-contrast images, they still suffer from noise amplification and color distortion. To address these issues, this paper proposes a low-light image enhancement network based on multi-scale feature complementation (LIEN-MFC), which is a U-shaped encoder-decoder network supervised by multiple images of different scales. In the encoder, four feature extraction branches are constructed to extract features of low-light images at different scales. In the decoder, to ensure the integrity of the learned features at each scale, a feature supplementary fusion module (FSFM) is proposed to complement and integrate features from different branches of the encoder and decoder. In addition, a feature restoration module (FRM) and an image reconstruction module (IRM) are built in each branch to reconstruct the restored features and output enhanced images. To better train the network, a joint loss function is defined, in which a discriminative loss term is designed to ensure that the enhanced results better meet the visual properties of the human eye. Extensive experiments on benchmark datasets show that the proposed method outperforms some state-of-the-art methods subjectively and objectively",
    "checked": true,
    "id": "b0287e28e409920e97b5b308974750914d60ba7b",
    "semantic_title": "low-light image enhancement network based on multi-scale feature complementation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25428": {
    "title": "Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation",
    "volume": "main",
    "abstract": "Referring image segmentation segments an image from a language expression. With the aim of producing high-quality masks, existing methods often adopt iterative learning approaches that rely on RNNs or stacked attention layers to refine vision-language features. Despite their complexity, RNN-based methods are subject to specific encoder choices, while attention-based methods offer limited gains. In this work, we introduce a simple yet effective alternative for progressively learning discriminative multi-modal features. The core idea of our approach is to leverage a continuously updated query as the representation of the target object and at each iteration, strengthen multi-modal features strongly correlated to the query while weakening less related ones. As the query is initialized by language features and successively updated by object features, our algorithm gradually shifts from being localization-centric to segmentation-centric. This strategy enables the incremental recovery of missing object parts and/or removal of extraneous parts through iteration. Compared to its counterparts, our method is more versatile—it can be plugged into prior arts straightforwardly and consistently bring improvements. Experimental results on the challenging datasets of RefCOCO, RefCOCO+, and G-Ref demonstrate its advantage with respect to the state-of-the-art methods",
    "checked": true,
    "id": "21cbb159992abffdee87c2a1bc15a3d98bdfde8a",
    "semantic_title": "semantics-aware dynamic localization and refinement for referring image segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25429": {
    "title": "LidarMultiNet: Towards a Unified Multi-Task Network for LiDAR Perception",
    "volume": "main",
    "abstract": "LiDAR-based 3D object detection, semantic segmentation, and panoptic segmentation are usually implemented in specialized networks with distinctive architectures that are difficult to adapt to each other. This paper presents LidarMultiNet, a LiDAR-based multi-task network that unifies these three major LiDAR perception tasks. Among its many benefits, a multi-task network can reduce the overall cost by sharing weights and computation among multiple tasks. However, it typically underperforms compared to independently combined single-task models. The proposed LidarMultiNet aims to bridge the performance gap between the multi-task network and multiple single-task networks. At the core of LidarMultiNet is a strong 3D voxel-based encoder-decoder architecture with a Global Context Pooling (GCP) module extracting global contextual features from a LiDAR frame. Task-specific heads are added on top of the network to perform the three LiDAR perception tasks. More tasks can be implemented simply by adding new task-specific heads while introducing little additional cost. A second stage is also proposed to refine the first-stage segmentation and generate accurate panoptic segmentation results. LidarMultiNet is extensively tested on both Waymo Open Dataset and nuScenes dataset, demonstrating for the first time that major LiDAR perception tasks can be unified in a single strong network that is trained end-to-end and achieves state-of-the-art performance. Notably, LidarMultiNet reaches the official 1 place in the Waymo Open Dataset 3D semantic segmentation challenge 2022 with the highest mIoU and the best accuracy for most of the 22 classes on the test set, using only LiDAR points as input. It also sets the new state-of-the-art for a single model on the Waymo 3D object detection benchmark and three nuScenes benchmarks",
    "checked": true,
    "id": "09ce90a7d46c297db17e41cfd4e7691933408540",
    "semantic_title": "lidarmultinet: towards a unified multi-task network for lidar perception",
    "citation_count": 24
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25430": {
    "title": "DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer",
    "volume": "main",
    "abstract": "Recently, Transformer-based methods, which predict polygon points or Bezier curve control points for localizing texts, are popular in scene text detection. However, these methods built upon detection transformer framework might achieve sub-optimal training efficiency and performance due to coarse positional query modeling. In addition, the point label form exploited in previous works implies the reading order of humans, which impedes the detection robustness from our observation. To address these challenges, this paper proposes a concise Dynamic Point Text DEtection TRansformer network, termed DPText-DETR. In detail, DPText-DETR directly leverages explicit point coordinates to generate position queries and dynamically updates them in a progressive way. Moreover, to improve the spatial inductive bias of non-local self-attention in Transformer, we present an Enhanced Factorized Self-Attention module which provides point queries within each instance with circular shape guidance. Furthermore, we design a simple yet effective positional label form to tackle the side effect of the previous form. To further evaluate the impact of different label forms on the detection robustness in real-world scenario, we establish an Inverse-Text test set containing 500 manually labeled images. Extensive experiments prove the high training efficiency, robustness, and state-of-the-art performance of our method on popular benchmarks. The code and the Inverse-Text test set are available at https://github.com/ymy-k/DPText-DETR",
    "checked": true,
    "id": "5f89d5e8e3c6e3bb035729c1f6039ee95149ce0b",
    "semantic_title": "dptext-detr: towards better scene text detection with dynamic points in transformer",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25431": {
    "title": "Learning Second-Order Attentive Context for Efficient Correspondence Pruning",
    "volume": "main",
    "abstract": "Correspondence pruning aims to search consistent correspondences (inliers) from a set of putative correspondences. It is challenging because of the disorganized spatial distribution of numerous outliers, especially when putative correspondences are largely dominated by outliers. It's more challenging to ensure effectiveness while maintaining efficiency. In this paper, we propose an effective and efficient method for correspondence pruning. Inspired by the success of attentive context in correspondence problems, we first extend the attentive context to the first-order attentive context and then introduce the idea of attention in attention (ANA) to model second-order attentive context for correspondence pruning. Compared with first-order attention that focuses on feature-consistent context, second-order attention dedicates to attention weights itself and provides an additional source to encode consistent context from the attention map. For efficiency, we derive two approximate formulations for the naive implementation of second-order attention to optimize the cubic complexity to linear complexity, such that second-order attention can be used with negligible computational overheads. We further implement our formulations in a second-order context layer and then incorporate the layer in an ANA block. Extensive experiments demonstrate that our method is effective and efficient in pruning outliers, especially in high-outlier-ratio cases. Compared with the state-of-the-art correspondence pruning approach LMCNet, our method runs 14 times faster while maintaining a competitive accuracy",
    "checked": true,
    "id": "f5faa47345366d623a8cd3b41e63276065c404a3",
    "semantic_title": "learning second-order attentive context for efficient correspondence pruning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25432": {
    "title": "Infusing Definiteness into Randomness: Rethinking Composition Styles for Deep Image Matting",
    "volume": "main",
    "abstract": "We study the composition style in deep image matting, a notion that characterizes a data generation flow on how to exploit limited foregrounds and random backgrounds to form a training dataset. Prior art executes this flow in a completely random manner by simply going through the foreground pool or by optionally combining two foregrounds before foreground-background composition. In this work, we first show that naive foreground combination can be problematic and therefore derive an alternative formulation to reasonably combine foregrounds. Our second contribution is an observation that matting performance can benefit from a certain occurrence frequency of combined foregrounds and their associated source foregrounds during training. Inspired by this, we introduce a novel composition style that binds the source and combined foregrounds in a definite triplet. In addition, we also find that different orders of foreground combination lead to different foreground patterns, which further inspires a quadruplet-based composition style. Results under controlled experiments on four matting baselines show that our composition styles outperform existing ones and invite consistent performance improvement on both composited and real-world datasets. Code is available at: https://github.com/coconuthust/composition_styles",
    "checked": true,
    "id": "28f4d454359641f62944674ea085e84aff5ce023",
    "semantic_title": "infusing definiteness into randomness: rethinking composition styles for deep image matting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25433": {
    "title": "Can We Find Strong Lottery Tickets in Generative Models?",
    "volume": "main",
    "abstract": "Yes. In this paper, we investigate strong lottery tickets in generative models, the subnetworks that achieve good generative performance without any weight update. Neural network pruning is considered the main cornerstone of model compression for reducing the costs of computation and memory. Unfortunately, pruning a generative model has not been extensively explored, and all existing pruning algorithms suffer from excessive weight-training costs, performance degradation, limited generalizability, or complicated training. To address these problems, we propose to find a strong lottery ticket via moment-matching scores. Our experimental results show that the discovered subnetwork can perform similarly or better than the trained dense model even when only 10% of the weights remain. To the best of our knowledge, we are the first to show the existence of strong lottery tickets in generative models and provide an algorithm to find it stably. Our code and supplementary materials are publicly available at https://lait-cvlab.github.io/SLT-in-Generative-Models/",
    "checked": true,
    "id": "74fb4456760e417ec2873cfab1e4c87ce59df8f6",
    "semantic_title": "can we find strong lottery tickets in generative models?",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25434": {
    "title": "Class-Independent Regularization for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Training deep neural networks (DNNs) with noisy labels often leads to poorly generalized models as DNNs tend to memorize the noisy labels in training. Various strategies have been developed for improving sample selection precision and mitigating the noisy label memorization issue. However, most existing works adopt a class-dependent softmax classifier that is vulnerable to noisy labels by entangling the classification of multi-class features. This paper presents a class-independent regularization (CIR) method that can effectively alleviate the negative impact of noisy labels in DNN training. CIR regularizes the class-dependent softmax classifier by introducing multi-binary classifiers each of which takes care of one class only. Thanks to its class-independent nature, CIR is tolerant to noisy labels as misclassification by one binary classifier does not affect others. For effective training of CIR, we design a heterogeneous adaptive co-teaching strategy that forces the class-independent and class-dependent classifiers to focus on sample selection and image classification, respectively, in a cooperative manner. Extensive experiments show that CIR achieves superior performance consistently across multiple benchmarks with both synthetic and real images. Code is available at https://github.com/RumengYi/CIR",
    "checked": true,
    "id": "284858ac99309568e83377a9968a1908ee28c717",
    "semantic_title": "class-independent regularization for learning with noisy labels",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25435": {
    "title": "Unbiased Heterogeneous Scene Graph Generation with Relation-Aware Message Passing Neural Network",
    "volume": "main",
    "abstract": "Recent scene graph generation (SGG) frameworks have focused on learning complex relationships among multiple objects in an image. Thanks to the nature of the message passing neural network (MPNN) that models high-order interactions between objects and their neighboring objects, they are dominant representation learning modules for SGG. However, existing MPNN-based frameworks assume the scene graph as a homogeneous graph, which restricts the context-awareness of visual relations between objects. That is, they overlook the fact that the relations tend to be highly dependent on the objects with which the relations are associated. In this paper, we propose an unbiased heterogeneous scene graph generation (HetSGG) framework that captures relation-aware context using message passing neural networks. We devise a novel message passing layer, called relation-aware message passing neural network (RMP), that aggregates the contextual information of an image considering the predicate type between objects. Our extensive evaluations demonstrate that HetSGG outperforms state-of-the-art methods, especially outperforming on tail predicate classes. The source code for HetSGG is available at https://github.com/KanghoonYoon/hetsgg-torch",
    "checked": true,
    "id": "b02f9674a07d6a7e4ab5f4846301dc8d15433e46",
    "semantic_title": "unbiased heterogeneous scene graph generation with relation-aware message passing neural network",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25436": {
    "title": "Lifelong Person Re-identification via Knowledge Refreshing and Consolidation",
    "volume": "main",
    "abstract": "Lifelong person re-identification (LReID) is in significant demand for real-world development as a large amount of ReID data is captured from diverse locations over time and cannot be accessed at once inherently. However, a key challenge for LReID is how to incrementally preserve old knowledge and gradually add new capabilities to the system. Unlike most existing LReID methods, which mainly focus on dealing with catastrophic forgetting, our focus is on a more challenging problem, which is, not only trying to reduce the forgetting on old tasks but also aiming to improve the model performance on both new and old tasks during the lifelong learning process. Inspired by the biological process of human cognition where the somatosensory neocortex and the hippocampus work together in memory consolidation, we formulated a model called Knowledge Refreshing and Consolidation (KRC) that achieves both positive forward and backward transfer. More specifically, a knowledge refreshing scheme is incorporated with the knowledge rehearsal mechanism to enable bi-directional knowledge transfer by introducing a dynamic memory model and an adaptive working model. Moreover, a knowledge consolidation scheme operating on the dual space further improves model stability over the long-term. Extensive evaluations show KRC's superiority over the state-of-the-art LReID methods with challenging pedestrian benchmarks. Code is available at https://github.com/cly234/LReID-KRKC",
    "checked": true,
    "id": "cdba9c57d5e372bee7a382c2f287eb20da244977",
    "semantic_title": "lifelong person re-identification via knowledge refreshing and consolidation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25437": {
    "title": "Generalizing Multiple Object Tracking to Unseen Domains by Introducing Natural Language Representation",
    "volume": "main",
    "abstract": "Although existing multi-object tracking (MOT) algorithms have obtained competitive performance on various benchmarks, almost all of them train and validate models on the same domain. The domain generalization problem of MOT is hardly studied. To bridge this gap, we first draw the observation that the high-level information contained in natural language is domain invariant to different tracking domains. Based on this observation, we propose to introduce natural language representation into visual MOT models for boosting the domain generalization ability. However, it is infeasible to label every tracking target with a textual description. To tackle this problem, we design two modules, namely visual context prompting (VCP) and visual-language mixing (VLM). Specifically, VCP generates visual prompts based on the input frames. VLM joints the information in the generated visual prompts and the textual prompts from a pre-defined Trackbook to obtain instance-level pseudo textual description, which is domain invariant to different tracking scenes. Through training models on MOT17 and validating them on MOT20, we observe that the pseudo textual descriptions generated by our proposed modules improve the generalization performance of query-based trackers by large margins",
    "checked": true,
    "id": "dddeca3509132717a8b5843f730b8b01a3887dcc",
    "semantic_title": "generalizing multiple object tracking to unseen domains by introducing natural language representation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25438": {
    "title": "Rethinking Rotation Invariance with Point Cloud Registration",
    "volume": "main",
    "abstract": "Recent investigations on rotation invariance for 3D point clouds have been devoted to devising rotation-invariant feature descriptors or learning canonical spaces where objects are semantically aligned. Examinations of learning frameworks for invariance have seldom been looked into. In this work, we review rotation invariance (RI) in terms of point cloud registration (PCR) and propose an effective framework for rotation invariance learning via three sequential stages, namely rotation-invariant shape encoding, aligned feature integration, and deep feature registration. We first encode shape descriptors constructed with respect to reference frames defined over different scales, e.g., local patches and global topology, to generate rotation-invariant latent shape codes. Within the integration stage, we propose an Aligned Integration Transformer (AIT) to produce a discriminative feature representation by integrating point-wise self- and cross-relations established within the shape codes. Meanwhile, we adopt rigid transformations between reference frames to align the shape codes for feature consistency across different scales. Finally, the deep integrated feature is registered to both rotation-invariant shape codes to maximize their feature similarities, such that rotation invariance of the integrated feature is preserved and shared semantic information is implicitly extracted from shape codes. Experimental results on 3D shape classification, part segmentation, and retrieval tasks prove the feasibility of our framework. Our project page is released at: https://rotation3d.github.io/",
    "checked": true,
    "id": "6ca5939d6e64dc751de8bd513980f57a2d43af55",
    "semantic_title": "rethinking rotation invariance with point cloud registration",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25439": {
    "title": "Frame-Level Label Refinement for Skeleton-Based Weakly-Supervised Action Recognition",
    "volume": "main",
    "abstract": "In recent years, skeleton-based action recognition has achieved remarkable performance in understanding human motion from sequences of skeleton data, which is an important medium for synthesizing realistic human movement in various applications. However, existing methods assume that each action clip is manually trimmed to contain one specific action, which requires a significant amount of effort for annotation. To solve this problem, we consider a novel problem of skeleton-based weakly-supervised temporal action localization (S-WTAL), where we need to recognize and localize human action segments in untrimmed skeleton videos given only the video-level labels. Although this task is challenging due to the sparsity of skeleton data and the lack of contextual clues from interaction with other objects and the environment, we present a frame-level label refinement framework based on a spatio-temporal graph convolutional network (ST-GCN) to overcome these difficulties. We use multiple instance learning (MIL) with video-level labels to generate the frame-level predictions. Inspired by advances in handling the noisy label problem, we introduce a label cleaning strategy of the frame-level pseudo labels to guide the learning process. The network parameters and the frame-level predictions are alternately updated to obtain the final results. We extensively evaluate the effectiveness of our learning approach on skeleton-based action recognition benchmarks. The state-of-the-art experimental results demonstrate that the proposed method can recognize and localize action segments of the skeleton data",
    "checked": true,
    "id": "ac0d48894ddbb9232730ab54f9a7754dc6e73c45",
    "semantic_title": "frame-level label refinement for skeleton-based weakly-supervised action recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25440": {
    "title": "Recurrent Structure Attention Guidance for Depth Super-resolution",
    "volume": "main",
    "abstract": "Image guidance is an effective strategy for depth super-resolution. Generally, most existing methods employ hand-crafted operators to decompose the high-frequency (HF) and low-frequency (LF) ingredients from low-resolution depth maps and guide the HF ingredients by directly concatenating them with image features. However, the hand-designed operators usually cause inferior HF maps (e.g., distorted or structurally missing) due to the diverse appearance of complex depth maps. Moreover, the direct concatenation often results in weak guidance because not all image features have a positive effect on the HF maps. In this paper, we develop a recurrent structure attention guided (RSAG) framework, consisting of two important parts. First, we introduce a deep contrastive network with multi-scale filters for adaptive frequency-domain separation, which adopts contrastive networks from large filters to small ones to calculate the pixel contrasts for adaptive high-quality HF predictions. Second, instead of the coarse concatenation guidance, we propose a recurrent structure attention block, which iteratively utilizes the latest depth estimation and the image features to jointly select clear patterns and boundaries, aiming at providing refined guidance for accurate depth recovery. In addition, we fuse the features of HF maps to enhance the edge structures in the decomposed LF maps. Extensive experiments show that our approach obtains superior performance compared with state-of-the-art depth super-resolution methods. Our code is available at: https://github.com/Yuanjiayii/DSR-RSAG",
    "checked": true,
    "id": "5010526e685c33293be4317720235e639a3bf2e7",
    "semantic_title": "recurrent structure attention guidance for depth super-resolution",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25441": {
    "title": "Structure Flow-Guided Network for Real Depth Super-resolution",
    "volume": "main",
    "abstract": "Real depth super-resolution (DSR), unlike synthetic settings, is a challenging task due to the structural distortion and the edge noise caused by the natural degradation in real-world low-resolution (LR) depth maps. These defeats result in significant structure inconsistency between the depth map and the RGB guidance, which potentially confuses the RGB-structure guidance and thereby degrades the DSR quality. In this paper, we propose a novel structure flow-guided DSR framework, where a cross-modality flow map is learned to guide the RGB-structure information transferring for precise depth upsampling. Specifically, our framework consists of a cross-modality flow-guided upsampling network (CFUNet) and a flow-enhanced pyramid edge attention network (PEANet). CFUNet contains a trilateral self-attention module combining both the geometric and semantic correlations for reliable cross-modality flow learning. Then, the learned flow maps are combined with the grid-sampling mechanism for coarse high-resolution (HR) depth prediction. PEANet targets at integrating the learned flow map as the edge attention into a pyramid network to hierarchically learn the edge-focused guidance feature for depth edge refinement. Extensive experiments on real and synthetic DSR datasets verify that our approach achieves excellent performance compared to state-of-the-art methods. Our code is available at: https://github.com/Yuanjiayii/DSR-SFG",
    "checked": true,
    "id": "98338f97c57fdaea642fa19df65d761349562ca4",
    "semantic_title": "structure flow-guided network for real depth super-resolution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25442": {
    "title": "Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network",
    "volume": "main",
    "abstract": "Model inversion (MI) attacks have raised increasing concerns about privacy, which can reconstruct training data from public models. Indeed, MI attacks can be formalized as an optimization problem that seeks private data in a certain space. Recent MI attacks leverage a generative adversarial network (GAN) as an image prior to narrow the search space, and can successfully reconstruct even the high-dimensional data (e.g., face images). However, these generative MI attacks do not fully exploit the potential capabilities of the target model, still leading to a vague and coupled search space, i.e., different classes of images are coupled in the search space. Besides, the widely used cross-entropy loss in these attacks suffers from gradient vanishing. To address these problems, we propose Pseudo Label-Guided MI (PLG-MI) attack via conditional GAN (cGAN). At first, a top-n selection strategy is proposed to provide pseudo-labels for public data, and use pseudo-labels to guide the training of the cGAN. In this way, the search space is decoupled for different classes of images. Then a max-margin loss is introduced to improve the search process on the subspace of a target class. Extensive experiments demonstrate that our PLG-MI attack significantly improves the attack success rate and visual quality for various datasets and models, notably, 2 ∼ 3× better than state-of-the-art attacks under large distributional shifts. Our code is available at: https://github.com/LetheSec/PLG-MI-Attack",
    "checked": true,
    "id": "b489c6fe772b861a99a14f5f6474b0d44a4a8f2e",
    "semantic_title": "pseudo label-guided model inversion attack via conditional generative adversarial network",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25443": {
    "title": "Cyclically Disentangled Feature Translation for Face Anti-spoofing",
    "volume": "main",
    "abstract": "Current domain adaptation methods for face anti-spoofing leverage labeled source domain data and unlabeled target domain data to obtain a promising generalizable decision boundary. However, it is usually difficult for these methods to achieve a perfect domain-invariant liveness feature disentanglement, which may degrade the final classification performance by domain differences in illumination, face category, spoof type, etc. In this work, we tackle cross-scenario face anti-spoofing by proposing a novel domain adaptation method called cyclically disentangled feature translation network (CDFTN). Specifically, CDFTN generates pseudo-labeled samples that possess: 1) source domain-invariant liveness features and 2) target domain-specific content features, which are disentangled through domain adversarial training. A robust classifier is trained based on the synthetic pseudo-labeled images under the supervision of source domain labels. We further extend CDFTN for multi-target domain adaptation by leveraging data from more unlabeled target domains. Extensive experiments on several public datasets demonstrate that our proposed approach significantly outperforms the state of the art. Code and models are available at https://github.com/vis-face/CDFTN",
    "checked": true,
    "id": "9abd5e50e280323e3446595d23aa696113346465",
    "semantic_title": "cyclically disentangled feature translation for face anti-spoofing",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25444": {
    "title": "FlowFace: Semantic Flow-Guided Shape-Aware Face Swapping",
    "volume": "main",
    "abstract": "In this work, we propose a semantic flow-guided two-stage framework for shape-aware face swapping, namely FlowFace. Unlike most previous methods that focus on transferring the source inner facial features but neglect facial contours, our FlowFace can transfer both of them to a target face, thus leading to more realistic face swapping. Concretely, our FlowFace consists of a face reshaping network and a face swapping network. The face reshaping network addresses the shape outline differences between the source and target faces. It first estimates a semantic flow (i.e. face shape differences) between the source and the target face, and then explicitly warps the target face shape with the estimated semantic flow. After reshaping, the face swapping network generates inner facial features that exhibit the identity of the source face. We employ a pre-trained face masked autoencoder (MAE) to extract facial features from both the source face and the target face. In contrast to previous methods that use identity embedding to preserve identity information, the features extracted by our encoder can better capture facial appearances and identity information. Then, we develop a cross-attention fusion module to adaptively fuse inner facial features from the source face with the target facial attributes, thus leading to better identity preservation. Extensive quantitative and qualitative experiments on in-the-wild faces demonstrate that our FlowFace outperforms the state-of-the-art significantly",
    "checked": true,
    "id": "166d8ab629f46c818f42fa4e802a6033d42ece16",
    "semantic_title": "flowface: semantic flow-guided shape-aware face swapping",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25445": {
    "title": "Multi-Modal Knowledge Hypergraph for Diverse Image Retrieval",
    "volume": "main",
    "abstract": "The task of keyword-based diverse image retrieval has received considerable attention due to its wide demand in real-world scenarios. Existing methods either rely on a multi-stage re-ranking strategy based on human design to diversify results, or extend sub-semantics via an implicit generator, which either relies on manual labor or lacks explainability. To learn more diverse and explainable representations, we capture sub-semantics in an explicit manner by leveraging the multi-modal knowledge graph (MMKG) that contains richer entities and relations. However, the huge domain gap between the off-the-shelf MMKG and retrieval datasets, as well as the semantic gap between images and texts, make the fusion of MMKG difficult. In this paper, we pioneer a degree-free hypergraph solution that models many-to-many relations to address the challenge of heterogeneous sources and heterogeneous modalities. Specifically, a hyperlink-based solution, Multi-Modal Knowledge Hyper Graph (MKHG) is proposed, which bridges heterogeneous data via various hyperlinks to diversify sub-semantics. Among them, a hypergraph construction module first customizes various hyperedges to link the heterogeneous MMKG and retrieval databases. A multi-modal instance bagging module then explicitly selects instances to diversify the semantics. Meanwhile, a diverse concept aggregator flexibly adapts key sub-semantics. Finally, several losses are adopted to optimize the semantic space. Extensive experiments on two real-world datasets have well verified the effectiveness and explainability of our proposed method",
    "checked": true,
    "id": "f1820749124aa1eaa51c47cb084f13c76b1a8ed2",
    "semantic_title": "multi-modal knowledge hypergraph for diverse image retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25446": {
    "title": "Learnable Blur Kernel for Single-Image Defocus Deblurring in the Wild",
    "volume": "main",
    "abstract": "Recent research showed that the dual-pixel sensor has made great progress in defocus map estimation and image defocus deblurring. However, extracting real-time dual-pixel views is troublesome and complex in algorithm deployment. Moreover, the deblurred image generated by the defocus deblurring network lacks high-frequency details, which is unsatisfactory in human perception. To overcome this issue, we propose a novel defocus deblurring method that uses the guidance of the defocus map to implement image deblurring. The proposed method consists of a learnable blur kernel to estimate the defocus map, which is an unsupervised method, and a single-image defocus deblurring generative adversarial network (DefocusGAN) for the first time. The proposed network can learn the deblurring of different regions and recover realistic details. We propose a defocus adversarial loss to guide this training process. Competitive experimental results confirm that with a learnable blur kernel, the generated defocus map can achieve results comparable to supervised methods. In the single-image defocus deblurring task, the proposed method achieves state-of-the-art results, especially significant improvements in perceptual quality, where PSNR reaches 25.56 dB and LPIPS reaches 0.111",
    "checked": true,
    "id": "297c953df13c4375527df215b57f9c72c036a569",
    "semantic_title": "learnable blur kernel for single-image defocus deblurring in the wild",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25447": {
    "title": "Darwinian Model Upgrades: Model Evolving with Selective Compatibility",
    "volume": "main",
    "abstract": "The traditional model upgrading paradigm for retrieval requires recomputing all gallery embeddings before deploying the new model (dubbed as \"backfilling\"), which is quite expensive and time-consuming considering billions of instances in industrial applications. BCT presents the first step towards backward-compatible model upgrades to get rid of backfilling. It is workable but leaves the new model in a dilemma between new feature discriminativeness and new-to-old compatibility due to the undifferentiated compatibility constraints. In this work, we propose Darwinian Model Upgrades (DMU), which disentangle the inheritance and variation in the model evolving with selective backward compatibility and forward adaptation, respectively. The old-to-new heritable knowledge is measured by old feature discriminativeness, and the gallery features, especially those of poor quality, are evolved in a lightweight manner to become more adaptive in the new latent space. We demonstrate the superiority of DMU through comprehensive experiments on large-scale landmark retrieval and face recognition benchmarks. DMU effectively alleviates the new-to-new degradation at the same time improving new-to-old compatibility, rendering a more proper model upgrading paradigm in large-scale retrieval systems.Code: https://github.com/TencentARC/OpenCompatible",
    "checked": true,
    "id": "7d49b452bf09020f032ff65aa115740e3bf99ea0",
    "semantic_title": "darwinian model upgrades: model evolving with selective compatibility",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25448": {
    "title": "Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "Existing methods of cross-modal domain adaptation for 3D semantic segmentation predict results only via 2D-3D complementarity that is obtained by cross-modal feature matching. However, as lacking supervision in the target domain, the complementarity is not always reliable. The results are not ideal when the domain gap is large. To solve the problem of lacking supervision, we introduce masked modeling into this task and propose a method Mx2M, which utilizes masked cross-modality modeling to reduce the large domain gap. Our Mx2M contains two components. One is the core solution, cross-modal removal and prediction (xMRP), which makes the Mx2M adapt to various scenarios and provides cross-modal self-supervision. The other is a new way of cross-modal feature matching, the dynamic cross-modal filter (DxMF) that ensures the whole method dynamically uses more suitable 2D-3D complementarity. Evaluation of the Mx2M on three DA scenarios, including Day/Night, USA/Singapore, and A2D2/SemanticKITTI, brings large improvements over previous methods on many metrics",
    "checked": true,
    "id": "dc430bba79178483c254a072c2151f0b3e7111af",
    "semantic_title": "mx2m: masked cross-modality modeling in domain adaptation for 3d semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25449": {
    "title": "Few-Shot 3D Point Cloud Semantic Segmentation via Stratified Class-Specific Attention Based Transformer Network",
    "volume": "main",
    "abstract": "3D point cloud semantic segmentation aims to group all points into different semantic categories, which benefits important applications such as point cloud scene reconstruction and understanding. Existing supervised point cloud semantic segmentation methods usually require large-scale annotated point clouds for training and cannot handle new categories. While a few-shot learning method was proposed recently to address these two problems, it suffers from high computational complexity caused by graph construction and inability to learn fine-grained relationships among points due to the use of pooling operations. In this paper, we further address these problems by developing a new multi-layer transformer network for few-shot point cloud semantic segmentation. In the proposed network, the query point cloud features are aggregated based on the class-specific support features in different scales. Without using pooling operations, our method makes full use of all pixel-level features from the support samples. By better leveraging the support features for few-shot learning, the proposed method achieves the new state-of-the-art performance, with 15% less inference time, over existing few-shot 3D point cloud segmentation models on the S3DIS dataset and the ScanNet dataset. Our code is available at https://github.com/czzhang179/SCAT",
    "checked": true,
    "id": "80df029becc3fdf6789c0d4c4fdf9a832f2e2672",
    "semantic_title": "few-shot 3d point cloud semantic segmentation via stratified class-specific attention based transformer network",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25450": {
    "title": "PaRot: Patch-Wise Rotation-Invariant Network via Feature Disentanglement and Pose Restoration",
    "volume": "main",
    "abstract": "Recent interest in point cloud analysis has led rapid progress in designing deep learning methods for 3D models. However, state-of-the-art models are not robust to rotations, which remains an unknown prior to real applications and harms the model performance. In this work, we introduce a novel Patch-wise Rotation-invariant network (PaRot), which achieves rotation invariance via feature disentanglement and produces consistent predictions for samples with arbitrary rotations. Specifically, we design a siamese training module which disentangles rotation invariance and equivariance from patches defined over different scales, e.g., the local geometry and global shape, via a pair of rotations. However, our disentangled invariant feature loses the intrinsic pose information of each patch. To solve this problem, we propose a rotation-invariant geometric relation to restore the relative pose with equivariant information for patches defined over different scales. Utilising the pose information, we propose a hierarchical module which implements intra-scale and inter-scale feature aggregation for 3D shape learning. Moreover, we introduce a pose-aware feature propagation process with the rotation-invariant relative pose information embedded. Experiments show that our disentanglement module extracts high-quality rotation-robust features and the proposed lightweight model achieves competitive results in rotated 3D object classification and part segmentation tasks",
    "checked": true,
    "id": "e6ad00f8c11a9237fc112ecff222e00fd71174ae",
    "semantic_title": "parot: patch-wise rotation-invariant network via feature disentanglement and pose restoration",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25451": {
    "title": "Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations",
    "volume": "main",
    "abstract": "Contrastive learning has been proven beneficial for self-supervised skeleton-based action recognition. Most contrastive learning methods utilize carefully designed augmentations to generate different movement patterns of skeletons for the same semantics. However, it is still a pending issue to apply strong augmentations, which distort the images/skeletons' structures and cause semantic loss, due to their resulting unstable training. In this paper, we investigate the potential of adopting strong augmentations and propose a general hierarchical consistent contrastive learning framework (HiCLR) for skeleton-based action recognition. Specifically, we first design a gradual growing augmentation policy to generate multiple ordered positive pairs, which guide to achieve the consistency of the learned representation from different views. Then, an asymmetric loss is proposed to enforce the hierarchical consistency via a directional clustering operation in the feature space, pulling the representations from strongly augmented views closer to those from weakly augmented views for better generalizability. Meanwhile, we propose and evaluate three kinds of strong augmentations for 3D skeletons to demonstrate the effectiveness of our method. Extensive experiments show that HiCLR outperforms the state-of-the-art methods notably on three large-scale datasets, i.e., NTU60, NTU120, and PKUMMD. Our project is publicly available at: https://jhang2020.github.io/Projects/HiCLR/HiCLR.html",
    "checked": true,
    "id": "51133d759e81916553edf4e35a20385d12090abe",
    "semantic_title": "hierarchical consistent contrastive learning for skeleton-based action recognition with growing augmentations",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25452": {
    "title": "ImageNet Pre-training Also Transfers Non-robustness",
    "volume": "main",
    "abstract": "ImageNet pre-training has enabled state-of-the-art results on many tasks. In spite of its recognized contribution to generalization, we observed in this study that ImageNet pre-training also transfers adversarial non-robustness from pre-trained model into fine-tuned model in the downstream classification tasks. We first conducted experiments on various datasets and network backbones to uncover the adversarial non-robustness in fine-tuned model. Further analysis was conducted on examining the learned knowledge of fine-tuned model and standard model, and revealed that the reason leading to the non-robustness is the non-robust features transferred from ImageNet pre-trained model. Finally, we analyzed the preference for feature learning of the pre-trained model, explored the factors influencing robustness, and introduced a simple robust ImageNet pre-training solution. Our code is available at https://github.com/jiamingzhang94/ImageNet-Pretraining-transfers-non-robustness",
    "checked": true,
    "id": "b28004c12d1073f3b186483ecb9e8fb816dd37c8",
    "semantic_title": "imagenet pre-training also transfers non-robustness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25453": {
    "title": "Language-Assisted 3D Feature Learning for Semantic Scene Understanding",
    "volume": "main",
    "abstract": "Learning descriptive 3D features is crucial for understanding 3D scenes with diverse objects and complex structures. However, it is usually unknown whether important geometric attributes and scene context obtain enough emphasis in an end-to-end trained 3D scene understanding network. To guide 3D feature learning toward important geometric attributes and scene context, we explore the help of textual scene descriptions. Given some free-form descriptions paired with 3D scenes, we extract the knowledge regarding the object relationships and object attributes. We then inject the knowledge to 3D feature learning through three classification-based auxiliary tasks. This language-assisted training can be combined with modern object detection and instance segmentation methods to promote 3D semantic scene understanding, especially in a label-deficient regime. Moreover, the 3D feature learned with language assistance is better aligned with the language features, which can benefit various 3D-language multimodal tasks. Experiments on several benchmarks of 3D-only and 3D-language tasks demonstrate the effectiveness of our language-assisted 3D feature learning. Code is available at https://github.com/Asterisci/Language-Assisted-3D",
    "checked": true,
    "id": "7e667180d34f837c7416358414a387595d54eee4",
    "semantic_title": "language-assisted 3d feature learning for semantic scene understanding",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25454": {
    "title": "IKOL: Inverse Kinematics Optimization Layer for 3D Human Pose and Shape Estimation via Gauss-Newton Differentiation",
    "volume": "main",
    "abstract": "This paper presents an inverse kinematic optimization layer (IKOL) for 3D human pose and shape estimation that leverages the strength of both optimization- and regression-based methods within an end-to-end framework. IKOL involves a nonconvex optimization that establishes an implicit mapping from an image's 3D keypoints and body shapes to the relative body-part rotations. The 3D keypoints and the body shapes are the inputs and the relative body-part rotations are the solutions. However, this procedure is implicit and hard to make differentiable. So, to overcome this issue, we designed a Gauss-Newton differentiation (GN-Diff) procedure to differentiate IKOL. GN-Diff iteratively linearizes the nonconvex objective function to obtain Gauss-Newton directions with closed form solutions. Then, an automatic differentiation procedure is directly applied to generate a Jacobian matrix for end-to-end training. Notably, the GN-Diff procedure works fast because it does not rely on a time-consuming implicit differentiation procedure. The twist rotation and shape parameters are learned from the neural networks and, as a result, IKOL has a much lower computational overhead than most existing optimization-based methods. Additionally, compared to existing regression-based methods, IKOL provides a more accurate mesh-image correspondence. This is because it iteratively reduces the distance between the keypoints and also enhances the reliability of the pose structures. Extensive experiments demonstrate the superiority of our proposed framework over a wide range of 3D human pose and shape estimation methods. Code is available at https://github.com/Juzezhang/IKOL",
    "checked": true,
    "id": "aa5262e4d46f8565de1d6049d1f85e8e2033601a",
    "semantic_title": "ikol: inverse kinematics optimization layer for 3d human pose and shape estimation via gauss-newton differentiation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25455": {
    "title": "Mind the Gap: Polishing Pseudo Labels for Accurate Semi-supervised Object Detection",
    "volume": "main",
    "abstract": "Exploiting pseudo labels (e.g., categories and bounding boxes) of unannotated objects produced by a teacher detector have underpinned much of recent progress in semi-supervised object detection (SSOD). However, due to the limited generalization capacity of the teacher detector caused by the scarce annotations, the produced pseudo labels often deviate from ground truth, especially those with relatively low classification confidences, thus limiting the generalization performance of SSOD. To mitigate this problem, we propose a dual pseudo-label polishing framework for SSOD. Instead of directly exploiting the pseudo labels produced by the teacher detector, we take the first attempt at reducing their deviation from ground truth using dual polishing learning, where two differently structured polishing networks are elaborately developed and trained using synthesized paired pseudo labels and the corresponding ground truth for categories and bounding boxes on the given annotated objects, respectively. By doing this, both polishing networks can infer more accurate pseudo labels for unannotated objects through sufficiently exploiting their context knowledge based on the initially produced pseudo labels, and thus improve the generalization performance of SSOD. Moreover, such a scheme can be seamlessly plugged into the existing SSOD framework for joint end-to-end learning. In addition, we propose to disentangle the polished pseudo categories and bounding boxes of unannotated objects for separate category classification and bounding box regression in SSOD, which enables introducing more unannotated objects during model training and thus further improves the performance. Experiments on both PASCAL VOC and MS-COCO benchmarks demonstrate the superiority of the proposed method over existing state-of-the-art baselines. The code can be found at https://github.com/snowdusky/DualPolishLearning",
    "checked": true,
    "id": "0b3854e82d71e1a36e049763bb270f1f19d2ea3c",
    "semantic_title": "mind the gap: polishing pseudo labels for accurate semi-supervised object detection",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25456": {
    "title": "ConvMatch: Rethinking Network Design for Two-View Correspondence Learning",
    "volume": "main",
    "abstract": "Multilayer perceptron (MLP) has been widely used in two-view correspondence learning for only unordered correspondences provided, and it extracts deep features from individual correspondence effectively. However, the problem of lacking context information limits its performance and hence, many extra complex blocks are designed to capture such information in the follow-up studies. In this paper, from a novel perspective, we design a correspondence learning network called ConvMatch that for the first time can leverage convolutional neural network (CNN) as the backbone to capture better context, thus avoiding the complex design of extra blocks. Specifically, with the observation that sparse motion vectors and dense motion field can be converted into each other with interpolating and sampling, we regularize the putative motion vectors by estimating dense motion field implicitly, then rectify the errors caused by outliers in local areas with CNN, and finally obtain correct motion vectors from the rectified motion field. Extensive experiments reveal that ConvMatch with a simple CNN backbone consistently outperforms state-of-the-arts including MLP-based methods for relative pose estimation and homography estimation, and shows promising generalization ability to different datasets and descriptors. Our code is publicly available at https://github.com/SuhZhang/ConvMatch",
    "checked": true,
    "id": "8d9662d2db763abb6d427d398e76288327457b1a",
    "semantic_title": "convmatch: rethinking network design for two-view correspondence learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25457": {
    "title": "Cross-View Geo-Localization via Learning Disentangled Geometric Layout Correspondence",
    "volume": "main",
    "abstract": "Cross-view geo-localization aims to estimate the location of a query ground image by matching it to a reference geo-tagged aerial images database. As an extremely challenging task, its difficulties root in the drastic view changes and different capturing time between two views. Despite these difficulties, recent works achieve outstanding progress on cross-view geo-localization benchmarks. However, existing methods still suffer from poor performance on the cross-area benchmarks, in which the training and testing data are captured from two different regions. We attribute this deficiency to the lack of ability to extract the spatial configuration of visual feature layouts and models' overfitting on low-level details from the training set. In this paper, we propose GeoDTR which explicitly disentangles geometric information from raw features and learns the spatial correlations among visual features from aerial and ground pairs with a novel geometric layout extractor module. This module generates a set of geometric layout descriptors, modulating the raw features and producing high-quality latent representations. In addition, we elaborate on two categories of data augmentations, (i) Layout simulation, which varies the spatial configuration while keeping the low-level details intact. (ii) Semantic augmentation, which alters the low-level details and encourages the model to capture spatial configurations. These augmentations help to improve the performance of the cross-view geo-localization models, especially on the cross-area benchmarks. Moreover, we propose a counterfactual-based learning process to benefit the geometric layout extractor in exploring spatial information. Extensive experiments show that GeoDTR not only achieves state-of-the-art results but also significantly boosts the performance on same-area and cross-area benchmarks. Our code can be found at https://gitlab.com/vail-uvm/geodtr",
    "checked": true,
    "id": "ae6bbeb670a0211bc2426b9afd867f8afb72e751",
    "semantic_title": "cross-view geo-localization via learning disentangled geometric layout correspondence",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25458": {
    "title": "Video Compression Artifact Reduction by Fusing Motion Compensation and Global Context in a Swin-CNN Based Parallel Architecture",
    "volume": "main",
    "abstract": "Video Compression Artifact Reduction aims to reduce the artifacts caused by video compression algorithms and improve the quality of compressed video frames. The critical challenge in this task is to make use of the redundant high-quality information in compressed frames for compensation as much as possible. Two important possible compensations: Motion compensation and global context, are not comprehensively considered in previous works, leading to inferior results. The key idea of this paper is to fuse the motion compensation and global context together to gain more compensation information to improve the quality of compressed videos. Here, we propose a novel Spatio-Temporal Compensation Fusion (STCF) framework with the Parallel Swin-CNN Fusion (PSCF) block, which can simultaneously learn and merge the motion compensation and global context to reduce the video compression artifacts. Specifically, a temporal self-attention strategy based on shifted windows is developed to capture the global context in an efficient way, for which we use the Swin transformer layer in the PSCF block. Moreover, an additional Ada-CNN layer is applied in the PSCF block to extract the motion compensation. Experimental results demonstrate that our proposed STCF framework outperforms the state-of-the-art methods up to 0.23dB (27% improvement) on the MFQEv2 dataset",
    "checked": true,
    "id": "63b3c5e3c6b15394743f9bb423ac2082b2443d24",
    "semantic_title": "video compression artifact reduction by fusing motion compensation and global context in a swin-cnn based parallel architecture",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25459": {
    "title": "MRCN: A Novel Modality Restitution and Compensation Network for Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "Visible-infrared person re-identification (VI-ReID), which aims to search identities across different spectra, is a challenging task due to large cross-modality discrepancy between visible and infrared images. The key to reduce the discrepancy is to filter out identity-irrelevant interference and effectively learn modality-invariant person representations. In this paper, we propose a novel Modality Restitution and Compensation Network (MRCN) to narrow the gap between the two modalities. Specifically, we first reduce the modality discrepancy by using two Instance Normalization (IN) layers. Next, to reduce the influence of IN layers on removing discriminative information and to reduce modality differences, we propose a Modality Restitution Module (MRM) and a Modality Compensation Module (MCM) to respectively distill modality-irrelevant and modality-relevant features from the removed information. Then, the modality-irrelevant features are used to restitute to the normalized visible and infrared features, while the modality-relevant features are used to compensate for the features of the other modality. Furthermore, to better disentangle the modality-relevant features and the modality-irrelevant features, we propose a novel Center-Quadruplet Causal (CQC) loss to encourage the network to effectively learn the modality-relevant features and the modality-irrelevant features. Extensive experiments are conducted to validate the superiority of our method on the challenging SYSU-MM01 and RegDB datasets. More remarkably, our method achieves 95.1% in terms of Rank-1 and 89.2% in terms of mAP on the RegDB dataset",
    "checked": true,
    "id": "ddd0cff0f7f959c324f9bb157f045302c66690ca",
    "semantic_title": "mrcn: a novel modality restitution and compensation network for visible-infrared person re-identification",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25460": {
    "title": "A Simple Baseline for Multi-Camera 3D Object Detection",
    "volume": "main",
    "abstract": "3D object detection with surrounding cameras has been a promising direction for autonomous driving. In this paper, we present SimMOD, a Simple baseline for Multi-camera Object Detection, to solve the problem. To incorporate multiview information as well as build upon previous efforts on monocular 3D object detection, the framework is built on sample-wise object proposals and designed to work in a twostage manner. First, we extract multi-scale features and generate the perspective object proposals on each monocular image. Second, the multi-view proposals are aggregated and then iteratively refined with multi-view and multi-scale visual features in the DETR3D-style. The refined proposals are endto-end decoded into the detection results. To further boost the performance, we incorporate the auxiliary branches alongside the proposal generation to enhance the feature learning. Also, we design the methods of target filtering and teacher forcing to promote the consistency of two-stage training. We conduct extensive experiments on the 3D object detection benchmark of nuScenes to demonstrate the effectiveness of SimMOD and achieve competitive performance. Code will be available at https://github.com/zhangyp15/SimMOD",
    "checked": true,
    "id": "b0226ed79f7b9efe06adfd0c093228a354bc5197",
    "semantic_title": "a simple baseline for multi-camera 3d object detection",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25461": {
    "title": "Positional Label for Self-Supervised Vision Transformer",
    "volume": "main",
    "abstract": "Positional encoding is important for vision transformer (ViT) to capture the spatial structure of the input image. General effectiveness has been proven in ViT. In our work we propose to train ViT to recognize the positional label of patches of the input image, this apparently simple task actually yields a meaningful self-supervisory task. Based on previous work on ViT positional encoding, we propose two positional labels dedicated to 2D images including absolute position and relative position. Our positional labels can be easily plugged into various current ViT variants. It can work in two ways: (a) As an auxiliary training target for vanilla ViT for better performance. (b) Combine the self-supervised ViT to provide a more powerful self-supervised signal for semantic feature learning. Experiments demonstrate that with the proposed self-supervised methods, ViT-B and Swin-B gain improvements of 1.20% (top-1 Acc) and 0.74% (top-1 Acc) on ImageNet, respectively, and 6.15% and 1.14% improvement on Mini-ImageNet. The code is publicly available at: https://github.com/zhangzhemin/PositionalLabel",
    "checked": true,
    "id": "9ea9e48950e311387c177b5195782c8c324ac119",
    "semantic_title": "positional label for self-supervised vision transformer",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25462": {
    "title": "Cross-Category Highlight Detection via Feature Decomposition and Modality Alignment",
    "volume": "main",
    "abstract": "Learning an autonomous highlight video detector with good transferability across video categories, called Cross-Category Video Highlight Detection(CC-VHD), is crucial for the practical application on video-based media platforms. To tackle this problem, we first propose a framework that treats the CC-VHD as learning category-independent highlight feature representation. Under this framework, we propose a novel module, named Multi-task Feature Decomposition Branch which jointly conducts label prediction, cyclic feature reconstruction, and adversarial feature reconstruction to decompose the video features into two independent components: highlight-related component and category-related component. Besides, we propose to align the visual and audio modalities to one aligned feature space before conducting modality fusion, which has not been considered in previous works. Finally, the extensive experimental results on three challenging public benchmarks validate the efficacy of our paradigm and the superiority over the existing state-of-the-art approaches to video highlight detection",
    "checked": true,
    "id": "45de3c9f58933d5ba67dbc9d4cc0266dc11eddf2",
    "semantic_title": "cross-category highlight detection via feature decomposition and modality alignment",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25463": {
    "title": "TrEP: Transformer-Based Evidential Prediction for Pedestrian Intention with Uncertainty",
    "volume": "main",
    "abstract": "With rapid development in hardware (sensors and processors) and AI algorithms, automated driving techniques have entered the public's daily life and achieved great success in supporting human driving performance. However, due to the high contextual variations and temporal dynamics in pedestrian behaviors, the interaction between autonomous-driving cars and pedestrians remains challenging, impeding the development of fully autonomous driving systems. This paper focuses on predicting pedestrian intention with a novel transformer-based evidential prediction (TrEP) algorithm. We develop a transformer module towards the temporal correlations among the input features within pedestrian video sequences and a deep evidential learning model to capture the AI uncertainty under scene complexities. Experimental results on three popular pedestrian intent benchmarks have verified the effectiveness of our proposed model over the state-of-the-art. The algorithm performance can be further boosted by controlling the uncertainty level. We systematically compare human disagreements with AI uncertainty to further evaluate AI performance in confusing scenes. The code is released at https://github.com/zzmonlyyou/TrEP.git",
    "checked": true,
    "id": "e9e8ed0ee97296c638329a30643c2456d95e944e",
    "semantic_title": "trep: transformer-based evidential prediction for pedestrian intention with uncertainty",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25464": {
    "title": "DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video",
    "volume": "main",
    "abstract": "For few-shot learning, it is still a critical challenge to realize photo-realistic face visually dubbing on high-resolution videos. Previous works fail to generate high-fidelity dubbing results. To address the above problem, this paper proposes a Deformation Inpainting Network (DINet) for high-resolution face visually dubbing. Different from previous works relying on multiple up-sample layers to directly generate pixels from latent embeddings, DINet performs spatial deformation on feature maps of reference images to better preserve high-frequency textural details. Specifically, DINet consists of one deformation part and one inpainting part. In the first part, five reference facial images adaptively perform spatial deformation to create deformed feature maps encoding mouth shapes at each frame, in order to align with input driving audio and also the head poses of input source images. In the second part, to produce face visually dubbing, a feature decoder is responsible for adaptively incorporating mouth movements from the deformed feature maps and other attributes (i.e., head pose and upper facial expression) from the source feature maps together. Finally, DINet achieves face visually dubbing with rich textural details. We conduct qualitative and quantitative comparisons to validate our DINet on high-resolution videos. The experimental results show that our method outperforms state-of-the-art works",
    "checked": true,
    "id": "f13d4d12d924727114182da54980a04be051fc87",
    "semantic_title": "dinet: deformation inpainting network for realistic face visually dubbing on high resolution video",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25465": {
    "title": "ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories",
    "volume": "main",
    "abstract": "Diffusion models have recently exhibited remarkable abilities to synthesize striking image samples since the introduction of denoising diffusion probabilistic models (DDPMs). Their key idea is to disrupt images into noise through a fixed forward process and learn its reverse process to generate samples from noise in a denoising way. For conditional DDPMs, most existing practices relate conditions only to the reverse process and fit it to the reversal of unconditional forward process. We find this will limit the condition modeling and generation in a small time window. In this paper, we propose a novel and flexible conditional diffusion model by introducing conditions into the forward process. We utilize extra latent space to allocate an exclusive diffusion trajectory for each condition based on some shifting rules, which will disperse condition modeling to all timesteps and improve the learning capacity of model. We formulate our method, which we call ShiftDDPMs, and provide a unified point of view on existing related methods. Extensive qualitative and quantitative experiments on image synthesis demonstrate the feasibility and effectiveness of ShiftDDPMs",
    "checked": true,
    "id": "6ae2e3d5682e755d560c34b05169a9660660f055",
    "semantic_title": "shiftddpms: exploring conditional diffusion models by shifting diffusion trajectories",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25466": {
    "title": "Combating Unknown Bias with Effective Bias-Conflicting Scoring and Gradient Alignment",
    "volume": "main",
    "abstract": "Models notoriously suffer from dataset biases which are detrimental to robustness and generalization. The identify-emphasize paradigm shows a promising effect in dealing with unknown biases. However, we find that it is still plagued by two challenges: A, the quality of the identified bias-conflicting samples is far from satisfactory; B, the emphasizing strategies just yield suboptimal performance. In this work, for challenge A, we propose an effective bias-conflicting scoring method to boost the identification accuracy with two practical strategies --- peer-picking and epoch-ensemble. For challenge B, we point out that the gradient contribution statistics can be a reliable indicator to inspect whether the optimization is dominated by bias-aligned samples. Then, we propose gradient alignment, which employs gradient statistics to balance the contributions of the mined bias-aligned and bias-conflicting samples dynamically throughout the learning process, forcing models to leverage intrinsic features to make fair decisions. Experiments are conducted on multiple datasets in various settings, demonstrating that the proposed solution can alleviate the impact of unknown biases and achieve state-of-the-art performance",
    "checked": true,
    "id": "5dc689a4d6827b5a441c50cb56c14d19b027d9f7",
    "semantic_title": "combating unknown bias with effective bias-conflicting scoring and gradient alignment",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25467": {
    "title": "RLogist: Fast Observation Strategy on Whole-Slide Images with Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Whole-slide images (WSI) in computational pathology have high resolution with gigapixel size, but are generally with sparse regions of interest, which leads to weak diagnostic relevance and data inefficiency for each area in the slide. Most of the existing methods rely on a multiple instance learning framework that requires densely sampling local patches at high magnification. The limitation is evident in the application stage as the heavy computation for extracting patch-level features is inevitable. In this paper, we develop RLogist, a benchmarking deep reinforcement learning (DRL) method for fast observation strategy on WSIs. Imitating the diagnostic logic of human pathologists, our RL agent learns how to find regions of observation value and obtain representative features across multiple resolution levels, without having to analyze each part of the WSI at the high magnification. We benchmark our method on two whole-slide level classification tasks, including detection of metastases in WSIs of lymph node sections, and subtyping of lung cancer. Experimental results demonstrate that RLogist achieves competitive classification performance compared to typical multiple instance learning algorithms, while having a significantly short observation path. In addition, the observation path given by RLogist provides good decision-making interpretability, and its ability of reading path navigation can potentially be used by pathologists for educational/assistive purposes. Our code is available at: https://github.com/tencent-ailab/RLogist",
    "checked": true,
    "id": "63195e3d637c2725913fc2fd999e3e3ae40362e4",
    "semantic_title": "rlogist: fast observation strategy on whole-slide images with deep reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25468": {
    "title": "Learning to Super-resolve Dynamic Scenes for Neuromorphic Spike Camera",
    "volume": "main",
    "abstract": "Spike camera is a kind of neuromorphic sensor that uses a novel ``integrate-and-fire'' mechanism to generate a continuous spike stream to record the dynamic light intensity at extremely high temporal resolution. However, as a trade-off for high temporal resolution, its spatial resolution is limited, resulting in inferior reconstruction details. To address this issue, this paper develops a network (SpikeSR-Net) to super-resolve a high-resolution image sequence from the low-resolution binary spike streams. SpikeSR-Net is designed based on the observation model of spike camera and exploits both the merits of model-based and learning-based methods. To deal with the limited representation capacity of binary data, a pixel-adaptive spike encoder is proposed to convert spikes to latent representation to infer clues on intensity and motion. Then, a motion-aligned super resolver is employed to exploit long-term correlation, so that the dense sampling in temporal domain can be exploited to enhance the spatial resolution without introducing motion blur. Experimental results show that SpikeSR-Net is promising in super-resolving higher-quality images for spike camera",
    "checked": true,
    "id": "98528cb319f7939749cafff8d1ff08823641c009",
    "semantic_title": "learning to super-resolve dynamic scenes for neuromorphic spike camera",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25469": {
    "title": "TinyNeRF: Towards 100 x Compression of Voxel Radiance Fields",
    "volume": "main",
    "abstract": "Voxel grid representation of 3D scene properties has been widely used to improve the training or rendering speed of the Neural Radiance Fields (NeRF) while at the same time achieving high synthesis quality. However, these methods accelerate the original NeRF at the expense of extra storage demand, which hinders their applications in many scenarios. To solve this limitation, we present TinyNeRF, a three-stage pipeline: frequency domain transformation, pruning and quantization that work together to reduce the storage demand of the voxel grids with little to no effects on their speed and synthesis quality. Based on the prior knowledge of visual signals sparsity in the frequency domain, we convert the original voxel grids in the frequency domain via block-wise discrete cosine transformation (DCT). Next, we apply pruning and quantization to enforce the DCT coefficients to be sparse and low-bit. Our method can be optimized from scratch in an end-to-end manner, and can typically compress the original models by 2 orders of magnitude with minimal sacrifice on speed and synthesis quality",
    "checked": true,
    "id": "a8f13253a047f76c5e9ee275d27b70274dfe2758",
    "semantic_title": "tinynerf: towards 100 x compression of voxel radiance fields",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25470": {
    "title": "BEST: BERT Pre-training for Sign Language Recognition with Coupling Tokenization",
    "volume": "main",
    "abstract": "In this work, we are dedicated to leveraging the BERT pre-training success and modeling the domain-specific statistics to fertilize the sign language recognition~(SLR) model. Considering the dominance of hand and body in sign language expression, we organize them as pose triplet units and feed them into the Transformer backbone in a frame-wise manner. Pre-training is performed via reconstructing the masked triplet unit from the corrupted input sequence, which learns the hierarchical correlation context cues among internal and external triplet units. Notably, different from the highly semantic word token in BERT, the pose unit is a low-level signal originally locating in continuous space, which prevents the direct adoption of the BERT cross entropy objective. To this end, we bridge this semantic gap via coupling tokenization of the triplet unit. It adaptively extracts the discrete pseudo label from the pose triplet unit, which represents the semantic gesture / body state. After pre-training, we fine-tune the pre-trained encoder on the downstream SLR task, jointly with the newly added task-specific layer. Extensive experiments are conducted to validate the effectiveness of our proposed method, achieving new state-of-the-art performance on all four benchmarks with a notable gain",
    "checked": true,
    "id": "919efc30ab8c45456ab72fb4ebeb5e25f2ad7642",
    "semantic_title": "best: bert pre-training for sign language recognition with coupling tokenization",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25471": {
    "title": "MulGT: Multi-Task Graph-Transformer with Task-Aware Knowledge Injection and Domain Knowledge-Driven Pooling for Whole Slide Image Analysis",
    "volume": "main",
    "abstract": "Whole slide image (WSI) has been widely used to assist automated diagnosis under the deep learning fields. However, most previous works only discuss the SINGLE task setting which is not aligned with real clinical setting, where pathologists often conduct multiple diagnosis tasks simultaneously. Also, it is commonly recognized that the multi-task learning paradigm can improve learning efficiency by exploiting commonalities and differences across multiple tasks. To this end, we present a novel multi-task framework (i.e., MulGT) for WSI analysis by the specially designed Graph-Transformer equipped with Task-aware Knowledge Injection and Domain Knowledge-driven Graph Pooling modules. Basically, with the Graph Neural Network and Transformer as the building commons, our framework is able to learn task-agnostic low-level local information as well as task-specific high-level global representation. Considering that different tasks in WSI analysis depend on different features and properties, we also design a novel Task-aware Knowledge Injection module to transfer the task-shared graph embedding into task-specific feature spaces to learn more accurate representation for different tasks. Further, we elaborately design a novel Domain Knowledge-driven Graph Pooling module for each task to improve both the accuracy and robustness of different tasks by leveraging different diagnosis patterns of multiple tasks. We evaluated our method on two public WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney carcinoma. Experimental results show that our method outperforms single-task counterparts and the state-of-theart methods on both tumor typing and staging tasks",
    "checked": true,
    "id": "10968bcb3e19eb3cbd137af1bf4b82ad1c04378c",
    "semantic_title": "mulgt: multi-task graph-transformer with task-aware knowledge injection and domain knowledge-driven pooling for whole slide image analysis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25472": {
    "title": "Grouped Knowledge Distillation for Deep Face Recognition",
    "volume": "main",
    "abstract": "Compared with the feature-based distillation methods, logits distillation can liberalize the requirements of consistent feature dimension between teacher and student networks, while the performance is deemed inferior in face recognition. One major challenge is that the light-weight student network has difficulty fitting the target logits due to its low model capacity, which is attributed to the significant number of identities in face recognition. Therefore, we seek to probe the target logits to extract the primary knowledge related to face identity, and discard the others, to make the distillation more achievable for the student network. Specifically, there is a tail group with near-zero values in the prediction, containing minor knowledge for distillation. To provide a clear perspective of its impact, we first partition the logits into two groups, i.e., Primary Group and Secondary Group, according to the cumulative probability of the softened prediction. Then, we reorganize the Knowledge Distillation (KD) loss of grouped logits into three parts, i.e., Primary-KD, Secondary-KD, and Binary-KD. Primary-KD refers to distilling the primary knowledge from the teacher, Secondary-KD aims to refine minor knowledge but increases the difficulty of distillation, and Binary-KD ensures the consistency of knowledge distribution between teacher and student. We experimentally found that (1) Primary-KD and Binary-KD are indispensable for KD, and (2) Secondary-KD is the culprit restricting KD at the bottleneck. Therefore, we propose a Grouped Knowledge Distillation (GKD) that retains the Primary-KD and Binary-KD but omits Secondary-KD in the ultimate KD loss calculation. Extensive experimental results on popular face recognition benchmarks demonstrate the superiority of proposed GKD over state-of-the-art methods",
    "checked": true,
    "id": "68ed8b94df543dba73a619980d1c5a3fcde417a5",
    "semantic_title": "grouped knowledge distillation for deep face recognition",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25473": {
    "title": "Style-Content Metric Learning for Multidomain Remote Sensing Object Recognition",
    "volume": "main",
    "abstract": "Previous remote sensing recognition approaches predominantly perform well on the training-testing dataset. However, due to large style discrepancies not only among multidomain datasets but also within a single domain, they suffer from obvious performance degradation when applied to unseen domains. In this paper, we propose a style-content metric learning framework to address the generalizable remote sensing object recognition issue. Specifically, we firstly design an inter-class dispersion metric to encourage the model to make decision based on content rather than the style, which is achieved by dispersing predictions generated from the contents of both positive sample and negative sample and the style of input image. Secondly, we propose an intra-class compactness metric to force the model to be less style-biased by compacting classifier's predictions from the content of input image and the styles of positive sample and negative sample. Lastly, we design an intra-class interaction metric to improve model's recognition accuracy by pulling in classifier's predictions obtained from the input image and positive sample. Extensive experiments on four datasets show that our style-content metric learning achieves superior generalization performance against the state-of-the-art competitors. Code and model are available at: https://github.com/wdzhao123/TSCM",
    "checked": true,
    "id": "2fbd355ed0cf0bb57a3bfea9b3f12cc1e180d701",
    "semantic_title": "style-content metric learning for multidomain remote sensing object recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25474": {
    "title": "Occupancy Planes for Single-View RGB-D Human Reconstruction",
    "volume": "main",
    "abstract": "Single-view RGB-D human reconstruction with implicit functions is often formulated as per-point classification. Specifically, a set of 3D locations within the view-frustum of the camera are first projected independently onto the image and a corresponding feature is subsequently extracted for each 3D location. The feature of each 3D location is then used to classify independently whether the corresponding 3D point is inside or outside the observed object. This procedure leads to sub-optimal results because correlations between predictions for neighboring locations are only taken into account implicitly via the extracted features. For more accurate results we propose the occupancy planes (OPlanes) representation, which enables to formulate single-view RGB-D human reconstruction as occupancy prediction on planes which slice through the camera's view frustum. Such a representation provides more flexibility than voxel grids and enables to better leverage correlations than per-point classification. On the challenging S3D data we observe a simple classifier based on the OPlanes representation to yield compelling results, especially in difficult situations with partial occlusions due to other objects and partial visibility, which haven't been addressed by prior work",
    "checked": true,
    "id": "dde7f2b5b925c7edb8ef9c5aabc1f679a6b6c104",
    "semantic_title": "occupancy planes for single-view rgb-d human reconstruction",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25475": {
    "title": "Deep Equilibrium Models for Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "The ability of snapshot compressive imaging (SCI) systems to efficiently capture high-dimensional (HD) data has led to an inverse problem, which consists of recovering the HD signal from the compressed and noisy measurement. While reconstruction algorithms grow fast to solve it with the recent advances of deep learning, the fundamental issue of accurate and stable recovery remains. To this end, we propose deep equilibrium models (DEQ) for video SCI, fusing data-driven regularization and stable convergence in a theoretically sound manner. Each equilibrium model implicitly learns a nonexpansive operator and analytically computes the fixed point, thus enabling unlimited iterative steps and infinite network depth with only a constant memory requirement in training and testing. Specifically, we demonstrate how DEQ can be applied to two existing models for video SCI reconstruction: recurrent neural networks (RNN) and Plug-and-Play (PnP) algorithms. On a variety of datasets and real data, both quantitative and qualitative evaluations of our results demonstrate the effectiveness and stability of our proposed method. The code and models are available at: https://github.com/IndigoPurple/DEQSCI",
    "checked": true,
    "id": "68bcc213f88a5ee875f7061cdc192cb5b4ad9832",
    "semantic_title": "deep equilibrium models for snapshot compressive imaging",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25476": {
    "title": "Unsupervised Deep Video Denoising with Untrained Network",
    "volume": "main",
    "abstract": "Deep learning has become a prominent tool for video denoising. However, most existing deep video denoising methods require supervised training using noise-free videos. Collecting noise-free videos can be costly and challenging in many applications. Therefore, this paper aims to develop an unsupervised deep learning method for video denoising that only uses a single test noisy video for training. To achieve this, an unsupervised loss function is presented that provides an unbiased estimator of its supervised counterpart defined on noise-free video. Additionally, a temporal attention mechanism is proposed to exploit redundancy among frames. The experiments on video denoising demonstrate that the proposed unsupervised method outperforms existing unsupervised methods and remains competitive against recent supervised deep learning methods",
    "checked": true,
    "id": "8132f60124089fd50fe2afbd3e60e73b05ef65e1",
    "semantic_title": "unsupervised deep video denoising with untrained network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25477": {
    "title": "Attack Can Benefit: An Adversarial Approach to Recognizing Facial Expressions under Noisy Annotations",
    "volume": "main",
    "abstract": "The real-world Facial Expression Recognition (FER) datasets usually exhibit complex scenarios with coupled noise annotations and imbalanced classes distribution, which undoubtedly impede the development of FER methods. To address the aforementioned issues, in this paper, we propose a novel and flexible method to spot noisy labels by leveraging adversarial attack, termed as Geometry Aware Adversarial Vulnerability Estimation (GAAVE). Different from existing state-of-the-art methods of noisy label learning (NLL), our method has no reliance on additional information and is thus easy to generalize to the large-scale real-world FER datasets. Besides, the combination of Dataset Splitting module and Subset Refactoring module mitigates the impact of class imbalance, and the Self-Annotator module facilitates the sufficient use of all training data. Extensive experiments on RAF-DB, FERPlus, AffectNet, and CIFAR-10 datasets validate the effectiveness of our method. The stabilized enhancement based on different methods demonstrates the flexibility of our proposed GAAVE",
    "checked": true,
    "id": "4266bba4e036807378abb05b2a9ebe7063c13a94",
    "semantic_title": "attack can benefit: an adversarial approach to recognizing facial expressions under noisy annotations",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25478": {
    "title": "Phrase-Level Temporal Relationship Mining for Temporal Sentence Localization",
    "volume": "main",
    "abstract": "In this paper, we address the problem of video temporal sentence localization, which aims to localize a target moment from videos according to a given language query. We observe that existing models suffer from a sheer performance drop when dealing with simple phrases contained in the sentence. It reveals the limitation that existing models only capture the annotation bias of the datasets but lack sufficient understanding of the semantic phrases in the query. To address this problem, we propose a phrase-level Temporal Relationship Mining (TRM) framework employing the temporal relationship relevant to the phrase and the whole sentence to have a better understanding of each semantic entity in the sentence. Specifically, we use phrase-level predictions to refine the sentence-level prediction, and use Multiple Instance Learning to improve the quality of phrase-level predictions. We also exploit the consistency and exclusiveness constraints of phrase-level and sentence-level predictions to regularize the training process, thus alleviating the ambiguity of each phrase prediction. The proposed approach sheds light on how machines can understand detailed phrases in a sentence and their compositions in their generality rather than learning the annotation biases. Experiments on the ActivityNet Captions and Charades-STA datasets show the effectiveness of our method on both phrase and sentence temporal localization and enable better model interpretability and generalization when dealing with unseen compositions of seen concepts. Code can be found at https://github.com/minghangz/TRM",
    "checked": true,
    "id": "4f70d5bdd5293b7fc53831624971fef83873d3f7",
    "semantic_title": "phrase-level temporal relationship mining for temporal sentence localization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25479": {
    "title": "Learning Semantic Degradation-Aware Guidance for Recognition-Driven Unsupervised Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Low-light images suffer severe degradation of low lightness and noise corruption, causing unsatisfactory visual quality and visual recognition performance. To solve this problem while meeting the unavailability of paired datasets in wide-range scenarios, unsupervised low-light image enhancement (ULLIE) techniques have been developed. However, these methods are primarily guided to alleviate the degradation effect on visual quality rather than semantic levels, hence limiting their performance in visual recognition tasks. To this end, we propose to learn a Semantic Degradation-Aware Guidance (SDAG) that perceives the low-light degradation effect on semantic levels in a self-supervised manner, which is further utilized to guide the ULLIE methods. The proposed SDAG utilizes the low-light degradation factors as augmented signals to degrade the low-light images, and then capture their degradation effect on semantic levels. Specifically, our SDAG employs the subsequent pre-trained recognition model extractor to extract semantic representations, and then learns to self-reconstruct the enhanced low-light image and its augmented degraded images. By constraining the relative reconstruction effect between the original enhanced image and the augmented formats, our SDAG learns to be aware of the degradation effect on semantic levels in a relative comparison manner. Moreover, our SDAG is general and can be plugged into the training paradigm of the existing ULLIE methods. Extensive experiments demonstrate its effectiveness for improving the ULLIE approaches on the downstream recognition tasks while maintaining a competitive visual quality. Code will be available at https://github.com/zheng980629/SDAG",
    "checked": true,
    "id": "00d66e7384eb46a290595bf54d62c2a614f8734d",
    "semantic_title": "learning semantic degradation-aware guidance for recognition-driven unsupervised low-light image enhancement",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25480": {
    "title": "Memory-Aided Contrastive Consensus Learning for Co-salient Object Detection",
    "volume": "main",
    "abstract": "Co-salient object detection (CoSOD) aims at detecting common salient objects within a group of relevant source images. Most of the latest works employ the attention mechanism for finding common objects. To achieve accurate CoSOD results with high-quality maps and high efficiency, we propose a novel Memory-aided Contrastive Consensus Learning (MCCL) framework, which is capable of effectively detecting co-salient objects in real time (∼150 fps). To learn better group consensus, we propose the Group Consensus Aggregation Module (GCAM) to abstract the common features of each image group; meanwhile, to make the consensus representation more discriminative, we introduce the Memory-based Contrastive Module (MCM), which saves and updates the consensus of images from different groups in a queue of memories. Finally, to improve the quality and integrity of the predicted maps, we develop an Adversarial Integrity Learning (AIL) strategy to make the segmented regions more likely composed of complete objects with less surrounding noise. Extensive experiments on all the latest CoSOD benchmarks demonstrate that our lite MCCL outperforms 13 cutting-edge models, achieving the new state of the art (∼5.9% and ∼6.2% improvement in S-measure on CoSOD3k and CoSal2015, respectively). Our source codes, saliency maps, and online demos are publicly available at https://github.com/ZhengPeng7/MCCL",
    "checked": true,
    "id": "4ec9799fba2f8e71a48c7606d1c725f2e2a30095",
    "semantic_title": "memory-aided contrastive consensus learning for co-salient object detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25481": {
    "title": "MaskBooster: End-to-End Self-Training for Sparsely Supervised Instance Segmentation",
    "volume": "main",
    "abstract": "The present paper introduces sparsely supervised instance segmentation, with the datasets being fully annotated bounding boxes and sparsely annotated masks. A direct solution to this task is self-training, which is not fully explored for instance segmentation yet. In this paper, we propose MaskBooster for sparsely supervised instance segmentation (SpSIS) with comprehensive usage of pseudo masks. MaskBooster is featured with (1) dynamic and progressive pseudo masks from an online updating teacher model, (2) refining binary pseudo masks with the help of bounding box prior, (3) learning inter-class prediction distribution via knowledge distillation for soft pseudo masks. As an end-to-end and universal self-training framework, MaskBooster can empower fully supervised algorithms and boost their segmentation performance on SpSIS. Abundant experiments are conducted on COCO and BDD100K datasets and validate the effectiveness of MaskBooster. Specifically, on different COCO protocols and BDD100K, we surpass sparsely supervised baseline by a large margin for both Mask RCNN and ShapeProp. MaskBooster on SpSIS also outperforms weakly and semi-supervised instance segmentation state-of-the-art on the datasets with similar annotation budgets",
    "checked": true,
    "id": "bc3fab136f5ee45103bad37eebafa1581781560d",
    "semantic_title": "maskbooster: end-to-end self-training for sparsely supervised instance segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25482": {
    "title": "RSPT: Reconstruct Surroundings and Predict Trajectory for Generalizable Active Object Tracking",
    "volume": "main",
    "abstract": "Active Object Tracking (AOT) aims to maintain a specific relation between the tracker and object(s) by autonomously controlling the motion system of a tracker given observations. It is widely used in various applications such as mobile robots and autonomous driving. However, Building a generalizable active tracker that works robustly across various scenarios remains a challenge, particularly in unstructured environments with cluttered obstacles and diverse layouts. To realize this, we argue that the key is to construct a state representation that can model the geometry structure of the surroundings and the dynamics of the target. To this end, we propose a framework called RSPT to form a structure-aware motion representation by Reconstructing Surroundings and Predicting the target Trajectory. Moreover, we further enhance the generalization of the policy network by training in the asymmetric dueling mechanism. Empirical results show that RSPT outperforms existing methods in unseen environments, especially those with cluttered obstacles and diverse layouts. We also demonstrate good sim-to-real transfer when deploying RSPT in real-world scenarios",
    "checked": false,
    "id": "0a21061ddf9c41b71c8d7b409f7dd97b18a5cd8b",
    "semantic_title": "rspt: reconstruct surroundings and predict trajectories for generalizable active object tracking",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25483": {
    "title": "STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training",
    "volume": "main",
    "abstract": "Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predicting actions found in the text. Extensive experiments on three downstream tasks (video captioning, text-video retrieval, and video question answering) demonstrate the effectiveness of our proposed STOA-VLP (e.g. 3.7 Rouge-L improvements on MSR-VTT video captioning benchmark, 2.9% accuracy improvements on MSVD video question answering benchmark, compared to previous approaches)",
    "checked": true,
    "id": "3d20d1d3ec1199c35d13e60351b358ac1e317401",
    "semantic_title": "stoa-vlp: spatial-temporal modeling of object and action for video-language pre-training",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25484": {
    "title": "Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning",
    "volume": "main",
    "abstract": "Video captioning aims to generate natural language sentences that describe the given video accurately. Existing methods obtain favorable generation by exploring richer visual representations in encode phase or improving the decoding ability. However, the long-tailed problem hinders these attempts at low-frequency tokens, which rarely occur but carry critical semantics, playing a vital role in the detailed generation. In this paper, we introduce a novel Refined Semantic enhancement method towards Frequency Diffusion (RSFD), a captioning model that constantly perceives the linguistic representation of the infrequent tokens. Concretely, a Frequency-Aware Diffusion (FAD) module is proposed to comprehend the semantics of low-frequency tokens to break through generation limitations. In this way, the caption is refined by promoting the absorption of tokens with insufficient occurrence. Based on FAD, we design a Divergent Semantic Supervisor (DSS) module to compensate for the information loss of high-frequency tokens brought by the diffusion process, where the semantics of low-frequency tokens is further emphasized to alleviate the long-tailed problem. Extensive experiments indicate that RSFD outperforms the state-of-the-art methods on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate that the enhancement of low-frequency tokens semantics can obtain a competitive generation effect. Code is available at https://github.com/lzp870/RSFD",
    "checked": true,
    "id": "154f9eb2f97cae3a7752cbc4e0261eb4e75008d4",
    "semantic_title": "refined semantic enhancement towards frequency diffusion for video captioning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25485": {
    "title": "Aesthetically Relevant Image Captioning",
    "volume": "main",
    "abstract": "Image aesthetic quality assessment (AQA) aims to assign numerical aesthetic ratings to images whilst image aesthetic captioning (IAC) aims to generate textual descriptions of the aesthetic aspects of images. In this paper, we study image AQA and IAC together and present a new IAC method termed Aesthetically Relevant Image Captioning (ARIC). Based on the observation that most textual comments of an image are about objects and their interactions rather than aspects of aesthetics, we first introduce the concept of Aesthetic Relevance Score (ARS) of a sentence and have developed a model to automatically label a sentence with its ARS. We then use the ARS to design the ARIC model which includes an ARS weighted IAC loss function and an ARS based diverse aesthetic caption selector (DACS). We present extensive experimental results to show the soundness of the ARS concept and the effectiveness of the ARIC model by demonstrating that texts with higher ARS's can predict the aesthetic ratings more accurately and that the new ARIC model can generate more accurate, aesthetically more relevant and more diverse image captions. Furthermore, a large new research database containing 510K images with over 5 million comments and 350K aesthetic scores, and code for implementing ARIC, are available at https://github.com/PengZai/ARIC",
    "checked": true,
    "id": "15d6ef576fb07bb5fc07fef6f63708e440396dd9",
    "semantic_title": "aesthetically relevant image captioning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25486": {
    "title": "Polarization-Aware Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Polarization-based vision algorithms have found uses in various applications since polarization provides additional physical constraints. However, in low-light conditions, their performance would be severely degenerated since the captured polarized images could be noisy, leading to noticeable degradation in the degree of polarization (DoP) and the angle of polarization (AoP). Existing low-light image enhancement methods cannot handle the polarized images well since they operate in the intensity domain, without effectively exploiting the information provided by polarization. In this paper, we propose a Stokes-domain enhancement pipeline along with a dual-branch neural network to handle the problem in a polarization-aware manner. Two application scenarios (reflection removal and shape from polarization) are presented to show how our enhancement can improve their results",
    "checked": true,
    "id": "1956f7770663fc45d9035278e39eb14fefcdd76a",
    "semantic_title": "polarization-aware low-light image enhancement",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25487": {
    "title": "Progressive Bayesian Inference for Scribble-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "The scribble-supervised semantic segmentation is an important yet challenging task in the field of computer vision. To deal with the pixel-wise sparse annotation problem, we propose a Progressive Bayesian Inference (PBI) framework to boost the performance of the scribble-supervised semantic segmentation, which can effectively infer the semantic distribution of these unlabeled pixels to guide the optimization of the segmentation network. The PBI dynamically improves the model learning from two aspects: the Bayesian inference module (i.e., semantic distribution learning) and the pixel-wise segmenter (i.e., model updating). Specifically, we effectively infer the semantic probability distribution of these unlabeled pixels with our designed Bayesian inference module, where its guidance is estimated through the Bayesian expectation maximization under the situation of partially observed data. The segmenter can be progressively improved under the joint guidance of the original scribble information and the learned semantic distribution. The segmenter optimization and semantic distribution promotion are encapsulated into a unified architecture where they could improve each other with mutual evolution in a progressive fashion. Comprehensive evaluations of several benchmark datasets demonstrate the effectiveness and superiority of our proposed PBI when compared with other state-of-the-art methods applied to the scribble-supervised semantic segmentation task",
    "checked": true,
    "id": "7a506fdeb24c051732d7c0aa74ae99db3cedbd0f",
    "semantic_title": "progressive bayesian inference for scribble-supervised semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25488": {
    "title": "Exploratory Inference Learning for Scribble Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Scribble supervised semantic segmentation has achieved great advances in pseudo label exploitation, yet suffers insufficient label exploration for the mass of unannotated regions. In this work, we propose a novel exploratory inference learning (EIL) framework, which facilitates efficient probing on unlabeled pixels and promotes selecting confident candidates for boosting the evolved segmentation. The exploration of unannotated regions is formulated as an iterative decision-making process, where a policy searcher learns to infer in the unknown space and the reward to the exploratory policy is based on a contrastive measurement of candidates. In particular, we devise the contrastive reward with the intra-class attraction and the inter-class repulsion in the feature space w.r.t the pseudo labels. The unlabeled exploration and the labeled exploitation are jointly balanced to improve the segmentation, and framed in a close-looping end-to-end network. Comprehensive evaluations on the benchmark datasets (PASCAL VOC 2012 and PASCAL Context) demonstrate the superiority of our proposed EIL when compared with other state-of-the-art methods for the scribble-supervised semantic segmentation problem",
    "checked": true,
    "id": "2ff5db744e5eefb95e17afd59260efbb3a78756c",
    "semantic_title": "exploratory inference learning for scribble supervised semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25489": {
    "title": "Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Learning discriminative features for effectively separating abnormal events from normality is crucial for weakly supervised video anomaly detection (WS-VAD) tasks. Existing approaches, both video and segment level label oriented, mainly focus on extracting representations for anomaly data while neglecting the implication of normal data. We observe that such a scheme is sub-optimal, i.e., for better distinguishing anomaly one needs to understand what is a normal state, and may yield a higher false alarm rate. To address this issue, we propose an Uncertainty Regulated Dual Memory Units (UR-DMU) model to learn both the representations of normal data and discriminative features of abnormal data. To be specific, inspired by the traditional global and local structure on graph convolutional networks, we introduce a Global and Local Multi-Head Self Attention (GL-MHSA) module for the Transformer network to obtain more expressive embeddings for capturing associations in videos. Then, we use two memory banks, one additional abnormal memory for tackling hard samples, to store and separate abnormal and normal prototypes and maximize the margins between the two representations. Finally, we propose an uncertainty learning scheme to learn the normal data latent space, that is robust to noise from camera switching, object changing, scene transforming, etc. Extensive experiments on XD-Violence and UCF-Crime datasets demonstrate that our method outperforms the state-of-the-art methods by a sizable margin",
    "checked": true,
    "id": "965a0ff397ff33dbb9ee9a5b725472af4ffe0892",
    "semantic_title": "dual memory units with uncertainty regulation for weakly supervised video anomaly detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25490": {
    "title": "Unsupervised Hierarchical Domain Adaptation for Adverse Weather Optical Flow",
    "volume": "main",
    "abstract": "Optical flow estimation has made great progress, but usually suffers from degradation under adverse weather. Although semi/full-supervised methods have made good attempts, the domain shift between the synthetic and real adverse weather images would deteriorate their performance. To alleviate this issue, our start point is to unsupervisedly transfer the knowledge from source clean domain to target degraded domain. Our key insight is that adverse weather does not change the intrinsic optical flow of the scene, but causes a significant difference for the warp error between clean and degraded images. In this work, we propose the first unsupervised framework for adverse weather optical flow via hierarchical motion-boundary adaptation. Specifically, we first employ image translation to construct the transformation relationship between clean and degraded domains. In motion adaptation, we utilize the flow consistency knowledge to align the cross-domain optical flows into a motion-invariance common space, where the optical flow from clean weather is used as the guidance-knowledge to obtain a preliminary optical flow for adverse weather. Furthermore, we leverage the warp error inconsistency which measures the motion misalignment of the boundary between the clean and degraded domains, and propose a joint intra- and inter-scene boundary contrastive adaptation to refine the motion boundary. The hierarchical motion and boundary adaptation jointly promotes optical flow in a unified framework. Extensive quantitative and qualitative experiments have been performed to verify the superiority of the proposed method",
    "checked": true,
    "id": "cd9aae06d5720908760b47e46516adcb9a89f03f",
    "semantic_title": "unsupervised hierarchical domain adaptation for adverse weather optical flow",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25491": {
    "title": "PASS: Patch Automatic Skip Scheme for Efficient Real-Time Video Perception on Edge Devices",
    "volume": "main",
    "abstract": "Real-time video perception tasks are often challenging over the resource-constrained edge devices due to the concerns of accuracy drop and hardware overhead, where saving computations is the key to performance improvement. Existing methods either rely on domain-specific neural chips or priorly searched models, which require specialized optimization according to different task properties. In this work, we propose a general and task-independent Patch Automatic Skip Scheme (PASS), a novel end-to-end learning pipeline to support diverse video perception settings by decoupling acceleration and tasks. The gist is to capture the temporal similarity across video frames and skip the redundant computations at patch level, where the patch is a non-overlapping square block in visual. PASS equips each convolution layer with a learnable gate to selectively determine which patches could be safely skipped without degrading model accuracy. As to each layer, a desired gate needs to make flexible skip decisions based on intermediate features without any annotations, which cannot be achieved by conventional supervised learning paradigm. To address this challenge, we are the first to construct a tough self-supervisory procedure for optimizing these gates, which learns to extract contrastive representation, i.e., distinguishing similarity and difference, from frame sequence. These high-capacity gates can serve as a plug-and-play module for convolutional neural network (CNN) backbones to implement patch-skippable architectures, and automatically generate proper skip strategy to accelerate different video-based downstream tasks, e.g., outperforming the state-of-the-art MobileHumanPose (MHP) in 3D pose estimation and FairMOT in multiple object tracking, by up to 9.43 times and 12.19 times speedups, respectively. By directly processing the raw data of frames, PASS can generalize to real-time video streams on commodity edge devices, e.g., NVIDIA Jetson Nano, with efficient performance in realistic deployment",
    "checked": true,
    "id": "ebb9f49e940bdd14925a9776998244ad8aa74b57",
    "semantic_title": "pass: patch automatic skip scheme for efficient real-time video perception on edge devices",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25492": {
    "title": "Robust Feature Rectification of Pretrained Vision Models for Object Recognition",
    "volume": "main",
    "abstract": "Pretrained vision models for object recognition often suffer a dramatic performance drop with degradations unseen during training. In this work, we propose a RObust FEature Rectification module (ROFER) to improve the performance of pretrained models against degradations. Specifically, ROFER first estimates the type and intensity of the degradation that corrupts the image features. Then, it leverages a Fully Convolutional Network (FCN) to rectify the features from the degradation by pulling them back to clear features. ROFER is a general-purpose module that can address various degradations simultaneously, including blur, noise, and low contrast. Besides, it can be plugged into pretrained models seamlessly to rectify the degraded features without retraining the whole model. Furthermore, ROFER can be easily extended to address composite degradations by adopting a beam search algorithm to find the composition order. Evaluations on CIFAR-10 and Tiny-ImageNet demonstrate that the accuracy of ROFER is 5% higher than that of SOTA methods on different degradations. With respect to composite degradations, ROFER improves the accuracy of a pretrained CNN by 10% and 6% on CIFAR-10 and Tiny-ImageNet respectively",
    "checked": true,
    "id": "1d4ddca6c68ed963763711f545844557d6085b8a",
    "semantic_title": "robust feature rectification of pretrained vision models for object recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25493": {
    "title": "Video Object of Interest Segmentation",
    "volume": "main",
    "abstract": "In this work, we present a new computer vision task named video object of interest segmentation (VOIS). Given a video and a target image of interest, our objective is to simultaneously segment and track all objects in the video that are relevant to the target image. This problem combines the traditional video object segmentation task with an additional image indicating the content that users are concerned with. Since no existing dataset is perfectly suitable for this new task, we specifically construct a large-scale dataset called LiveVideos, which contains 2418 pairs of target images and live videos with instance-level annotations. In addition, we propose a transformer-based method for this task. We revisit Swin Transformer and design a dual-path structure to fuse video and image features. Then, a transformer decoder is employed to generate object proposals for segmentation and tracking from the fused features. Extensive experiments on LiveVideos dataset show the superiority of our proposed method",
    "checked": true,
    "id": "21c088c0620f6bf9d7699c9547af47318257df7a",
    "semantic_title": "video object of interest segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25494": {
    "title": "Tree-Structured Trajectory Encoding for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Over the past few years, the research on vision-and-language navigation (VLN) has made tremendous progress. Many previous works attempted to improve the performance from different aspects like training strategy, data augmentation, pre-training, etc. This work focuses on a rarely-explored aspect in VLN, namely the trajectory organization and encoding during the navigation. Most of existing state-of-the-art VLN models adopt a vanilla sequential strategy for encoding the trajectories. Such strategy takes the whole trajectory as a single sequence to estimate the current state, no matter whether the agent moved smoothly or perhaps made mistakes and backtracked in the past. We show that the sequential encoding may largely lose this kind of fine-grained structure in the trajectory, which could hamper the later state estimation and decision making. In order to solve this problem, this work proposes a novel tree-structured trajectory encoding strategy. The whole trajectory is organized as a tree rooted from the starting position, and encoded using our Tree-Transformer module to fully extract the fine-grained historical information. Besides, as the spatial topology could be easily embedded in the trajectory tree, we further design a tree-based action space to allow the agent making long-range error-correction in one decision. We implement the holistic agent based on cross-modal transformer and train it with a newly-proposed Tree-nDTW reward. On the benchmark dataset R2R, our model achieves a surpassing success rate (SR) of 68% on val-unseen and 66% on test. We further conduct extensive ablation studies and analyses to provide more insights for the effectiveness our designs",
    "checked": true,
    "id": "293951f26d10fed6950b2949e0f90571e0d67cd6",
    "semantic_title": "tree-structured trajectory encoding for vision-and-language navigation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25495": {
    "title": "Self-Supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences",
    "volume": "main",
    "abstract": "Self-supervised learning has demonstrated remarkable capability in representation learning for skeleton-based action recognition. Existing methods mainly focus on applying global data augmentation to generate different views of the skeleton sequence for contrastive learning. However, due to the rich action clues in the skeleton sequences, existing methods may only take a global perspective to learn to discriminate different skeletons without thoroughly leveraging the local relationship between different skeleton joints and video frames, which is essential for real-world applications. In this work, we propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the local relationship from a partial skeleton sequences built by a unique spatio-temporal masking strategy. Specifically, we construct a negative-sample-free triplet steam structure that is composed of an anchor stream without any masking, a spatial masking stream with Central Spatial Masking (CSM), and a temporal masking stream with Motion Attention Temporal Masking (MATM). The feature cross-correlation matrix is measured between the anchor stream and the other two masking streams, respectively. (1) Central Spatial Masking discards selected joints from the feature calculation process, where the joints with a higher degree of centrality have a higher possibility of being selected. (2) Motion Attention Temporal Masking leverages the motion of action and remove frames that move faster with a higher possibility. Our method achieves state-of-the-art performance on NTURGB+D 60, NTURGB+D 120 and PKU-MMD under various downstream tasks. Furthermore, to simulate the real-world scenarios, a practical evaluation is performed where some skeleton joints are lost in downstream tasks.In contrast to previous methods that suffer from large performance drops, our PSTL can still achieve remarkable results under this challenging setting, validating the robustness of our method",
    "checked": true,
    "id": "185e0240f15d84f805d36ed30f70ebd6e85be2b4",
    "semantic_title": "self-supervised action representation learning from partial spatio-temporal skeleton sequences",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25496": {
    "title": "Debiased Fine-Tuning for Vision-Language Models by Prompt Regularization",
    "volume": "main",
    "abstract": "We present a new paradigm for fine-tuning large-scale vision-language pre-trained models on downstream task, dubbed Prompt Regularization (ProReg). Different from traditional fine-tuning which easily overfits to the downstream task data, ProReg uses the prediction by prompting the pretrained model to regularize the fine-tuning. The motivation is: by prompting the large model \"a photo of a [CLASS]\", the fill-in answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased. Specifically, given a training sample prediction during fine-tuning, we first calculate its Kullback-Leibler loss of the prompt prediction and Cross-Entropy loss of the ground-truth label, and then combine them with a proposed sample-wise adaptive trade- off weight, which automatically adjusts the transfer between the pretrained and downstream domains. On various out-of-distribution benchmarks, we show the consistently strong performance of ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning, and other state-of-the-art methods",
    "checked": true,
    "id": "e8b73abefd998229f35e810f465854bdea7512f8",
    "semantic_title": "debiased fine-tuning for vision-language models by prompt regularization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25497": {
    "title": "Improving Scene Text Image Super-resolution via Dual Prior Modulation Network",
    "volume": "main",
    "abstract": "Scene text image super-resolution (STISR) aims to simultaneously increase the resolution and legibility of the text images, and the resulting images will significantly affect the performance of downstream tasks. Although numerous progress has been made, existing approaches raise two crucial issues: (1) They neglect the global structure of the text, which bounds the semantic determinism of the scene text. (2) The priors, e.g., text prior or stroke prior, employed in existing works, are extracted from pre-trained text recognizers. That said, such priors suffer from the domain gap including low resolution and blurriness caused by poor imaging conditions, leading to incorrect guidance. Our work addresses these gaps and proposes a plug-and-play module dubbed Dual Prior Modulation Network (DPMN), which leverages dual image-level priors to bring performance gain over existing approaches. Specifically, two types of prior-guided refinement modules, each using the text mask or graphic recognition result of the low-quality SR image from the preceding layer, are designed to improve the structural clarity and semantic accuracy of the text, respectively. The following attention mechanism hence modulates two quality-enhanced images to attain a superior SR result. Extensive experiments validate that our method improves the image quality and boosts the performance of downstream tasks over five typical approaches on the benchmark. Substantial visualizations and ablation studies demonstrate the advantages of the proposed DPMN. Code is available at: https://github.com/jdfxzzy/DPMN",
    "checked": true,
    "id": "e45d243316096cfc709e96027ebb2d353475ede6",
    "semantic_title": "improving scene text image super-resolution via dual prior modulation network",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25498": {
    "title": "SRoUDA: Meta Self-Training for Robust Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "As acquiring manual labels on data could be costly, unsupervised domain adaptation (UDA), which transfers knowledge learned from a rich-label dataset to the unlabeled target dataset, is gaining increasingly more popularity. While extensive studies have been devoted to improving the model accuracy on target domain, an important issue of model robustness is neglected. To make things worse, conventional adversarial training (AT) methods for improving model robustness are inapplicable under UDA scenario since they train models on adversarial examples that are generated by supervised loss function. In this paper, we present a new meta self-training pipeline, named SRoUDA, for improving adversarial robustness of UDA models. Based on self-training paradigm, SRoUDA starts with pre-training a source model by applying UDA baseline on source labeled data and taraget unlabeled data with a developed random masked augmentation (RMA), and then alternates between adversarial target model training on pseudo-labeled target data and fine-tuning source model by a meta step. While self-training allows the direct incorporation of AT in UDA, the meta step in SRoUDA further helps in mitigating error propagation from noisy pseudo labels. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of SRoUDA where it achieves significant model robustness improvement without harming clean accuracy",
    "checked": true,
    "id": "0a7ab69f162e706901c6aa1e391004971a463a3a",
    "semantic_title": "srouda: meta self-training for robust unsupervised domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25499": {
    "title": "Gradient-Based Graph Attention for Scene Text Image Super-resolution",
    "volume": "main",
    "abstract": "Scene text image super-resolution (STISR) in the wild has been shown to be beneficial to support improved vision-based text recognition from low-resolution imagery. An intuitive way to enhance STISR performance is to explore the well-structured and repetitive layout characteristics of text and exploit these as prior knowledge to guide model convergence. In this paper, we propose a novel gradient-based graph attention method to embed patch-wise text layout contexts into image feature representations for high-resolution text image reconstruction in an implicit and elegant manner. We introduce a non-local group-wise attention module to extract text features which are then enhanced by a cascaded channel attention module and a novel gradient-based graph attention module in order to obtain more effective representations by exploring correlations of regional and local patch-wise text layout properties. Extensive experiments on the benchmark TextZoom dataset convincingly demonstrate that our method supports excellent text recognition and outperforms the current state-of-the-art in STISR. The source code is available at https://github.com/xyzhu1/TSAN",
    "checked": true,
    "id": "30db786f5810602af4680dc4372c265e4597666e",
    "semantic_title": "gradient-based graph attention for scene text image super-resolution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25500": {
    "title": "RGBD1K: A Large-Scale Dataset and Benchmark for RGB-D Object Tracking",
    "volume": "main",
    "abstract": "RGB-D object tracking has attracted considerable attention recently, achieving promising performance thanks to the symbiosis between visual and depth channels. However, given a limited amount of annotated RGB-D tracking data, most state-of-the-art RGB-D trackers are simple extensions of high-performance RGB-only trackers, without fully exploiting the underlying potential of the depth channel in the offline training stage. To address the dataset deficiency issue, a new RGB-D dataset named RGBD1K is released in this paper. The RGBD1K contains 1,050 sequences with about 2.5M frames in total. To demonstrate the benefits of training on a larger RGB-D data set in general, and RGBD1K in particular, we develop a transformer-based RGB-D tracker, named SPT, as a baseline for future visual object tracking studies using the new dataset. The results, of extensive experiments using the SPT tracker demonstrate the potential of the RGBD1K dataset to improve the performance of RGB-D tracking, inspiring future developments of effective tracker designs. The dataset and codes will be available on the project homepage: https://github.com/xuefeng-zhu5/RGBD1K",
    "checked": true,
    "id": "b52382b22d25dd63c2a68424304e39024bf6e15f",
    "semantic_title": "rgbd1k: a large-scale dataset and benchmark for rgb-d object tracking",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25501": {
    "title": "Learn More for Food Recognition via Progressive Self-Distillation",
    "volume": "main",
    "abstract": "Food recognition has a wide range of applications, such as health-aware recommendation and self-service restaurants. Most previous methods of food recognition firstly locate informative regions in some weakly-supervised manners and then aggregate their features. However, location errors of informative regions limit the effectiveness of these methods to some extent. Instead of locating multiple regions, we propose a Progressive Self-Distillation (PSD) method, which progressively enhances the ability of network to mine more details for food recognition. The training of PSD simultaneously contains multiple self-distillations, in which a teacher network and a student network share the same embedding network. Since the student network receives a modified image from its teacher network by masking some informative regions, the teacher network outputs stronger semantic representations than the student network. Guided by such teacher network with stronger semantics, the student network is encouraged to mine more useful regions from the modified image by enhancing its own ability. The ability of the teacher network is also enhanced with the shared embedding network. By using progressive training, the teacher network incrementally improves its ability to mine more discriminative regions. In inference phase, only the teacher network is used without the help of the student network. Extensive experiments on three datasets demonstrate the effectiveness of our proposed method and state-of-the-art performance",
    "checked": true,
    "id": "ab094795f8a5a48b2cbf7b2889c59f5c1d329f2c",
    "semantic_title": "learn more for food recognition via progressive self-distillation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25502": {
    "title": "Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning",
    "volume": "main",
    "abstract": "This paper presents a new adversarial training framework for image inpainting with segmentation confusion adversarial training (SCAT) and contrastive learning. SCAT plays an adversarial game between an inpainting generator and a segmentation network, which provides pixel-level local training signals and can adapt to images with free-form holes. By combining SCAT with standard global adversarial training, the new adversarial training framework exhibits the following three advantages simultaneously: (1) the global consistency of the repaired image, (2) the local fine texture details of the repaired image, and (3) the flexibility of handling images with free-form holes. Moreover, we propose the textural and semantic contrastive learning losses to stabilize and improve our inpainting model's training by exploiting the feature representation space of the discriminator, in which the inpainting images are pulled closer to the ground truth images but pushed farther from the corrupted images. The proposed contrastive losses better guide the repaired images to move from the corrupted image data points to the real image data points in the feature representation space, resulting in more realistic completed images. We conduct extensive experiments on two benchmark datasets, demonstrating our model's effectiveness and superiority both qualitatively and quantitatively",
    "checked": true,
    "id": "65f5f5d54bde99776de1840e7cd59d30bbf74390",
    "semantic_title": "generative image inpainting with segmentation confusion adversarial training and contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25503": {
    "title": "Improved Algorithms for Maximum Satisfiability and Its Special Cases",
    "volume": "main",
    "abstract": "The Maximum Satisfiability (MAXSAT) problem is an optimization version of the Satisfiability problem (SAT) in which one is given a CNF formula with n variables and needs to find the maximum number of simultaneously satisfiable clauses. Recent works achieved significant progress in proving new upper bounds on the worst-case computational complexity of MAXSAT. All these works reduce general MAXSAT to a special case of MAXSAT where each variable appears a small number of times. So, it is important to design fast algorithms for (n,k)-MAXSAT to construct an efficient exact algorithm for MAXSAT. (n,k)-MAXSAT is a special case of MAXSAT where each variable appears at most k times in the input formula. For the (n,3)-MAXSAT problem, we design a O*(1.1749^n) algorithm improving on the previous record running time of O*(1.191^n). For the (n,4)-MAXSAT problem, we construct a O*(1.3803^n) algorithm improving on the previous best running time of O*(1.4254^n). Using the results, we develop a O*(1.0911^L) algorithm for the MAXSAT where L is a length of the input formula which improves previous algorithm with O*(1.0927^L) running time",
    "checked": true,
    "id": "e8e39247da25bb69cd9ceae1c5c5d0d4c354dd47",
    "semantic_title": "improved algorithms for maximum satisfiability and its special cases",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25504": {
    "title": "Lifting (D)QBF Preprocessing and Solving Techniques to (D)SSAT",
    "volume": "main",
    "abstract": "Dependency stochastic Boolean satisfiability (DSSAT) generalizes stochastic Boolean satisfiability (SSAT) in existential variables being Henkinized allowing their dependencies on randomized variables to be explicitly specified. It allows NEXPTIME problems of reasoning under uncertainty and partial information to be compactly encoded. To date, no decision procedure has been implemented for solving DSSAT formulas. This work provides the first such tool by converting DSSAT into SSAT with dependency elimination, similar to converting dependency quantified Boolean formula (DQBF) to quantified Boolean formula (QBF). Moreover, we extend (D)QBF preprocessing techniques and implement the first standalone (D)SSAT preprocessor. Experimental results show that solving DSSAT via dependency elimination is highly applicable and that existing SSAT solvers may benefit from preprocessing",
    "checked": true,
    "id": "6ae0b1ba9c743ad04020efe39e1237a5cbc7b1f5",
    "semantic_title": "lifting (d)qbf preprocessing and solving techniques to (d)ssat",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25505": {
    "title": "NuWLS: Improving Local Search for (Weighted) Partial MaxSAT by New Weighting Techniques",
    "volume": "main",
    "abstract": "Maximum Satisfiability (MaxSAT) is a prototypical constraint optimization problem, and its generalized version is the (Weighted) Partial MaxSAT problem, denoted as (W)PMS, which deals with hard and soft clauses. Considerable progress has been made on stochastic local search (SLS) algorithms for solving (W)PMS, which mainly focus on clause weighting techniques. In this work, we identify two issues of existing clause weighting techniques for (W)PMS, and propose two ideas correspondingly. First, we observe that the initial values of soft clause weights have a big effect on the performance of the SLS solver for solving (W)PMS, and propose a weight initialization method. Second, we propose a new clause weighting scheme that for the first time employs different conditions for updating hard and soft clause weights. Based on these two ideas, we develop a new SLS solver for (W)PMS named NuWLS. Through extensive experiments, NuWLS performs much better than existing SLS solvers on all 6 benchmarks from the incomplete tracks of MaxSAT Evaluations (MSEs) 2019, 2020, and 2021. In terms of the number of winning instances, NuWLS outperforms state-of-the-art SAT-based incomplete solvers on all the 6 benchmarks. More encouragingly, a hybrid solver that combines NuWLS and an SAT-based solver won all four categories in the incomplete track of the MaxSAT Evaluation 2022",
    "checked": true,
    "id": "68d54d627cfaa914d277a3f4a5508de1c4abe4ef",
    "semantic_title": "nuwls: improving local search for (weighted) partial maxsat by new weighting techniques",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25506": {
    "title": "Separate but Equal: Equality in Belief Propagation for Single Cycle Graphs",
    "volume": "main",
    "abstract": "Belief propagation is a widely used incomplete optimization algorithm, whose main theoretical properties hold only under the assumptions that beliefs are not equal. Nevertheless, there is much evidence that equality between beliefs does occur. A method to overcome belief equality by using unary function-nodes is assumed to resolve the problem. We focus on Min-sum, the belief propagation version for solving constraint optimization problems. We prove that on a single cycle graph, belief equality can be avoided only when the algorithm converges to the optimal solution. In any other case, the unary function methods will not prevent equality, rendering some existing results in need of reassessment. We differentiate between belief equality, which includes equal beliefs in a single message, and assignment equality, that prevents a coherent selection of assignments to variables. We show the necessary and satisfying conditions for both",
    "checked": true,
    "id": "276906bc63d919a9d576b80786c42254be84a0c7",
    "semantic_title": "separate but equal: equality in belief propagation for single cycle graphs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25507": {
    "title": "Complexity of Reasoning with Cardinality Minimality Conditions",
    "volume": "main",
    "abstract": "Many AI-related reasoning problems are based on the problem of satisfiability of propositional formulas with some cardinality-minimality condition. While the complexity of the satisfiability problem (SAT) is well understood when considering systematically all fragments of propositional logic within Schaefer's framework, this is not the case when such minimality condition is added. We consider the CardMinSat problem, which asks, given a formula φ and an atom x, whether x is true in some cardinality-minimal model of φ. We completely classify the computational complexity of the CardMinSat problem within Schaefer's framework, thus paving the way for a better understanding of the tractability frontier of many AI-related reasoning problems. To this end we use advanced algebraic tools",
    "checked": true,
    "id": "225a18fc8ae50641c5da8171cd11d00a428d32c9",
    "semantic_title": "complexity of reasoning with cardinality minimality conditions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25508": {
    "title": "DASH: A Distributed and Parallelizable Algorithm for Size-Constrained Submodular Maximization",
    "volume": "main",
    "abstract": "MapReduce (MR) algorithms for maximizing monotone, submodular functions subject to a cardinality constraint (SMCC) are currently restricted to the use of the linear-adaptive (non-parallelizable) algorithm GREEDY. Low-adaptive algorithms do not satisfy the requirements of these distributed MR frameworks, thereby limiting their performance. We study the SMCC problem in a distributed setting and propose the first MR algorithms with sublinear adaptive complexity. Our algorithms, R-DASH, T-DASH and G-DASH provide 0.316 - ε, 3/8 - ε , and (1 - 1/e - ε) approximation ratios, respectively, with nearly optimal adaptive complexity and nearly linear time complexity. Additionally, we provide a framework to increase, under some mild assumptions, the maximum permissible cardinality constraint from O( n / ℓ^2) of prior MR algorithms to O( n / ℓ ), where n is the data size and ℓ is the number of machines; under a stronger condition on the objective function, we increase the maximum constraint value to n. Finally, we provide empirical evidence to demonstrate that our sublinear-adaptive, distributed algorithms provide orders of magnitude faster runtime compared to current state-of-the-art distributed algorithms",
    "checked": true,
    "id": "e912edd7f1b82df95cc569ef73cf602bf7118ea8",
    "semantic_title": "dash: a distributed and parallelizable algorithm for size-constrained submodular maximization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25509": {
    "title": "SharpSSAT: A Witness-Generating Stochastic Boolean Satisfiability Solver",
    "volume": "main",
    "abstract": "Stochastic Boolean satisfiability (SSAT) is a formalism allowing decision-making for optimization under quantitative constraints. Although SSAT solvers are under active development, existing solvers do not provide Skolem-function witnesses, which are crucial for practical applications. In this work, we develop a new witness-generating SSAT solver, SharpSSAT, which integrates techniques, including component caching, clause learning, and pure literal detection. It can generate a set of Skolem functions witnessing the attained satisfying probability of a given SSAT formula. We also equip the solver ClauSSat with witness generation capability for comparison. Experimental results show that SharpSSAT outperforms current state-of-the-art solvers and can effectively generate compact Skolem-function witnesses. The new witness-generating solver may broaden the applicability of SSAT to practical applications",
    "checked": true,
    "id": "d9e4474d023987ed4c5f409176fe870db873ca85",
    "semantic_title": "sharpssat: a witness-generating stochastic boolean satisfiability solver",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25510": {
    "title": "Submodular Maximization under the Intersection of Matroid and Knapsack Constraints",
    "volume": "main",
    "abstract": "Submodular maximization arises in many applications, and has attracted a lot of research attentions from various areas such as artificial intelligence, finance and operations research. Previous studies mainly consider only one kind of constraint, while many real-world problems often involve several constraints. In this paper, we consider the problem of submodular maximization under the intersection of two commonly used constraints, i.e., k-matroid constraint and m-knapsack constraint, and propose a new algorithm SPROUT by incorporating partial enumeration into the simultaneous greedy framework. We prove that SPROUT can achieve a polynomial-time approximation guarantee better than the state-of-the-art algorithms. Then, we introduce the random enumeration and smooth techniques into SPROUT to improve its efficiency, resulting in the SPROUT++ algorithm, which can keep a similar approximation guarantee. Experiments on the applications of movie recommendation and weighted max-cut demonstrate the superiority of SPROUT++ in practice",
    "checked": true,
    "id": "d8f527c2046fcf50d9418d4d694d86c4601ac7ff",
    "semantic_title": "submodular maximization under the intersection of matroid and knapsack constraints",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25511": {
    "title": "A Framework to Design Approximation Algorithms for Finding Diverse Solutions in Combinatorial Problems",
    "volume": "main",
    "abstract": "Finding a \\emph{single} best solution is the most common objective in combinatorial optimization problems. However, such a single solution may not be applicable to real-world problems as objective functions and constraints are only ``approximately'' formulated for original real-world problems. To solve this issue, finding \\emph{multiple} solutions is a natural direction, and diversity of solutions is an important concept in this context. Unfortunately, finding diverse solutions is much harder than finding a single solution. To cope with the difficulty, we investigate the approximability of finding diverse solutions. As a main result, we propose a framework to design approximation algorithms for finding diverse solutions, which yields several outcomes including constant-factor approximation algorithms for finding diverse matchings in graphs and diverse common bases in two matroids and PTASes for finding diverse minimum cuts and interval schedulings",
    "checked": true,
    "id": "807cfcbca58e27d087a913be5c2481e524aeb7f9",
    "semantic_title": "a framework to design approximation algorithms for finding diverse solutions in combinatorial problems",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25512": {
    "title": "An Improved Approximation Algorithm for Wage Determination and Online Task Allocation in Crowd-Sourcing",
    "volume": "main",
    "abstract": "Crowd-sourcing has attracted much attention due to its growing importance to society, and numerous studies have been conducted on task allocation and wage determination. Recent works have focused on optimizing task allocation and workers' wages, simultaneously. However, existing methods do not provide good solutions for real-world crowd-sourcing platforms due to the low approximation ratio or myopic problem settings. We tackle an optimization problem for wage determination and online task allocation in crowd-sourcing and propose a fast 1-1/(k+3)^(1/2)-approximation algorithm, where k is the minimum of tasks' budgets (numbers of possible assignments). This approximation ratio is greater than or equal to the existing method. The proposed method reduces the tackled problem to a non-convex multi-period continuous optimization problem by approximating the objective function. Then, the method transforms the reduced problem into a minimum convex cost flow problem, which is a well-known combinatorial optimization problem, and solves it by the capacity scaling algorithm. Synthetic experiments and simulation experiments using real crowd-sourcing data show that the proposed method solves the problem faster and outputs higher objective values than existing methods",
    "checked": true,
    "id": "bc8beb173d4dad28357d848b2240d560b81f39f3",
    "semantic_title": "an improved approximation algorithm for wage determination and online task allocation in crowd-sourcing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25513": {
    "title": "Predict+Optimize for Packing and Covering LPs with Unknown Parameters in Constraints",
    "volume": "main",
    "abstract": "Predict+Optimize is a recently proposed framework which combines machine learning and constrained optimization, tackling optimization problems that contain parameters that are unknown at solving time. The goal is to predict the unknown parameters and use the estimates to solve for an estimated optimal solution to the optimization problem. However, all prior works have focused on the case where unknown parameters appear only in the optimization objective and not the constraints, for the simple reason that if the constraints were not known exactly, the estimated optimal solution might not even be feasible under the true parameters. The contributions of this paper are two-fold. First, we propose a novel and practically relevant framework for the Predict+Optimize setting, but with unknown parameters in both the objective and the constraints. We introduce the notion of a correction function, and an additional penalty term in the loss function, modelling practical scenarios where an estimated optimal solution can be modified into a feasible solution after the true parameters are revealed, but at an additional cost. Second, we propose a corresponding algorithmic approach for our framework, which handles all packing and covering linear programs. Our approach is inspired by the prior work of Mandi and Guns, though with crucial modifications and re-derivations for our very different setting. Experimentation demonstrates the superior empirical performance of our method over classical approaches",
    "checked": true,
    "id": "e0c94f185099505025b5b2ee94aa567409cdaf19",
    "semantic_title": "predict+optimize for packing and covering lps with unknown parameters in constraints",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25514": {
    "title": "Solving Explainability Queries with Quantification: The Case of Feature Relevancy",
    "volume": "main",
    "abstract": "Trustable explanations of machine learning (ML) models are vital in high-risk uses of artificial intelligence (AI). Apart from the computation of trustable explanations, a number of explainability queries have been identified and studied in recent work. Some of these queries involve solving quantification problems, either in propositional or in more expressive logics. This paper investigates one of these quantification problems, namely the feature relevancy problem (FRP), i.e.\\ to decide whether a (possibly sensitive) feature can occur in some explanation of a prediction. In contrast with earlier work, that studied FRP for specific classifiers, this paper proposes a novel algorithm for the \\fprob quantification problem which is applicable to any ML classifier that meets minor requirements. Furthermore, the paper shows that the novel algorithm is efficient in practice. The experimental results, obtained using random forests (RFs) induced from well-known publicly available datasets, demonstrate that the proposed solution outperforms existing state-of-the-art solvers for Quantified Boolean Formulas (QBF) by orders of magnitude. Finally, the paper also identifies a novel family of formulas that are challenging for currently state-of-the-art QBF solvers",
    "checked": true,
    "id": "622657250ba771cb4af66f7e54868314b777bbd4",
    "semantic_title": "solving explainability queries with quantification: the case of feature relevancy",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25515": {
    "title": "Second-Order Quantified Boolean Logic",
    "volume": "main",
    "abstract": "Second-order quantified Boolean formulas (SOQBFs) generalize quantified Boolean formulas (QBFs) by admitting second-order quantifiers on function variables in addition to first-order quantifiers on atomic variables. Recent endeavors establish that the complexity of SOQBF satisfiability corresponds to the exponential-time hierarchy (EXPH), similar to that of QBF satisfiability corresponding to the polynomial-time hierarchy (PH). This fact reveals the succinct expression power of SOQBFs in encoding decision problems not efficiently doable by QBFs. In this paper, we investigate the second-order quantified Boolean logic with the following main results: First, we present a procedure of quantifier elimination converting SOQBFs to QBFs and a game interpretation of SOQBF semantics. Second, we devise a sound and complete refutation-proof system for SOQBF. Third, we develop an algorithm for countermodel extraction from a refutation proof. Finally, we show potential applications of SOQBFs in system design and multi-agent planning. With these advances, we anticipate practical tools for development",
    "checked": true,
    "id": "84c64c851b2bbaa8533258b16dee807a25f52cbe",
    "semantic_title": "second-order quantified boolean logic",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25516": {
    "title": "Learning Markov Random Fields for Combinatorial Structures via Sampling through Lovász Local Lemma",
    "volume": "main",
    "abstract": "Learning to generate complex combinatorial structures satisfying constraints will have transformative impacts in many application domains. However, it is beyond the capabilities of existing approaches due to the highly intractable nature of the embedded probabilistic inference. Prior works spend most of the training time learning to separate valid from invalid structures but do not learn the inductive biases of valid structures. We develop NEural Lovasz Sampler (NELSON), which embeds the sampler through Lovasz Local Lemma (LLL) as a fully differentiable neural network layer. Our NELSON-CD embeds this sampler into the contrastive divergence learning process of Markov random fields. NELSON allows us to obtain valid samples from the current model distribution. Contrastive divergence is then applied to separate these samples from those in the training set. NELSON is implemented as a fully differentiable neural net, taking advantage of the parallelism of GPUs. Experimental results on several real-world domains reveal that NELSON learns to generate 100% valid structures, while baselines either time out or cannot ensure validity. NELSON also outperforms other approaches in running time, log-likelihood, and MAP scores",
    "checked": true,
    "id": "cd51ce7bb450a1e8bbf3727db6b809ca45b13518",
    "semantic_title": "learning markov random fields for combinatorial structures via sampling through lovász local lemma",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25517": {
    "title": "Fast Converging Anytime Model Counting",
    "volume": "main",
    "abstract": "Model counting is a fundamental problem which has been influential in many applications, from artificial intelligence to formal verification. Due to the intrinsic hardness of model counting, approximate techniques have been developed to solve real-world instances of model counting. This paper designs a new anytime approach called PartialKC for approximate model counting. The idea is a form of partial knowledge compilation to provide an unbiased estimate of the model count which can converge to the exact count. Our empirical analysis demonstrates that PartialKC achieves significant scalability and accuracy over prior state-of-the-art approximate counters, including satss and STS. Interestingly, the empirical results show that PartialKC reaches convergence for many instances and therefore provides exact model counting performance comparable to state-of-the-art exact counters",
    "checked": true,
    "id": "f1fc7323bb9f59f444be47e1624e31320e417f6b",
    "semantic_title": "fast converging anytime model counting",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25518": {
    "title": "Finding Good Partial Assignments during Restart-Based Branch and Bound Search",
    "volume": "main",
    "abstract": "Restart-based Branch-and-Bound Search (BBS) is a standard algorithm for solving Constraint Optimization Problems (COPs). In this paper, we propose an approach to find good partial assignments to jumpstart search at each restart for general COPs, which are identified by comparing different best solutions found in different restart runs. We consider information extracted from historical solutions to evaluate the quality of the partial assignments. Thus the good partial assignments are dynamically updated as the current best solution evolves. Our approach makes restart-based BBS explore different promising sub-search-spaces to find high-quality solutions. Experiments on the MiniZinc benchmark suite show how our approach brings significant improvements to a black-box COP solver equipped with the state of the art search techniques. Our method finds better solutions and proves optimality for more instances",
    "checked": true,
    "id": "51870d8e06dff589258700debae3f69dc993772b",
    "semantic_title": "finding good partial assignments during restart-based branch and bound search",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25519": {
    "title": "Hybrid Learning with New Value Function for the Maximum Common Induced Subgraph Problem",
    "volume": "main",
    "abstract": "Maximum Common Induced Subgraph (MCIS) is an important NP-hard problem with wide real-world applications. An efficient class of MCIS algorithms uses Branch-and-Bound (BnB), consisting in successively selecting vertices to match and pruning when it is discovered that a solution better than the best solution found so far does not exist. The method of selecting the vertices to match is essential for the performance of BnB. In this paper, we propose a new value function and a hybrid selection strategy used in reinforcement learning to define a new vertex selection method, and propose a new BnB algorithm, called McSplitDAL, for MCIS. Extensive experiments show that McSplitDAL significantly improves the current best BnB algorithms, McSplit+LL and McSplit+RL. An empirical analysis is also performed to illustrate why the new value function and the hybrid selection strategy are effective",
    "checked": true,
    "id": "dd2f352795968d59bd82169a5c5c9547b054f9e8",
    "semantic_title": "hybrid learning with new value function for the maximum common induced subgraph problem",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25520": {
    "title": "Self-Supervised Primal-Dual Learning for Constrained Optimization",
    "volume": "main",
    "abstract": "This paper studies how to train machine-learning models that directly approximate the optimal solutions of constrained optimization problems. This is an empirical risk minimization under constraints, which is challenging as training must balance optimality and feasibility conditions. Supervised learning methods often approach this challenge by training the model on a large collection of pre-solved instances. This paper takes a different route and proposes the idea of Primal-Dual Learning (PDL), a self-supervised training method that does not require a set of pre-solved instances or an optimization solver for training and inference. Instead, PDL mimics the trajectory of an Augmented Lagrangian Method (ALM) and jointly trains primal and dual neural networks. Being a primal-dual method, PDL uses instance-specific penalties of the constraint terms in the loss function used to train the primal network. Experiments show that, on a set of nonlinear optimization benchmarks, PDL typically exhibits negligible constraint violations and minor optimality gaps, and is remarkably close to the ALM optimization. PDL also demonstrated improved or similar performance in terms of the optimality gaps, constraint violations, and training times compared to existing approaches",
    "checked": true,
    "id": "27171044552454edcbd1d6a20ac0714ee3c46686",
    "semantic_title": "self-supervised primal-dual learning for constrained optimization",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25521": {
    "title": "Reinforcement Learning for Branch-and-Bound Optimisation Using Retrospective Trajectories",
    "volume": "main",
    "abstract": "Combinatorial optimisation problems framed as mixed integer linear programmes (MILPs) are ubiquitous across a range of real-world applications. The canonical branch-and-bound algorithm seeks to exactly solve MILPs by constructing a search tree of increasingly constrained sub-problems. In practice, its solving time performance is dependent on heuristics, such as the choice of the next variable to constrain ('branching'). Recently, machine learning (ML) has emerged as a promising paradigm for branching. However, prior works have struggled to apply reinforcement learning (RL), citing sparse rewards, difficult exploration, and partial observability as significant challenges. Instead, leading ML methodologies resort to approximating high quality handcrafted heuristics with imitation learning (IL), which precludes the discovery of novel policies and requires expensive data labelling. In this work, we propose retro branching; a simple yet effective approach to RL for branching. By retrospectively deconstructing the search tree into multiple paths each contained within a sub-tree, we enable the agent to learn from shorter trajectories with more predictable next states. In experiments on four combinatorial tasks, our approach enables learning-to-branch without any expert guidance or pre-training. We outperform the current state-of-the-art RL branching algorithm by 3-5x and come within 20% of the best IL method's performance on MILPs with 500 constraints and 1000 variables, with ablations verifying that our retrospectively constructed trajectories are essential to achieving these results",
    "checked": true,
    "id": "3212c957a11a46d059e981826372914045f5cc4a",
    "semantic_title": "reinforcement learning for branch-and-bound optimisation using retrospective trajectories",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25522": {
    "title": "Constraint Optimization over Semirings",
    "volume": "main",
    "abstract": "Interpretations of logical formulas over semirings (other than the Boolean semiring) have applications in various areas of computer science including logic, AI, databases, and security. Such interpretations provide richer information beyond the truth or falsity of a statement. Examples of such semirings include Viterbi semiring, min-max or access control semiring, tropical semiring, and fuzzy semiring. The present work investigates the complexity of constraint optimization problems over semirings. The generic optimization problem we study is the following: Given a propositional formula phi over n variable and a semiring (K,+, . ,0,1), find the maximum value over all possible interpretations of phi over K. This can be seen as a generalization of the well-known satisfiability problem (a propositional formula is satisfiable if and only if the maximum value over all interpretations/assignments over the Boolean semiring is 1). A related problem is to find an interpretation that achieves the maximum value. In this work, we first focus on these optimization problems over the Viterbi semiring, which we call optConfVal and optConf. We first show that for general propositional formulas in negation normal form, optConfVal and optConf are in FP^NP. We then investigate optConf when the input formula phi is represented in the conjunctive normal form. For CNF formulae, we first derive an upper bound on the value of optConf as a function of the number of maximum satisfiable clauses. In particular, we show that if r is the maximum number of satisfiable clauses in a CNF formula with m clauses, then its optConf value is at most 1/4^(m-r). Building on this we establish that optConf for CNF formulae is hard for the complexity class FP^NP[log]. We also design polynomial-time approximation algorithms and establish an inapproximability for optConfVal. We establish similar complexity results for these optimization problems over other semirings including tropical, fuzzy, and access control semirings",
    "checked": true,
    "id": "39fdccb34c7eae74dac974354db59d0ca48fc8c0",
    "semantic_title": "constraint optimization over semirings",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25523": {
    "title": "Generalized Confidence Constraints",
    "volume": "main",
    "abstract": "In robust optimization, finding a solution that solely respects the constraints is not enough. Usually, the uncertainty and unknown parameters of the model are represented by random variables. In such conditions, a good solution is a solution robust to most-likely assignments of these random variables. Recently, the Confidence constraint has been introduced by Mercier-Aubin et al. in order to enforce this type of robustness in constraint programming. Unfortunately, it is restricted to a conjunction of binary inequalities In this paper, we generalize the Confidence constraint to any constraint and propose an implementation based on Multi-valued Decision Diagrams (MDDs). The Confidence constraint is defined over a vector of random variables. For a given constraint C, and given a threshold, the Confidence constraint ensures that the probability for C to be satisfied by a sample of the random variables is greater than the threshold. We propose to use MDDs to represent the constraints on the random variables. MDDs are an efficient tool for representing combinatorial constraints, thanks to their exponential compression power. Here, both random and decision variables are stored in the MDD, and propagation rules are proposed for removing values of decision variables that cannot lead to robust solutions. Furthermore, for several constraints, we show that decision variables can be omitted from the MDD because lighter filtering algorithms are sufficient. This leads to gain an exponential factor in the MDD size. The experimental results obtained on a chemical deliveries problem in factories – where the chemicals consumption are uncertain – shows the efficiency of the proposed approach",
    "checked": true,
    "id": "7ed381fb10c3178120b182c5e4e70fb89791f466",
    "semantic_title": "generalized confidence constraints",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25524": {
    "title": "Circuit Minimization with QBF-Based Exact Synthesis",
    "volume": "main",
    "abstract": "This paper presents a rewriting method for Boolean circuits that minimizes small subcircuits with exact synthesis. Individual synthesis tasks are encoded as Quantified Boolean Formulas (QBFs) that capture the full flexibility for implementing multi-output subcircuits. This is in contrast to SAT-based resynthesis, where \"don't cares\" are computed for an individual gate, and replacements are confined to the circuitry used exclusively by that gate. An implementation of our method achieved substantial size reductions compared to state-of-the-art methods across a wide range of benchmark circuits",
    "checked": true,
    "id": "25f369e3318ba4ee373a5dcf2c7234e37b4ba2c8",
    "semantic_title": "circuit minimization with qbf-based exact synthesis",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25525": {
    "title": "Probabilistic Generalization of Backdoor Trees with Application to SAT",
    "volume": "main",
    "abstract": "The concept of Strong Backdoor Sets (SBS) for Constraint Satisfaction Problems is well known as one of the attempts to exploit structural peculiarities in hard instances. However, in practice, finding an SBS for a particular instance is often harder than solving it. Recently, a probabilistic weakened variant of the SBS was introduced: in the SBS, all subproblems must be polynomially solvable, whereas in the probabilistic SBS only a large fraction ρ of them should have this property. This new variant of backdoors called ρ-backdoors makes it possible to use the Monte Carlo method and metaheuristic optimization to find ρ-backdoors with ρ very close to 1, and relatively fast. Despite the fact that in a ρ-backdoor-based decomposition a portion of hard subproblems remain, in practice the narrowing of the search space often allows solving the problem faster with such a backdoor than without it. In this paper, we significantly improve on the concept of ρ-backdoors by extending this concept to backdoor trees: we introduce ρ-backdoor trees, show the interconnections between SBS, ρ-backdoors, and the corresponding backdoor trees, and establish some new theoretical properties of backdoor trees. In the experimental part of the paper, we show that moving from the metaheuristic search for ρ-backdoors to that of ρ-backdoor trees allows drastically reducing the time required to construct the required decompositions without compromising their quality",
    "checked": true,
    "id": "0b0738a5a45dee62b540b60b9bb33a82e41f8a12",
    "semantic_title": "probabilistic generalization of backdoor trees with application to sat",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25526": {
    "title": "The Expressive Power of Ad-Hoc Constraints for Modelling CSPs",
    "volume": "main",
    "abstract": "Ad-hoc constraints (also called generic constraints) are important for modelling Constraint Satisfaction Problems (CSPs). Many representations have been proposed to define ad-hoc constraints, such as tables, decision diagrams, binary constraint trees, automata and context-free grammars. However, prior works mainly focus on efficient Generalized Arc Consistency (GAC) propagators of ad-hoc constraints using the representations. In this paper, we ask a more fundamental question which bears on modelling constraints in a CSP as ad-hoc constraints, how the choice of constraints and operations affect tractability. Rather than ad-hoc constraints and their GAC propagators, our focus is on their expressive power in terms of succinctness (polysize) and cost of operations/queries (polytime). We use a large set of constraint families to investigate the expressive power of 14 existing ad-hoc constraints. We show a complete map of the succinctness of the ad-hoc constraints. We also present results on the tractability of applying various operations and queries on the ad-hoc constraints. Finally, we give case studies illustrating how our results can be useful for questions in the modelling of CSPs",
    "checked": true,
    "id": "b7e5501bfb07b6f3535263caf90d59a21724e510",
    "semantic_title": "the expressive power of ad-hoc constraints for modelling csps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25527": {
    "title": "Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",
    "volume": "main",
    "abstract": "The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the performance of general artificial intelligence algorithms. The ARC's focus on broad generalization and few-shot learning has made it difficult to solve using pure machine learning. A more promising approach has been to perform program synthesis within an appropriately designed Domain Specific Language (DSL). However, these too have seen limited success. We propose Abstract Reasoning with Graph Abstractions (ARGA), a new object-centric framework that first represents images using graphs and then performs a search for a correct program in a DSL that is based on the abstracted graph space. The complexity of this combinatorial search is tamed through the use of constraint acquisition, state hashing, and Tabu search. An extensive set of experiments demonstrates the promise of ARGA in tackling some of the complicated object-centric tasks of the ARC rather efficiently, producing programs that are correct and easy to understand",
    "checked": true,
    "id": "dcbda9002def84b4b4467e54e1d06a18ac227103",
    "semantic_title": "graphs, constraints, and search for the abstraction and reasoning corpus",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25528": {
    "title": "Eliminating the Impossible, Whatever Remains Must Be True: On Extracting and Applying Background Knowledge in the Context of Formal Explanations",
    "volume": "main",
    "abstract": "The rise of AI methods to make predictions and decisions has led to a pressing need for more explainable artificial intelligence (XAI) methods. One common approach for XAI is to produce a post-hoc explanation, explaining why a black box ML model made a certain prediction. Formal approaches to post-hoc explanations provide succinct reasons for why a prediction was made, as well as why not another prediction was made. But these approaches assume that features are independent and uniformly distributed. While this means that \"why\" explanations are correct, they may be longer than required. It also means the \"why not\" explanations may be suspect as the counterexamples they rely on may not be meaningful. In this paper, we show how one can apply background knowledge to give more succinct \"why\" formal explanations, that are presumably easier to interpret by humans, and give more accurate \"why not\" explanations. In addition, we show how to use existing rule induction techniques to efficiently extract background information from a dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25529": {
    "title": "Farsighted Probabilistic Sampling: A General Strategy for Boosting Local Search MaxSAT Solvers",
    "volume": "main",
    "abstract": "Local search has been demonstrated as an efficient approach for two practical generalizations of the MaxSAT problem, namely Partial MaxSAT (PMS) and Weighted PMS (WPMS). In this work, we observe that most local search (W)PMS solvers usually flip a single variable per iteration. Such a mechanism may lead to relatively low-quality local optimal solutions, and may limit the diversity of search directions to escape from local optima. To address this issue, we propose a general strategy, called farsighted probabilistic sampling (FPS), to replace the single flipping mechanism so as to boost the local search (W)PMS algorithms. FPS considers the benefit of continuously flipping a pair of variables in order to find higher-quality local optimal solutions. Moreover, FPS proposes an effective approach to escape from local optima by preferring the best to flip among the best sampled single variable and the best sampled variable pair. Extensive experiments demonstrate that our proposed FPS strategy significantly improves the state-of-the-art (W)PMS solvers, and FPS has an excellent generalization capability to various local search MaxSAT solvers",
    "checked": true,
    "id": "56c858b81ce04d884a711a70a0b40d5f9f166beb",
    "semantic_title": "farsighted probabilistic sampling: a general strategy for boosting local search maxsat solvers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25530": {
    "title": "LANCER: A Lifetime-Aware News Recommender System",
    "volume": "main",
    "abstract": "From the observation that users reading news tend to not click outdated news, we propose the notion of 'lifetime' of news, with two hypotheses: (i) news has a shorter lifetime, compared to other types of items such as movies or e-commerce products; (ii) news only competes with other news whose lifetimes have not ended, and which has an overlapping lifetime (i.e., limited competitions). By further developing the characteristics of the lifetime of news, then we present a novel approach for news recommendation, namely, Lifetime-Aware News reCommEndeR System (LANCER) that carefully exploits the lifetime of news during training and recommendation. Using real-world news datasets (e.g., Adressa and MIND), we successfully demonstrate that state-of-the-art news recommendation models can get significantly benefited by integrating the notion of lifetime and LANCER, by up to about 40% increases in recommendation accuracy",
    "checked": true,
    "id": "23627f20f53611a39a5df35642f99d3007863496",
    "semantic_title": "lancer: a lifetime-aware news recommender system",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25531": {
    "title": "Win-Win: A Privacy-Preserving Federated Framework for Dual-Target Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by transferring knowledge from an informative source domain to the target domain, which inevitably proposes stern challenges to data privacy and transferability during the transfer process. A small amount of recent CDR works have investigated privacy protection, while they still suffer from satisfying practical requirements (e.g., limited privacy-preserving ability) and preventing the potential risk of negative transfer. To address the above challenging problems, we propose a novel and unified privacy-preserving federated framework for dual-target CDR, namely P2FCDR. We design P2FCDR as peer-to-peer federated network architecture to ensure the local data storage and privacy protection of business partners. Specifically, for the special knowledge transfer process in CDR under federated settings, we initialize an optimizable orthogonal mapping matrix to learn the embedding transformation across domains and adopt the local differential privacy technique on the transformed embedding before exchanging across domains, which provides more reliable privacy protection. Furthermore, we exploit the similarity between in-domain and cross-domain embedding, and develop a gated selecting vector to refine the information fusion for more accurate dual transfer. Extensive experiments on three real-world datasets demonstrate that P2FCDR significantly outperforms the state-of-the-art methods and effectively protects data privacy",
    "checked": true,
    "id": "4fcda17617d3887eb657d017bc15017fc7b3641c",
    "semantic_title": "win-win: a privacy-preserving federated framework for dual-target cross-domain recommendation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25532": {
    "title": "Enhanced Multi-Relationships Integration Graph Convolutional Network for Inferring Substitutable and Complementary Items",
    "volume": "main",
    "abstract": "Understanding the relationships between items can improve the accuracy and interpretability of recommender systems. Among these relationships, the substitute and complement relationships attract the most attention in e-commerce platforms. The substitutable items are interchangeable and might be compared with each other before purchasing, while the complementary items are used in conjunction and are usually bought together with the query item. In this paper, we focus on two issues of inferring the substitutable and complementary items: 1) how to model their mutual influence to improve the performance of downstream tasks, 2) how to further discriminate them by considering the strength of relationship for different item pairs. We propose a novel multi-task learning framework named Enhanced Multi-Relationships Integration Graph Convolutional Network (EMRIGCN). We regard the relationship inference task as a link prediction task in heterogeneous graph with different types of edges between nodes (items). To model the mutual influence between substitute and complement, EMRIGCN adopts a two-level integration module, i.e., feature and structure integration, based on experts sharing mechanism during message passing. To obtain the strength of relationship for item pairs, we build an auxiliary loss function to further increase or decrease the distances between embeddings of items with weak or strong relation in latent space. Extensive experiments on both public and industrial datasets prove that EMRIGCN significantly outperforms the state-of-the-art solutions. We also conducted A/B tests on real world recommender systems of Meituan Maicai, an online supermarket platform in China, and obtained 15.3% improvement on VBR and 15.34% improvement on RPM",
    "checked": true,
    "id": "fa0090adfbc80de8dcc82c8d094ce800f674b094",
    "semantic_title": "enhanced multi-relationships integration graph convolutional network for inferring substitutable and complementary items",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25533": {
    "title": "PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs",
    "volume": "main",
    "abstract": "Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches type restriction to candidate constraints according to their measuring scores. We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively, the experimental results show that pattern-based automatic constraint mining is powerful in generating valuable temporal constraints",
    "checked": true,
    "id": "57a13f7f82a11887af3baceda506e2c579d7466d",
    "semantic_title": "patecon: a pattern-based temporal constraint mining method for conflict detection on knowledge graphs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25534": {
    "title": "End-to-End Entity Linking with Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "Entity linking (EL) is the task of linking the text segments to the referring entities in the knowledge graph, typically decomposed into mention detection, and entity disambiguation. Compared to traditional methods treating the two tasks separately, recent end-to-end entity linking methods exploit the mutual dependency between mentions and entities to achieve better performance. However, existing end-to-end EL methods have problems utilizing the dependency of mentions and entities in the task. To this end, we propose to model the EL task as a hierarchical decision-making process and design a hierarchical reinforcement learning algorithm to solve the problem. We conduct extensive experiments to show that the proposed method achieves state-of-the-art performance in several EL benchmark datasets. Our code is publicly available at https://github.com/lhlclhl/he2eel",
    "checked": true,
    "id": "aec8eed33a7f9195ba0925c798e6431702d22e1a",
    "semantic_title": "end-to-end entity linking with hierarchical reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25535": {
    "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency",
    "checked": true,
    "id": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
    "semantic_title": "entity-agnostic representation learning for parameter-efficient knowledge graph embedding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25536": {
    "title": "Dual Low-Rank Graph Autoencoder for Semantic and Topological Networks",
    "volume": "main",
    "abstract": "Due to the powerful capability to gather the information of neighborhood nodes, Graph Convolutional Network (GCN) has become a widely explored hotspot in recent years. As a well-established extension, Graph AutoEncoder (GAE) succeeds in mining underlying node representations via evaluating the quality of adjacency matrix reconstruction from learned features. However, limited works on GAE were devoted to leveraging both semantic and topological graphs, and they only indirectly extracted the relationships between graphs via weights shared by features. To better capture the connections between nodes from these two types of graphs, this paper proposes a graph neural network dubbed Dual Low-Rank Graph AutoEncoder (DLR-GAE), which takes both semantic and topological homophily into consideration. Differing from prior works that share common weights between GCNs, the presented DLR-GAE conducts sustained exploration of low-rank information between two distinct graphs, and reconstructs adjacency matrices from learned latent factors and embeddings. In order to obtain valid adjacency matrices that meet certain conditions, we design some surrogates and projections to restrict the learned factor matrix. We compare the proposed model with state-of-the-art methods on several datasets, which demonstrates the superior accuracy of DLR-GAE in semi-supervised classification",
    "checked": true,
    "id": "a7adc0b3b0a87de6f57cffcd3f6133cf1041214a",
    "semantic_title": "dual low-rank graph autoencoder for semantic and topological networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25537": {
    "title": "Dynamic Multi-Behavior Sequence Modeling for Next Item Recommendation",
    "volume": "main",
    "abstract": "Sequential Recommender Systems (SRSs) aim to predict the next item that users will consume, by modeling the user interests within their item sequences. While most existing SRSs focus on a single type of user behavior, only a few pay attention to multi-behavior sequences, although they are very common in real-world scenarios. It is challenging to effectively capture the user interests within multi-behavior sequences, because the information about user interests is entangled throughout the sequences in complex relationships. To this end, we first address the characteristics of multi-behavior sequences that should be considered in SRSs, and then propose novel methods for Dynamic Multi-behavior Sequence modeling named DyMuS, which is a light version, and DyMuS+, which is an improved version, considering the characteristics. DyMuS first encodes each behavior sequence independently, and then combines the encoded sequences using dynamic routing, which dynamically integrates information required in the final result from among many candidates, based on correlations between the sequences. DyMuS+, furthermore, applies the dynamic routing even to encoding each behavior sequence to further capture the correlations at item-level. Moreover, we release a new, large and up-to-date dataset for multi-behavior recommendation. Our experiments on DyMuS and DyMuS+ show their superiority and the significance of capturing the characteristics of multi-behavior sequences",
    "checked": true,
    "id": "b838508bcc1f591b7db00dabe19678fe8b0b3ce0",
    "semantic_title": "dynamic multi-behavior sequence modeling for next item recommendation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25538": {
    "title": "Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction",
    "volume": "main",
    "abstract": "Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets T1 and T2 where T1 is (Academy_Awards, Nominates, Avatar) and T2 is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that T1 is a prerequisite for T2. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level triplets, with additional consideration of the augmented triplets. We propose two new tasks: triplet prediction and conditional link prediction. Given a triplet T1 and a higher-level relation, the triplet prediction predicts a triplet that is likely to be connected to T1 by the higher-level relation, e.g., . The conditional link prediction predicts a missing entity in a triplet conditioned on another triplet, e.g., . Experimental results show that BiVE significantly outperforms all other methods in the two new tasks and the typical base-level link prediction in real-world bi-level knowledge graphs",
    "checked": true,
    "id": "cce3bb3d79c8c6e6f1b6b825f444a98d12f30833",
    "semantic_title": "learning representations of bi-level knowledge graphs for reasoning beyond link prediction",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25539": {
    "title": "Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs",
    "volume": "main",
    "abstract": "Existing knowledge graph (KG) embedding models have primarily focused on static KGs. However, real-world KGs do not remain static, but rather evolve and grow in tandem with the development of KG applications. Consequently, new facts and previously unseen entities and relations continually emerge, necessitating an embedding model that can quickly learn and transfer new knowledge through growth. Motivated by this, we delve into an expanding field of KG embedding in this paper, i.e., lifelong KG embedding. We consider knowledge transfer and retention of the learning on growing snapshots of a KG without having to learn embeddings from scratch. The proposed model includes a masked KG autoencoder for embedding learning and update, with an embedding transfer strategy to inject the learned knowledge into the new entity and relation embeddings, and an embedding regularization method to avoid catastrophic forgetting. To investigate the impacts of different aspects of KG growth, we construct four datasets to evaluate the performance of lifelong KG embedding. Experimental results show that the proposed model outperforms the state-of-the-art inductive and lifelong embedding baselines",
    "checked": true,
    "id": "7b7bfdc2936f09b09943d78d7b4d596f27e3274b",
    "semantic_title": "lifelong embedding learning and transfer for growing knowledge graphs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25540": {
    "title": "Uniform Sequence Better: Time Interval Aware Data Augmentation for Sequential Recommendation",
    "volume": "main",
    "abstract": "Sequential recommendation is an important task to predict the next-item to access based on a sequence of interacted items. Most existing works learn user preference as the transition pattern from the previous item to the next one, ignoring the time interval between these two items. However, we observe that the time interval in a sequence may vary significantly different, and thus result in the ineffectiveness of user modeling due to the issue of preference drift. In fact, we conducted an empirical study to validate this observation, and found that a sequence with uniformly distributed time interval (denoted as uniform sequence) is more beneficial for performance improvement than that with greatly varying time interval. Therefore, we propose to augment sequence data from the perspective of time interval, which is not studied in the literature. Specifically, we design five operators (Ti-Crop, Ti-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original non-uniform sequence to uniform sequence with the consideration of variance of time intervals. Then, we devise a control strategy to execute data augmentation on item sequences in different lengths. Finally, we implement these improvements on a state-of-the-art model CoSeRec and validate our approach on four real datasets. The experimental results show that our approach reaches significantly better performance than the other 9 competing methods. Our implementation is available: https://github.com/KingGugu/TiCoSeRec",
    "checked": true,
    "id": "614938bed58a2e496cfc373b2f70f11462506131",
    "semantic_title": "uniform sequence better: time interval aware data augmentation for sequential recommendation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25541": {
    "title": "Rule Induction in Knowledge Graphs Using Linear Programming",
    "volume": "main",
    "abstract": "We present a simple linear programming (LP) based method to learn compact and interpretable sets of rules encoding the facts in a knowledge graph (KG) and use these rules to solve the KG completion problem. Our LP model chooses a set of rules of bounded complexity from a list of candidate first-order logic rules and assigns weights to them. The complexity bound is enforced via explicit constraints. We combine simple rule generation heuristics with our rule selection LP to obtain predictions with accuracy comparable to state-of-the-art codes, even while generating much more compact rule sets. Furthermore, when we take as input rules generated by other codes, we often improve interpretability by reducing the number of chosen rules, while maintaining accuracy",
    "checked": true,
    "id": "b8169e28b81bce7e4fb878a3935d44829035ae2d",
    "semantic_title": "rule induction in knowledge graphs using linear programming",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25542": {
    "title": "Spatio-Temporal Neural Structural Causal Models for Bike Flow Prediction",
    "volume": "main",
    "abstract": "As a representative of public transportation, the fundamental issue of managing bike-sharing systems is bike flow prediction. Recent methods overemphasize the spatio-temporal correlations in the data, ignoring the effects of contextual conditions on the transportation system and the inter-regional time-varying causality. In addition, due to the disturbance of incomplete observations in the data, random contextual conditions lead to spurious correlations between data and features, making the prediction of the model ineffective in special scenarios. To overcome this issue, we propose a Spatio-temporal Neural Structure Causal Model(STNSCM) from the perspective of causality. First, we build a causal graph to describe the traffic prediction, and further analyze the causal relationship between the input data, contextual conditions, spatio-temporal states, and prediction results. Second, we propose to apply the frontdoor criterion to eliminate confounding biases in the feature extraction process. Finally, we propose a counterfactual representation reasoning module to extrapolate the spatio-temporal state under the factual scenario to future counterfactual scenarios to improve the prediction performance. Experiments on real-world datasets demonstrate the superior performance of our model, especially its resistance to fluctuations caused by the external environment. The source code and data will be released",
    "checked": true,
    "id": "9fab6048fc657ec13d4a59fe21b823ecd0ab0b95",
    "semantic_title": "spatio-temporal neural structural causal models for bike flow prediction",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25543": {
    "title": "DAMix: Exploiting Deep Autoregressive Model Zoo for Improving Lossless Compression Generalization",
    "volume": "main",
    "abstract": "Deep generative models have demonstrated superior performance in lossless compression on identically distributed data. However, in real-world scenarios, data to be compressed are of various distributions and usually cannot be known in advance. Thus, commercially expected neural compression must have strong Out-of-Distribution (OoD) generalization capabilities. Compared with traditional compression methods, deep learning methods have intrinsic flaws for OoD generalization. In this work, we make the attempt to tackle this challenge via exploiting a zoo of Deep Autoregressive models (DAMix). We build a model zoo consisting of autoregressive models trained on data from diverse distributions. In the test phase, we select useful expert models by a simple model evaluation score and adaptively aggregate the predictions of selected models. By assuming the outputs from each expert model are biased in favor of their training distributions, a von Mises-Fisher based filter is proposed to recover the value of unbiased predictions that provides more accurate density estimations than a single model. We derive the posterior of unbiased predictions as well as concentration parameters in the filter, and a novel temporal Stein variational gradient descent for sequential data is proposed to adaptively update the posterior distributions. We evaluate DAMix on 22 image datasets, including in-distribution and OoD data, and demonstrate that making use of unbiased predictions has up to 45.6% improvement over the single model trained on ImageNet",
    "checked": true,
    "id": "a8467d3032147d2d3508b2f98e09aea566d8a03e",
    "semantic_title": "damix: exploiting deep autoregressive model zoo for improving lossless compression generalization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25544": {
    "title": "Soft Target-Enhanced Matching Framework for Deep Entity Matching",
    "volume": "main",
    "abstract": "Deep Entity Matching (EM) is one of the core research topics in data integration. Typical existing works construct EM models by training deep neural networks (DNNs) based on the training samples with onehot labels. However, these sharp supervision signals of onehot labels harm the generalization of EM models, causing them to overfit the training samples and perform badly in unseen datasets. To solve this problem, we first propose that the challenge of training a well-generalized EM model lies in achieving the compromise between fitting the training samples and imposing regularization, i.e., the bias-variance tradeoff. Then, we propose a novel Soft Target-EnhAnced Matching (Steam) framework, which exploits the automatically generated soft targets as label-wise regularizers to constrain the model training. Specifically, Steam regards the EM model trained in previous iteration as a virtual teacher and takes its softened output as the extra regularizer to train the EM model in the current iteration. As such, Steam effectively calibrates the obtained EM model, achieving the bias-variance tradeoff without any additional computational cost. We conduct extensive experiments over open datasets and the results show that our proposed Steam outperforms the state-of-the-art EM approaches in terms of effectiveness and label efficiency",
    "checked": true,
    "id": "6894a73276a06f6093c23d4320d7fae2b09f4a54",
    "semantic_title": "soft target-enhanced matching framework for deep entity matching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25545": {
    "title": "DropMessage: Unifying Random Dropping for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph representation learning. Despite their rapid development, GNNs also face some challenges, such as over-fitting, over-smoothing, and non-robustness. Previous works indicate that these problems can be alleviated by random dropping methods, which integrate augmented data into models by randomly masking parts of the input. However, some open problems of random dropping on GNNs remain to be solved. First, it is challenging to find a universal method that are suitable for all cases considering the divergence of different datasets and models. Second, augmented data introduced to GNNs causes the incomplete coverage of parameters and unstable training process. Third, there is no theoretical analysis on the effectiveness of random dropping methods on GNNs. In this paper, we propose a novel random dropping method called DropMessage, which performs dropping operations directly on the propagated messages during the message-passing process. More importantly, we find that DropMessage provides a unified framework for most existing random dropping methods, based on which we give theoretical analysis of their effectiveness. Furthermore, we elaborate the superiority of DropMessage: it stabilizes the training process by reducing sample variance; it keeps information diversity from the perspective of information theory, enabling it become a theoretical upper bound of other methods. To evaluate our proposed method, we conduct experiments that aims for multiple tasks on five public datasets and two industrial datasets with various backbone models. The experimental results show that DropMessage has the advantages of both effectiveness and generalization, and can significantly alleviate the problems mentioned above. A detailed version with full appendix can be found on arXiv: https://arxiv.org/abs/2204.10037",
    "checked": true,
    "id": "688a20c88a022febbd7432c5231963235d4729bd",
    "semantic_title": "dropmessage: unifying random dropping for graph neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25546": {
    "title": "Contrastive Pre-training with Adversarial Perturbations for Check-In Sequence Representation Learning",
    "volume": "main",
    "abstract": "A core step of mining human mobility data is to learn accurate representations for user-generated check-in sequences. The learned representations should be able to fully describe the spatial-temporal mobility patterns of users and the high-level semantics of traveling. However, existing check-in sequence representation learning is usually implicitly achieved by end-to-end models designed for specific downstream tasks, resulting in unsatisfactory generalizable abilities and poor performance. Besides, although the sequence representation learning models that follow the contrastive learning pre-training paradigm have achieved breakthroughs in many fields like NLP, they fail to simultaneously consider the unique spatial-temporal characteristics of check-in sequences and need manual adjustments on the data augmentation strategies. So, directly applying them to check-in sequences cannot yield a meaningful pretext task. To this end, in this paper we propose a contrastive pre-training model with adversarial perturbations for check-in sequence representation learning (CACSR). Firstly, we design a novel spatial-temporal augmentation block for disturbing the spatial-temporal features of check-in sequences in the latent space to relieve the stress of designing manual data augmentation strategies. Secondly, to construct an effective contrastive pretext task, we generate \"hard\" positive and negative pairs for the check-in sequence by adversarial training. These two designs encourage the model to capture the high-level spatial-temporal patterns and semantics of check-in sequences while ignoring the noisy and unimportant details. We demonstrate the effectiveness and versatility of CACSR on two kinds of downstream tasks using three real-world datasets. The results show that our model outperforms both the state-of-the-art pre-training methods and the end-to-end models",
    "checked": true,
    "id": "d75010b4a317f6ea5a9a26bae72505b22cb8d134",
    "semantic_title": "contrastive pre-training with adversarial perturbations for check-in sequence representation learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25547": {
    "title": "MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Contrastive learning (CL), which can extract the information shared between different contrastive views, has become a popular paradigm for vision representation learning. Inspired by the success in computer vision, recent work introduces CL into graph modeling, dubbed as graph contrastive learning (GCL). However, generating contrastive views in graphs is more challenging than that in images, since we have little prior knowledge on how to significantly augment a graph without changing its labels. We argue that typical data augmentation techniques (e.g., edge dropping) in GCL cannot generate diverse enough contrastive views to filter out noises. Moreover, previous GCL methods employ two view encoders with exactly the same neural architecture and tied parameters, which further harms the diversity of augmented views. To address this limitation, we propose a novel paradigm named model augmented GCL (MA-GCL), which will focus on manipulating the architectures of view encoders instead of perturbing graph inputs. Specifically, we present three easy-to-implement model augmentation tricks for GCL, namely asymmetric, random and shuffling, which can respectively help alleviate high-frequency noises, enrich training instances and bring safer augmentations. All three tricks are compatible with typical data augmentations. Experimental results show that MA-GCL can achieve state-of-the-art performance on node classification benchmarks by applying the three tricks on a simple base model. Extensive studies also validate our motivation and the effectiveness of each trick. (Code, data and appendix are available at https://github.com/GXM1141/MA-GCL. )",
    "checked": true,
    "id": "3fd66f10b978d44e063aa23c64cdfe98722a3812",
    "semantic_title": "ma-gcl: model augmentation tricks for graph contrastive learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25548": {
    "title": "Generic and Dynamic Graph Representation Learning for Crowd Flow Modeling",
    "volume": "main",
    "abstract": "Many deep spatio-temporal learning methods have been proposed for crowd flow modeling in recent years. However, most of them focus on designing a spatial and temporal convolution mechanism to aggregate information from nearby nodes and historical observations for a pre-defined prediction task. Different from the existing research, this paper aims to provide a generic and dynamic representation learning method for crowd flow modeling. The main idea of our method is to maintain a continuous-time representation for each node, and update the representations of all nodes continuously according to the streaming observed data. Along this line, a particular encoder-decoder architecture is proposed, where the encoder converts the newly happened transactions into a timestamped message, and then the representations of related nodes are updated according to the generated message. The role of the decoder is to guide the representation learning process by reconstructing the observed transactions based on the most recent node representations. Moreover, a number of virtual nodes are added to discover macro-level spatial patterns and also share the representations among spatially-interacted stations. Experiments have been conducted on two real-world datasets for four popular prediction tasks in crowd flow modeling. The result demonstrates that our method could achieve better prediction performance for all the tasks than baseline methods",
    "checked": true,
    "id": "a9a358cf2e3f85fa7f286e392bfb0ad232c109e5",
    "semantic_title": "generic and dynamic graph representation learning for crowd flow modeling",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25549": {
    "title": "Conditional Diffusion Based on Discrete Graph Structures for Molecular Graph Generation",
    "volume": "main",
    "abstract": "Learning the underlying distribution of molecular graphs and generating high-fidelity samples is a fundamental research problem in drug discovery and material science. However, accurately modeling distribution and rapidly generating novel molecular graphs remain crucial and challenging goals. To accomplish these goals, we propose a novel Conditional Diffusion model based on discrete Graph Structures (CDGS) for molecular graph generation. Specifically, we construct a forward graph diffusion process on both graph structures and inherent features through stochastic differential equations (SDE) and derive discrete graph structures as the condition for reverse generative processes. We present a specialized hybrid graph noise prediction model that extracts the global context and the local node-edge dependency from intermediate graph states. We further utilize ordinary differential equation (ODE) solvers for efficient graph sampling, based on the semi-linear structure of the probability flow ODE. We also combine the solvers with gradient guidance from the molecule property predictor for similarity-constrained molecule optimization. Experiments on diverse datasets validate the effectiveness of our framework. Particularly, the proposed method still generates high-quality molecular graphs in a limited number of steps",
    "checked": true,
    "id": "67b5d57c08176787ae7b54b65ae3bf0ac11b9b04",
    "semantic_title": "conditional diffusion based on discrete graph structures for molecular graph generation",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25550": {
    "title": "SAH: Shifting-Aware Asymmetric Hashing for Reverse k Maximum Inner Product Search",
    "volume": "main",
    "abstract": "This paper investigates a new yet challenging problem called Reverse k-Maximum Inner Product Search (RkMIPS). Given a query (item) vector, a set of item vectors, and a set of user vectors, the problem of RkMIPS aims to find a set of user vectors whose inner products with the query vector are one of the k largest among the query and item vectors. We propose the first subquadratic-time algorithm, i.e., Shifting-aware Asymmetric Hashing (SAH), to tackle the RkMIPS problem. To speed up the Maximum Inner Product Search (MIPS) on item vectors, we design a shifting-invariant asymmetric transformation and develop a novel sublinear-time Shifting-Aware Asymmetric Locality Sensitive Hashing (SA-ALSH) scheme. Furthermore, we devise a new blocking strategy based on the Cone-Tree to effectively prune user vectors (in a batch). We prove that SAH achieves a theoretical guarantee for solving the RMIPS problem. Experimental results on five real-world datasets show that SAH runs 4~8x faster than the state-of-the-art methods for RkMIPS while achieving F1-scores of over 90%. The code is available at https://github.com/HuangQiang/SAH",
    "checked": false,
    "id": "e76548be2a8c67f437daee555bc7b246c540de42",
    "semantic_title": "sah: shifting-aware asymmetric hashing for reverse k-maximum inner product search",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25551": {
    "title": "Learned Distributed Image Compression with Multi-Scale Patch Matching in Feature Domain",
    "volume": "main",
    "abstract": "Beyond achieving higher compression efficiency over classical image compression codecs, deep image compression is expected to be improved with additional side information, e.g., another image from a different perspective of the same scene. To better utilize the side information under the distributed compression scenario, the existing method only implements patch matching at the image domain to solve the parallax problem caused by the difference in viewing points. However, the patch matching at the image domain is not robust to the variance of scale, shape, and illumination caused by the different viewing angles, and can not make full use of the rich texture information of the side information image. To resolve this issue, we propose Multi-Scale Feature Domain Patch Matching (MSFDPM) to fully utilizes side information at the decoder of the distributed image compression model. Specifically, MSFDPM consists of a side information feature extractor, a multi-scale feature domain patch matching module, and a multi-scale feature fusion network. Furthermore, we reuse inter-patch correlation from the shallow layer to accelerate the patch matching of the deep layer. Finally, we find that our patch matching in a multi-scale feature domain further improves compression rate by about 20% compared with the patch matching method at image domain",
    "checked": false,
    "id": "102a0f62845c3e2c351969bb210bbdc1dbc9ed6d",
    "semantic_title": "learned distributed image compression with multi-scale patch matching in feature domai",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25552": {
    "title": "Constrained Market Share Maximization by Signal-Guided Optimization",
    "volume": "main",
    "abstract": "With the rapid development of the airline industry, maximizing the market share with a constrained budget is an urgent econometric problem for an airline. We investigate the problem by adjusting flight frequencies on different flight routes. Owing to the large search space of solutions and the difficulty of predicting the market, this problem is in general daunting to solve. This paper proposes a novel two-stage optimization method to address the challenges. On the higher level, we use a signal to guide the optimization process toward a constrained satisfying solution. On the lower level, we consider the consecutive itineraries in real scenarios and model the unseen correlations between routes in itineraries for market share prediction. In theory, we prove the convergence of our optimization approach. In the experiment, we empirically verify the superiority of both our prediction model and optimization approach over existing works with large-scale real-world data. Our code has been released at: https://github.com/codingAndBS/AirlineMarket",
    "checked": true,
    "id": "6d1cbfd0ab6880a5fe0f5ce95e7d4a3072955839",
    "semantic_title": "constrained market share maximization by signal-guided optimization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25553": {
    "title": "T2-GNN: Graph Neural Networks for Graphs with Incomplete Features and Structure via Teacher-Student Distillation",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been a prevailing technique for tackling various analysis tasks on graph data. A key premise for the remarkable performance of GNNs relies on complete and trustworthy initial graph descriptions (i.e., node features and graph structure), which is often not satisfied since real-world graphs are often incomplete due to various unavoidable factors. In particular, GNNs face greater challenges when both node features and graph structure are incomplete at the same time. The existing methods either focus on feature completion or structure completion. They usually rely on the matching relationship between features and structure, or employ joint learning of node representation and feature (or structure) completion in the hope of achieving mutual benefit. However, recent studies confirm that the mutual interference between features and structure leads to the degradation of GNN performance. When both features and structure are incomplete, the mismatch between features and structure caused by the missing randomness exacerbates the interference between the two, which may trigger incorrect completions that negatively affect node representation. To this end, in this paper we propose a general GNN framework based on teacher-student distillation to improve the performance of GNNs on incomplete graphs, namely T2-GNN. To avoid the interference between features and structure, we separately design feature-level and structure-level teacher models to provide targeted guidance for student model (base GNNs, such as GCN) through distillation. Then we design two personalized methods to obtain well-trained feature and structure teachers. To ensure that the knowledge of the teacher model is comprehensively and effectively distilled to the student model, we further propose a dual distillation mode to enable the student to acquire as much expert knowledge as possible. Extensive experiments on eight benchmark datasets demonstrate the effectiveness and robustness of the new framework on graphs with incomplete features and structure",
    "checked": true,
    "id": "9b17f8d306e0e8264830394229e38c5d79b77523",
    "semantic_title": "t2-gnn: graph neural networks for graphs with incomplete features and structure via teacher-student distillation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25554": {
    "title": "Detecting Sources of Healthcare Associated Infections",
    "volume": "main",
    "abstract": "Healthcare acquired infections (HAIs) (e.g., Methicillin-resistant Staphylococcus aureus infection) have complex transmission pathways, spreading not just via direct person-to-person contacts, but also via contaminated surfaces. Prior work in mathematical epidemiology has led to a class of models – which we call load sharing models – that provide a discrete-time, stochastic formalization of HAI-spread on temporal contact networks. The focus of this paper is the source detection problem for the load sharing model. The source detection problem has been studied extensively in SEIR type models, but this prior work does not apply to load sharing models. We show that a natural formulation of the source detection problem for the load sharing model is computationally hard, even to approximate. We then present two alternate formulations that are much more tractable. The tractability of our problems depends crucially on the submodularity of the expected number of infections as a function of the source set. Prior techniques for showing submodularity, such as the \"live graph\" technique are not applicable for the load sharing model and our key technical contribution is to use a more sophisticated \"coupling\" technique to show the submodularity result. We propose algorithms for our two problem formulations by extending existing algorithmic results from submodular optimization and combining these with an expectation propagation heuristic for the load sharing model that leads to orders-of-magnitude speedup. We present experimental results on temporal contact networks based on fine-grained EMR data from three different hospitals. Our results on synthetic outbreaks on these networks show that our algorithms outperform baselines by up to 5.97 times. Furthermore, case studies based on hospital outbreaks of Clostridioides difficile infection show that our algorithms identify clinically meaningful sources",
    "checked": true,
    "id": "d0a7b41426bbae754e4ff50dd2dad0741b199a15",
    "semantic_title": "detecting sources of healthcare associated infections",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25555": {
    "title": "Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction",
    "volume": "main",
    "abstract": "Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specifically, our ST-SSL is built over an integrated module with temporal and spatial convolutions for encoding the information across space and time. To achieve the adaptive spatio-temporal self-supervised learning, our ST-SSL first performs the adaptive augmentation over the traffic flow graph data at both attribute- and structure-levels. On top of the augmented traffic graph, two SSL auxiliary tasks are constructed to supplement the main traffic prediction task with spatial and temporal heterogeneity-aware augmentation. Experiments on four benchmark datasets demonstrate that ST-SSL consistently outperforms various state-of-the-art baselines. Since spatio-temporal heterogeneity widely exists in practical datasets, the proposed framework may also cast light on other spatial-temporal applications. Model implementation is available at https://github.com/Echo-Ji/ST-SSL",
    "checked": true,
    "id": "abf773ddd5cb8f42a20e77effcf7e1f826991d42",
    "semantic_title": "spatio-temporal self-supervised learning for traffic flow prediction",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25556": {
    "title": "PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction",
    "volume": "main",
    "abstract": "As a core technology of Intelligent Transportation System, traffic flow prediction has a wide range of applications. The fundamental challenge in traffic flow prediction is to effectively model the complex spatial-temporal dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN) models have emerged as one of the most promising methods to solve this problem. However, GNN-based models have three major limitations for traffic prediction: i) Most methods model spatial dependencies in a static manner, which limits the ability to learn dynamic urban traffic patterns; ii) Most methods only consider short-range spatial information and are unable to capture long-range spatial dependencies; iii) These methods ignore the fact that the propagation of traffic conditions between locations has a time delay in traffic systems. To this end, we propose a novel Propagation Delay-aware dynamic long-range transFormer, namely PDFormer, for accurate traffic flow prediction. Specifically, we design a spatial self-attention module to capture the dynamic spatial dependencies. Then, two graph masking matrices are introduced to highlight spatial dependencies from short- and long-range views. Moreover, a traffic delay-aware feature transformation module is proposed to empower PDFormer with the capability of explicitly modeling the time delay of spatial information propagation. Extensive experimental results on six real-world public traffic datasets show that our method can not only achieve state-of-the-art performance but also exhibit competitive computational efficiency. Moreover, we visualize the learned spatial-temporal attention map to make our model highly interpretable",
    "checked": true,
    "id": "38381097a0add12ff685dfaac0191c31c9ae429a",
    "semantic_title": "pdformer: propagation delay-aware dynamic long-range transformer for traffic flow prediction",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25557": {
    "title": "Continuous Trajectory Generation Based on Two-Stage GAN",
    "volume": "main",
    "abstract": "Simulating the human mobility and generating large-scale trajectories are of great use in many real-world applications, such as urban planning, epidemic spreading analysis, and geographic privacy protect. Although many previous works have studied the problem of trajectory generation, the continuity of the generated trajectories has been neglected, which makes these methods useless for practical urban simulation scenarios. To solve this problem, we propose a novel two-stage generative adversarial framework to generate the continuous trajectory on the road network, namely TS-TrajGen, which efficiently integrates prior domain knowledge of human mobility with model-free learning paradigm. Specifically, we build the generator under the human mobility hypothesis of the A* algorithm to learn the human mobility behavior. For the discriminator, we combine the sequential reward with the mobility yaw reward to enhance the effectiveness of the generator. Finally, we propose a novel two-stage generation process to overcome the weak point of the existing stochastic generation process. Extensive experiments on two real-world datasets and two case studies demonstrate that our framework yields significant improvements over the state-of-the-art methods",
    "checked": true,
    "id": "ba316f5916807fd0b49b54452f7216c366208e1e",
    "semantic_title": "continuous trajectory generation based on two-stage gan",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25558": {
    "title": "Let Graph Be the Go Board: Gradient-Free Node Injection Attack for Graph Neural Networks via Reinforcement Learning",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have drawn significant attentions over the years and been broadly applied to essential applications requiring solid robustness or vigorous security standards, such as product recommendation and user behavior modeling. Under these scenarios, exploiting GNN's vulnerabilities and further downgrading its performance become extremely incentive for adversaries. Previous attackers mainly focus on structural perturbations or node injections to the existing graphs, guided by gradients from the surrogate models. Although they deliver promising results, several limitations still exist. For the structural perturbation attack, to launch a proposed attack, adversaries need to manipulate the existing graph topology, which is impractical in most circumstances. Whereas for the node injection attack, though being more practical, current approaches require training surrogate models to simulate a white-box setting, which results in significant performance downgrade when the surrogate architecture diverges from the actual victim model. To bridge these gaps, in this paper, we study the problem of black-box node injection attack, without training a potentially misleading surrogate model. Specifically, we model the node injection attack as a Markov decision process and propose Gradient-free Graph Advantage Actor Critic, namely G2A2C, a reinforcement learning framework in the fashion of advantage actor critic. By directly querying the victim model, G2A2C learns to inject highly malicious nodes with extremely limited attacking budgets, while maintaining a similar node feature distribution. Through our comprehensive experiments over eight acknowledged benchmark datasets with different characteristics, we demonstrate the superior performance of our proposed G2A2C over the existing state-of-the-art attackers. Source code is publicly available at: https://github.com/jumxglhf/G2A2C",
    "checked": true,
    "id": "3b494763868d078aa306c1f1b3f7c21ffd83feac",
    "semantic_title": "let graph be the go board: gradient-free node injection attack for graph neural networks via reinforcement learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25559": {
    "title": "GLCC: A General Framework for Graph-Level Clustering",
    "volume": "main",
    "abstract": "This paper studies the problem of graph-level clustering, which is a novel yet challenging task. This problem is critical in a variety of real-world applications such as protein clustering and genome analysis in bioinformatics. Recent years have witnessed the success of deep clustering coupled with graph neural networks (GNNs). However, existing methods focus on clustering among nodes given a single graph, while exploring clustering on multiple graphs is still under-explored. In this paper, we propose a general graph-level clustering framework named Graph-Level Contrastive Clustering (GLCC) given multiple graphs. Specifically, GLCC first constructs an adaptive affinity graph to explore instance- and cluster-level contrastive learning (CL). Instance-level CL leverages graph Laplacian based contrastive loss to learn clustering-friendly representations while cluster-level CL captures discriminative cluster representations incorporating neighbor information of each sample. Moreover, we utilize neighbor-aware pseudo-labels to reward the optimization of representation learning. The two steps can be alternatively trained to collaborate and benefit each other. Experiments on a range of well-known datasets demonstrate the superiority of our proposed GLCC over competitive baselines",
    "checked": true,
    "id": "cbcf89becf091e3abdf86f692fc3150f0dab33a9",
    "semantic_title": "glcc: a general framework for graph-level clustering",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25560": {
    "title": "Parameterized Algorithms for Colored Clustering",
    "volume": "main",
    "abstract": "In the Colored Clustering problem, one is asked to cluster edge-colored (hyper-)graphs whose colors represent interaction types. More specifically, the goal is to select as many edges as possible without choosing two edges that share an endpoint and are colored differently. Equivalently, the goal can also be described as assigning colors to the vertices in a way that fits the edge-coloring as well as possible. As this problem is NP-hard, we build on previous work by studying its parameterized complexity. We give a 2ᴼ⁽ᵏ⁾·nᴼ⁽¹⁾-time algorithm where k is the number of edges to be selected and n the number of vertices. We also prove the existence of a problem kernel of size O(k⁵ᐟ²), resolving an open problem posed in the literature. We consider parameters that are smaller than k, the number of edges to be selected, and r, the number of edges that can be deleted. Such smaller parameters are obtained by considering the difference between k or r and some lower bound on these values. We give both algorithms and lower bounds for Colored Clustering with such parameterizations. Finally, we settle the parameterized complexity of Colored Clustering with respect to structural graph parameters by showing that it is W[1]-hard with respect to both vertex cover number and tree-cut width, but fixed-parameter tractable with respect to local feedback edge number",
    "checked": true,
    "id": "2dfe8538b77cdccf89434f1091463dd1c0db9caa",
    "semantic_title": "parameterized algorithms for colored clustering",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25561": {
    "title": "Towards Reliable Item Sampling for Recommendation Evaluation",
    "volume": "main",
    "abstract": "Since Rendle and Krichene argued that commonly used sampling-based evaluation metrics are ``inconsistent'' with respect to the global metrics (even in expectation), there have been a few studies on the sampling-based recommender system evaluation. Existing methods try either mapping the sampling-based metrics to their global counterparts or more generally, learning the empirical rank distribution to estimate the top-K metrics. However, despite existing efforts, there is still a lack of rigorous theoretical understanding of the proposed metric estimators, and the basic item sampling also suffers from the ``blind spot'' issue, i.e., estimation accuracy to recover the top-K metrics when K is small can still be rather substantial. In this paper, we provide an in-depth investigation into these problems and make two innovative contributions. First, we propose a new item-sampling estimator that explicitly optimizes the error with respect to the ground truth, and theoretically highlights its subtle difference against prior work. Second, we propose a new adaptive sampling method that aims to deal with the ``blind spot'' problem and also demonstrate the expectation-maximization (EM) algorithm can be generalized for such a setting. Our experimental results confirm our statistical analysis and the superiority of the proposed works. This study helps lay the theoretical foundation for adopting item sampling metrics for recommendation evaluation and provides strong evidence for making item sampling a powerful and reliable tool for recommendation evaluation",
    "checked": true,
    "id": "8fdb76a8fdc41eacf3c2c87a9f3d1dba9cc0e068",
    "semantic_title": "towards reliable item sampling for recommendation evaluation",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25562": {
    "title": "Multiple Robust Learning for Recommendation",
    "volume": "main",
    "abstract": "In recommender systems, a common problem is the presence of various biases in the collected data, which deteriorates the generalization ability of the recommendation models and leads to inaccurate predictions. Doubly robust (DR) learning has been studied in many tasks in RS, with the advantage that unbiased learning can be achieved when either a single imputation or a single propensity model is accurate. In this paper, we propose a multiple robust (MR) estimator that can take the advantage of multiple candidate imputation and propensity models to achieve unbiasedness. Specifically, the MR estimator is unbiased when any of the imputation or propensity models, or a linear combination of these models is accurate. Theoretical analysis shows that the proposed MR is an enhanced version of DR when only having a single imputation and propensity model, and has a smaller bias. Inspired by the generalization error bound of MR, we further propose a novel multiple robust learning approach with stabilization. We conduct extensive experiments on real-world and semi-synthetic datasets, which demonstrates the superiority of the proposed approach over state-of-the-art methods",
    "checked": true,
    "id": "50ba1ee99c52448b0abdc0143315449fa4f5664b",
    "semantic_title": "multiple robust learning for recommendation",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25563": {
    "title": "Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors",
    "volume": "main",
    "abstract": "Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery is aimed at segmenting anomaly patterns of the earth deviating from normal patterns, which plays an important role in various Earth vision applications. However, it is a challenging task due to the complex distribution and the irregular shapes of objects, and the lack of abnormal samples. To tackle these problems, an anomaly segmentation model based on pixel descriptors (ASD) is proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class classification is introduced for anomaly segmentation in the feature space with discriminative pixel descriptors. The ASD model incorporates the data argument for generating virtual abnormal samples, which can force the pixel descriptors to be compact for normal data and meanwhile to be diverse to avoid the model collapse problems when only positive samples participated in the training. In addition, the ASD introduced a multi-level and multi-scale feature extraction strategy for learning the low-level and semantic information to make the pixel descriptors feature-rich. The proposed ASD model was validated using four HSR datasets and compared with the recent state-of-the-art models, showing its potential value in Earth vision applications",
    "checked": true,
    "id": "be8e35b1753cc7aed92767db46b99289997af62d",
    "semantic_title": "anomaly segmentation for high-resolution remote sensing images based on pixel descriptors",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25564": {
    "title": "Adaptive Low-Precision Training for Embeddings in Click-Through Rate Prediction",
    "volume": "main",
    "abstract": "Embedding tables are usually huge in click-through rate (CTR) prediction models. To train and deploy the CTR models efficiently and economically, it is necessary to compress their embedding tables. To this end, we formulate a novel quantization training paradigm to compress the embeddings from the training stage, termed low-precision training (LPT). Also, we provide theoretical analysis on its convergence. The results show that stochastic weight quantization has a faster convergence rate and a smaller convergence error than deterministic weight quantization in LPT. Further, to reduce accuracy degradation, we propose adaptive low-precision training (ALPT) which learns the step size (i.e., the quantization resolution). Experiments on two real-world datasets confirm our analysis and show that ALPT can significantly improve the prediction accuracy, especially at extremely low bit width. For the first time in CTR models, we successfully train 8-bit embeddings without sacrificing prediction accuracy",
    "checked": true,
    "id": "624c0373019dabb9f5b69464c0f094013c2b101c",
    "semantic_title": "adaptive low-precision training for embeddings in click-through rate prediction",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25565": {
    "title": "Signed Laplacian Graph Neural Networks",
    "volume": "main",
    "abstract": "This paper studies learning meaningful node representations for signed graphs, where both positive and negative links exist. This problem has been widely studied by meticulously designing expressive signed graph neural networks, as well as capturing the structural information of the signed graph through traditional structure decomposition methods, e.g., spectral graph theory. In this paper, we propose a novel signed graph representation learning framework, called Signed Laplacian Graph Neural Network (SLGNN), which combines the advantages of both. Specifically, based on spectral graph theory and graph signal processing, we first design different low-pass and high-pass graph convolution filters to extract low-frequency and high-frequency information on positive and negative links, respectively, and then combine them into a unified message passing framework. To effectively model signed graphs, we further propose a self-gating mechanism to estimate the impacts of low-frequency and high-frequency information during message passing. We mathematically establish the relationship between the aggregation process in SLGNN and signed Laplacian regularization in signed graphs, and theoretically analyze the expressiveness of SLGNN. Experimental results demonstrate that SLGNN outperforms various competitive baselines and achieves state-of-the-art performance",
    "checked": true,
    "id": "23141437bffed6d5457c676bff0b59adf6d64a9a",
    "semantic_title": "signed laplacian graph neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25566": {
    "title": "PPGenCDR: A Stable and Robust Framework for Privacy-Preserving Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "Privacy-preserving cross-domain recommendation (PPCDR) refers to preserving the privacy of users when transferring the knowledge from source domain to target domain for better performance, which is vital for the long-term development of recommender systems. Existing work on cross-domain recommendation (CDR) reaches advanced and satisfying recommendation performance, but mostly neglects preserving privacy. To fill this gap, we propose a privacy-preserving generative cross-domain recommendation (PPGenCDR) framework for PPCDR. PPGenCDR includes two main modules, i.e., stable privacy-preserving generator module, and robust cross-domain recommendation module. Specifically, the former isolates data from different domains with a generative adversarial network (GAN) based model, which stably estimates the distribution of private data in the source domain with ́Renyi differential privacy (RDP) technique. Then the latter aims to robustly leverage the perturbed but effective knowledge from the source domain with the raw data in target domain to improve recommendation performance. Three key modules, i.e., (1) selective privacy preserver, (2) GAN stabilizer, and (3) robustness conductor, guarantee the cost-effective trade-off between utility and privacy, the stability of GAN when using RDP, and the robustness of leveraging transferable knowledge accordingly. The extensive empirical studies on Douban and Amazon datasets demonstrate that PPGenCDR significantly outperforms the state-of-the-art recommendation models while preserving privacy",
    "checked": true,
    "id": "370cacdf83aa10e60d79025fad336ac467eb415e",
    "semantic_title": "ppgencdr: a stable and robust framework for privacy-preserving cross-domain recommendation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25567": {
    "title": "COLA: Improving Conversational Recommender Systems by Collaborative Augmentation",
    "volume": "main",
    "abstract": "Conversational recommender systems (CRS) aim to employ natural language conversations to suggest suitable products to users. Understanding user preferences for prospective items and learning efficient item representations are crucial for CRS. Despite various attempts, earlier studies mostly learned item representations based on individual conversations, ignoring item popularity embodied among all others. Besides, they still need support in efficiently capturing user preferences since the information reflected in a single conversation is limited. Inspired by collaborative filtering, we propose a collaborative augmentation (COLA) method to simultaneously improve both item representation learning and user preference modeling to address these issues. We construct an interactive user-item graph from all conversations, which augments item representations with user-aware information, i.e., item popularity. To improve user preference modeling, we retrieve similar conversations from the training corpus, where the involved items and attributes that reflect the user's potential interests are used to augment the user representation through gate control. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our method. Our code and data are available at https://github.com/DongdingLin/COLA",
    "checked": true,
    "id": "d02536f52b22c303e902f1a5e34ac8cfeb56a293",
    "semantic_title": "cola: improving conversational recommender systems by collaborative augmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25568": {
    "title": "Scalable and Effective Conductance-Based Graph Clustering",
    "volume": "main",
    "abstract": "Conductance-based graph clustering has been recognized as a fundamental operator in numerous graph analysis applications. Despite the significant success of conductance-based graph clustering, existing algorithms are either hard to obtain satisfactory clustering qualities, or have high time and space complexity to achieve provable clustering qualities. To overcome these limitations, we devise a powerful peeling-based graph clustering framework PCon. We show that many existing solutions can be reduced to our framework. Namely, they first define a score function for each vertex, then iteratively remove the vertex with the smallest score. Finally, they output the result with the smallest conductance during the peeling process. Based on our framework, we propose two novel algorithms PCon_core and PCon_de with linear time and space complexity, which can efficiently and effectively identify clusters from massive graphs with more than a few billion edges. Surprisingly, we prove that PCon_de can identify clusters with near-constant approximation ratio, resulting in an important theoretical improvement over the well-known quadratic Cheeger bound. Empirical results on real-life and synthetic datasets show that our algorithms can achieve 5~42 times speedup with a high clustering accuracy, while using 1.4~7.8 times less memory than the baseline algorithms",
    "checked": true,
    "id": "05930e5e7684c05fe1461f404bda10434ca143f4",
    "semantic_title": "scalable and effective conductance-based graph clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25569": {
    "title": "Multi-Domain Generalized Graph Meta Learning",
    "volume": "main",
    "abstract": "Graph meta learning aims to learn historical knowledge from training graph neural networks (GNNs) models and adapt it to downstream learning tasks in a target graph, which has drawn increasing attention due to its ability of knowledge transfer and fast adaptation. While existing graph meta learning approaches assume the learning tasks are from the same graph domain but lack the solution for multi-domain adaptation. In this paper, we address the multi-domain generalized graph meta learning problem, which is challenging due to non-Euclidean data, inequivalent feature spaces, and heterogeneous distributions. To this end, we propose a novel solution called MD-Gram for multi-domain graph generalization. It introduces an empirical graph generalization method that uses empirical vectors to form a unified expression of non-Euclidean graph data. Then it proposes a multi-domain graphs transformation approach to transform the learning tasks from multiple source-domain graphs with inequivalent feature spaces into a common domain, where graph meta learning is conducted to learn generalized knowledge. It further adopts a domain-specific GNN enhancement method to learn a customized GNN model to achieve fast adaptation in the unseen target domain. Extensive experiments based on four real-world graph domain datasets show that the proposed method significantly outperforms the state-of-the-art in multi-domain graph meta learning tasks",
    "checked": true,
    "id": "3ae83395b9ca551e0efd74fac10551e396974040",
    "semantic_title": "multi-domain generalized graph meta learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25570": {
    "title": "IterDE: An Iterative Knowledge Distillation Framework for Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "Knowledge distillation for knowledge graph embedding (KGE) aims to reduce the KGE model size to address the challenges of storage limitations and knowledge reasoning efficiency. However, current work still suffers from the performance drops when compressing a high-dimensional original KGE model to a low-dimensional distillation KGE model. Moreover, most work focuses on the reduction of inference time but ignores the time-consuming training process of distilling KGE models. In this paper, we propose IterDE, a novel knowledge distillation framework for KGEs. First, IterDE introduces an iterative distillation way and enables a KGE model to alternately be a student model and a teacher model during the iterative distillation process. Consequently, knowledge can be transferred in a smooth manner between high-dimensional teacher models and low-dimensional student models, while preserving good KGE performances. Furthermore, in order to optimize the training process, we consider that different optimization objects between hard label loss and soft label loss can affect the efficiency of training, and then we propose a soft-label weighting dynamic adjustment mechanism that can balance the inconsistency of optimization direction between hard and soft label loss by gradually increasing the weighting of soft label loss. Our experimental results demonstrate that IterDE achieves a new state-of-the-art distillation performance for KGEs compared to strong baselines on the link prediction task. Significantly, IterDE can reduce the training time by 50% on average. Finally, more exploratory experiments show that the soft-label weighting dynamic adjustment mechanism and more fine-grained iterations can improve distillation performance",
    "checked": true,
    "id": "0a400be3a281ac5f0a30df63d224f00bdaad2ad8",
    "semantic_title": "iterde: an iterative knowledge distillation framework for knowledge graph embeddings",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25571": {
    "title": "Learning by Applying: A General Framework for Mathematical Reasoning via Enhancing Explicit Knowledge Learning",
    "volume": "main",
    "abstract": "Mathematical reasoning is one of the crucial abilities of general artificial intelligence, which requires machines to master mathematical logic and knowledge from solving problems. However, existing approaches are not transparent (thus not interpretable) in terms of what knowledge has been learned and applied in the reasoning process. In this paper, we propose a general Learning by Applying (LeAp) framework to enhance existing models (backbones) in a principled way by explicit knowledge learning. In LeAp, we perform knowledge learning in a novel problem-knowledge-expression paradigm, with a Knowledge Encoder to acquire knowledge from problem data and a Knowledge Decoder to apply knowledge for expression reasoning. The learned mathematical knowledge, including word-word relations and word-operator relations, forms an explicit knowledge graph, which bridges the knowledge \"learning\" and \"applying\" organically. Moreover, for problem solving, we design a semantics-enhanced module and a reasoning-enhanced module that apply knowledge to improve the problem comprehension and symbol reasoning abilities of any backbone, respectively. We theoretically prove the superiority of LeAp's autonomous learning mechanism. Experiments on three real-world datasets show that LeAp improves all backbones' performances, learns accurate knowledge, and achieves a more interpretable reasoning process",
    "checked": true,
    "id": "0cc9d031ca2f85c1412d5eab9449416c47b8cacd",
    "semantic_title": "learning by applying: a general framework for mathematical reasoning via enhancing explicit knowledge learning",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25572": {
    "title": "Low-Resource Personal Attribute Prediction from Conversations",
    "volume": "main",
    "abstract": "Personal knowledge bases (PKBs) are crucial for a broad range of applications such as personalized recommendation and Web-based chatbots. A critical challenge to build PKBs is extracting personal attribute knowledge from users' conversation data. Given some users of a conversational system, a personal attribute and these users' utterances, our goal is to predict the ranking of the given personal attribute values for each user. Previous studies often rely on a relative number of resources such as labeled utterances and external data, yet the attribute knowledge embedded in unlabeled utterances is underutilized and their performance of predicting some difficult personal attributes is still unsatisfactory. In addition, it is found that some text classification methods could be employed to resolve this task directly. However, they also perform not well over those difficult personal attributes. In this paper, we propose a novel framework PEARL to predict personal attributes from conversations by leveraging the abundant personal attribute knowledge from utterances under a low-resource setting in which no labeled utterances or external data are utilized. PEARL combines the biterm semantic information with the word co-occurrence information seamlessly via employing the updated prior attribute knowledge to refine the biterm topic model's Gibbs sampling process in an iterative manner. The extensive experimental results show that PEARL outperforms all the baseline methods not only on the task of personal attribute prediction from conversations over two data sets, but also on the more general weakly supervised text classification task over one data set",
    "checked": false,
    "id": "f1e588f1fc1e6456c7ec14a984c630ef58f8df9c",
    "semantic_title": "low-resource personal attribute prediction from conversation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25573": {
    "title": "Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating",
    "volume": "main",
    "abstract": "Unsupervised graph representation learning (UGRL) has drawn increasing research attention and achieved promising results in several graph analytic tasks. Relying on the homophily assumption, existing UGRL methods tend to smooth the learned node representations along all edges, ignoring the existence of heterophilic edges that connect nodes with distinct attributes. As a result, current methods are hard to generalize to heterophilic graphs where dissimilar nodes are widely connected, and also vulnerable to adversarial attacks. To address this issue, we propose a novel unsupervised Graph Representation learning method with Edge hEterophily discriminaTing (GREET) which learns representations by discriminating and leveraging homophilic edges and heterophilic edges. To distinguish two types of edges, we build an edge discriminator that infers edge homophily/heterophily from feature and structure information. We train the edge discriminator in an unsupervised way through minimizing the crafted pivot-anchored ranking loss, with randomly sampled node pairs acting as pivots. Node representations are learned through contrasting the dual-channel encodings obtained from the discriminated homophilic and heterophilic edges. With an effective interplaying scheme, edge discriminating and representation learning can mutually boost each other during the training phase. We conducted extensive experiments on 14 benchmark datasets and multiple learning scenarios to demonstrate the superiority of GREET",
    "checked": true,
    "id": "b29aec89e64bb20f2b963e5615c79b9008ecfd88",
    "semantic_title": "beyond smoothing: unsupervised graph representation learning with edge heterophily discriminating",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25574": {
    "title": "On Generalized Degree Fairness in Graph Neural Networks",
    "volume": "main",
    "abstract": "Conventional graph neural networks (GNNs) are often confronted with fairness issues that may stem from their input, including node attributes and neighbors surrounding a node. While several recent approaches have been proposed to eliminate the bias rooted in sensitive attributes, they ignore the other key input of GNNs, namely the neighbors of a node, which can introduce bias since GNNs hinge on neighborhood structures to generate node representations. In particular, the varying neighborhood structures across nodes, manifesting themselves in drastically different node degrees, give rise to the diverse behaviors of nodes and biased outcomes. In this paper, we first define and generalize the degree bias using a generalized definition of node degree as a manifestation and quantification of different multi-hop structures around different nodes. To address the bias in the context of node classification, we propose a novel GNN framework called Generalized Degree Fairness-centric Graph Neural Network (DegFairGNN). Specifically, in each GNN layer, we employ a learnable debiasing function to generate debiasing contexts, which modulate the layer-wise neighborhood aggregation to eliminate the degree bias originating from the diverse degrees among nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our model on both accuracy and fairness metrics",
    "checked": true,
    "id": "6c26e7f14b52332087c9a1e07e09a2b882665ef3",
    "semantic_title": "on generalized degree fairness in graph neural networks",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25575": {
    "title": "Time Series Contrastive Learning with Information-Aware Augmentations",
    "volume": "main",
    "abstract": "Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where \"desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high fidelity and variety based on information theory. A theoretical analysis leads to the criteria for selecting feasible data augmentations. On top of that, we propose a new contrastive learning approach with information-aware augmentations, InfoTS, that adaptively selects optimal augmentations for time series representation learning. Experiments on various datasets show highly competitive performance with up to a 12.0% reduction in MSE on forecasting tasks and up to 3.7% relative improvement in accuracy on classification tasks over the leading baselines",
    "checked": true,
    "id": "6e70a2b7512fde9d25176c508f9cad35e47f66ad",
    "semantic_title": "time series contrastive learning with information-aware augmentations",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25576": {
    "title": "NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs",
    "volume": "main",
    "abstract": "Complex query answering (CQA) is an essential task for multi-hop and logical reasoning on knowledge graphs (KGs). Currently, most approaches are limited to queries among binary relational facts and pay less attention to n-ary facts (n≥2) containing more than two entities, which are more prevalent in the real world. Moreover, previous CQA methods can only make predictions for a few given types of queries and cannot be flexibly extended to more complex logical queries, which significantly limits their applications. To overcome these challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model for CQA over hyper-relational knowledge graphs (HKGs), which include massive n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and fuzzy logic theory to satisfy all n-ary FOL queries, including existential quantifiers (∃), conjunction (∧), disjunction (∨), and negation (¬). We also propose a parallel processing algorithm that can train or predict arbitrary n-ary FOL queries in a single batch, regardless of the kind of each query, with good flexibility and extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and other standard CQA datasets show that NQE is the state-of-the-art CQA method over HKGs with good generalization capability. Our code and dataset are publicly available",
    "checked": true,
    "id": "d94e7aeab88d07f0b50b7866ceb5baa3c29be859",
    "semantic_title": "nqe: n-ary query embedding for complex query answering over hyper-relational knowledge graphs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25577": {
    "title": "FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction",
    "volume": "main",
    "abstract": "Click-through rate (CTR) prediction is one of the fundamental tasks in online advertising and recommendation. Multi-layer perceptron (MLP) serves as a core component in many deep CTR prediction models, but it has been widely shown that applying a vanilla MLP network alone is ineffective in learning complex feature interactions. As such, many two-stream models (e.g., Wide&Deep, DeepFM, and DCN) have recently been proposed, aiming to integrate two parallel sub-networks to learn feature interactions from two different views for enhanced CTR prediction. In addition to one MLP stream that learns feature interactions implicitly, most of the existing research focuses on designing another stream to complement the MLP stream with explicitly enhanced feature interactions. Instead, this paper presents a simple two-stream feature interaction model, namely FinalMLP, which employs only MLPs in both streams yet achieves surprisingly strong performance. In contrast to sophisticated network design in each stream, our work enhances CTR modeling through a feature selection module, which produces differentiated feature inputs to two streams, and a group-wise bilinear fusion module, which effectively captures stream-level interactions across two streams. We show that FinalMLP achieves competitive or even better performance against many existing two-stream CTR models on four open benchmark datasets and also brings significant CTR improvements during an online A/B test in our industrial news recommender system. We envision that the simple yet effective FinalMLP model could serve as a new strong baseline for future development of two-stream CTR models. Our source code will be available at MindSpore/models and FuxiCTR/model_zoo",
    "checked": true,
    "id": "4af58fc20efaa3856df8609921b6a022f8f9d3ac",
    "semantic_title": "finalmlp: an enhanced two-stream mlp model for ctr prediction",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25578": {
    "title": "GMDNet: A Graph-Based Mixture Density Network for Estimating Packages' Multimodal Travel Time Distribution",
    "volume": "main",
    "abstract": "In the logistics network, accurately estimating packages' Travel Time Distribution (TTD) given the routes greatly benefits both consumers and platforms. Although recent works perform well in predicting an expected time or a time distribution in a road network, they could not be well applied to estimate TTD in logistics networks. Because TTD prediction in the logistics network requires modeling packages' multimodal TTD (MTTD, i.e., there can be more than one likely output with a given input) while leveraging the complex correlations in the logistics network. To this end, this work opens appealing research opportunities in studying MTTD learning conditioned on graph-structure data by investigating packages' travel time distribution in the logistics network. We propose a Graph-based Mixture Density Network, named GMDNet, which takes the benefits of both graph neural network and mixture density network for estimating MTTD conditioned on graph-structure data (i.e., the logistics network). Furthermore, we adopt the Expectation-Maximization (EM) framework in the training process to guarantee local convergence and thus obtain more stable results than gradient descent. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed model",
    "checked": true,
    "id": "0a9d4820dee3d1de377a7ba16223653701ba726e",
    "semantic_title": "gmdnet: a graph-based mixture density network for estimating packages' multimodal travel time distribution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25579": {
    "title": "Logic and Commonsense-Guided Temporal Knowledge Graph Completion",
    "volume": "main",
    "abstract": "A temporal knowledge graph (TKG) stores the events derived from the data involving time. Predicting events is extremely challenging due to the time-sensitive property of events. Besides, the previous TKG completion (TKGC) approaches cannot represent both the timeliness and the causality properties of events, simultaneously. To address these challenges, we propose a Logic and Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive representation involving timeliness and causality of events, together with the time-independent representation of events from the perspective of commonsense. Specifically, we design a temporal rule learning algorithm to construct a rule-guided predicate embedding regularization strategy for learning the causality among events. Furthermore, we could accurately evaluate the plausibility of events via auxiliary commonsense knowledge. The experimental results of TKGC task illustrate the significant performance improvements of our model compared with the existing approaches. More interestingly, our model is able to provide the explainability of the predicted results in the view of causal inference. The appendix, source code and datasets of this paper are available at https://github.com/ngl567/LCGE",
    "checked": true,
    "id": "3e0a7f3f98e49d0a690842a5692e723615c44928",
    "semantic_title": "logic and commonsense-guided temporal knowledge graph completion",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25580": {
    "title": "Graph Structure Learning on User Mobility Data for Social Relationship Inference",
    "volume": "main",
    "abstract": "With the prevalence of smart mobile devices and location-based services, uncovering social relationships from human mobility data is of great value in real-world spatio-temporal applications ranging from friend recommendation, advertisement targeting to transportation scheduling. While a handful of sophisticated graph embedding techniques are developed for social relationship inference, they are significantly limited to the sparse and noisy nature of user mobility data, as they all ignore the essential problem of the existence of a large amount of noisy data unrelated to social activities in such mobility data. In this work, we present Social Relationship Inference Network (SRINet), a novel Graph Neural Network (GNN) framework, to improve inference performance by learning to remove noisy data. Specifically, we first construct a multiplex user meeting graph to model the spatial-temporal interactions among users in different semantic contexts. Our proposed SRINet tactfully combines the representation learning ability of Graph Convolutional Networks (GCNs) with the power of removing noisy edges of graph structure learning, which can learn effective user embeddings on the multiplex user meeting graph in a semi-supervised manner. Extensive experiments on three real-world datasets demonstrate the superiority of SRINet against state-of-the-art techniques in inferring social relationships from user mobility data. The source code of our method is available at https://github.com/qinguangming1999/SRINet",
    "checked": true,
    "id": "cca56116eea606d1be0dd258f03f4c3c679c065a",
    "semantic_title": "graph structure learning on user mobility data for social relationship inference",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25581": {
    "title": "Online Random Feature Forests for Learning in Varying Feature Spaces",
    "volume": "main",
    "abstract": "In this paper, we propose a new online learning algorithm tailored for data streams described by varying feature spaces (VFS), wherein new features constantly emerge and old features may stop to be observed over various time spans. Our proposed algorithm, named Online Random Feature Forests for Feature space Variabilities (ORF3V), provides a strategy to respect such feature dynamics by generating, updating, pruning, as well as online re-weighing an ensemble of what we call feature forests, which are generated and updated based on a compressed and storage efficient representation for each observed feature. We benchmark our algorithm on 12 datasets, including one novel real-world dataset of government COVID-19 responses collected through a crowd-sensing program in Spain. The empirical results substantiate the viability and effectiveness of our ORF3V algorithm and its superior accuracy performance over the state-of-the-art rival models",
    "checked": true,
    "id": "4482de2f9d3ff6b74d774c9a67a9d6cf5fa5fc59",
    "semantic_title": "online random feature forests for learning in varying feature spaces",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25582": {
    "title": "Scaling Law for Recommendation Models: Towards General-Purpose User Representations",
    "volume": "main",
    "abstract": "Recent advancement of large-scale pretrained models such as BERT, GPT-3, CLIP, and Gopher, has shown astonishing achievements across various task domains. Unlike vision recognition and language models, studies on general-purpose user representation at scale still remain underexplored. Here we explore the possibility of general-purpose user representation learning by training a universal user encoder at large scales. We demonstrate that the scaling law is present in user representation learning areas, where the training error scales as a power-law with the amount of computation. Our Contrastive Learning User Encoder (CLUE), optimizes task-agnostic objectives, and the resulting user embeddings stretch our expectation of what is possible to do in various downstream tasks. CLUE also shows great transferability to other domains and companies, as performances on an online experiment shows significant improvements in Click-Through-Rate (CTR). Furthermore, we also investigate how the model performance is influenced by the scale factors, such as training data size, model capacity, sequence length, and batch size. Finally, we discuss the broader impacts of CLUE in general",
    "checked": true,
    "id": "7567744a0e23174166575e8d98590967684696b4",
    "semantic_title": "scaling law for recommendation models: towards general-purpose user representations",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25583": {
    "title": "Cross-Domain Adaptative Learning for Online Advertisement Customer Lifetime Value Prediction",
    "volume": "main",
    "abstract": "Accurate estimation of customer lifetime value (LTV), which reflects the potential consumption of a user over a period of time, is crucial for the revenue management of online advertising platforms. However, predicting LTV in real-world applications is not an easy task since the user consumption data is usually insufficient within a specific domain. To tackle this problem, we propose a novel cross-domain adaptative framework (CDAF) to leverage consumption data from different domains. The proposed method is able to simultaneously mitigate the data scarce problem and the distribution gap problem caused by data from different domains. To be specific, our method firstly learns a LTV prediction model from a different but related platform with sufficient data provision. Subsequently, we exploit domain-invariant information to mitigate data scarce problem by minimizing the Wasserstein discrepancy between the encoded user representations of two domains. In addition, we design a dual-predictor schema which not only enhances domain-invariant information in the semantic space but also preserves domain-specific information for accurate target prediction. The proposed framework is evaluated on five datasets collected from real historical data on the advertising platform of Tencent Games. Experimental results verify that the proposed framework is able to significantly improve the LTV prediction performance on this platform. For instance, our method can boost DCNv2 with the improvement of 13.7% in terms of AUC on dataset G2. Code: https://github.com/TL-UESTC/CDAF",
    "checked": true,
    "id": "562f254f7b0fccba6377a513fc95cbbb4bf80ee7",
    "semantic_title": "cross-domain adaptative learning for online advertisement customer lifetime value prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25584": {
    "title": "Self-Supervised Interest Transfer Network via Prototypical Contrastive Learning for Recommendation",
    "volume": "main",
    "abstract": "Cross-domain recommendation has attracted increasing attention from industry and academia recently. However, most existing methods do not exploit the interest invariance between domains, which would yield sub-optimal solutions. In this paper, we propose a cross-domain recommendation method: Self-supervised Interest Transfer Network (SITN), which can effectively transfer invariant knowledge between domains via prototypical contrastive learning. Specifically, we perform two levels of cross-domain contrastive learning: 1) instance-to-instance contrastive learning, 2) instance-to-cluster contrastive learning. Not only that, we also take into account users' multi-granularity and multi-view interests. With this paradigm, SITN can explicitly learn the invariant knowledge of interest clusters between domains and accurately capture users' intents and preferences. We conducted extensive experiments on a public dataset and a large-scale industrial dataset collected from one of the world's leading e-commerce corporations. The experimental results indicate that SITN achieves significant improvements over state-of-the-art recommendation methods. Additionally, SITN has been deployed on a micro-video recommendation platform, and the online A/B testing results further demonstrate its practical value. Supplement is available at: https://github.com/fanqieCoffee/SITN-Supplement",
    "checked": true,
    "id": "09aedbfbc314006bac8e7cead1b838439705f449",
    "semantic_title": "self-supervised interest transfer network via prototypical contrastive learning for recommendation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25585": {
    "title": "Opinion Optimization in Directed Social Networks",
    "volume": "main",
    "abstract": "Shifting social opinions has far-reaching implications in various aspects, such as public health campaigns, product marketing, and political candidates. In this paper, we study a problem of opinion optimization based on the popular Friedkin-Johnsen (FJ) model for opinion dynamics in an unweighted directed social network with n nodes and m edges. In the FJ model, the internal opinion of every node lies in the closed interval [0, 1], with 0 and 1 being polar opposites of opinions about a certain issue. Concretely, we focus on the problem of selecting a small number of k<",
    "checked": true,
    "id": "a4984e10597e3c8a77f1675a48eb7dc6d5252e72",
    "semantic_title": "opinion optimization in directed social networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25586": {
    "title": "Self-Supervised Continual Graph Learning in Adaptive Riemannian Spaces",
    "volume": "main",
    "abstract": "Continual graph learning routinely finds its role in a variety of real-world applications where the graph data with different tasks come sequentially. Despite the success of prior works, it still faces great challenges. On the one hand, existing methods work with the zero-curvature Euclidean space, and largely ignore the fact that curvature varies over the com- ing graph sequence. On the other hand, continual learners in the literature rely on abundant labels, but labeling graph in practice is particularly hard especially for the continuously emerging graphs on-the-fly. To address the aforementioned challenges, we propose to explore a challenging yet practical problem, the self-supervised continual graph learning in adaptive Riemannian spaces. In this paper, we propose a novel self-supervised Riemannian Graph Continual Learner (RieGrace). In RieGrace, we first design an Adaptive Riemannian GCN (AdaRGCN), a unified GCN coupled with a neural curvature adapter, so that Riemannian space is shaped by the learnt curvature adaptive to each graph. Then, we present a Label-free Lorentz Distillation approach, in which we create teacher-student AdaRGCN for the graph sequence. The student successively performs intra-distillation from itself and inter-distillation from the teacher so as to consolidate knowledge without catastrophic forgetting. In particular, we propose a theoretically grounded Generalized Lorentz Projection for the contrastive distillation in Riemannian space. Extensive experiments on the benchmark datasets show the superiority of RieGrace, and additionally, we investigate on how curvature changes over the graph sequence",
    "checked": true,
    "id": "6796467c5662e07f5f7bbd685f6a9c59cfbf99ec",
    "semantic_title": "self-supervised continual graph learning in adaptive riemannian spaces",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25587": {
    "title": "Self-Organization Preserved Graph Structure Learning with Principle of Relevant Information",
    "volume": "main",
    "abstract": "Most Graph Neural Networks follow the message-passing paradigm, assuming the observed structure depicts the ground-truth node relationships. However, this fundamental assumption cannot always be satisfied, as real-world graphs are always incomplete, noisy, or redundant. How to reveal the inherent graph structure in a unified way remains under-explored. We proposed PRI-GSL, a Graph Structure Learning framework guided by the Principle of Relevant Information, providing a simple and unified framework for identifying the self-organization and revealing the hidden structure. PRI-GSL learns a structure that contains the most relevant yet least redundant information quantified by von Neumann entropy and Quantum Jensen Shannon divergence. PRI-GSL incorporates the evolution of quantum continuous walk with graph wavelets to encode node structural roles, showing in which way the nodes interplay and self-organize with the graph structure. Extensive experiments demonstrate the superior effectiveness and robustness of PRI-GSL",
    "checked": true,
    "id": "b501be9cac69f1fa58ef2887017d0dd94d233b68",
    "semantic_title": "self-organization preserved graph structure learning with principle of relevant information",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25588": {
    "title": "Efficient Embeddings of Logical Variables for Query Answering over Incomplete Knowledge Graphs",
    "volume": "main",
    "abstract": "The problem of answering complex First-order Logic queries over incomplete knowledge graphs is receiving growing attention in the literature. A promising recent approach to this problem has been to exploit neural link predictors, which can be effective in identifying individual missing triples in the incomplete graph, in order to efficiently answer complex queries. A crucial advantage of this approach over other methods is that it does not require example answers to complex queries for training, as it relies only on the availability of a trained link predictor for the knowledge graph at hand. This approach, however, can be computationally expensive during inference, and cannot deal with queries involving negation. In this paper, we propose a novel approach that addresses all of these limitations. Experiments on established benchmark datasets demonstrate that our approach offers superior performance while significantly reducing inference times",
    "checked": true,
    "id": "7d275bb20b36926e039fb604f9fb93340a70e63c",
    "semantic_title": "efficient embeddings of logical variables for query answering over incomplete knowledge graphs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25589": {
    "title": "Human-Instructed Deep Hierarchical Generative Learning for Automated Urban Planning",
    "volume": "main",
    "abstract": "The essential task of urban planning is to generate the optimal land-use configuration of a target area. However, traditional urban planning is time-consuming and labor-intensive. Deep generative learning gives us hope that we can automate this planning process and come up with the ideal urban plans. While remarkable achievements have been obtained, they have exhibited limitations in lacking awareness of: 1) the hierarchical dependencies between functional zones and spatial grids; 2) the peer dependencies among functional zones; and 3) human regulations to ensure the usability of generated configurations. To address these limitations, we develop a novel human-instructed deep hierarchical generative model. We rethink the urban planning generative task from a unique functionality perspective, where we summarize planning requirements into different functionality projections for better urban plan generation. To this end, we develop a three-stage generation process from a target area to zones to grids. The first stage is to label the grids of a target area with latent functionalities to discover functional zones. The second stage is to perceive the planning requirements to form urban functionality projections. We propose a novel module: functionalizer to project the embedding of human instructions and geospatial contexts to the zone-level plan to obtain such projections. Each projection includes the information of land-use portfolios and the structural dependencies across spatial grids in terms of a specific urban function. The third stage is to leverage multi-attentions to model the zone-zone peer dependencies of the functionality projections to generate grid-level land-use configurations. Finally, we present extensive experiments to demonstrate the effectiveness of our framework",
    "checked": true,
    "id": "3480471572fd3908e72cf827b3e4ca532f286fc5",
    "semantic_title": "human-instructed deep hierarchical generative learning for automated urban planning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25590": {
    "title": "Easy Begun Is Half Done: Spatial-Temporal Graph Modeling with ST-Curriculum Dropout",
    "volume": "main",
    "abstract": "Spatial-temporal (ST) graph modeling, such as traffic speed forecasting and taxi demand prediction, is an important task in deep learning area. However, for the nodes in the graph, their ST patterns can vary greatly in difficulties for modeling, owning to the heterogeneous nature of ST data. We argue that unveiling the nodes to the model in a meaningful order, from easy to complex, can provide performance improvements over traditional training procedure. The idea has its root in Curriculum Learning, which suggests in the early stage of training models can be sensitive to noise and difficult samples. In this paper, we propose ST-Curriculum Dropout, a novel and easy-to-implement strategy for spatial-temporal graph modeling. Specifically, we evaluate the learning difficulty of each node in high-level feature space and drop those difficult ones out to ensure the model only needs to handle fundamental ST relations at the beginning, before gradually moving to hard ones. Our strategy can be applied to any canonical deep learning architecture without extra trainable parameters, and extensive experiments on a wide range of datasets are conducted to illustrate that, by controlling the difficulty level of ST relations as the training progresses, the model is able to capture better representation of the data and thus yields better generalization",
    "checked": true,
    "id": "e07be0014393a4a8c9521e49bc640074c78b6333",
    "semantic_title": "easy begun is half done: spatial-temporal graph modeling with st-curriculum dropout",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25591": {
    "title": "Cross-Domain Graph Anomaly Detection via Anomaly-Aware Contrastive Alignment",
    "volume": "main",
    "abstract": "Cross-domain graph anomaly detection (CD-GAD) describes the problem of detecting anomalous nodes in an unlabelled target graph using auxiliary, related source graphs with labelled anomalous and normal nodes. Although it presents a promising approach to address the notoriously high false positive issue in anomaly detection, little work has been done in this line of research. There are numerous domain adaptation methods in the literature, but it is difficult to adapt them for GAD due to the unknown distributions of the anomalies and the complex node relations embedded in graph data. To this end, we introduce a novel domain adaptation approach, namely Anomaly-aware Contrastive alignmenT (ACT), for GAD. ACT is designed to jointly optimise: (i) unsupervised contrastive learning of normal representations of nodes in the target graph, and (ii) anomaly-aware one-class alignment that aligns these contrastive node representations and the representations of labelled normal nodes in the source graph, while enforcing significant deviation of the representations of the normal nodes from the labelled anomalous nodes in the source graph. In doing so, ACT effectively transfers anomaly-informed knowledge from the source graph to learn the complex node relations of the normal class for GAD on the target graph without any specification of the anomaly distributions. Extensive experiments on eight CD-GAD settings demonstrate that our approach ACT achieves substantially improved detection performance over 10 state-of-the-art GAD methods. Code is available at https://github.com/QZ-WANG/ACT",
    "checked": true,
    "id": "c310a633d4fe1b3be759588656c0763cacc4071e",
    "semantic_title": "cross-domain graph anomaly detection via anomaly-aware contrastive alignment",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25592": {
    "title": "WSiP: Wave Superposition Inspired Pooling for Dynamic Interactions-Aware Trajectory Prediction",
    "volume": "main",
    "abstract": "Predicting motions of surrounding vehicles is critically important to help autonomous driving systems plan a safe path and avoid collisions. Although recent social pooling based LSTM models have achieved significant performance gains by considering the motion interactions between vehicles close to each other, vehicle trajectory prediction still remains as a challenging research issue due to the dynamic and high-order interactions in the real complex driving scenarios. To this end, we propose a wave superposition inspired social pooling (Wave-pooling for short) method for dynamically aggregating the high-order interactions from both local and global neighbor vehicles. Through modeling each vehicle as a wave with the amplitude and phase, Wave-pooling can more effectively represent the dynamic motion states of vehicles and capture their high-order dynamic interactions by wave superposition. By integrating Wave-pooling, an encoder-decoder based learning framework named WSiP is also proposed. Extensive experiments conducted on two public highway datasets NGSIM and highD verify the effectiveness of WSiP by comparison with current state-of-the-art baselines. More importantly, the result of WSiP is more interpretable as the interaction strength between vehicles can be intuitively reflected by their phase difference. The code of the work is publicly available at https://github.com/Chopin0123/WSiP",
    "checked": true,
    "id": "ded88b250761f197a5a8b7f225c6f914cc6b49fd",
    "semantic_title": "wsip: wave superposition inspired pooling for dynamic interactions-aware trajectory prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25593": {
    "title": "Beyond Graph Convolutional Network: An Interpretable Regularizer-Centered Optimization Framework",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCNs) have been attracting widespread attentions due to their encouraging performance and powerful generalizations. However, few work provide a general view to interpret various GCNs and guide GCNs' designs. In this paper, by revisiting the original GCN, we induce an interpretable regularizer-centerd optimization framework, in which by building appropriate regularizers we can interpret most GCNs, such as APPNP, JKNet, DAGNN, and GNN-LF/HF. Further, under the proposed framework, we devise a dual-regularizer graph convolutional network (dubbed tsGCN) to capture topological and semantic structures from graph data. Since the derived learning rule for tsGCN contains an inverse of a large matrix and thus is time-consuming, we leverage the Woodbury matrix identity and low-rank approximation tricks to successfully decrease the high computational complexity of computing infinite-order graph convolutions. Extensive experiments on eight public datasets demonstrate that tsGCN achieves superior performance against quite a few state-of-the-art competitors w.r.t. classification tasks",
    "checked": true,
    "id": "4239fce31468d3188ce9baed53280051da19c494",
    "semantic_title": "beyond graph convolutional network: an interpretable regularizer-centered optimization framework",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25594": {
    "title": "Augmenting Affective Dependency Graph via Iterative Incongruity Graph Learning for Sarcasm Detection",
    "volume": "main",
    "abstract": "Recently, progress has been made towards improving automatic sarcasm detection in computer science. Among existing models, manually constructing static graphs for texts and then using graph neural networks (GNNs) is one of the most effective approaches for drawing long-range incongruity patterns. However, the manually constructed graph structure might be prone to errors (e.g., noisy or incomplete) and not optimal for the sarcasm detection task. Errors produced during the graph construction step cannot be remedied and may accrue to the following stages, resulting in poor performance. To surmount the above limitations, we explore a novel Iterative Augmenting Affective Graph and Dependency Graph (IAAD) framework to jointly and iteratively learn the incongruity graph structure. IAAD can alternatively update the incongruity graph structure and node representation until the learning graph structure is optimal for the metrics of sarcasm detection. More concretely, we begin with deriving an affective and a dependency graph for each instance, then an iterative incongruity graph learning module is employed to augment affective and dependency graphs for obtaining the optimal inconsistent semantic graph with the goal of optimizing the graph for the sarcasm detection task. Extensive experiments on three datasets demonstrate that the proposed model outperforms state-of-the-art baselines for sarcasm detection with significant margins",
    "checked": true,
    "id": "f50f7e50573876fde5e9ae714ffb95354b68a0bb",
    "semantic_title": "augmenting affective dependency graph via iterative incongruity graph learning for sarcasm detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25595": {
    "title": "Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems",
    "volume": "main",
    "abstract": "Recommender systems now consume large-scale data and play a significant role in improving user experience. Graph Neural Networks (GNNs) have emerged as one of the most effective recommender system models because they model the rich relational information. The ever-growing volume of data can make training GNNs prohibitively expensive. To address this, previous attempts propose to train the GNN models incrementally as new data blocks arrive. Feature and structure knowledge distillation techniques have been explored to allow the GNN model to train in a fast incremental fashion while alleviating the catastrophic forgetting problem. However, preserving the same amount of the historical information for all users is sub-optimal since it fails to take into account the dynamics of each user's change of preferences. For the users whose interests shift substantially, retaining too much of the old knowledge can overly constrain the model, preventing it from quickly adapting to the users' novel interests. In contrast, for users who have static preferences, model performance can benefit greatly from preserving as much of the user's long-term preferences as possible. In this work, we propose a novel training strategy that adaptively learns personalized imitation weights for each user to balance the contribution from the recent data and the amount of knowledge to be distilled from previous time periods. We demonstrate the effectiveness of learning imitation weights via a comparison on five diverse datasets for three state-of-art structure distillation based recommender systems. The performance shows consistent improvement over competitive incremental learning techniques",
    "checked": true,
    "id": "26ebeeb1b9172df34ad21f1000bb6f3c374a222e",
    "semantic_title": "structure aware incremental learning with personalized imitation weights for recommender systems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25596": {
    "title": "Online Semi-supervised Learning with Mix-Typed Streaming Features",
    "volume": "main",
    "abstract": "Online learning with feature spaces that are not fixed but can vary over time renders a seemingly flexible learning paradigm thus has drawn much attention. Unfortunately, two restrictions prohibit a ubiquitous application of this learning paradigm in practice. First, whereas prior studies mainly assume a homogenous feature type, data streams generated from real applications can be heterogeneous in which Boolean, ordinal, and continuous co-exist. Existing methods that prescribe parametric distributions such as Gaussians would not suffice to model the correlation among such mixtyped features. Second, while full supervision seems to be a default setup, providing labels to all arriving data instances over a long time span is tangibly onerous, laborious, and economically unsustainable. Alas, a semi-supervised online learner that can deal with mix-typed, varying feature spaces is still missing. To fill the gap, this paper explores a novel problem, named Online Semi-supervised Learning with Mixtyped streaming Features (OSLMF), which strives to relax the restrictions on the feature type and supervision information. Our key idea to solve the new problem is to leverage copula model to align the data instances with different feature spaces so as to make their distance measurable. A geometric structure underlying data instances is then established in an online fashion based on their distances, through which the limited labeling information is propagated, from the scarce labeled instances to their close neighbors. Experimental results are documented to evidence the viability and effectiveness of our proposed approach. Code is released in https://github.com/wudi1989/OSLMF",
    "checked": true,
    "id": "c5e68be7506eadf90435539490f6b75343eb7347",
    "semantic_title": "online semi-supervised learning with mix-typed streaming features",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25597": {
    "title": "Few-Shot Composition Learning for Image Retrieval with Prompt Tuning",
    "volume": "main",
    "abstract": "We study the problem of composition learning for image retrieval, for which we learn to retrieve target images with search queries in the form of a composition of a reference image and a modification text that describes desired modifications of the image. Existing models of composition learning for image retrieval are generally built with large-scale datasets, demanding extensive training samples, i.e., query-target pairs, as supervision, which restricts their application for the scenario of few-shot learning with only few query-target pairs available. Recently, prompt tuning with frozen pretrained language models has shown remarkable performance when the amount of training data is limited. Inspired by this, we propose a prompt tuning mechanism with the pretrained CLIP model for the task of few-shot composition learning for image retrieval. Specifically, we regard the representation of the reference image as a trainable visual prompt, prefixed to the embedding of the text sequence. One challenge is to efficiently train visual prompt with few-shot samples. To deal with this issue, we further propose a self-upervised auxiliary task via ensuring that the reference image can retrieve itself when no modification information is given from the text, which facilitates training for the visual prompt, while not requiring additional annotations for query-target pairs. Experiments on multiple benchmarks show that our proposed model can yield superior performance when trained with only few query-target pairs",
    "checked": true,
    "id": "510f41c71a8751832cc97d79a0df673a0c2b0539",
    "semantic_title": "few-shot composition learning for image retrieval with prompt tuning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25598": {
    "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
    "volume": "main",
    "abstract": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae",
    "checked": true,
    "id": "63a38cb55e0b9f91c0676efd79089debe61c7768",
    "semantic_title": "contextual masked auto-encoder for dense passage retrieval",
    "citation_count": 21
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25599": {
    "title": "Jointly Imputing Multi-View Data with Optimal Transport",
    "volume": "main",
    "abstract": "The multi-view data with incomplete information hinder the effective data analysis. Existing multi-view imputation methods that learn the mapping between complete view and completely missing view are not able to deal with the common multi-view data with missing feature information. In this paper, we propose a generative imputation model named Git with optimal transport theory to jointly impute the missing features/values, conditional on all observed values from the multi-view data. Git consists of two modules, i.e., a multi-view joint generator (MJG) and a masking energy discriminator (MED). The generator MJG incorporates a joint autoencoder with the multiple imputation rule to learn the data distribution from all observed multi-view data. The discriminator MED leverages a new masking energy divergence function to make Git differentiable for imputation enhancement. Extensive experiments on several real-world multi-view data sets demonstrate that, Git yields over 35% accuracy gain, compared to the state-of-the-art approaches",
    "checked": true,
    "id": "781fad0aaf23ca028fa35ba7ad8b3a97d349052b",
    "semantic_title": "jointly imputing multi-view data with optimal transport",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25600": {
    "title": "Knowledge Graph Embedding by Normalizing Flows",
    "volume": "main",
    "abstract": "A key to knowledge graph embedding (KGE) is to choose a proper representation space, e.g., point-wise Euclidean space and complex vector space. In this paper, we propose a unified perspective of embedding and introduce uncertainty into KGE from the view of group theory. Our model can incorporate existing models (i.e., generality), ensure the computation is tractable (i.e., efficiency) and enjoy the expressive power of complex random variables (i.e., expressiveness). The core idea is that we embed entities/relations as elements of a symmetric group, i.e., permutations of a set. Permutations of different sets can reflect different properties of embedding. And the group operation of symmetric groups is easy to compute. In specific, we show that the embedding of many existing models, point vectors, can be seen as elements of a symmetric group. To reflect uncertainty, we first embed entities/relations as permutations of a set of random variables. A permutation can transform a simple random variable into a complex random variable for greater expressiveness, called a normalizing flow. We then define scoring functions by measuring the similarity of two normalizing flows, namely NFE. We construct several instantiating models and prove that they are able to learn logical rules. Experimental results demonstrate the effectiveness of introducing uncertainty and our model. The code is available at https://github.com/changyi7231/NFE",
    "checked": true,
    "id": "2e3cdedd8c2ce304fdfe861f3c2f4c68e85c9aed",
    "semantic_title": "knowledge graph embedding by normalizing flows",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25601": {
    "title": "Temporal Knowledge Graph Reasoning with Historical Contrastive Learning",
    "volume": "main",
    "abstract": "Temporal knowledge graph, serving as an effective way to store and model dynamic relations, shows promising prospects in event forecasting. However, most temporal knowledge graph reasoning methods are highly dependent on the recurrence or periodicity of events, which brings challenges to inferring future events related to entities that lack historical interaction. In fact, the current moment is often the combined effect of a small part of historical information and those unobserved underlying factors. To this end, we propose a new event forecasting model called Contrastive Event Network (CENET), based on a novel training framework of historical contrastive learning. CENET learns both the historical and non-historical dependency to distinguish the most potential entities that can best match the given query. Simultaneously, it trains representations of queries to investigate whether the current moment depends more on historical or non-historical events by launching contrastive learning. The representations further help train a binary classifier whose output is a boolean mask to indicate related entities in the search space. During the inference process, CENET employs a mask-based strategy to generate the final results. We evaluate our proposed model on five benchmark graphs. The results demonstrate that CENET significantly outperforms all existing methods in most metrics, achieving at least 8.3% relative improvement of Hits@1 over previous state-of-the-art baselines on event-based datasets",
    "checked": true,
    "id": "22157f0bcde877743dc95715cf570d672f76520a",
    "semantic_title": "temporal knowledge graph reasoning with historical contrastive learning",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25602": {
    "title": "SCI: A Spectrum Concentrated Implicit Neural Compression for Biomedical Data",
    "volume": "main",
    "abstract": "Massive collection and explosive growth of biomedical data, demands effective compression for efficient storage, transmission and sharing. Readily available visual data compression techniques have been studied extensively but tailored for natural images/videos, and thus show limited performance on biomedical data which are of different features and larger diversity. Emerging implicit neural representation (INR) is gaining momentum and demonstrates high promise for fitting diverse visual data in target-data-specific manner, but a general compression scheme covering diverse biomedical data is so far absent. To address this issue, we firstly derive a mathematical explanation for INR's spectrum concentration property and an analytical insight on the design of INR based compressor. Further, we propose a Spectrum Concentrated Implicit neural compression (SCI) which adaptively partitions the complex biomedical data into blocks matching INR's concentrated spectrum envelop, and design a funnel shaped neural network capable of representing each block with a small number of parameters. Based on this design, we conduct compression via optimization under given budget and allocate the available parameters with high representation accuracy. The experiments show SCI's superior performance to state-of-the-art methods including commercial compressors, data-driven ones, and INR based counterparts on diverse biomedical data. The source code can be found at https://github.com/RichealYoung/ImplicitNeuralCompression.git",
    "checked": true,
    "id": "d589135b7cc3019a2170676d69b1a4c1505e775c",
    "semantic_title": "sci: a spectrum concentrated implicit neural compression for biomedical data",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25603": {
    "title": "Unsupervised Legal Evidence Retrieval via Contrastive Learning with Approximate Aggregated Positive",
    "volume": "main",
    "abstract": "Verifying the facts alleged by the prosecutors before the trial requires the judges to retrieve evidence within the massive materials accompanied. Existing Legal AI applications often assume the facts are already determined and fail to notice the difficulty of reconstructing them. To build a practical Legal AI application and free the judges from the manually searching work, we introduce the task of Legal Evidence Retrieval, which aims at automatically retrieving the precise fact-related verbal evidence within a single case. We formulate the task in a dense retrieval paradigm, and jointly learn the constrastive representations and alignments between facts and evidence. To get rid of the tedious annotations, we construct an approximated positive vector for a given fact by aggregating a set of evidence from the same case. An entropy-based denoise technique is further applied to mitigate the impact of false positive samples. We train our models on tens of thousands of unlabeled cases and evaluate them on a labeled dataset containing 919 cases and 4,336 queries. Experimental results indicate that our approach is effective and outperforms other state-of-the-art representation and retrieval models. The dataset and code are available at https://github.com/yaof20/LER",
    "checked": true,
    "id": "cc8ba45e69cd4073e43926a7abd8403236f8870b",
    "semantic_title": "unsupervised legal evidence retrieval via contrastive learning with approximate aggregated positive",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25604": {
    "title": "One-for-All: Proposal Masked Cross-Class Anomaly Detection",
    "volume": "main",
    "abstract": "One of the most challenges for anomaly detection (AD) is how to learn one unified and generalizable model to adapt to multi-class especially cross-class settings: the model is trained with normal samples from seen classes with the objective to detect anomalies from both seen and unseen classes. In this work, we propose a novel Proposal Masked Anomaly Detection (PMAD) approach for such challenging multi- and cross-class anomaly detection. The proposed PMAD can be adapted to seen and unseen classes by two key designs: MAE-based patch-level reconstruction and prototype-guided proposal masking. First, motivated by MAE (Masked AutoEncoder), we develop a patch-level reconstruction model rather than the image-level reconstruction adopted in most AD methods for this reason: the masked patches in unseen classes can be reconstructed well by using the visible patches and the adaptive reconstruction capability of MAE. Moreover, we improve MAE by ViT encoder-decoder architecture, combinational masking, and visual tokens as reconstruction objectives to make it more suitable for anomaly detection. Second, we develop a two-stage anomaly detection manner during inference. In the proposal masking stage, the prototype-guided proposal masking module is utilized to generate proposals for suspicious anomalies as much as possible, then masked patches can be generated from the proposal regions. By masking most likely anomalous patches, the \"shortcut reconstruction\" issue (i.e., anomalous regions can be well reconstructed) can be mostly avoided. In the reconstruction stage, these masked patches are then reconstructed by the trained patch-level reconstruction model to determine if they are anomalies. Extensive experiments show that the proposed PMAD can outperform current state-of-the-art models significantly under the multi- and especially cross-class settings. Code will be publicly available at https://github.com/xcyao00/PMAD",
    "checked": true,
    "id": "e29734b4945611c4eeb49bd176086881e71ba25a",
    "semantic_title": "one-for-all: proposal masked cross-class anomaly detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25605": {
    "title": "Analogical Inference Enhanced Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference",
    "checked": true,
    "id": "354b651dbc3ba2af4c3785ccbecd3df0585d30b2",
    "semantic_title": "analogical inference enhanced knowledge graph embedding",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25606": {
    "title": "A Noise-Tolerant Differentiable Learning Approach for Single Occurrence Regular Expression with Interleaving",
    "volume": "main",
    "abstract": "We study the problem of learning a single occurrence regular expression with interleaving (SOIRE) from a set of text strings possibly with noise. SOIRE fully supports interleaving and covers a large portion of regular expressions used in practice. Learning SOIREs is challenging because it requires heavy computation and text strings usually contain noise in practice. Most of the previous studies only learn restricted SOIREs and are not robust on noisy data. To tackle these issues, we propose a noise-tolerant differentiable learning approach SOIREDL for SOIRE. We design a neural network to simulate SOIRE matching and theoretically prove that certain assignments of the set of parameters learnt by the neural network, called faithful encodings, are one-to-one corresponding to SOIREs for a bounded size. Based on this correspondence, we interpret the target SOIRE from an assignment of the set of parameters of the neural network by exploring the nearest faithful encodings. Experimental results show that SOIREDL outperforms the state-of-the-art approaches, especially on noisy data",
    "checked": true,
    "id": "2764258ec19ed233f2871574f31f90e587cdac01",
    "semantic_title": "a noise-tolerant differentiable learning approach for single occurrence regular expression with interleaving",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25607": {
    "title": "Learning from the Wisdom of Crowds: Exploiting Similar Sessions for Session Search",
    "volume": "main",
    "abstract": "Search engines are essential internet services, enabling users to efficiently find the information they need. Session search employs users' session logs of queries to solve complex retrieval tasks, in which users search multiple times until interested documents are found. Most existing session search models focus on the contextual information within the current search, ignoring the evidence from historical search sessions. Considering the fact that many ongoing retrieval tasks should have already been carried out by other users with a similar intent, we argue that historical sessions with similar intents can help improve the accuracy of the current search task. We propose a novel Similar Session-enhanced Ranking (SSR) model to improve the session search performance using historical sessions with similar intents. Specifically, the candidate historical sessions are matched by query-level and session-level semantic similarity, and then query-level neighbor behaviors are aggregated by a Query-guided GNN (QGNN) while session-level neighbor behaviors are aggregated using the attention mechanism. Finally, we integrate the refined and aggregated historical neighbor information into the current search session. Experimental results on AOL and Tiangong-ST datasets show that our SSR model significantly outperforms the state-of-the-art models",
    "checked": true,
    "id": "163252e0a3de4b60382e158f0447c647782b591c",
    "semantic_title": "learning from the wisdom of crowds: exploiting similar sessions for session search",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25608": {
    "title": "Next POI Recommendation with Dynamic Graph and Explicit Dependency",
    "volume": "main",
    "abstract": "Next Point-Of-Interest (POI) recommendation plays an important role in various location-based services. Its main objective is to predict the user's next interested POI based on her previous check-in information. Most existing methods directly use users' historical check-in trajectories to construct various graphs to assist sequential models to complete this task. However, as users' check-in data is extremely sparse, it is difficult to capture the potential relations between POIs by directly using these check-in data. To this end, we propose the Sequence-based Neighbour search and Prediction Model (SNPM) for next POI recommendation. In SNPM, the RotatE knowledge graph embedding and Eigenmap methods are used to extract POI relationships implied in check-in data, and build the POI similarity graph. Then, we enhance the model's generalized representations of POIs' general features by aggregating similar POIs. As the context is typically rich and valuable when making Next POI predictions, the sequence model selects which POIs to aggregate not only depends on the current state, but also needs to consider the previous POI sequence. Therefore, we construct a Sequence-based, Dynamic Neighbor Graph (SDNG) to find the similarity neighbourhood and develop a Multi-Step Dependency Prediction model (MSDP) inspired by RotatE, which explicitly leverage information from previous states. We evaluate the proposed model on two real-world datasets, and the experimental results show that the proposed method significantly outperforms existing state-of-the-art POI recommendation methods",
    "checked": true,
    "id": "b54d3c2e7d971ae03017b0ae196d4abc61c920ce",
    "semantic_title": "next poi recommendation with dynamic graph and explicit dependency",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25609": {
    "title": "Predicting Temporal Sets with Simplified Fully Connected Networks",
    "volume": "main",
    "abstract": "Given a sequence of sets, where each set contains an arbitrary number of elements, temporal sets prediction aims to predict which elements will appear in the subsequent set. Existing methods for temporal sets prediction are developed on sophisticated components (e.g., recurrent neural networks, attention or gating mechanisms, and graph neural networks), which inevitably increase the model complexity due to more trainable parameters and higher computational costs. Moreover, the involved nonlinear activation may contribute little or even degrade the performance. In this paper, we present a succinct architecture that is solely built on the Simplified Fully Connected Networks (SFCNs) for temporal sets prediction to bring both effectiveness and efficiency together. In particular, given a user's sequence of sets, we employ SFCNs to derive representations of the user by learning inter-set temporal dependencies, intra-set element relationships, and intra-embedding channel correlations. Two families of general functions are introduced to preserve the permutation-invariant property of each set and the permutation-equivariant property of elements in each set. Moreover, we design a user representations adaptive fusing module to aggregate user representations according to each element for improving the prediction performance. Experiments on four benchmarks show the superiority of our approach over the state-of-the-art under both transductive and inductive settings. We also theoretically and empirically demonstrate that our model has lower space and time complexity than baselines. Codes and datasets are available at https://github.com/yule-BUAA/SFCNTSP",
    "checked": true,
    "id": "f9c59365254eed5fa64cc9b44c29a23760dc01de",
    "semantic_title": "predicting temporal sets with simplified fully connected networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25610": {
    "title": "Learning to Count Isomorphisms with Graph Neural Networks",
    "volume": "main",
    "abstract": "Subgraph isomorphism counting is an important problem on graphs, as many graph-based tasks exploit recurring subgraph patterns. Classical methods usually boil down to a backtracking framework that needs to navigate a huge search space with prohibitive computational cost. Some recent studies resort to graph neural networks (GNNs) to learn a low-dimensional representation for both the query and input graphs, in order to predict the number of subgraph isomorphisms on the input graph. However, typical GNNs employ a node-centric message passing scheme that receives and aggregates messages on nodes, which is inadequate in complex structure matching for isomorphism counting. Moreover, on an input graph, the space of possible query graphs is enormous, and different parts of the input graph will be triggered to match different queries. Thus, expecting a fixed representation of the input graph to match diversely structured query graphs is unrealistic. In this paper, we propose a novel GNN called Count-GNN for subgraph isomorphism counting, to deal with the above challenges. At the edge level, given that an edge is an atomic unit of encoding graph structures, we propose an edge-centric message passing scheme, where messages on edges are propagated and aggregated based on the edge adjacency to preserve fine-grained structural information. At the graph level, we modulate the input graph representation conditioned on the query, so that the input graph can be adapted to each query individually to improve their matching. Finally, we conduct extensive experiments on a number of benchmark datasets to demonstrate the superior performance of Count-GNN",
    "checked": true,
    "id": "674e14227bd9aacb41a02fb195951ddddd30cd66",
    "semantic_title": "learning to count isomorphisms with graph neural networks",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25611": {
    "title": "Untargeted Attack against Federated Recommendation Systems via Poisonous Item Embeddings and the Defense",
    "volume": "main",
    "abstract": "Federated recommendation (FedRec) can train personalized recommenders without collecting user data, but the decentralized nature makes it susceptible to poisoning attacks. Most previous studies focus on the targeted attack to promote certain items, while the untargeted attack that aims to degrade the overall performance of the FedRec system remains less explored. In fact, untargeted attacks can disrupt the user experience and bring severe ﬁnancial loss to the service provider. However, existing untargeted attack methods are either inapplicable or ineffective against FedRec systems. In this paper, we delve into the untargeted attack and its defense for FedRec systems. (i) We propose ClusterAttack, a novel untargeted attack method. It uploads poisonous gradients that converge the item embeddings into several dense clusters, which make the recommender generate similar scores for these items in the same cluster and perturb the ranking order. (ii) We propose a uniformity-based defense mechanism (UNION) to protect FedRec systems from such attacks. We design a contrastive learning task that regularizes the item embeddings toward a uniform distribution. Then the server ﬁlters out these malicious gradients by estimating the uniformity of updated item embeddings. Experiments on two public datasets show that ClusterAttack can effectively degrade the performance of FedRec systems while circumventing many defense methods, and UNION can improve the resistance of the system against various untargeted attacks, including our ClusterAttack",
    "checked": true,
    "id": "77f1d71efcfc733fa9efcdb7492d1310185b0de6",
    "semantic_title": "untargeted attack against federated recommendation systems via poisonous item embeddings and the defense",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25612": {
    "title": "Practical Cross-System Shilling Attacks with Limited Access to Data",
    "volume": "main",
    "abstract": "In shilling attacks, an adversarial party injects a few fake user profiles into a Recommender System (RS) so that the target item can be promoted or demoted. Although much effort has been devoted to developing shilling attack methods, we find that existing approaches are still far from practical. In this paper, we analyze the properties a practical shilling attack method should have and propose a new concept of Cross-system Attack. With the idea of Cross-system Attack, we design a Practical Cross-system Shilling Attack (PC-Attack) framework that requires little information about the victim RS model and the target RS data for conducting attacks. PC-Attack is trained to capture graph topology knowledge from public RS data in a self-supervised manner. Then, it is fine-tuned on a small portion of target data that is easy to access to construct fake profiles. Extensive experiments have demonstrated the superiority of PC-Attack over state-of-the-art baselines. Our implementation of PC-Attack is available at https://github.com/KDEGroup/PC-Attack",
    "checked": true,
    "id": "c81c5307175bc683b096496ff44fdb6419ae18ae",
    "semantic_title": "practical cross-system shilling attacks with limited access to data",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25613": {
    "title": "Query-Aware Quantization for Maximum Inner Product Search",
    "volume": "main",
    "abstract": "Maximum Inner Product Search (MIPS) plays an essential role in many applications ranging from information retrieval, recommender systems to natural language processing. However, exhaustive MIPS is often expensive and impractical when there are a large number of candidate items. The state-of-the-art quantization method of approximated MIPS is product quantization with a score-aware loss, developed by assuming that queries are uniformly distributed in the unit sphere. However, in real-world datasets, the above assumption about queries does not necessarily hold. To this end, we propose a quantization method based on the distribution of queries combined with sampled softmax. Further, we introduce a general framework encompassing the proposed method and multiple quantization methods, and we develop an effective optimization for the proposed general framework. The proposed method is evaluated on three real-world datasets. The experimental results show that it outperforms the state-of-the-art baselines",
    "checked": true,
    "id": "8e74644b3d1b92037b478303df0860591a030fd3",
    "semantic_title": "query-aware quantization for maximum inner product search",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25614": {
    "title": "TOT：Topology-Aware Optimal Transport for Multimodal Hate Detection",
    "volume": "main",
    "abstract": "Multimodal hate detection, which aims to identify the harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gap issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce a kernel Hilbert space (RKHS), which reflects significance for eliminating the distributional modality gap. Moreover, we perceive the topology information based on aligned representations to conduct bipartite graph path reasoning. The newly achieved state-of-the-art performance on two publicly available benchmark datasets, together with further visual analysis, demonstrate the superiority of TOT in capturing implicit cross-modal alignment",
    "checked": false,
    "id": "c90d18d5cd8b1e07f8a8b6b9d14a85b03d55b4b6",
    "semantic_title": "tot: topology-aware optimal transport for multimodal hate detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25615": {
    "title": "Cross-Domain Few-Shot Graph Classification with a Reinforced Task Coordinator",
    "volume": "main",
    "abstract": "Cross-domain graph few-shot learning attempts to address the prevalent data scarcity issue in graph mining problems. However, the utilization of cross-domain data induces another intractable domain shift issue which severely degrades the generalization ability of cross-domain graph few-shot learning models. The combat with the domain shift issue is hindered due to the coarse utilization of source domains and the ignorance of accessible prompts. To address these challenges, in this paper, we design a novel Cross-domain Task Coordinator to leverage a small set of labeled target domain data as prompt tasks, then model the association and discover the relevance between meta-tasks from the source domain and the prompt tasks. Based on the discovered relevance, our model achieves adaptive task selection and enables the optimization of a graph learner using the selected fine-grained meta-tasks. Extensive experiments conducted on molecular property prediction benchmarks validate the effectiveness of our proposed method by comparing it with state-of-the-art baselines",
    "checked": true,
    "id": "12313819cdb18f6e11da1a449a087220810bdd99",
    "semantic_title": "cross-domain few-shot graph classification with a reinforced task coordinator",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25616": {
    "title": "AutoSTL: Automated Spatio-Temporal Multi-Task Learning",
    "volume": "main",
    "abstract": "Spatio-temporal prediction plays a critical role in smart city construction. Jointly modeling multiple spatio-temporal tasks can further promote an intelligent city life by integrating their inseparable relationship. However, existing studies fail to address this joint learning problem well, which generally solve tasks individually or a fixed task combination. The challenges lie in the tangled relation between different properties, the demand for supporting flexible combinations of tasks and the complex spatio-temporal dependency. To cope with the problems above, we propose an Automated Spatio-Temporal multi-task Learning (AutoSTL) method to handle multiple spatio-temporal tasks jointly. Firstly, we propose a scalable architecture consisting of advanced spatio-temporal operations to exploit the complicated dependency. Shared modules and feature fusion mechanism are incorporated to further capture the intrinsic relationship between tasks. Furthermore, our model automatically allocates the operations and fusion weight. Extensive experiments on benchmark datasets verified that our model achieves state-of-the-art performance. As we can know, AutoSTL is the first automated spatio-temporal multi-task learning method",
    "checked": true,
    "id": "64ff803d5256b6b48c74ac5229db4c7a864172a7",
    "semantic_title": "autostl: automated spatio-temporal multi-task learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25617": {
    "title": "Fair Representation Learning for Recommendation: A Mutual Information Perspective",
    "volume": "main",
    "abstract": "Recommender systems have been widely used in recent years. By exploiting historical user-item interactions, recommender systems can model personalized potential interests of users and have been widely applied to a wide range of scenarios. Despite their impressive performance, most of them may be subject to unwanted biases related to sensitive attributes (e.g., race and gender), leading to unfairness. An intuitive idea to alleviate this problem is to ensure that there is no mutual information between recommendation results and sensitive attributes. However, keeping independence conditions solely achieves fairness improvement while causing an obvious degradation of recommendation accuracy, which is not a desired result. To this end, in this paper, we re-define recommendation fairness with a novel two-fold mutual information objective. In concerned details, we define fairness as mutual information minimization between embeddings and sensitive information, and mutual information maximization between embeddings and non-sensitive information. Then, a flexible Fair Mutual Information (FairMI) framework is designed to achieve this goal. FairMI first employs a sensitive attribute encoder to capture sensitive information in the data. Then, based on results from the sensitive attribute encoder, an interest encoder is developed to generate sensitive-free embeddings, which are expected to contain rich non-sensitive information of input data. Moreover, we propose novel mutual information (upper/lower) bounds with contrastive information estimation for model optimization. Extensive experiments over two real-world datasets demonstrate the effectiveness of our proposed FairMI in reducing unfairness and improving recommendation accuracy simultaneously",
    "checked": true,
    "id": "ef720e1b70188ef7f31f73a6940229d7e7dce73b",
    "semantic_title": "fair representation learning for recommendation: a mutual information perspective",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25618": {
    "title": "Deep Graph Structural Infomax",
    "volume": "main",
    "abstract": "In the scene of self-supervised graph learning, Mutual Information (MI) was recently introduced for graph encoding to generate robust node embeddings. A successful representative is Deep Graph Infomax (DGI), which essentially operates on the space of node features but ignores topological structures, and just considers global graph summary. In this paper, we present an effective model called Deep Graph Structural Infomax (DGSI) to learn node representation. We explore to derive the structural mutual information from the perspective of Information Bottleneck (IB), which defines a trade-off between the sufficiency and minimality of representation on the condition of the topological structure preservation. Intuitively, the derived constraints formally maximize the structural mutual information both edge-wise and local neighborhood-wise. Besides, we develop a general framework that incorporates the global representational mutual information, local representational mutual information, and sufficient structural information into the node representation. Essentially, our DGSI extends DGI and could capture more fine-grained semantic information as well as beneficial structural information in a self-supervised manner, thereby improving node representation and further boosting the learning performance. Extensive experiments on different types of datasets demonstrate the effectiveness and superiority of the proposed method",
    "checked": true,
    "id": "c38be73c2e17cd2a0f808fe6ae008c1f248fa989",
    "semantic_title": "deep graph structural infomax",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25619": {
    "title": "Causal Conditional Hidden Markov Model for Multimodal Traffic Prediction",
    "volume": "main",
    "abstract": "Multimodal traffic flow can reflect the health of the transportation system, and its prediction is crucial to urban traffic management. Recent works overemphasize spatio-temporal correlations of traffic flow, ignoring the physical concepts that lead to the generation of observations and their causal relationship. Spatio-temporal correlations are considered unstable under the influence of different conditions, and spurious correlations may exist in observations. In this paper, we analyze the physical concepts affecting the generation of multimode traffic flow from the perspective of the observation generation principle and propose a Causal Conditional Hidden Markov Model (CCHMM) to predict multimodal traffic flow. In the latent variables inference stage, a posterior network disentangles the causal representations of the concepts of interest from conditional information and observations, and a causal propagation module mines their causal relationship. In the data generation stage, a prior network samples the causal latent variables from the prior distribution and feeds them into the generator to generate multimodal traffic flow. We use a mutually supervised training method for the prior and posterior to enhance the identifiability of the model. Experiments on real-world datasets show that CCHMM can effectively disentangle causal representations of concepts of interest and identify causality, and accurately predict multimodal traffic flow",
    "checked": true,
    "id": "f4472361e74e441a1539251868e077884c7b7042",
    "semantic_title": "causal conditional hidden markov model for multimodal traffic prediction",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25620": {
    "title": "ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels",
    "volume": "main",
    "abstract": "Existing works on anomaly detection (AD) rely on clean labels from human annotators that are expensive to acquire in practice. In this work, we propose a method to leverage weak/noisy labels (e.g., risk scores generated by machine rules for detecting malware) that are cheaper to obtain for anomaly detection. Specifically, we propose ADMoE, the first framework for anomaly detection algorithms to learn from noisy labels. In a nutshell, ADMoE leverages mixture-of-experts (MoE) architecture to encourage specialized and scalable learning from multiple noisy sources. It captures the similarities among noisy labels by sharing most model parameters, while encouraging specialization by building \"expert\" sub-networks. To further juice out the signals from noisy labels, ADMoE uses them as input features to facilitate expert learning. Extensive results on eight datasets (including a proprietary enterprise security dataset) demonstrate the effectiveness of ADMoE, where it brings up to 34% performance improvement over not using it. Also, it outperforms a total of 13 leading baselines with equivalent network parameters and FLOPS. Notably, ADMoE is model-agnostic to enable any neural network-based detection methods to handle noisy labels, where we showcase its results on both multiple-layer perceptron (MLP) and the leading AD method DeepSAD",
    "checked": true,
    "id": "f1305bbd54db0345533906726e3425f742312c55",
    "semantic_title": "admoe: anomaly detection with mixture-of-experts from noisy labels",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25621": {
    "title": "A Provable Framework of Learning Graph Embeddings via Summarization",
    "volume": "main",
    "abstract": "Given a large graph, can we learn its node embeddings from a smaller summary graph? What is the relationship between embeddings learned from original graphs and their summary graphs? Graph representation learning plays an important role in many graph mining applications, but learning em-beddings of large-scale graphs remains a challenge. Recent works try to alleviate it via graph summarization, which typ-ically includes the three steps: reducing the graph size by combining nodes and edges into supernodes and superedges,learning the supernode embedding on the summary graph and then restoring the embeddings of the original nodes. How-ever, the justification behind those steps is still unknown. In this work, we propose GELSUMM, a well-formulated graph embedding learning framework based on graph sum-marization, in which we show the theoretical ground of learn-ing from summary graphs and the restoration with the three well-known graph embedding approaches in a closed form.Through extensive experiments on real-world datasets, we demonstrate that our methods can learn graph embeddings with matching or better performance on downstream tasks.This work provides theoretical analysis for learning node em-beddings via summarization and helps explain and under-stand the mechanism of the existing works",
    "checked": true,
    "id": "bc4bf8864aff1d41ad76d677714ea21606b290c4",
    "semantic_title": "a provable framework of learning graph embeddings via summarization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25622": {
    "title": "GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have achieved great success in node classification tasks. However, existing GNNs naturally bias towards the majority classes with more labelled data and ignore those minority classes with relatively few labelled ones. The traditional techniques often resort over-sampling methods, but they may cause overfitting problem. More recently, some works propose to synthesize additional nodes for minority classes from the labelled nodes, however, there is no any guarantee if those generated nodes really stand for the the corresponding minority classes. In fact, improperly synthesized nodes may result in insufficient generalization of the algorithm. To resolve the problem, in this paper we seek to automatically augment the minority classes from the massive unlabelled nodes of the graph. Specifically, we propose \\textit{GraphSR}, a novel self-training strategy to augment the minority classes with significant diversity of unlabelled nodes, which is based on a Similarity-based selection module and a Reinforcement Learning(RL) selection module. The first module finds a subset of unlabelled nodes which are most similar to those labelled minority nodes, and the second one further determines the representative and reliable nodes from the subset via RL technique. Furthermore, the RL-based module can adaptively determine the sampling scale according to current training data. This strategy is general and can be easily combined with different GNNs models. Our experiments demonstrate the proposed approach outperforms the state-of-the-art baselines on various class-imbalanced datasets",
    "checked": true,
    "id": "296049ee701634e7919b8f334da285ca7c54bfba",
    "semantic_title": "graphsr: a data augmentation algorithm for imbalanced node classification",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25623": {
    "title": "Detecting Multivariate Time Series Anomalies with Zero Known Label",
    "volume": "main",
    "abstract": "Multivariate time series anomaly detection has been extensively studied under the one-class classification setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach forMultivariate Time series anomaly detection via dynamic Graph and entityaware normalizing Flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges to density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we propose to learn the mutual and dynamic relations among entities via a graph structure learning model, which helps to model the accurate distribution of multivariate time series. Moreover, taking account of distinct characteristics of the individual entities, an entity-aware normalizing flow is developed to describe each entity into a parameterized normal distribution, thereby producing fine-grained density estimation. Incorporating these two strategies, MTGFlow achieves superior anomaly detection performance. Experiments on five public datasets with seven baselines are conducted, MTGFlow outperforms the SOTA methods by up to 5.0 AUROC%",
    "checked": true,
    "id": "4ba2f8dfc1a4fccec80cf95ce3f0eeff3066f21e",
    "semantic_title": "detecting multivariate time series anomalies with zero known label",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25624": {
    "title": "GRLSTM: Trajectory Similarity Computation with Graph-Based Residual LSTM",
    "volume": "main",
    "abstract": "The computation of trajectory similarity is a crucial task in many spatial data analysis applications. However, existing methods have been designed primarily for trajectories in Euclidean space, which overlooks the fact that real-world trajectories are often generated on road networks. This paper addresses this gap by proposing a novel framework, called GRLSTM (Graph-based Residual LSTM). To jointly capture the properties of trajectories and road networks, the proposed framework incorporates knowledge graph embedding (KGE), graph neural network (GNN), and the residual network into the multi-layer LSTM (Residual-LSTM). Specifically, the framework constructs a point knowledge graph to study the multi-relation of points, as points may belong to both the trajectory and the road network. KGE is introduced to learn point embeddings and relation embeddings to build the point fusion graph, while GNN is used to capture the topology structure information of the point fusion graph. Finally, Residual-LSTM is used to learn the trajectory embeddings.To further enhance the accuracy and robustness of the final trajectory embeddings, we introduce two new neighbor-based point loss functions, namely, graph-based point loss function and trajectory-based point loss function. The GRLSTM is evaluated using two real-world trajectory datasets, and the experimental results demonstrate that GRLSTM outperforms all the state-of-the-art methods significantly",
    "checked": true,
    "id": "48402bec037fc2a5033c4dc7f90733b7e86da446",
    "semantic_title": "grlstm: trajectory similarity computation with graph-based residual lstm",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25625": {
    "title": "Heterogeneous Region Embedding with Prompt Learning",
    "volume": "main",
    "abstract": "The prevalence of region-based urban data has opened new possibilities for exploring correlations among regions to improve urban planning and smart-city solutions. Region embedding, which plays a critical role in this endeavor, faces significant challenges related to the varying nature of city data and the effectiveness of downstream applications. In this paper, we propose a novel framework, HREP (Heterogeneous Region Embedding with Prompt learning), which addresses both intra-region and inter-region correlations through two key modules: Heterogeneous Region Embedding (HRE) and prompt learning for different downstream tasks. The HRE module constructs a heterogeneous region graph based on three categories of data, capturing inter-region contexts such as human mobility and geographic neighbors, and intraregion contexts such as POI (Point-of-Interest) information. We use relation-aware graph embedding to learn region and relation embeddings of edge types, and introduce selfattention to capture global correlations among regions. Additionally, we develop an attention-based fusion module to integrate shared information among different types of correlations. To enhance the effectiveness of region embedding in downstream tasks, we incorporate prompt learning, specifically prefix-tuning, which guides the learning of downstream tasks and results in better prediction performance. Our experiment results on real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods",
    "checked": true,
    "id": "2c6f266b833aa65bc8014f8cd052b823929ca2c1",
    "semantic_title": "heterogeneous region embedding with prompt learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25626": {
    "title": "Show Me the Way! Bilevel Search for Synthesizing Programmatic Strategies",
    "volume": "main",
    "abstract": "The synthesis of programmatic strategies requires one to search in large non-differentiable spaces of computer programs. Current search algorithms use self-play approaches to guide this search. The issue with these approaches is that the guiding function often provides a weak search signal. This is because self-play functions only measure how well a program performs against other programs. Thus, while small changes to a losing program might not transform it into a winning one, such changes might represent steps in the direction of a winning program. In this paper we introduce a bilevel search algorithm that searches concurrently in the space of programs and in a space of state features. Each iteration of the search in the space of features defines a set of target features that the search in the program space attempts to achieve (i.e., features one observes while following the strategy encoded in a program). We hypothesize the combination of a self-play function and a feature-based one provides a stronger search signal for synthesis. While both functions are used to guide the search in the program space, the self-play function is used to guide the search in the feature space, to allow for the selection of target features that are more likely to lead to winning programs. We evaluated our bilevel algorithm in MicroRTS, a real-time strategy game. Our results show that the bilevel search synthesizes stronger strategies than methods that search only in the program space. Also, the strategies our method synthesizes obtained the highest winning rate in a simulated tournament with several baseline agents, including the best agents from the two latest MicroRTS competitions",
    "checked": true,
    "id": "2a9f5132f22418f4c3eec95521dc7d3fb959b4f3",
    "semantic_title": "show me the way! bilevel search for synthesizing programmatic strategies",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25627": {
    "title": "Anytime User Engagement Prediction in Information Cascades for Arbitrary Observation Periods",
    "volume": "main",
    "abstract": "Predicting user engagement -- whether a user will engage in a given information cascade -- is an important problem in the context of social media, as it is useful to online marketing and misinformation mitigation just to name a couple major applications. Based on split population multi-variate survival processes, we develop a discriminative approach that, unlike prior works, leads to a single model for predicting whether individual users of an information network will engage a given cascade for arbitrary forecast horizons and observation periods. Being probabilistic in nature, this model retains the interpretability of its generative counterpart and renders count prediction intervals in a disciplined manner. Our results indicate that our model is highly competitive, if not superior, to current approaches, when compared over varying observed cascade histories and forecast horizons",
    "checked": true,
    "id": "c538e5cfc2c908bf833f8453c72f82a63465a26b",
    "semantic_title": "anytime user engagement prediction in information cascades for arbitrary observation periods",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25628": {
    "title": "Principled Data-Driven Decision Support for Cyber-Forensic Investigations",
    "volume": "main",
    "abstract": "In the wake of a cybersecurity incident, it is crucial to promptly discover how the threat actors breached security in order to assess the impact of the incident and to develop and deploy countermeasures that can protect against further attacks. To this end, defenders can launch a cyber-forensic investigation, which discovers the techniques that the threat actors used in the incident. A fundamental challenge in such an investigation is prioritizing the investigation of particular techniques since the investigation of each technique requires time and effort, but forensic analysts cannot know which ones were actually used before investigating them. To ensure prompt discovery, it is imperative to provide decision support that can help forensic analysts with this prioritization. A recent study demonstrated that data-driven decision support, based on a dataset of prior incidents, can provide state-of-the-art prioritization. However, this data-driven approach, called DISCLOSE, is based on a heuristic that utilizes only a subset of the available information and does not approximate optimal decisions. To improve upon this heuristic, we introduce a principled approach for data-driven decision support for cyber-forensic investigations. We formulate the decision-support problem using a Markov decision process, whose states represent the states of a forensic investigation. To solve the decision problem, we propose a Monte Carlo tree search based method, which relies on a k-NN regression over prior incidents to estimate state-transition probabilities. We evaluate our proposed approach on multiple versions of the MITRE ATT&CK dataset, which is a knowledge base of adversarial techniques and tactics based on real-world cyber incidents, and demonstrate that our approach outperforms DISCLOSE in terms of techniques discovered per effort spent",
    "checked": true,
    "id": "36c3e9fce1e1eec51cea57ec5ebc339ba0e11e2c",
    "semantic_title": "principled data-driven decision support for cyber-forensic investigations",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25629": {
    "title": "BETA-CD: A Bayesian Meta-Learned Cognitive Diagnosis Framework for Personalized Learning",
    "volume": "main",
    "abstract": "Personalized learning is a promising educational approach that aims to provide high-quality personalized services for each student with minimum demands for practice data. The key to achieving that lies in the cognitive diagnosis task, which estimates the cognitive state of the student through his/her logged data of doing practice quizzes. Nevertheless, in the personalized learning scenario, existing cognitive diagnosis models suffer from the inability to (1) quickly adapt to new students using a small amount of data, and (2) measure the reliability of the diagnosis result to avoid improper services that mismatch the student's actual state. In this paper, we propose a general Bayesian mETA-learned Cognitive Diagnosis framework (BETA-CD), which addresses the two challenges by prior knowledge exploitation and model uncertainty quantification, respectively. Specifically, we firstly introduce Bayesian hierarchical modeling to associate each student's cognitive state with a shared prior distribution encoding prior knowledge and a personal posterior distribution indicating model uncertainty. Furthermore, we formulate a meta-learning objective to automatically exploit prior knowledge from historical students, and efficiently solve it with a gradient-based variational inference method. The code will be publicly available at https://github.com/AyiStar/pyat",
    "checked": true,
    "id": "44deaf38f9a4b9a0c274fbc4acc5ace2d6f06c01",
    "semantic_title": "beta-cd: a bayesian meta-learned cognitive diagnosis framework for personalized learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25630": {
    "title": "Set-to-Sequence Ranking-Based Concept-Aware Learning Path Recommendation",
    "volume": "main",
    "abstract": "With the development of the online education system, personalized education recommendation has played an essential role. In this paper, we focus on developing path recommendation systems that aim to generating and recommending an entire learning path to the given user in each session. Noticing that existing approaches fail to consider the correlations of concepts in the path, we propose a novel framework named Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation (SRC), which formulates the recommendation task under a set-to-sequence paradigm. Specifically, we first design a concept-aware encoder module which can capture the correlations among the input learning concepts. The outputs are then fed into a decoder module that sequentially generates a path through an attention mechanism that handles correlations between the learning and target concepts. Our recommendation policy is optimized by policy gradient. In addition, we also introduce an auxiliary module based on knowledge tracing to enhance the model's stability by evaluating students' learning effects on learning concepts. We conduct extensive experiments on two real-world public datasets and one industrial dataset, and the experimental results demonstrate the superiority and effectiveness of SRC. Code now is available at https://gitee.com/mindspore/models/tree/master/research/recommend/SRC",
    "checked": true,
    "id": "4a2ac894fad19b6c8bd0984ed5e205d63376cdb7",
    "semantic_title": "set-to-sequence ranking-based concept-aware learning path recommendation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25631": {
    "title": "Unsupervised Deep Embedded Fusion Representation of Single-Cell Transcriptomics",
    "volume": "main",
    "abstract": "Cell clustering is a critical step in analyzing single-cell RNA sequencing (scRNA-seq) data, which allows us to characterize the cellular heterogeneity of transcriptional profiling at the single-cell level. Single-cell deep embedded representation models have recently become popular since they can learn feature representation and clustering simultaneously. However, the model still suffers from a variety of significant challenges, including the massive amount of data, pervasive dropout events, and complicated noise patterns in transcriptional profiling. Here, we propose a Single-Cell Deep Embedding Fusion Representation (scDEFR) model, which develop a deep embedded fusion representation to learn fused heterogeneous latent embedding that contains both the transcriptome gene-level information and the cell topology information. We first fuse them layer by layer to obtain compressed representations of intercellular relationships and transcriptome information. After that, the zero-inflated negative binomial model (ZINB)-based decoder is proposed to capture the global probabilistic structure of the data and reconstruct the final gene expression information and cell graph. Finally, by simultaneously integrating the clustering loss, crossentropy loss, ZINB loss, and the cell graph reconstruction loss, scDEFR can optimize clustering performance and learn the latent representation in fused information under a joint mutual supervised strategy. We conducted extensive and comprehensive experiments on 15 single-cell RNA-seq datasets from different sequencing platforms to demonstrate the superiority of scDEFR over a variety of state-of-the-art methods",
    "checked": true,
    "id": "79bbb02ade124c82c663b5f1ee9ef9fdce0d4f6d",
    "semantic_title": "unsupervised deep embedded fusion representation of single-cell transcriptomics",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25632": {
    "title": "Constrained Submodular Optimization for Vaccine Design",
    "volume": "main",
    "abstract": "Advances in machine learning have enabled the prediction of immune system responses to prophylactic and therapeutic vaccines. However, the engineering task of designing vaccines remains a challenge. In particular, the genetic variability of the human immune system makes it difficult to design peptide vaccines that provide widespread immunity in vaccinated populations. We introduce a framework for evaluating and designing peptide vaccines that uses probabilistic machine learning models, and demonstrate its ability to produce designs for a SARS-CoV-2 vaccine that outperform previous designs. We provide a theoretical analysis of the approximability, scalability, and complexity of our framework",
    "checked": true,
    "id": "9ded91f056e1d073029b662d69b1847e1cc1060e",
    "semantic_title": "constrained submodular optimization for vaccine design",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25633": {
    "title": "Flow-Based Robust Watermarking with Invertible Noise Layer for Black-Box Distortions",
    "volume": "main",
    "abstract": "Deep learning-based digital watermarking frameworks have been widely studied recently. Most existing methods adopt an ``encoder-noise layer-decoder''-based architecture where the embedding and extraction processes are accomplished separately by the encoder and the decoder. However, one potential drawback of such a framework is that the encoder and the decoder may not be well coupled, resulting in the fact that the encoder may embed some redundant features into the host image thus influencing the invisibility and robustness of the whole algorithm. To address this limitation, this paper proposes a flow-based robust watermarking framework. The basic component of such framework is an invertible up-down-sampling neural block that can realize the embedding and extraction simultaneously. As a consequence, the encoded feature could keep high consistency with the feature that the decoder needed, which effectively avoids the embedding of redundant features. In addition, to ensure the robustness of black-box distortion, an invertible noise layer (INL) is designed to simulate the distortion and is served as a noise layer in the training stage. Benefiting from its reversibility, INL is also applied as a preprocessing before extraction to eliminate the distortion, which further improves the robustness of the algorithm. Extensive experiments demonstrate the superiority of the proposed framework in terms of visual quality and robustness. Compared with the state-of-the-art architecture, the visual quality (measured by PSNR) of the proposed framework improves by 2dB and the extraction accuracy after JPEG compression (QF=50) improves by more than 4%. Besides, the robustness against black-box distortions can be greatly achieved with more than 95% extraction accuracy",
    "checked": true,
    "id": "9be90c6279e4133c7b2891bb5513b417da490857",
    "semantic_title": "flow-based robust watermarking with invertible noise layer for black-box distortions",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25634": {
    "title": "Identifying and Eliminating Majority Illusion in Social Networks",
    "volume": "main",
    "abstract": "Majority illusion occurs in a social network when the majority of the network vertices belong to a certain type but the majority of each vertex's neighbours belong to a different type, therefore creating the wrong perception, i.e., the illusion, that the majority type is different from the actual one. From a system engineering point of view, this motivates the search for algorithms to detect and, where possible, correct this undesirable phenomenon. In this paper we initiate the computational study of majority illusion in social networks, providing NP-hardness and parametrised complexity results for its occurrence and elimination",
    "checked": true,
    "id": "2195aff9d693a39c92c469cd64aef681b6716ad7",
    "semantic_title": "identifying and eliminating majority illusion in social networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25635": {
    "title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling",
    "volume": "main",
    "abstract": "Following the success of the transformer architecture in the natural language domain, transformer-like architectures have been widely applied to the domain of symbolic music recently. Symbolic music and text, however, are two different modalities. Symbolic music contains multiple attributes, both absolute attributes (e.g., pitch) and relative attributes (e.g., pitch interval). These relative attributes shape human perception of musical motifs. These important relative attributes, however, are mostly ignored in existing symbolic music modelling methods with the main reason being the lack of a musically-meaningful embedding space where both the absolute and relative embeddings of the symbolic music tokens can be efficiently represented. In this paper, we propose the Fundamental Music Embedding (FME) for symbolic music based on a bias-adjusted sinusoidal encoding within which both the absolute and the relative attributes can be embedded and the fundamental musical properties (e.g., translational invariance) are explicitly preserved. Taking advantage of the proposed FME, we further propose a novel attention mechanism based on the relative index, pitch and onset embeddings (RIPO attention) such that the musical domain knowledge can be fully utilized for symbolic music modelling. Experiment results show that our proposed model: RIPO transformer which utilizes FME and RIPO attention outperforms the state-of-the-art transformers (i.e., music transformer, linear transformer) in a melody completion task. Moreover, using the RIPO transformer in a downstream music generation task, we notice that the notorious degeneration phenomenon no longer exists and the music generated by the RIPO transformer outperforms the music generated by state-of-the-art transformer models in both subjective and objective evaluations. The code of the proposed method is available online: github.com/guozixunnicolas/FundamentalMusicEmbedding",
    "checked": true,
    "id": "aec9ae3c0e5a5784e171dbb7aa58c209de89e404",
    "semantic_title": "a domain-knowledge-inspired music embedding space and a novel attention mechanism for symbolic music modeling",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25636": {
    "title": "MSDC: Exploiting Multi-State Power Consumption in Non-intrusive Load Monitoring Based on a Dual-CNN Model",
    "volume": "main",
    "abstract": "Non-intrusive load monitoring (NILM) aims to decompose aggregated electrical usage signal into appliance-specific power consumption and it amounts to a classical example of blind source separation tasks. Leveraging recent progress on deep learning techniques, we design a new neural NILM model {\\em Multi-State Dual CNN} (MSDC). Different from previous models, MSDC explicitly extracts information about the appliance's multiple states and state transitions, which in turn regulates the prediction of signals for appliances. More specifically, we employ a dual-CNN architecture: one CNN for outputting state distributions and the other for predicting the power of each state. A new technique is invented that utilizes conditional random fields (CRF) to capture state transitions. Experiments on two real-world datasets REDD and UK-DALE demonstrate that our model significantly outperform state-of-the-art models while having good generalization capacity, achieving 6%-10% MAE gain and 33%-51% SAE gain to unseen appliances",
    "checked": true,
    "id": "7c238db86fc92dbd85544f4c89c3c82cf008d06e",
    "semantic_title": "msdc: exploiting multi-state power consumption in non-intrusive load monitoring based on a dual-cnn model",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25637": {
    "title": "Integrating Reward Maximization and Population Estimation: Sequential Decision-Making for Internal Revenue Service Audit Selection",
    "volume": "main",
    "abstract": "We introduce a new setting, optimize-and-estimate structured bandits. Here, a policy must select a batch of arms, each characterized by its own context, that would allow it to both maximize reward and maintain an accurate (ideally unbiased) population estimate of the reward. This setting is inherent to many public and private sector applications and often requires handling delayed feedback, small data, and distribution shifts. We demonstrate its importance on real data from the United States Internal Revenue Service (IRS). The IRS performs yearly audits of the tax base. Two of its most important objectives are to identify suspected misreporting and to estimate the \"tax gap\" -- the global difference between the amount paid and true amount owed. Based on a unique collaboration with the IRS, we cast these two processes as a unified optimize-and-estimate structured bandit. We analyze optimize-and-estimate approaches to the IRS problem and propose a novel mechanism for unbiased population estimation that achieves rewards comparable to baseline approaches. This approach has the potential to improve audit efficacy, while maintaining policy-relevant estimates of the tax gap. This has important social consequences given that the current tax gap is estimated at nearly half a trillion dollars. We suggest that this problem setting is fertile ground for further research and we highlight its interesting challenges. The results of this and related research are currently being incorporated into the continual improvement of the IRS audit selection methods",
    "checked": true,
    "id": "e17ba7c1a91e51070a308039ef53f0deec2cee2a",
    "semantic_title": "integrating reward maximization and population estimation: sequential decision-making for internal revenue service audit selection",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25638": {
    "title": "MGTCF: Multi-Generator Tropical Cyclone Forecasting with Heterogeneous Meteorological Data",
    "volume": "main",
    "abstract": "Accurate forecasting of tropical cyclone (TC) plays a critical role in the prevention and defense of TC disasters. We must explore a more accurate method for TC prediction. Deep learning methods are increasingly being implemented to make TC prediction more accurate. However, most existing methods lack a generic framework for adapting heterogeneous meteorological data and do not focus on the importance of the environment. Therefore, we propose a Multi-Generator Tropical Cyclone Forecasting model (MGTCF), a generic, extensible, multi-modal TC prediction model with the key modules of Generator Chooser Network (GC-Net) and Environment Net (Env-Net). The proposed method can utilize heterogeneous meteorologic data efficiently and mine environmental factors. In addition, the Multi-generator with Generator Chooser Net is proposed to tackle the drawbacks of single-generator TC prediction methods: the prediction of undesired out-of-distribution samples and the problems stemming from insufficient learning ability. To prove the effectiveness of MGTCF, we conduct extensive experiments on the China Meteorological Administration Tropical Cyclone Best Track Dataset. MGTCF obtains better performance compared with other deep learning methods and outperforms the official prediction method of the China Central Meteorological Observatory in most indexes",
    "checked": true,
    "id": "8651b010f8615d6dacd089f510b222e434c46f28",
    "semantic_title": "mgtcf: multi-generator tropical cyclone forecasting with heterogeneous meteorological data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25639": {
    "title": "MDM: Molecular Diffusion Model for 3D Molecule Generation",
    "volume": "main",
    "abstract": "Molecule generation, especially generating 3D molecular geometries from scratch (i.e., 3D de novo generation), has become a fundamental task in drug design. Existing diffusion based 3D molecule generation methods could suffer from unsatisfactory performances, especially when generating large molecules. At the same time, the generated molecules lack enough diversity. This paper proposes a novel diffusion model to address those two challenges. First, interatomic relations are not included in molecules' 3D point cloud representations. Thus, it is difficult for existing generative models to capture the potential interatomic forces and abundant local constraints. To tackle this challenge, we propose to augment the potential interatomic forces and further involve dual equivariant encoders to encode interatomic forces of different strengths. Second, existing diffusion-based models essentially shift elements in geometry along the gradient of data density. Such a process lacks enough exploration in the intermediate steps of the Langevin dynamics. To address this issue, we introduce a distributional controlling variable in each diffusion/reverse step to enforce thorough explorations and further improve generation diversity. Extensive experiments on multiple benchmarks demonstrate that the proposed model significantly outperforms existing methods for both unconditional and conditional generation tasks. We also conduct case studies to help understand the physicochemical properties of the generated molecules. The codes are available at https://github.com/tencent-ailab/MDM",
    "checked": true,
    "id": "161858efd41df44a826feb52c27f29eaf8bce80d",
    "semantic_title": "mdm: molecular diffusion model for 3d molecule generation",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25640": {
    "title": "Learning Chemical Rules of Retrosynthesis with Pre-training",
    "volume": "main",
    "abstract": "Retrosynthesis aided by artificial intelligence has been a very active and bourgeoning area of research, for its critical role in drug discovery as well as material science. Three categories of solutions, i.e., template-based, template-free, and semi-template methods, constitute mainstream solutions to this problem. In this paper, we focus on template-free methods which are known to be less bothered by the template generalization issue and the atom mapping challenge. Among several remaining problems regarding template-free methods, failing to conform to chemical rules is pronounced. To address the issue, we seek for a pre-training solution to empower the pre-trained model with chemical rules encoded. Concretely, we enforce the atom conservation rule via a molecule reconstruction pre-training task, and the reaction rule that dictates reaction centers via a reaction type guided contrastive pre-training task. In our empirical evaluation, the proposed pre-training solution substantially improves the single-step retrosynthesis accuracies in three downstream datasets",
    "checked": true,
    "id": "7cbe75ef6748e461d18f964d99ea7f4e8f8d539a",
    "semantic_title": "learning chemical rules of retrosynthesis with pre-training",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25641": {
    "title": "Online Symbolic Regression with Informative Query",
    "volume": "main",
    "abstract": "Symbolic regression, the task of extracting mathematical expressions from the observed data, plays a crucial role in scientific discovery. Despite the promising performance of existing methods, most of them conduct symbolic regression in an offline setting. That is, they treat the observed data points as given ones that are simply sampled from uniform distributions without exploring the expressive potential of data. However, for real-world scientific problems, the data used for symbolic regression are usually actively obtained by doing experiments, which is an online setting. Thus, how to obtain informative data that can facilitate the symbolic regression process is an important problem that remains challenging. In this paper, we propose QUOSR, a query-based framework for online symbolic regression that can automatically obtain informative data in an iterative manner. Specifically, at each step, QUOSR receives historical data points, generates new x, and then queries the symbolic expression to get the corresponding y, where the (x, y) serves as new data points. This process repeats until the maximum number of query steps is reached. To make the generated data points informative, we implement the framework with a neural network and train it by maximizing the mutual information between generated data points and the target expression. Through comprehensive experiments, we show that QUOSR can facilitate modern symbolic regression methods by generating informative data",
    "checked": true,
    "id": "b3eee07ad5e785d9d6124a08cce985fe034c83bf",
    "semantic_title": "online symbolic regression with informative query",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25642": {
    "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
    "volume": "main",
    "abstract": "Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages",
    "checked": true,
    "id": "453a8fac3be9282be53908f0735160d0d21e0f48",
    "semantic_title": "repair is nearly generation: multilingual program repair with llms",
    "citation_count": 23
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25643": {
    "title": "Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis",
    "volume": "main",
    "abstract": "Routine clinical visits of a patient produce not only image data, but also non-image data containing clinical information regarding the patient, i.e., medical data is multi-modal in nature. Such heterogeneous modalities offer different and complementary perspectives on the same patient, resulting in more accurate clinical decisions when they are properly combined. However, despite its significance, how to effectively fuse the multi-modal medical data into a unified framework has received relatively little attention. In this paper, we propose an effective graph-based framework called HetMed (Heterogeneous Graph Learning for Multi-modal Medical Data Analysis) for fusing the multi-modal medical data. Specifically, we construct a multiplex network that incorporates multiple types of non-image features of patients to capture the complex relationship between patients in a systematic way, which leads to more accurate clinical decisions. Extensive experiments on various real-world datasets demonstrate the superiority and practicality of HetMed. The source code for HetMed is available at https://github.com/Sein-Kim/Multimodal-Medical",
    "checked": true,
    "id": "ef7ef2cb643acb5bba3e8c249d0663ee1a4d8108",
    "semantic_title": "heterogeneous graph learning for multi-modal medical data analysis",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25644": {
    "title": "Rolling Horizon Based Temporal Decomposition for the Offline Pickup and Delivery Problem with Time Windows",
    "volume": "main",
    "abstract": "The offline pickup and delivery problem with time windows (PDPTW) is a classical combinatorial optimization problem in the transportation community, which has proven to be very challenging computationally. Due to the complexity of the problem, practical problem instances can be solved only via heuristics, which trade-off solution quality for computational tractability. Among the various heuristics, a common strategy is problem decomposition, that is, the reduction of a large-scale problem into a collection of smaller sub-problems, with spatial and temporal decompositions being two natural approaches. While spatial decomposition has been successful in certain settings, effective temporal decomposition has been challenging due to the difficulty of stitching together the sub-problem solutions across the decomposition boundaries. In this work, we introduce a novel temporal decomposition scheme for solving a class of PDPTWs that have narrow time windows, for which it is able to provide both fast and high-quality solutions. We utilize techniques that have been popularized recently in the context of online dial-a-ride problems along with the general idea of rolling horizon optimization. To the best of our knowledge, this is the first attempt to solve offline PDPTWs using such an approach. To show the performance and scalability of our framework, we use the optimization of paratransit services as a motivating example. Due to the lack of benchmark solvers similar to ours (i.e., temporal decomposition with an online solver), we compare our results with an offline heuristic algorithm using Google OR-Tools. In smaller problem instances (with an average of 129 requests per instance), the baseline approach is as competitive as our framework. However, in larger problem instances (approximately 2,500 requests per instance), our framework is more scalable and can provide good solutions to problem instances of varying degrees of difficulty, while the baseline algorithm often fails to find a feasible solution within comparable compute times",
    "checked": true,
    "id": "02aeb1c12b95530cd5381d13e0bc9edb121dba4b",
    "semantic_title": "rolling horizon based temporal decomposition for the offline pickup and delivery problem with time windows",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25645": {
    "title": "GRIP: Graph Representation of Immune Repertoire Using Graph Neural Network and Transformer",
    "volume": "main",
    "abstract": "The immune repertoire is a collection of immune recep-tors that has emerged as an important biomarker for both diagnostic and therapeutic of cancer patients. In terms of deep learning, analyzing immune repertoire is a challeng-ing multiple-instance learning problem in which the im-mune repertoire of an individual is a bag, and the immune receptor is an instance. Although several deep learning methods for immune repertoire analysis are introduced, they consider the immune repertoire as a set-like struc-ture that doesn't take account of the nature of the im-mune response. When the immune response occurs, mu-tations are introduced to the immune receptor sequence sequentially to optimize the immune response against the pathogens that enter our body. As a result, immune receptors for the specific pathogen have the lineage of evolution; thus, immune repertoire is better represented as a graph-like structure. In this work, we present our novel method graph representation of immune repertoire (GRIP), which analyzes the immune repertoire as a hier-archical graph structure and utilize the collection of graph neural network followed by graph pooling and transformer to efficiently represents the immune reper-toire as an embedding vector. We show that GRIP predict the survival probability of cancer patients better than the set-based methods and graph-based structure is critical for performance. Also, GRIP provides interpretable re-sults, which prove that GRIP adequately use the progno-sis-related immune receptor and give further possibility to use the GRIP as the novel biomarker searching tool",
    "checked": true,
    "id": "a15c413c629113dde457ffdf10d9a55b76e35a43",
    "semantic_title": "grip: graph representation of immune repertoire using graph neural network and transformer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25646": {
    "title": "LagNet: Deep Lagrangian Mechanics for Plug-and-Play Molecular Representation Learning",
    "volume": "main",
    "abstract": "Molecular representation learning is a fundamental problem in the field of drug discovery and molecular science. Whereas incorporating molecular 3D information in the representations of molecule seems beneficial, which is related to computational chemistry with the basic task of predicting stable 3D structures (conformations) of molecules. Existing machine learning methods either rely on 1D and 2D molecular properties or simulate molecular force field to use additional 3D structure information via Hamiltonian network. The former has the disadvantage of ignoring important 3D structure features, while the latter has the disadvantage that existing Hamiltonian neural network must satisfy the \"canonial\" constraint, which is difficult to be obeyed in many cases. In this paper, we propose a novel plug-and-play architecture LagNet by simulating molecular force field only with parameterized position coordinates, which implements Lagrangian mechanics to learn molecular representation by preserving 3D conformation without obeying any additional restrictions. LagNet is designed to generate known conformations and generalize for unknown ones from molecular SMILES. Implicit positions in LagNet are learned iteratively using discrete-time Lagrangian equations. Experimental results show that LagNet can well learn 3D molecular structure features, and outperforms previous state-of-the-art baselines related molecular representation by a significant margin",
    "checked": true,
    "id": "a422712a543a43b3c64be2505aa8435333270b1d",
    "semantic_title": "lagnet: deep lagrangian mechanics for plug-and-play molecular representation learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25647": {
    "title": "Steganography of Steganographic Networks",
    "volume": "main",
    "abstract": "Steganography is a technique for covert communication between two parties. With the rapid development of deep neural networks (DNN), more and more steganographic networks are proposed recently, which are shown to be promising to achieve good performance. Unlike the traditional handcrafted steganographic tools, a steganographic network is relatively large in size. It raises concerns on how to covertly transmit the steganographic network in public channels, which is a crucial stage in the pipeline of steganography in real world applications. To address such an issue, we propose a novel scheme for steganography of steganographic networks in this paper. Unlike the existing steganographic schemes which focus on the subtle modification of the cover data to accommodate the secrets. We propose to disguise a steganographic network (termed as the secret DNN model) into a stego DNN model which performs an ordinary machine learning task (termed as the stego task). During the model disguising, we select and tune a subset of filters in the secret DNN model to preserve its function on the secret task, where the remaining filters are reactivated according to a partial optimization strategy to disguise the whole secret DNN model into a stego DNN model. The secret DNN model can be recovered from the stego DNN model when needed. Various experiments have been conducted to demonstrate the advantage of our proposed method for covert communication of steganographic networks as well as general DNN models",
    "checked": true,
    "id": "7a672ec588ce8c11ef7b68ee8585ccf5766e7827",
    "semantic_title": "steganography of steganographic networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25648": {
    "title": "PEN: Prediction-Explanation Network to Forecast Stock Price Movement with Better Explainability",
    "volume": "main",
    "abstract": "Nowadays explainability in stock price movement prediction is attracting increasing attention in banks, hedge funds and asset managers, primarily due to audit or regulatory reasons. Text data such as financial news and social media posts can be part of the reasons for stock price movement. To this end, we propose a novel framework of Prediction-Explanation Network (PEN) jointly modeling text streams and price streams with alignment. The key component of the PEN model is an shared representation learning module that learns which texts are possibly associated with the stock price movement by modeling the interaction between the text data and stock price data with a salient vector characterizing their correlation. In this way, the PEN model is able to predict the stock price movement by identifying and utilizing abundant messages while on the other hand, the selected text messages also explain the stock price movement. Experiments on real-world datasets demonstrate that we are able to kill two birds with one stone: in terms of accuracy, the proposed PEN model outperforms the state-of-art baseline; on explainability, the PEN model are demonstrated to be far superior to attention mechanism, capable of picking out the crucial texts with a very high confidence",
    "checked": true,
    "id": "3901de60ef6fc8f73d0720bd76b0918254d22613",
    "semantic_title": "pen: prediction-explanation network to forecast stock price movement with better explainability",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25649": {
    "title": "Decision-Making Context Interaction Network for Click-Through Rate Prediction",
    "volume": "main",
    "abstract": "Click-through rate (CTR) prediction is crucial in recommendation and online advertising systems. Existing methods usually model user behaviors, while ignoring the informative context which influences the user to make a click decision, e.g., click pages and pre-ranking candidates that inform inferences about user interests, leading to suboptimal performance. In this paper, we propose a Decision-Making Context Interaction Network (DCIN), which deploys a carefully designed Context Interaction Unit (CIU) to learn decision-making contexts and thus benefits CTR prediction. In addition, the relationship between different decision-making context sources is explored by the proposed Adaptive Interest Aggregation Unit (AIAU) to improve CTR prediction further. In the experiments on public and industrial datasets, DCIN significantly outperforms the state-of-the-art methods. Notably, the model has obtained the improvement of CTR+2.9%/CPM+2.1%/GMV+1.5% for online A/B testing and served the main traffic of Meituan Waimai advertising system",
    "checked": true,
    "id": "608632865430a81e20528a57bff20cecc4f3ce1b",
    "semantic_title": "decision-making context interaction network for click-through rate prediction",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25650": {
    "title": "Fine-Grained Position Helps Memorizing More, a Novel Music Compound Transformer Model with Feature Interaction Fusion",
    "volume": "main",
    "abstract": "Due to the particularity of the simultaneous occurrence of multiple events in music sequences, compound Transformer is proposed to deal with the challenge of long sequences. However, there are two deficiencies in the compound Transformer. First, since the order of events is more important for music than natural language, the information provided by the original absolute position embedding is not precise enough. Second, there is an important correlation between the tokens in the compound word, which is ignored by the current compound Transformer. Therefore, in this work, we propose an improved compound Transformer model for music understanding. Specifically, we propose an attribute embedding fusion module and a novel position encoding scheme with absolute-relative consideration. In the attribute embedding fusion module, different attributes are fused through feature permutation by using a multi-head self-attention mechanism in order to capture rich interactions between attributes. In the novel position encoding scheme, we propose RoAR position encoding, which realizes rotational absolute position encoding, relative position encoding, and absolute-relative position interactive encoding, providing clear and rich orders for musical events. Empirical study on four typical music understanding tasks shows that our attribute fusion approach and RoAR position encoding brings large performance gains. In addition, we further investigate the impact of masked language modeling and casual language modeling pre-training on music understanding",
    "checked": true,
    "id": "79efb734cef71f93ad4c875044f9fb4d2cd6079d",
    "semantic_title": "fine-grained position helps memorizing more, a novel music compound transformer model with feature interaction fusion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25651": {
    "title": "Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning",
    "volume": "main",
    "abstract": "The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual response augmentation method is used to improve model training. Extensive experiments conducted on three real-world datasets demonstrate that our proposed model achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages",
    "checked": true,
    "id": "376345572946f8710e265e10be270373587d4c4e",
    "semantic_title": "zero-shot rumor detection with propagation structure via prompt learning",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25652": {
    "title": "On Manipulating Weight Predictions in Signed Weighted Networks",
    "volume": "main",
    "abstract": "Adversarial social network analysis studies how graphs can be rewired or otherwise manipulated to evade social network analysis tools. While there is ample literature on manipulating simple networks, more sophisticated network types are much less understood in this respect. In this paper, we focus on the problem of evading FGA---an edge weight prediction method for signed weighted networks by Kumar et al. 2016. Among others, this method can be used for trust prediction in reputation systems. We study the theoretical underpinnings of FGA and its computational properties in terms of manipulability. Our positive finding is that, unlike many other tools, this measure is not only difficult to manipulate optimally, but also it can be difficult to manipulate in practice",
    "checked": true,
    "id": "6cf75aa820ccfbb7b07f80ceb4bc446f879ce65a",
    "semantic_title": "on manipulating weight predictions in signed weighted networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25653": {
    "title": "Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion",
    "volume": "main",
    "abstract": "Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our experiments show that providing access to the function implementation and function usages greatly improves the argument completion performance. Our ablation study provides further insights on how different types of information available from the program analyzer and different ways of incorporating the information affect the model performance",
    "checked": true,
    "id": "65f86451e96ad61ffca50eed6a007a19bc03093d",
    "semantic_title": "better context makes better code language models: a case study on function call argument completion",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25654": {
    "title": "MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning",
    "volume": "main",
    "abstract": "Representation learning of source code is essential for applying machine learning to software engineering tasks. Learning code representation from a multilingual source code dataset has been shown to be more effective than learning from single-language datasets separately, since more training data from multilingual dataset improves the model's ability to extract language-agnostic information from source code. However, existing multilingual training overlooks the language-specific information which is crucial for modeling source code across different programming languages, while only focusing on learning a unified model with shared parameters among different languages for language-agnostic information modeling. To address this problem, we propose MetaTPTrans, a meta learning approach for multilingual code representation learning. MetaTPTrans generates different parameters for the feature extractor according to the specific programming language type of the input code snippet, enabling the model to learn both language-agnostic and language-specific information with dynamic parameters in the feature extractor. We conduct experiments on the code summarization and code completion tasks to verify the effectiveness of our approach. The results demonstrate the superiority of our approach with significant improvements on state-of-the-art baselines",
    "checked": true,
    "id": "31a84391e1b47fa15f3a521c43d62385a7757637",
    "semantic_title": "metatptrans: a meta learning approach for multilingual code representation learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25655": {
    "title": "HG-SL: Jointly Learning of Global and Local User Spreading Behavior for Fake News Early Detection",
    "volume": "main",
    "abstract": "Recently, fake news forgery technology has become more and more sophisticated, and even the profiles of participants may be faked, which challenges the robustness and effectiveness of traditional detection methods involving text or user identity. Most propagation-only approaches mainly rely on neural networks to learn the diffusion pattern of individual news, which is insufficient to describe the differences in news spread ability, and also ignores the valuable global connections of news and users, limiting the performance of detection. Therefore, we propose a joint learning model named HG-SL, which is blind to news content and user identities, but capable of catching the differences between true and fake news in the early stages of propagation through global and local user spreading behavior. Specifically, we innovatively design a Hypergraph-based Global interaction learning module to capture the global preferences of users from their co-spreading relationships, and introduce node centrality encoding to complement user influence in hypergraph learning. Moreover, the designed Self-attention-based Local context learning module first introduce spread status to highlight the propagation ability of news and users, thus providing additional signals for verifying news authenticity. Experiments on real-world datasets indicate that our HG-SL, which solely relies on user behavior, outperforms SOTA baselines utilizing multidimensional features in both fake news detection and early detection task",
    "checked": true,
    "id": "d3c6898458ee72eb35276a1244049b17f5f9be5c",
    "semantic_title": "hg-sl: jointly learning of global and local user spreading behavior for fake news early detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25656": {
    "title": "Defending against Backdoor Attacks in Natural Language Generation",
    "volume": "main",
    "abstract": "The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the one-to-many issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks concealed in deep NLG systems and inspire more future work (both attack and defense) towards this direction",
    "checked": true,
    "id": "834891acb1dfeacb9ff75f923feeca66347167d7",
    "semantic_title": "defending against backdoor attacks in natural language generation",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25657": {
    "title": "GenéLive! Generating Rhythm Actions in Love Live!",
    "volume": "main",
    "abstract": "This article presents our generative model for rhythm action games together with applications in business operation. Rhythm action games are video games in which the player is challenged to issue commands at the right timings during a music session. The timings are rendered in the chart, which consists of visual symbols, called notes, flying through the screen. We introduce our deep generative model, GenéLive!, which outperforms the state-of-the-art model by taking into account musical structures through beats and temporal scales. Thanks to its favorable performance, GenéLive! was put into operation at KLab Inc., a Japan-based video game developer, and reduced the business cost of chart generation by as much as half. The application target included the phenomenal \"Love Live!\", which has more than 10 million users across Asia and beyond, and is one of the few rhythm action franchises that has led the online era of the genre. In this article, we evaluate the generative performance of GenéLive! using production datasets at KLab as well as open datasets for reproducibility, while the model continues to operate in their business. Our code and the model, tuned and trained using a supercomputer, are publicly available",
    "checked": true,
    "id": "27c4f8efcc548e74983c165f99b6711ec1c0699d",
    "semantic_title": "genélive! generating rhythm actions in love live!",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25658": {
    "title": "Deepfake Video Detection via Facial Action Dependencies Estimation",
    "volume": "main",
    "abstract": "Deepfake video detection has drawn significant attention from researchers due to the security issues induced by deepfake videos. Unfortunately, most of the existing deepfake detection approaches have not competently modeled the natural structures and movements of human faces. In this paper, we formulate the deepfake video detection problem into a graph classification task, and propose a novel paradigm named Facial Action Dependencies Estimation (FADE) for deepfake video detection. We propose a Multi-Dependency Graph Module (MDGM) to capture abundant dependencies among facial action units, and extracts subtle clues in these dependencies. MDGM can be easily integrated into the existing frame-level detection schemes to provide significant performance gains. Extensive experiments demonstrate the superiority of our method against the state-of-the-art methods",
    "checked": true,
    "id": "e3e269034ebfc94da52d2133239114046c8c12ae",
    "semantic_title": "deepfake video detection via facial action dependencies estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25659": {
    "title": "Contrastive Attention Networks for Attribution of Early Modern Print",
    "volume": "main",
    "abstract": "In this paper, we develop machine learning techniques to identify unknown printers in early modern (c.~1500--1800) English printed books. Specifically, we focus on matching uniquely damaged character type-imprints in anonymously printed books to works with known printers in order to provide evidence of their origins. Until now, this work has been limited to manual investigations by analytical bibliographers. We present a Contrastive Attention-based Metric Learning approach to identify similar damage across character image pairs, which is sensitive to very subtle differences in glyph shapes, yet robust to various confounding sources of noise associated with digitized historical books. To overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process. Our method successfully improves downstream damaged type-imprint matching among printed works from this period, as validated by in-domain human experts. The results of our approach on two important philosophical works from the Early Modern period demonstrate potential to extend the extant historical research about the origins and content of these books",
    "checked": true,
    "id": "d9dc309f719233be9f2a6b6910072e537f96eec8",
    "semantic_title": "contrastive attention networks for attribution of early modern print",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25660": {
    "title": "AdapSafe: Adaptive and Safe-Certified Deep Reinforcement Learning-Based Frequency Control for Carbon-Neutral Power Systems",
    "volume": "main",
    "abstract": "With the increasing penetration of inverter-based renewable energy resources, deep reinforcement learning (DRL) has been proposed as one of the most promising solutions to realize real-time and autonomous control for future carbon-neutral power systems. In particular, DRL-based frequency control approaches have been extensively investigated to overcome the limitations of model-based approaches, such as the computational cost and scalability for large-scale systems. Nevertheless, the real-world implementation of DRLbased frequency control methods is facing the following fundamental challenges: 1) safety guarantee during the learning and decision-making processes; 2) adaptability against the dynamic system operating conditions. To this end, this is the first work that proposes an Adaptive and Safe-Certified DRL (AdapSafe) algorithm for frequency control to simultaneously address the aforementioned challenges. In particular, a novel self-tuning control barrier function is designed to actively compensate the unsafe frequency control strategies under variational safety constraints and thus achieve guaranteed safety. Furthermore, the concept of meta-reinforcement learning is integrated to significantly enhance its adaptiveness in non-stationary power system environments without sacrificing the safety cost. Experiments are conducted based on GB 2030 power system, and the results demonstrate that the proposed AdapSafe exhibits superior performance in terms of its guaranteed safety in both training and test phases, as well as its considerable adaptability against the dynamics changes of system parameters",
    "checked": true,
    "id": "11389a70e115c21169f365f3e944f517cebfe555",
    "semantic_title": "adapsafe: adaptive and safe-certified deep reinforcement learning-based frequency control for carbon-neutral power systems",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25661": {
    "title": "Don't Predict Counterfactual Values, Predict Expected Values Instead",
    "volume": "main",
    "abstract": "Counterfactual Regret Minimization algorithms are the most popular way of estimating the Nash Equilibrium in imperfect-information zero-sum games. In particular, DeepStack -- the state-of-the-art Poker bot -- employs the so-called Deep Counterfactual Value Network (DCVN) to learn the Counterfactual Values (CFVs) associated with various states in the game. Each CFV is a multiplication of two factors: (1) the probability that the opponent would reach a given state in a game, which can be explicitly calculated from the input data, and (2) the expected value (EV) of a payoff in that state, which is a complex function of the input data, hard to calculate. In this paper, we propose a simple yet powerful modification to the CFVs estimation process, which consists in utilizing a deep neural network to estimate only the EV factor of CFV. This new target setting significantly simplifies the learning problem and leads to much more accurate CFVs estimation. A direct comparison, in terms of CFVs prediction losses, shows a significant prediction accuracy improvement of the proposed approach (DEVN) over the original DCVN formulation (relatively by 9.18-15.70% when using card abstraction, and by 3.37-8.39% without card abstraction, depending on a particular setting). Furthermore, the application of DEVN improves the theoretical lower bound of the error by 29.05-31.83% compared to the DCVN pipeline when card abstraction is applied. Additionally, DEVN is able to achieve the goal using significantly smaller, and faster to infer, networks. While the proposed modification may seem to be of a rather technical nature, it, in fact, presents a fundamentally different approach to the overall process of learning and estimating CFVs, since the distributions of the training signals differ significantly between DCVN and DEVN. The former estimates CFVs, which are biased by the probability of reaching a given game state, while training the latter relies on a direct EV estimation, regardless of the state probability. In effect, the learning signal of DEVN presents a better estimation of the true value of a given state, thus allowing more accurate CFVs estimation",
    "checked": true,
    "id": "88d1fc95d1d8452d1cbb41b07fa0829e62f1fb66",
    "semantic_title": "don't predict counterfactual values, predict expected values instead",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25662": {
    "title": "Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs",
    "volume": "main",
    "abstract": "Procuring expressive molecular representations underpins AI-driven molecule design and scientific discovery. The research mainly focuses on atom-level homogeneous molecular graphs, ignoring the rich information in subgraphs or motifs. However, it has been widely accepted that substructures play a dominant role in identifying and determining molecular properties. To address such issues, we formulate heterogeneous molecular graphs (HMGs) and introduce a novel architecture to exploit both molecular motifs and 3D geometry. Precisely, we extract functional groups as motifs for small molecules and employ reinforcement learning to adaptively select quaternary amino acids as motif candidates for proteins. Then HMGs are constructed with both atom-level and motif-level nodes. To better accommodate those HMGs, we introduce a variant of the Transformer named Molformer, which adopts a heterogeneous self-attention layer to distinguish the interactions between multi-level nodes. Besides, it is also coupled with a multi-scale mechanism to capture fine-grained local patterns with increasing contextual scales. An attentive farthest point sampling algorithm is also proposed to obtain the molecular representations. We validate Molformer across a broad range of domains, including quantum chemistry, physiology, and biophysics. Extensive experiments show that Molformer outperforms or achieves the comparable performance of several state-of-the-art baselines. Our work provides a promising way to utilize informative motifs from the perspective of multi-level graph construction. The code is available at https://github.com/smiles724/Molformer",
    "checked": true,
    "id": "540ed994eb00b5279748d1f26d04371e3a67ec0d",
    "semantic_title": "molformer: motif-based transformer on 3d heterogeneous molecular graphs",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25663": {
    "title": "DiffMD: A Geometric Diffusion Model for Molecular Dynamics Simulations",
    "volume": "main",
    "abstract": "Molecular dynamics (MD) has long been the de facto choice for simulating complex atomistic systems from first principles. Recently deep learning models become a popular way to accelerate MD. Notwithstanding, existing models depend on intermediate variables such as the potential energy or force fields to update atomic positions, which requires additional computations to perform back-propagation. To waive this requirement, we propose a novel model called DiffMD by directly estimating the gradient of the log density of molecular conformations. DiffMD relies on a score-based denoising diffusion generative model that perturbs the molecular structure with a conditional noise depending on atomic accelerations and treats conformations at previous timeframes as the prior distribution for sampling. Another challenge of modeling such a conformation generation process is that a molecule is kinetic instead of static, which no prior works have strictly studied. To solve this challenge, we propose an equivariant geometric Transformer as the score function in the diffusion process to calculate corresponding gradients. It incorporates the directions and velocities of atomic motions via 3D spherical Fourier-Bessel representations. With multiple architectural improvements, we outperform state-of-the-art baselines on MD17 and isomers of C7O2H10 datasets. This work contributes to accelerating material and drug discovery",
    "checked": true,
    "id": "6fe2208f1c4dba1c9959f2dd4fdb6c2dc245b713",
    "semantic_title": "diffmd: a geometric diffusion model for molecular dynamics simulations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25664": {
    "title": "Retrosynthesis Prediction with Local Template Retrieval",
    "volume": "main",
    "abstract": "Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contains the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy, we improved 7.1% on the USPTO-50K dataset and 12.0% on the USPTO-MIT dataset.These results demonstrate the effectiveness of our method",
    "checked": true,
    "id": "7045e1e30c2eb8f305dcb8ded655ff659f53fefd",
    "semantic_title": "retrosynthesis prediction with local template retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25665": {
    "title": "Multi-Relational Contrastive Learning Graph Neural Network for Drug-Drug Interaction Event Prediction",
    "volume": "main",
    "abstract": "Drug-drug interactions (DDIs) could lead to various unexpected adverse consequences, so-called DDI events. Predicting DDI events can reduce the potential risk of combinatorial therapy and improve the safety of medication use, and has attracted much attention in the deep learning community. Recently, graph neural network (GNN)-based models have aroused broad interest and achieved satisfactory results in the DDI event prediction. Most existing GNN-based models ignore either drug structural information or drug interactive information, but both aspects of information are important for DDI event prediction. Furthermore, accurately predicting rare DDI events is hindered by their inadequate labeled instances. In this paper, we propose a new method, Multi-Relational Contrastive learning Graph Neural Network, MRCGNN for brevity, to predict DDI events. Specifically, MRCGNN integrates the two aspects of information by deploying a GNN on the multi-relational DDI event graph attributed with the drug features extracted from drug molecular graphs. Moreover, we implement a multi-relational graph contrastive learning with a designed dual-view negative counterpart augmentation strategy, to capture implicit information about rare DDI events. Extensive experiments on two datasets show that MRCGNN outperforms the state-of-the-art methods. Besides, we observe that MRCGNN achieves satisfactory performance when predicting rare DDI events",
    "checked": true,
    "id": "0ba58a80635fad308adfa41d76931c4f73ba8c54",
    "semantic_title": "multi-relational contrastive learning graph neural network for drug-drug interaction event prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25666": {
    "title": "Tighter Robust Upper Bounds for Options via No-Regret Learning",
    "volume": "main",
    "abstract": "Classic option pricing models, such as the Black-Scholes formula, often depend on some rigid assumptions on the dynamics of the underlying asset prices. These assumptions are inevitably violated in practice and thus induce the model risk. To mitigate this, robust option pricing that only requires the no-arbitrage principle has attracted a great deal of attention among researchers. In this paper, we give new robust upper bounds for option prices based on a novel η-momentum trading strategy. Our bounds for European options are tighter for most common moneyness, volatility, and expiration date setups than those presented in the existing literature. Our bounds for average strike Asian options are the first closed-form robust upper bounds for those options. Numerical simulations demonstrate that our bounds significantly outperform the benchmarks for both European and Asian options",
    "checked": true,
    "id": "b764a3ccd99b4bbac95d5f321eb4ad1ac4173224",
    "semantic_title": "tighter robust upper bounds for options via no-regret learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25667": {
    "title": "KerPrint: Local-Global Knowledge Graph Enhanced Diagnosis Prediction for Retrospective and Prospective Interpretations",
    "volume": "main",
    "abstract": "While recent developments of deep learning models have led to record-breaking achievements in many areas, the lack of sufficient interpretation remains a problem for many specific applications, such as the diagnosis prediction task in healthcare. The previous knowledge graph(KG) enhanced approaches mainly focus on learning clinically meaningful representations, the importance of medical concepts, and even the knowledge paths from inputs to labels. However, it is infeasible to interpret the diagnosis prediction, which needs to consider different medical concepts, various medical relationships, and the time-effectiveness of knowledge triples in different patient contexts. More importantly, the retrospective and prospective interpretations of disease processes are valuable to clinicians for the patients' confounding diseases. We propose KerPrint, a novel KG enhanced approach for retrospective and prospective interpretations to tackle these problems. Specifically, we propose a time-aware KG attention method to solve the problem of knowledge decay over time for trustworthy retrospective interpretation. We also propose a novel element-wise attention method to select candidate global knowledge using comprehensive representations from the local KG for prospective interpretation. We validate the effectiveness of our KerPrint through an extensive experimental study on a real-world dataset and a public dataset. The results show that our proposed approach not only achieves significant improvement over knowledge-enhanced methods but also gives the interpretability of diagnosis prediction in both retrospective and prospective views",
    "checked": true,
    "id": "2b5eef580fb9fa9e0f9d946feb2819ad63eb011a",
    "semantic_title": "kerprint: local-global knowledge graph enhanced diagnosis prediction for retrospective and prospective interpretations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25668": {
    "title": "Multi-Label Few-Shot ICD Coding as Autoregressive Generation with Prompt",
    "volume": "main",
    "abstract": "Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with an average of 3,000+ tokens. This task is challenging due to the high-dimensional space of multi-label assignment (155,000+ ICD code candidates) and the long-tail challenge - Many ICD codes are infrequently assigned yet infrequent ICD codes are important clinically. This study addresses the long-tail challenge by transforming this multi-label classification task into an autoregressive generation task. Specifically, we first introduce a novel pretraining objective to generate free text diagnosis and procedure descriptions using the SOAP structure, the medical logic physicians use for note documentation. Second, instead of directly predicting the high dimensional space of ICD codes, our model generates the lower dimension of text descriptions, which then infer ICD codes. Third, we designed a novel prompt template for multi-label classification. We evaluate our Generation with Prompt (GP) model with the benchmark of all code assignment (MIMIC-III-full) and few shot ICD code assignment evaluation benchmark (MIMIC-III-few). Experiments on MIMIC-III-few show that our model performs with a marco F1 30.2, which substantially outperforms the previous MIMIC-III-full SOTA model (marco F1 4.3) and the model specifically designed for few/zero shot setting (marco F1 18.7). Finally, we design a novel ensemble learner, a cross attention reranker with prompts, to integrate previous SOTA and our best few-shot coding predictions. Experiments on MIMIC-III-full show that our ensemble learner substantially improves both macro and micro F1, from 10.4 to 14.6 and from 58.2 to 59.1, respectively",
    "checked": true,
    "id": "6b87c9700b8de4912fe7c361574640b5dc536ca9",
    "semantic_title": "multi-label few-shot icd coding as autoregressive generation with prompt",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25669": {
    "title": "DMIS: Dynamic Mesh-Based Importance Sampling for Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "Modeling dynamics in the form of partial differential equations (PDEs) is an effectual way to understand real-world physics processes. For complex physics systems, analytical solutions are not available and numerical solutions are widely-used. However, traditional numerical algorithms are computationally expensive and challenging in handling multiphysics systems. Recently, using neural networks to solve PDEs has made significant progress, called physics-informed neural networks (PINNs). PINNs encode physical laws into neural networks and learn the continuous solutions of PDEs. For the training of PINNs, existing methods suffer from the problems of inefficiency and unstable convergence, since the PDE residuals require calculating automatic differentiation. In this paper, we propose Dynamic Mesh-based Importance Sampling (DMIS) to tackle these problems. DMIS is a novel sampling scheme based on importance sampling, which constructs a dynamic triangular mesh to estimate sample weights efficiently. DMIS has broad applicability and can be easily integrated into existing methods. The evaluation of DMIS on three widely-used benchmarks shows that DMIS improves the convergence speed and accuracy in the meantime. Especially in solving the highly nonlinear Schrödinger Equation, compared with state-of-the-art methods, DMIS shows up to 46% smaller root mean square error and five times faster convergence speed. Code is available at https://github.com/MatrixBrain/DMIS",
    "checked": true,
    "id": "45017fc77acd06a13d231900f548806bf804beb5",
    "semantic_title": "dmis: dynamic mesh-based importance sampling for training physics-informed neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25670": {
    "title": "Bootstrapping Multi-View Representations for Fake News Detection",
    "volume": "main",
    "abstract": "Previous researches on multimedia fake news detection include a series of complex feature extraction and fusion networks to gather useful information from the news. However, how cross-modal consistency relates to the fidelity of news and how features from different modalities affect the decision-making are still open questions. This paper presents a novel scheme of Bootstrapping Multi-view Representations (BMR) for fake news detection. Given a multi-modal news, we extract representations respectively from the views of the text, the image pattern and the image semantics. Improved Multi-gate Mixture-of-Expert networks (iMMoE) are proposed for feature refinement and fusion. Representations from each view are separately used to coarsely predict the fidelity of the whole news, and the multimodal representations are able to predict the cross-modal consistency. With the prediction scores, we reweigh each view of the representations and bootstrap them for fake news detection. Extensive experiments conducted on typical fake news detection datasets prove that BMR outperforms state-of-the-art schemes",
    "checked": true,
    "id": "38ff6bae846d70d341c8732717c0f593180f318b",
    "semantic_title": "bootstrapping multi-view representations for fake news detection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25671": {
    "title": "Overcoming Forgetting in Fine-Grained Urban Flow Inference via Adaptive Knowledge Replay",
    "volume": "main",
    "abstract": "Fine-grained urban flow inference (FUFI) problem aims at inferring the high-resolution flow maps from the coarse-grained ones, which plays an important role in sustainable and economic urban computing and traffic management. Previous models addressed the FUFI problem from spatial constraint, external factors, and memory cost. However, utilizing the new urban flow maps to calibrate the learned model is very challenging due to the \"catastrophic forgetting\" problem and is still under-explored. In this paper, we make the first step in FUFI and present CUFAR -- Continual Urban Flow inference with Adaptive knowledge Replay -- a novel framework for inferring the fine-grained citywide traffic flows. Specifically, (1) we design a spatial-temporal inference network that can extract better flow map features from both local and global levels; (2) then we present an adaptive knowledge replay (AKR) training algorithm to selectively replay the learned knowledge to facilitate the learning process of the model on new knowledge without forgetting. In addition, we also propose a knowledge discriminator to avoid \"negative replaying\" issue introduced by noisy urban flow maps. Extensive experiments on four large-scale real-world FUFI datasets demonstrate that our proposed model consistently outperforms strong baselines and effectively mitigates the forgetting problem. Source code is available at: https://github.com/PattonYu/CUFAR",
    "checked": true,
    "id": "54eec69dadad0093b38be8488968e7786a5fb376",
    "semantic_title": "overcoming forgetting in fine-grained urban flow inference via adaptive knowledge replay",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25672": {
    "title": "Generalized Cell Type Annotation and Discovery for Single-Cell RNA-Seq Data",
    "volume": "main",
    "abstract": "The rapid development of single-cell RNA sequencing (scRNA-seq) technology allows us to study gene expression heterogeneity at the cellular level. Cell annotation is the basis for subsequent downstream analysis in single-cell data mining. Existing methods rarely explore the fine-grained semantic knowledge of novel cell types absent from the reference data and usually susceptible to batch effects on the classification of seen cell types. Taking into consideration these limitations, this paper proposes a new and practical task called generalized cell type annotation and discovery for scRNA-seq data. In this task, cells of seen cell types are given class labels, while cells of novel cell types are given cluster labels instead of a unified \"unassigned\" label. To address this problem, we carefully design a comprehensive evaluation benchmark and propose a novel end-to-end algorithm framework called scGAD. Specifically, scGAD first builds the intrinsic correspondence across the reference and target data by retrieving the geometrically and semantically mutual nearest neighbors as anchor pairs. Then we introduce an anchor-based self-supervised learning module with a connectivity-aware attention mechanism to facilitate model prediction capability on unlabeled target data. To enhance the inter-type separation and intra-type compactness, we further propose a confidential prototypical self-supervised learning module to uncover the consensus category structure of the reference and target data. Extensive results on massive real datasets demonstrate the superiority of scGAD over various state-of-the-art clustering and annotation methods",
    "checked": true,
    "id": "7ca5419b63753fbb249860d45a66cbc4052397bd",
    "semantic_title": "generalized cell type annotation and discovery for single-cell rna-seq data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25673": {
    "title": "Mining and Applying Composition Knowledge of Dance Moves for Style-Concentrated Dance Generation",
    "volume": "main",
    "abstract": "Choreography refers to creation of dance motions according to both music and dance knowledge, where the created dances should be style-specific and consistent. However, most of the existing methods generate dances using the given music as the only reference, lacking the stylized dancing knowledge, namely, the flag motion patterns contained in different styles. Without the stylized prior knowledge, these approaches are not promising to generate controllable style or diverse moves for each dance style, nor new dances complying with stylized knowledge. To address this issue, we propose a novel music-to-dance generation framework guided by style embedding, considering both input music and stylized dancing knowledge. These style embeddings are learnt representations of style-consistent kinematic abstraction of reference dance videos, which can act as controllable factors to impose style constraints on dance generation in a latent manner. Hence, we can make the style embedding fit into any given style while allowing the flexibility to generate new compatible dance moves by modifying the style embedding according to the learnt representations of a certain style. We are the first to achieve knowledge-driven style control in dance generation tasks. To support this study, we build a large multi-style music-to-dance dataset referred to as I-Dance. The qualitative and quantitative evaluations demonstrate the advantage of the proposed framework, as well as the ability to synthesize diverse moves under a dance style directed by style embedding",
    "checked": true,
    "id": "b1574e3fff207020073b54b7367f2da80f64740b",
    "semantic_title": "mining and applying composition knowledge of dance moves for style-concentrated dance generation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25674": {
    "title": "Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer with Multi-Level Flow Representation",
    "volume": "main",
    "abstract": "Traffic classification is a critical task in network security and management. Recent research has demonstrated the effectiveness of the deep learning-based traffic classification method. However, the following limitations remain: (1) the traffic representation is simply generated from raw packet bytes, resulting in the absence of important information; (2) the model structure of directly applying deep learning algorithms does not take traffic characteristics into account; and (3) scenario-specific classifier training usually requires a labor-intensive and time-consuming process to label data. In this paper, we introduce a masked autoencoder (MAE) based traffic transformer with multi-level flow representation to tackle these problems. To model raw traffic data, we design a formatted traffic representation matrix with hierarchical flow information. After that, we develop an efficient Traffic Transformer, in which packet-level and flow-level attention mechanisms implement more efficient feature extraction with lower complexity. At last, we utilize the MAE paradigm to pre-train our classifier with a large amount of unlabeled data, and perform fine-tuning with a few labeled data for a series of traffic classification tasks. Experiment findings reveal that our method outperforms state-of-the-art methods on five real-world traffic datasets by a large margin. The code is available at https://github.com/NSSL-SJTU/YaTC",
    "checked": true,
    "id": "4ba78edb48d729339b109b1ae707daafcca8e005",
    "semantic_title": "yet another traffic classifier: a masked autoencoder based traffic transformer with multi-level flow representation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25675": {
    "title": "Loan Fraud Users Detection in Online Lending Leveraging Multiple Data Views",
    "volume": "main",
    "abstract": "In recent years, online lending platforms have been becoming attractive for micro-financing and popular in financial industries. However, such online lending platforms face a high risk of failure due to the lack of expertise on borrowers' creditworthness. Thus, risk forecasting is important to avoid economic loss. Detecting loan fraud users in advance is at the heart of risk forecasting. The purpose of fraud user (borrower) detection is to predict whether one user will fail to make required payments in the future. Detecting fraud users depend on historical loan records. However, a large proportion of users lack such information, especially for new users. In this paper, we attempt to detect loan fraud users from cross domain heterogeneous data views, including user attributes, installed app lists, app installation behaviors, and app-in logs, which compensate for the lack of historical loan records. However, it is difficult to effectively fuse the multiple heterogeneous data views. Moreover, some samples miss one or even more data views, increasing the difficulty in fusion. To address the challenges, we propose a novel end-to-end deep multiview learning approach, which encodes heterogeneous data views into homogeneous ones, generates the missing views based on the learned relationship among all the views, and then fuses all the views together to a comprehensive view for identifying fraud users. Our model is evaluated on a real-world large-scale dataset consisting of 401,978 loan records of 228,117 users from January 1, 2019, to September 30, 2019, achieving the state-of-the-art performance",
    "checked": true,
    "id": "5631b6811a2cff4d35aea151398154a929da8b24",
    "semantic_title": "loan fraud users detection in online lending leveraging multiple data views",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25676": {
    "title": "Sparse Maximum Margin Learning from Multimodal Human Behavioral Patterns",
    "volume": "main",
    "abstract": "We propose a multimodal data fusion framework to systematically analyze human behavioral data from specialized domains that are inherently dynamic, sparse, and heterogeneous. We develop a two-tier architecture of probabilistic mixtures, where the lower tier leverages parametric distributions from the exponential family to extract significant behavioral patterns from each data modality. These patterns are then organized into a dynamic latent state space at the higher tier to fuse patterns from different modalities. In addition, our framework jointly performs pattern discovery and maximum-margin learning for downstream classification tasks by using a group-wise sparse prior that regularizes the coefficients of the maximum-margin classifier. Therefore, the discovered patterns are highly interpretable and discriminative to support downstream classification tasks. Experiments on real-world behavioral data from medical and psychological domains demonstrate that our framework discovers meaningful multimodal behavioral patterns with improved interpretability and prediction performance",
    "checked": true,
    "id": "f4150ac3683f5bb310a409c280123238e0da701f",
    "semantic_title": "sparse maximum margin learning from multimodal human behavioral patterns",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25677": {
    "title": "Direct Heterogeneous Causal Learning for Resource Allocation Problems in Marketing",
    "volume": "main",
    "abstract": "Marketing is an important mechanism to increase user engagement and improve platform revenue, and heterogeneous causal learning can help develop more effective strategies. Most decision-making problems in marketing can be formulated as resource allocation problems and have been studied for decades. Existing works usually divide the solution procedure into two fully decoupled stages, i.e., machine learning (ML) and operation research (OR) --- the first stage predicts the model parameters and they are fed to the optimization in the second stage. However, the error of the predicted parameters in ML cannot be respected and a series of complex mathematical operations in OR lead to the increased accumulative errors. Essentially, the improved precision on the prediction parameters may not have a positive correlation on the final solution due to the side-effect from the decoupled design. In this paper, we propose a novel approach for solving resource allocation problems to mitigate the side-effects. Our key intuition is that we introduce the decision factor to establish a bridge between ML and OR such that the solution can be directly obtained in OR by only performing the sorting or comparison operations on the decision factor. Furthermore, we design a customized loss function that can conduct direct heterogeneous causal learning on the decision factor, an unbiased estimation of which can be guaranteed when the loss convergences. As a case study, we apply our approach to two crucial problems in marketing: the binary treatment assignment problem and the budget allocation problem with multiple treatments. Both large-scale simulations and online A/B Tests demonstrate that our approach achieves significant improvement compared with state-of-the-art",
    "checked": true,
    "id": "f1ca2cce6c8e53488983a89bccfe34c976202f7b",
    "semantic_title": "direct heterogeneous causal learning for resource allocation problems in marketing",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25678": {
    "title": "Mediated Cheap Talk Design",
    "volume": "main",
    "abstract": "We study an information design problem with two informed senders and a receiver in which, in contrast to traditional Bayesian persuasion settings, senders do not have commitment power. In our setting, a trusted mediator/platform gathers data from the senders and recommends the receiver which action to play. We characterize the set of feasible action distributions that can be obtained in equilibrium, and provide an O(n log n) algorithm (where n is the number of states) that computes the optimal equilibrium for the senders. Additionally, we show that the optimal equilibrium for the receiver can be obtained by a simple revelation mechanism",
    "checked": true,
    "id": "0fabed956606dff3be30ee895d6d2b3ff2ae8a4a",
    "semantic_title": "mediated cheap talk design",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25679": {
    "title": "Bidding Graph Games with Partially-Observable Budgets",
    "volume": "main",
    "abstract": "Two-player zero-sum \"graph games\" are central in logic, verification, and multi-agent systems. The game proceeds by placing a token on a vertex of a graph, and allowing the players to move it to produce an infinite path, which determines the winner or payoff of the game. Traditionally, the players alternate turns in moving the token. In \"bidding games\", however, the players have budgets and in each turn, an auction (bidding) determines which player moves the token. So far, bidding games have only been studied as full-information games. In this work we initiate the study of partial-information bidding games: we study bidding games in which a player's initial budget is drawn from a known probability distribution. We show that while for some bidding mechanisms and objectives, it is straightforward to adapt the results from the full-information setting to the partial-information setting, for others, the analysis is significantly more challenging, requires new techniques, and gives rise to interesting results. Specifically, we study games with \"mean-payoff\" objectives in combination with \"poorman\" bidding. We construct optimal strategies for a partially-informed player who plays against a fully-informed adversary. We show that, somewhat surprisingly, the \"value\" under pure strategies does not necessarily exist in such games",
    "checked": true,
    "id": "cf7c118beeca6e1ed555164151a084a6286a8dc9",
    "semantic_title": "bidding graph games with partially-observable budgets",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25680": {
    "title": "Fairness Concepts for Indivisible Items with Externalities",
    "volume": "main",
    "abstract": "We study a fair allocation problem of indivisible items under additive externalities in which each agent also receives utility from items that are assigned to other agents. This allows us to capture scenarios in which agents benefit from or compete against one another. We extend the well-studied properties of envy-freeness up to one item (EF1) and envy-freeness up to any item (EFX) to this setting, and we propose a new fairness concept called general fair share (GFS), which applies to a more general public decision making model. We undertake a detailed study and present algorithms for finding fair allocations",
    "checked": true,
    "id": "f92552c73d2b67e671fec7d5fced0f2195dc0502",
    "semantic_title": "fairness concepts for indivisible items with externalities",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25681": {
    "title": "Finding Fair Allocations under Budget Constraints",
    "volume": "main",
    "abstract": "We study the fair allocation of indivisible goods among agents with identical, additive valuations but individual budget constraints. Here, the indivisible goods--each with a specific size and value--need to be allocated such that the bundle assigned to each agent is of total size at most the agent's budget. Since envy-free allocations do not necessarily exist in the indivisible goods context, compelling relaxations--in particular, the notion of envy-freeness up to k goods (EFk)--have received significant attention in recent years. In an EFk allocation, each agent prefers its own bundle over that of any other agent, up to the removal of k goods, and the agents have similarly bounded envy against the charity (which corresponds to the set of all unallocated goods). It has been shown in prior work that an allocation that satisfies the budget constraints and maximizes the Nash social welfare is 1/4-approximately EF1. However, the computation (or even existence) of exact EFk allocations remained an intriguing open problem. We make notable progress towards this by proposing a simple, greedy, polynomial-time algorithm that computes EF2 allocations under budget constraints. Our algorithmic result implies the universal existence of EF2 allocations in this fair division context. The analysis of the algorithm exploits intricate structural properties of envy-freeness. Interestingly, the same algorithm also provides EF1 guarantees for important special cases. Specifically, we settle the existence of EF1 allocations for instances in which: (i) the value of each good is proportional to its size, (ii) all the goods have the same size, or (iii) all the goods have the same value. Our EF2 result even extends to the setting wherein the goods' sizes are agent specific",
    "checked": true,
    "id": "51e020d339e00afca58dc703ef91d217d00ea869",
    "semantic_title": "finding fair allocations under budget constraints",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25682": {
    "title": "Now We're Talking: Better Deliberation Groups through Submodular Optimization",
    "volume": "main",
    "abstract": "Citizens' assemblies are groups of randomly selected constituents who are tasked with providing recommendations on policy questions. Assembly members form their recommendations through a sequence of discussions in small groups (deliberation), in which group members exchange arguments and experiences. We seek to support this process through optimization, by studying how to assign participants to discussion groups over multiple sessions, in a way that maximizes interaction between participants and satisfies diversity constraints within each group. Since repeated meetings between a given pair of participants have diminishing marginal returns, we capture interaction through a submodular function, which is approximately optimized by a greedy algorithm making calls to an ILP solver. This framework supports different submodular objective functions, and we identify sensible options, but we also show it is not necessary to commit to a particular choice: Our main theoretical result is a (practically efficient) algorithm that simultaneously approximates every possible objective function of the form we are interested in. Experiments with data from real citizens' assemblies demonstrate that our approach substantially outperforms the heuristic algorithm currently used by practitioners",
    "checked": true,
    "id": "aead5a64ee1ef40829300401debe649772d7c26c",
    "semantic_title": "now we're talking: better deliberation groups through submodular optimization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25683": {
    "title": "Causes of Stability in Dynamic Coalition Formation",
    "volume": "main",
    "abstract": "We study the formation of stable outcomes via simple dynamics in cardinal hedonic games, where the utilities of agents change over time depending on the history of the coalition formation process. Specifically, we analyze situations where members of a coalition decrease their utility for a leaving agent (resent) or increase their utility for a joining agent (appreciation). We show that in contrast to classical dynamics, for resentful or appreciative agents, dynamics are guaranteed to converge under mild conditions for various stability concepts. Thereby, we establish that both resent and appreciation are strong stability-driving forces",
    "checked": true,
    "id": "d1664faf020d247b8b0a9cc775fb11152a6c38dc",
    "semantic_title": "causes of stability in dynamic coalition formation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25684": {
    "title": "Properties of Position Matrices and Their Elections",
    "volume": "main",
    "abstract": "We study the properties of elections that have a given position matrix (in such elections each candidate is ranked on each position by a number of voters specified in the matrix). We show that counting elections that generate a given position matrix is #P-complete. Consequently, sampling such elections uniformly at random seems challenging and we propose a simpler algorithm, without hard guarantees. Next, we consider the problem of testing if a given matrix can be implemented by an election with a certain structure (such as single-peakedness or group-separability). Finally, we consider the problem of checking if a given position matrix can be implemented by an election with a Condorcet winner. We complement our theoretical findings with experiments",
    "checked": true,
    "id": "0eafc75d4dd73a9491dd031e54af5b048936b83a",
    "semantic_title": "properties of position matrices and their elections",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25685": {
    "title": "Rank Aggregation Using Scoring Rules",
    "volume": "main",
    "abstract": "To aggregate rankings into a social ranking, one can use scoring systems such as Plurality, Veto, and Borda. We distinguish three types of methods: ranking by score, ranking by repeatedly choosing a winner that we delete and rank at the top, and ranking by repeatedly choosing a loser that we delete and rank at the bottom. The latter method captures the frequently studied voting rules Single Transferable Vote (aka Instant Runoff Voting), Coombs, and Baldwin. In an experimental analysis, we show that the three types of methods produce different rankings in practice. We also provide evidence that sequentially selecting winners is most suitable to detect the \"true\" ranking of candidates. For different rules in our classes, we then study the (parameterized) computational complexity of deciding in which positions a given candidate can appear in the chosen ranking. As part of our analysis, we also consider the Winner Determination problem for STV, Coombs, and Baldwin and determine their complexity when there are few voters or candidates",
    "checked": true,
    "id": "36814dc10e6f759c87520b080a6dd339f74c32be",
    "semantic_title": "rank aggregation using scoring rules",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25686": {
    "title": "Proportionality in Approval-Based Participatory Budgeting",
    "volume": "main",
    "abstract": "The ability to measure the satisfaction of (groups of) voters is a crucial prerequisite for formulating proportionality axioms in approval-based participatory budgeting elections. Two common -- but very different -- ways to measure the satisfaction of a voter consider (i) the number of approved projects and (ii) the total cost of approved projects, respectively. In general, it is difficult to decide which measure of satisfaction best reflects the voters' true utilities. In this paper, we study proportionality axioms with respect to large classes of approval-based satisfaction functions. We establish logical implications among our axioms and related notions from the literature, and we ask whether outcomes can be achieved that are proportional with respect to more than one satisfaction function. We show that this is impossible for the two commonly used satisfaction functions when considering proportionality notions based on extended justified representation, but achievable for a notion based on proportional justified representation. For the latter result, we introduce a strengthening of priceability and show that it is satisfied by several polynomial-time computable rules, including the Method of Equal Shares and Phragmén's sequential rule",
    "checked": true,
    "id": "8c4aed0f334de39822ece80507b83786f9dbc60f",
    "semantic_title": "proportionality in approval-based participatory budgeting",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25687": {
    "title": "Multiwinner Voting with Possibly Unavailable Candidates",
    "volume": "main",
    "abstract": "Selecting a committee that meets diversity and proportionality criteria is a challenging endeavor that has been studied extensively in recent years. This task becomes even more challenging when some of the selected candidates decline the invitation to join the committee. Since the unavailability of one candidate may impact the rest of the selection, inviting all candidates at the same time may lead to a suboptimal committee. Instead, invitations should be sequential and conditional on which candidates invited so far accepted the invitation: the solution to the committee selection problem is a query policy. If invitation queries are binding, they should be safe: one should not query a candidate without being sure that whatever the set of available candidates possible at that stage, her inclusion will not jeopardize committee optimality. Assuming approval-based inputs, we characterize the set of rules for which a safe query exists at every stage. In order to parallelize the invitation process, we investigate the computation of safe parallel queries, and show that it is often hard. We also study the existence of safe parallel queries with respect to proportionality axioms such as extended justified representation",
    "checked": true,
    "id": "c15f4da85b4d487ffed1b9c731f8411551a8a856",
    "semantic_title": "multiwinner voting with possibly unavailable candidates",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25688": {
    "title": "Fair Division with Prioritized Agents",
    "volume": "main",
    "abstract": "We consider the fair division problem of indivisible items. It is well-known that an envy-free allocation may not exist, and a relaxed version of envy-freeness, envy-freeness up to one item (EF1), has been widely considered. In an EF1 allocation, an agent may envy others' allocated shares, but only up to one item. In many applications, we may wish to specify a subset of prioritized agents where strict envy-freeness needs to be guaranteed from these agents to the remaining agents, while ensuring the whole allocation is still EF1. Prioritized agents may be those agents who are envious in a previous EF1 allocation, those agents who belong to underrepresented groups, etc. Motivated by this, we propose a new fairness notion named envy-freeness with prioritized agents EFprior, and study the existence and the algorithmic aspects for the problem of computing an EFprior allocation. With additive valuations, the simple round-robin algorithm is able to compute an EFprior allocation. In this paper, we mainly focus on general valuations. In particular, we present a polynomial-time algorithm that outputs an EFprior allocation with most of the items allocated. When all the items need to be allocated, we also present polynomial-time algorithms for some well-motivated special cases",
    "checked": true,
    "id": "7e2eb9aa34c5d218e195a5c9f7706a5be895081e",
    "semantic_title": "fair division with prioritized agents",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25689": {
    "title": "Topological Distance Games",
    "volume": "main",
    "abstract": "We introduce a class of strategic games in which agents are assigned to nodes of a topology graph and the utility of an agent depends on both the agent's inherent utilities for other agents as well as her distance from these agents on the topology graph. This model of topological distance games (TDGs) offers an appealing combination of important aspects of several prominent settings in coalition formation, including (additively separable) hedonic games, social distance games, and Schelling games. We study the existence and complexity of stable outcomes in TDGs—for instance, while a jump stable assignment may not exist in general, we show that the existence is guaranteed in several special cases. We also investigate the dynamics induced by performing beneficial jumps",
    "checked": true,
    "id": "31cc4da32ba25ce18bace942361fd8eff3fad994",
    "semantic_title": "topological distance games",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25690": {
    "title": "Game Implementation: What Are the Obstructions?",
    "volume": "main",
    "abstract": "In many applications, we want to influence the decisions of independent agents by designing incentives for their actions. We revisit a fundamental problem in this area, called GAME IMPLEMENTATION: Given a game in standard form and a set of desired strategies, can we design a set of payment promises such that if the players take the payment promises into account, then all undominated strategies are desired? Furthermore, we aim to minimize the cost, that is, the worst-case amount of payments. We study the tractability of computing such payment promises and determine more closely what obstructions we may have to overcome in doing so. We show that GAME IMPLEMENTATION is NP-hard even for two players, solving in particular a long-standing open question and suggesting more restrictions are necessary to obtain tractability results. We thus study the regime in which players have only a small constant number of strategies and obtain the following. First, this case remains NP-hard even if each player's utility depends only on three others. Second, we repair a flawed efficient algorithm for the case of both small number of strategies and small number of players. Among further results, we characterize sets of desired strategies that can be implemented at zero cost as a generalization of Nash equilibria",
    "checked": true,
    "id": "bbe7f2e5001559daa2f06b99c7cf5456966953e5",
    "semantic_title": "game implementation: what are the obstructions?",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25691": {
    "title": "A Pair-Approximation Method for Modelling the Dynamics of Multi-Agent Stochastic Games",
    "volume": "main",
    "abstract": "Developing a dynamical model for learning in games has attracted much recent interest. In stochastic games, agents need to make decisions in multiple states, and transitions between states, in turn, influence the dynamics of strategies. While previous works typically focus either on 2-agent stochastic games or on normal form games under an infinite-agent setting, we aim at formally modelling the learning dynamics in stochastic games under the infinite-agent setting. With a novel use of pair-approximation method, we develop a formal model for myopic Q-learning in stochastic games with symmetric state transition. We verify the descriptive power of our model (a partial differential equation) across various games through comparisons with agent-based simulation results. Based on our proposed model, we can gain qualitative and quantitative insights into the influence of transition probabilities on the dynamics of strategies. In particular, we illustrate that a careful design of transition probabilities can help players overcome the social dilemmas and promote cooperation, even if agents are myopic learners",
    "checked": true,
    "id": "ab178e4e4389cc2578617dec3186c313b4a35cb2",
    "semantic_title": "a pair-approximation method for modelling the dynamics of multi-agent stochastic games",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25692": {
    "title": "Complexity of Probabilistic Inference in Random Dichotomous Hedonic Games",
    "volume": "main",
    "abstract": "Hedonic games model cooperative games where agents desire to form coalitions, and only care about the composition of the coalitions of which they are members. Focusing on various classes of dichotomous hedonic games, where each agent either approves or disapproves a given coalition, we propose the random extension, where players have an independent participation probability. We initiate the research on the computational complexity of computing the probability that coalitions and partitions are optimal or stable. While some cases admit efficient algorithms (e.g., agents approve only few coalitions), they become computationally hard (#P-hard) in their complementary scenario. We then investigate the distribution of coalitions in perfect partitions and their performance in majority games, where an agent approves coalitions in which the agent is friends with the majority of its members. When friendships independently form with a constant probability, we prove that the number of coalitions of size 3 converges in distribution to a Poisson random variable",
    "checked": true,
    "id": "153d83340025647e8dc36945478cd4ff4eda043e",
    "semantic_title": "complexity of probabilistic inference in random dichotomous hedonic games",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25693": {
    "title": "Combinatorial Civic Crowdfunding with Budgeted Agents: Welfare Optimality at Equilibrium and Optimal Deviation",
    "volume": "main",
    "abstract": "Civic Crowdfunding (CC) uses the ``power of the crowd\" to garner contributions towards public projects. As these projects are non-excludable, agents may prefer to ``free-ride,\" resulting in the project not being funded. Researchers introduce refunds for single project CC to incentivize agents to contribute, guaranteeing the project's funding. These funding guarantees are applicable only when agents have an unlimited budget. This paper focuses on a combinatorial setting, where multiple projects are available for CC and agents have a limited budget. We study specific conditions where funding can be guaranteed. Naturally, funding the optimal social welfare subset of projects is desirable when every available project cannot be funded due to budget restrictions. We prove the impossibility of achieving optimal welfare at equilibrium for any monotone refund scheme. Further, given the contributions of other agents, we prove that it is NP-Hard for an agent to determine its optimal strategy. That is, while profitable deviations may exist for agents instead of funding the optimal welfare subset, it is computationally hard for an agent to find its optimal deviation. Consequently, we study different heuristics agents can use to contribute to the projects in practice. We demonstrate the heuristics' performance as the average-case trade-off between the welfare obtained and an agent's utility through simulations",
    "checked": true,
    "id": "7780bfd2cc56a38d0cf34edf88f7be79e9d25dd2",
    "semantic_title": "combinatorial civic crowdfunding with budgeted agents: welfare optimality at equilibrium and optimal deviation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25694": {
    "title": "Strategyproofness and Proportionality in Party-Approval Multiwinner Elections",
    "volume": "main",
    "abstract": "In party-approval multiwinner elections the goal is to allocate the seats of a fixed-size committee to parties based on the approval ballots of the voters over the parties. In particular, each voter can approve multiple parties and each party can be assigned multiple seats. Two central requirements in this setting are proportional representation and strategyproofness. Intuitively, proportional representation requires that every sufficiently large group of voters with similar preferences is represented in the committee. Strategyproofness demands that no voter can benefit by misreporting her true preferences. We show that these two axioms are incompatible for anonymous party-approval multiwinner voting rules, thus proving a far-reaching impossibility theorem. The proof of this result is obtained by formulating the problem in propositional logic and then letting a SAT solver show that the formula is unsatisfiable. Additionally, we demonstrate how to circumvent this impossibility by considering a weakening of strategyproofness which requires that only voters who do not approve any elected party cannot manipulate. While most common voting rules fail even this weak notion of strategyproofness, we characterize Chamberlin-Courant approval voting within the class of Thiele rules based on this strategyproofness notion",
    "checked": true,
    "id": "3f3c8d4ad8a89239bbd9b2d31a6c54cb396d3930",
    "semantic_title": "strategyproofness and proportionality in party-approval multiwinner elections",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25695": {
    "title": "Tight Inapproximability for Graphical Games",
    "volume": "main",
    "abstract": "We provide a complete characterization for the computational complexity of finding approximate equilibria in two-action graphical games. We consider the two most well-studied approximation notions: ε-Nash equilibria (ε-NE) and ε-well-supported Nash equilibria (ε-WSNE), where ε is in [0,1]. We prove that computing an ε-NE is PPAD-complete for any constant ε smaller than 1/2, while a very simple algorithm (namely, letting all players mix uniformly between their two actions) yields a 1/2-NE. On the other hand, we show that computing an ε-WSNE is PPAD-complete for any constant ε smaller than 1, while a 1-WSNE is trivial to achieve, because any strategy profile is a 1-WSNE. All of our lower bounds immediately also apply to graphical games with more than two actions per player",
    "checked": true,
    "id": "ca20eb26ec877f403db50049252511e47741c23f",
    "semantic_title": "tight inapproximability for graphical games",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25696": {
    "title": "From Monopoly to Competition: Optimal Contests Prevail",
    "volume": "main",
    "abstract": "We study competition among contests in a general model that allows for an arbitrary and heterogeneous space of contest design and symmetric contestants. The goal of the contest designers is to maximize the contestants' sum of efforts. Our main result shows that optimal contests in the monopolistic setting (i.e., those that maximize the sum of efforts in a model with a single contest) form an equilibrium in the model with competition among contests. Under a very natural assumption these contests are in fact dominant, and the equilibria that they form are unique. Moreover, equilibria with the optimal contests are Pareto-optimal even in cases where other equilibria emerge. In many natural cases, they also maximize the social welfare",
    "checked": true,
    "id": "3a0108111588f59d44689c7162e930fec1d1701d",
    "semantic_title": "from monopoly to competition: optimal contests prevail",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25697": {
    "title": "Commitment Games with Conditional Information Disclosure",
    "volume": "main",
    "abstract": "The conditional commitment abilities of mutually transparent computer agents have been studied in previous work on commitment games and program equilibrium. This literature has shown how these abilities can help resolve Prisoner's Dilemmas and other failures of cooperation in complete information settings. But inefficiencies due to private information have been neglected thus far in this literature, despite the fact that these problems are pervasive and might also be addressed by greater mutual transparency. In this work, we introduce a framework for commitment games with a new kind of conditional commitment device, which agents can use to conditionally disclose private information. We prove a folk theorem for this setting that provides sufficient conditions for ex post efficiency, and thus represents a model of ideal cooperation between agents without a third-party mediator. Further, extending previous work on program equilibrium, we develop an implementation of conditional information disclosure. We show that this implementation forms program ε-Bayesian Nash equilibria corresponding to the Bayesian Nash equilibria of these commitment games",
    "checked": true,
    "id": "54226bde1774d6a113ad871445775a84962aab35",
    "semantic_title": "commitment games with conditional information disclosure",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25698": {
    "title": "Rawlsian Fairness in Online Bipartite Matching: Two-Sided, Group, and Individual",
    "volume": "main",
    "abstract": "Online bipartite-matching platforms are ubiquitous and find applications in important areas such as crowdsourcing and ridesharing. In the most general form, the platform consists of three entities: two sides to be matched and a platform operator that decides the matching. The design of algorithms for such platforms has traditionally focused on the operator's (expected) profit. Since fairness has become an important consideration that was ignored in the existing algorithms a collection of online matching algorithms have been developed that give a fair treatment guarantee for one side of the market at the expense of a drop in the operator's profit. In this paper, we generalize the existing work to offer fair treatment guarantees to both sides of the market simultaneously, at a calculated worst case drop to operator profit. We consider group and individual Rawlsian fairness criteria. Moreover, our algorithms have theoretical guarantees and have adjustable parameters that can be tuned as desired to balance the trade-off between the utilities of the three sides. We also derive hardness results that give clear upper bounds over the performance of any algorithm",
    "checked": true,
    "id": "e31ff605e3db3449e0c999998365f6c95c2767e6",
    "semantic_title": "rawlsian fairness in online bipartite matching: two-sided, group, and individual",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25699": {
    "title": "Participatory Budgeting Designs for the Real World",
    "volume": "main",
    "abstract": "Participatory budgeting engages the public in the process of allocating public money to different types of projects. PB designs differ in how voters are asked to express their preferences over candidate projects and how these preferences are aggregated to determine which projects to fund. This paper studies two fundamental questions in PB design. Which voting format and aggregation method to use, and how to evaluate the outcomes of these design decisions? We conduct an extensive empirical study in which 1 800 participants vote in four participatory budgeting elections in a controlled setting to evaluate the practical effects of the choice of voting format and aggregation rule.We find that k-approval leads to the best user experience. With respect to the aggregation rule, greedy aggregation leads to outcomes that are highly sensitive to the input format used and the fraction of the population that participates. The method of equal shares, in contrast, leads to outcomes that are not sensitive to the type of voting format used, and these outcomes are remarkably stable even when the majority of the population does not participate in the election. These results carry valuable insights for PB practitioners and social choice researchers",
    "checked": true,
    "id": "183f37a03ca82e44a6468baa7e464c013e892011",
    "semantic_title": "participatory budgeting designs for the real world",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25700": {
    "title": "PAC Learning and Stabilizing Hedonic Games: Towards a Unifying Approach",
    "volume": "main",
    "abstract": "We study PAC learnability and PAC stabilizability of Hedonic Games (HGs), i.e., efficiently inferring preferences or core-stable partitions from samples. We first expand the known learnability/stabilizability landscape for some of the most prominent HGs classes, providing results for Friends and Enemies Games, Bottom Responsive, and Anonymous HGs. Then, having a broader view in mind, we attempt to shed light on the structural properties leading to learnability/stabilizability, or lack thereof, for specific HGs classes. Along this path, we focus on the fully expressive Hedonic Coalition Nets representation of HGs. We identify two sets of conditions that lead to efficient learnability, and which encompass all of the known positive learnability results. On the side of stability, we reveal that, while the freedom of choosing an ad hoc adversarial distribution is the most obvious hurdle to achieving PAC stability, it is not the only one. First, we show a distribution independent necessary condition for PAC stability. Then, we focus on W-games, where players have individual preferences over other players and evaluate coalitions based on the least preferred member. We prove that these games are PAC stabilizable under the class of bounded distributions, which assign positive probability mass to all coalitions. Finally, we discuss why such a result is not easily extendable to other HGs classes even in this promising scenario. Namely, we establish a purely computational property necessary for achieving PAC stability",
    "checked": true,
    "id": "27a17341f41c0af03606f43f56cba72b32ffbf94",
    "semantic_title": "pac learning and stabilizing hedonic games: towards a unifying approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25701": {
    "title": "Scalable Edge Blocking Algorithms for Defending Active Directory Style Attack Graphs",
    "volume": "main",
    "abstract": "Active Directory (AD) is the default security management system for Windows domain networks. An AD environment naturally describes an attack graph where nodes represent computers/accounts/security groups, and edges represent existing accesses/known exploits that allow the attacker to gain access from one node to another. Motivated by practical AD use cases, we study a Stackelberg game between one attacker and one defender. There are multiple entry nodes for the attacker to choose from and there is a single target (Domain Admin). Every edge has a failure rate. The attacker chooses the attack path with the maximum success rate. The defender can block a limited number of edges (i.e., revoke accesses) from a set of blockable edges, limited by budget. The defender's aim is to minimize the attacker's success rate. We exploit the tree-likeness of practical AD graphs to design scalable algorithms. We propose two novel methods that combine theoretical fixed parameter analysis and practical optimisation techniques. For graphs with small tree widths, we propose a tree decomposition based dynamic program. We then propose a general method for converting tree decomposition based dynamic programs to reinforcement learning environments, which leads to an anytime algorithm that scales better, but loses the optimality guarantee. For graphs with small numbers of non-splitting paths (a parameter we invent specifically for AD graphs), we propose a kernelization technique that significantly downsizes the model, which is then solved via mixed-integer programming. Experimentally, our algorithms scale to handle synthetic AD graphs with tens of thousands of nodes",
    "checked": true,
    "id": "7c67df2dc35e8d99dc419fbc138bbc1fab579f51",
    "semantic_title": "scalable edge blocking algorithms for defending active directory style attack graphs",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25702": {
    "title": "Representation with Incomplete Votes",
    "volume": "main",
    "abstract": "Platforms for online civic participation rely heavily on methods for condensing thousands of comments into a relevant handful, based on whether participants agree or disagree with them. These methods should guarantee fair representation of the participants, as their outcomes may affect the health of the conversation and inform impactful downstream decisions. To that end, we draw on the literature on approval-based committee elections. Our setting is novel in that the approval votes are incomplete since participants will typically not vote on all comments. We prove that this complication renders non-adaptive algorithms impractical in terms of the amount of information they must gather. Therefore, we develop an adaptive algorithm that uses information more efficiently by presenting incoming participants with statements that appear promising based on votes by previous participants. We prove that this method satisfies commonly used notions of fair representation, even when participants only vote on a small fraction of comments. Finally, an empirical evaluation using real data shows that the proposed algorithm provides representative outcomes in practice",
    "checked": true,
    "id": "5ae0490ac110effedd47d13eeb1e71d33f31da47",
    "semantic_title": "representation with incomplete votes",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25703": {
    "title": "Optimizing Multiple Simultaneous Objectives for Voting and Facility Location",
    "volume": "main",
    "abstract": "We study the classic facility location setting, where we are given n clients and m possible facility locations in some arbitrary metric space, and want to choose a location to build a facility. The exact same setting also arises in spatial social choice, where voters are the clients and the goal is to choose a candidate or outcome, with the distance from a voter to an outcome representing the cost of this outcome for the voter (e.g., based on their ideological differences). Unlike most previous work, we do not focus on a single objective to optimize (e.g., the total distance from clients to the facility, or the maximum distance, etc.), but instead attempt to optimize several different objectives simultaneously. More specifically, we consider the l-centrum family of objectives, which includes the total distance, max distance, and many others. We present tight bounds on how well any pair of such objectives (e.g., max and sum) can be simultaneously approximated compared to their optimum outcomes. In particular, we show that for any such pair of objectives, it is always possible to choose an outcome which simultaneously approximates both objectives within a factor of 1 plus square root of 2, and give a precise characterization of how this factor improves as the two objectives being optimized become more similar. For q>2 different centrum objectives, we show that it is always possible to approximate all q of these objectives within a small constant, and that this constant approaches 3 as q increases. Our results show that when optimizing only a few simultaneous objectives, it is always possible to form an outcome which is a significantly better than 3 approximation for all of these objectives",
    "checked": true,
    "id": "e0cf0244be1a0ed9d761742614adec024bfa3469",
    "semantic_title": "optimizing multiple simultaneous objectives for voting and facility location",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25704": {
    "title": "Class Fairness in Online Matching",
    "volume": "main",
    "abstract": "We initiate the study of fairness among classes of agents in online bipartite matching where there is a given set of offline vertices (aka agents) and another set of vertices (aka items) that arrive online and must be matched irrevocably upon arrival. In this setting, agents are partitioned into a set of classes and the matching is required to be fair with respect to the classes. We adopt popular fairness notions (e.g. envy-freeness, proportionality, and maximin share) and their relaxations to this setting and study deterministic and randomized algorithms for matching indivisible items (leading to integral matchings) and for matching divisible items (leading to fractional matchings). For matching indivisible items, we propose an adaptive-priority-based algorithm, MATCH-AND-SHIFT, prove that it achieves (1/2)-approximation of both class envy-freeness up to one item and class maximin share fairness, and show that each guarantee is tight. For matching divisible items, we design a water-filling-based algorithm, EQUAL-FILLING, that achieves (1-1/e)-approximation of class envy-freeness and class proportionality; we prove (1-1/e) to be tight for class proportionality and establish a 3/4 upper bound on class envy-freeness",
    "checked": true,
    "id": "462c81b0c6b03df11b4bfa0f0345e100e3b33fbc",
    "semantic_title": "class fairness in online matching",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25705": {
    "title": "How to Cut a Discrete Cake Fairly",
    "volume": "main",
    "abstract": "Cake-cutting is a fundamental model of dividing a heterogeneous resource, such as land, broadcast time, and advertisement space. In this study, we consider the problem of dividing indivisible goods fairly under the connectivity constraints of a path. We prove that a connected division of indivisible items satisfying a discrete counterpart of envy-freeness, called envy-freeness up to one good (EF1), always exists for any number of agents n with monotone valuations. Our result settles an open question raised by Bilò et al. (2019), who proved that an EF1 connected division always exists for four agents with monotone valuations. Moreover, the proof can be extended to show the following (1) ``secretive\" and (2) ``extra\" versions: (1) for n agents with monotone valuations, the path can be divided into n connected bundles such that an EF1 assignment of the remaining bundles can be made to the other agents for any selection made by the \"secretive agent\"; (2) for n+1 agents with monotone valuations, the path can be divided into n connected bundles such that when any ``extra agent\" leaves, an EF1 assignment of the bundles can be made to the remaining agents",
    "checked": true,
    "id": "555d80712ae5fcef4b319c3ba0265171cad33768",
    "semantic_title": "how to cut a discrete cake fairly",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25706": {
    "title": "Competition, Alignment, and Equilibria in Digital Marketplaces",
    "volume": "main",
    "abstract": "Competition between traditional platforms is known to improve user utility by aligning the platform's actions with user preferences. But to what extent is alignment exhibited in data-driven marketplaces? To study this question from a theoretical perspective, we introduce a duopoly market where platform actions are bandit algorithms and the two platforms compete for user participation. A salient feature of this market is that the quality of recommendations depends on both the bandit algorithm and the amount of data provided by interactions from users. This interdependency between the algorithm performance and the actions of users complicates the structure of market equilibria and their quality in terms of user utility. Our main finding is that competition in this market does not perfectly align market outcomes with user utility. Interestingly, market outcomes exhibit misalignment not only when the platforms have separate data repositories, but also when the platforms have a shared data repository. Nonetheless, the data sharing assumptions impact what mechanism drives misalignment and also affect the specific form of misalignment (e.g. the quality of the best-case and worst-case market outcomes). More broadly, our work illustrates that competition in digital marketplaces has subtle consequences for user utility that merit further investigation",
    "checked": true,
    "id": "9a2b3696dab23be25c8cda81490c54c5e6ea53c6",
    "semantic_title": "competition, alignment, and equilibria in digital marketplaces",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25707": {
    "title": "Voting with Preference Intensities",
    "volume": "main",
    "abstract": "When an agent votes, she typically ranks the set of available alternatives. Occasionally, she may also wish to report the intensity of her preferences by indicating adjacent pairs of alternatives in her ranking between which her preference is acutely decisive; for instance, she may suggest that she likes alternative a more than b, but b much more than c. We design near-optimal voting rules which aggregate such preference rankings with intensities using the recently-popular distortion framework. We also show that traditional voting rules, which aggregate preference rankings while ignoring (or not eliciting) intensities, can incur significant welfare loss",
    "checked": true,
    "id": "0f161bcb249a2ce6a2c97b5d4a93184d89ff39fa",
    "semantic_title": "voting with preference intensities",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25708": {
    "title": "Approximations for Indivisible Concave Allocations with Applications to Nash Welfare Maximization",
    "volume": "main",
    "abstract": "We study a general allocation setting where agent valuations are concave additive. In this model, a collection of items must be uniquely distributed among a set of agents, where each agent-item pair has a specified utility. The objective is to maximize the sum of agent valuations, each of which is an arbitrary non-decreasing concave function of the agent's total additive utility. This setting was studied by Devanur and Jain (STOC 2012) in the online setting for divisible items. In this paper, we obtain both multiplicative and additive approximations in the offline setting for indivisible items. Our approximations depend on novel parameters that measure the local multiplicative/additive curvatures of each agent valuation, which we show correspond directly to the integrality gap of the natural assignment convex program of the problem. Furthermore, we extend our additive guarantees to obtain constant multiplicative approximations for Asymmetric Nash Welfare Maximization when agents have smooth valuations. This algorithm also yields an interesting tatonnement-style interpretation, where agents adjust uniform prices and items are assigned according to maximum weighted bang-per-buck ratios",
    "checked": true,
    "id": "bd29605b3a91f1d1166a42514a5c96a6d577ccdd",
    "semantic_title": "approximations for indivisible concave allocations with applications to nash welfare maximization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25709": {
    "title": "Strategic Facility Location with Clients That Minimize Total Waiting Time",
    "volume": "main",
    "abstract": "We study a non-cooperative two-sided facility location game in which facilities and clients behave strategically. This is in contrast to many other facility location games in which clients simply visit their closest facility. Facility agents select a location on a graph to open a facility to attract as much purchasing power as possible, while client agents choose which facilities to patronize by strategically distributing their purchasing power in order to minimize their total waiting time. Here, the waiting time of a facility depends on its received total purchasing power. We show that our client stage is an atomic splittable congestion game, which implies existence, uniqueness and efficient computation of a client equilibrium. Therefore, facility agents can efficiently predict client behavior and make strategic decisions accordingly. Despite that, we prove that subgame perfect equilibria do not exist in all instances of this game and that their existence is NP-hard to decide. On the positive side, we provide a simple and efficient algorithm to compute 3-approximate subgame perfect equilibria",
    "checked": true,
    "id": "a22b3b66794a0d2e06f57753b555b82ee006e591",
    "semantic_title": "strategic facility location with clients that minimize total waiting time",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25710": {
    "title": "Proportional Decisions in Perpetual Voting",
    "volume": "main",
    "abstract": "Perpetual voting is a framework for long-term collective decision making. In this framework, we consider a sequence of subsequent approval-based elections and try to achieve a fair overall outcome. To achieve fairness over time, perpetual voting rules take the history of previous decisions into account and identify voters that were dissatisfied with previous decisions. In this paper, we look at perpetual voting rules from an axiomatic perspective. First, we define two classes of perpetual voting rules that are particularly easy to explain to voters and explore the bounds imposed by this simplicity. Second, we study proportionality in the perpetual setting and identify two rules with strong proportionality guarantees. However, both rules yield different guarantees and we prove them to be incompatible with each other",
    "checked": true,
    "id": "a21a5610157afb24f4a3229f31adea835f507ca2",
    "semantic_title": "proportional decisions in perpetual voting",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25711": {
    "title": "Multiagent MST Cover: Pleasing All Optimally via a Simple Voting Rule",
    "volume": "main",
    "abstract": "Given a connected graph on whose edges we can build roads to connect the nodes, a number of agents hold possibly different perspectives on which edges should be selected by assigning different edge weights. Our task is to build a minimum number of roads so that every agent has a spanning tree in the built subgraph whose weight is the same as a minimum spanning tree in the original graph. We first show that this problem is NP-hard and does not admit better than ((1-o(1)) ln k)-approximation polynomial-time algorithms unless P = NP, where k is the number of agents. We then give a simple voting algorithm with an optimal approximation ratio. Moreover, our algorithm only needs to access the agents' rankings on the edges. Finally, we extend our problem to submodular objective functions and Matroid rank constraints",
    "checked": true,
    "id": "c8e78fea7189e12c1778fc110852253b0f1ccca4",
    "semantic_title": "multiagent mst cover: pleasing all optimally via a simple voting rule",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25712": {
    "title": "When Congestion Games Meet Mobile Crowdsourcing: Selective Information Disclosure",
    "volume": "main",
    "abstract": "In congestion games, users make myopic routing decisions to jam each other, and the social planner with the full information designs mechanisms on information or payment side to regulate. However, it is difficult to obtain time-varying traffic conditions, and emerging crowdsourcing platforms (e.g., Waze and Google Maps) provide a convenient way for mobile users travelling on the paths to learn and share the traffic conditions over time. When congestion games meet mobile crowdsourcing, it is critical to incentive selfish users to change their myopic routing policy and reach the best exploitation-exploration trade-off. By considering a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for atomic users, we prove that the myopic routing policy's price of anarchy (PoA) can be arbitrarily large as the discount factor approaches 1. To remedy such huge efficiency loss, we propose a selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore the stochastic paths, while hiding such information when they want to under-explore. We prove that our mechanism reduces PoA to less than 2. Besides the worst-case performance, we further examine our mechanism's average-case performance by using extensive simulations",
    "checked": true,
    "id": "2310aaa5d2e3cf9d47b1168cd4a560a144cbf28a",
    "semantic_title": "when congestion games meet mobile crowdsourcing: selective information disclosure",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25713": {
    "title": "Partitioning Friends Fairly",
    "volume": "main",
    "abstract": "We consider the problem of partitioning n agents in an undirected social network into k almost equal in size (differing by at most one) groups, where the utility of an agent for a group is the number of her neighbors in the group. The core and envy-freeness are two compelling axiomatic fairness guarantees in such settings. The former demands that there be no coalition of agents such that each agent in the coalition has more utility for that coalition than for her own group, while the latter demands that no agent envy another agent for the group they are in. We provide (often tight) approximations to both fairness guarantees, and many of our positive results are obtained via efficient algorithms",
    "checked": true,
    "id": "1fdc1d82b449862d77dbaf26b4d007c80cd74434",
    "semantic_title": "partitioning friends fairly",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25714": {
    "title": "Differentially Private Condorcet Voting",
    "volume": "main",
    "abstract": "Designing private voting rules is an important and pressing problem for trustworthy democracy. In this paper, under the framework of differential privacy, we propose a novel famliy of randomized voting rules based on the well-known Condorcet method, and focus on three classes of voting rules in this family: Laplacian Condorcet method (CMLAP), exponential Condorcet method (CMEXP), and randomized response Condorcet method (CMRR), where λ represents the level of noise. We prove that all of our rules satisfy absolute monotonicity, lexi-participation, probabilistic Pareto efficiency, approximate probabilistic Condorcet criterion, and approximate SD-strategyproofness. In addition, CMRR satisfies (non-approximate) probabilistic Condorcet criterion, while CMLAP and CMEXP satisfy strong lexi-participation. Finally, we regard differential privacy as a voting axiom, and discuss its relations to other axioms",
    "checked": true,
    "id": "31d165e97e8e2c13fe3b3dc55e0e338491633b9a",
    "semantic_title": "differentially private condorcet voting",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25715": {
    "title": "Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games",
    "volume": "main",
    "abstract": "Function approximation (FA) has been a critical component in solving large zero-sum games. Yet, little attention has been given towards FA in solving general-sum extensive-form games, despite them being widely regarded as being computationally more challenging than their fully competitive or cooperative counterparts. A key challenge is that for many equilibria in general-sum games, no simple analogue to the state value function used in Markov Decision Processes and zero-sum games exists. In this paper, we propose learning the Enforceable Payoff Frontier (EPF)---a generalization of the state value function for general-sum games. We approximate the optimal Stackelberg extensive-form correlated equilibrium by representing EPFs with neural networks and training them by using appropriate backup operations and loss functions. This is the first method that applies FA to the Stackelberg setting, allowing us to scale to much larger games while still enjoying performance guarantees based on FA error. Additionally, our proposed method guarantees incentive compatibility and is easy to evaluate without having to depend on self-play or approximate best-response oracles",
    "checked": true,
    "id": "9f8ff18066b6de9249c76a23d4a8d265bc08ce5b",
    "semantic_title": "function approximation for solving stackelberg equilibrium in large perfect information games",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25716": {
    "title": "Optimal Pricing Schemes for Identical Items with Time-Sensitive Buyers",
    "volume": "main",
    "abstract": "Time or money? That is a question! In this paper, we consider this dilemma in the pricing regime, in which we try to find the optimal pricing scheme for identical items with heterogenous time-sensitive buyers. We characterize the revenue-optimal solution and propose an efficient algorithm to find it in a Bayesian setting. Our results also demonstrate the tight ratio between the value of wasted time and the seller's revenue, as well as that of two common-used pricing schemes, the k-step function and the fixed pricing. To explore the nature of the optimal scheme in the general setting, we present the closed forms over the product distribution and show by examples that positive correlation between the valuation of the item and the cost per unit time could help increase revenue. To the best of our knowledge, it is the first step towards understanding the impact of the time factor as a part of the buyer cost in pricing problems, in the computational view",
    "checked": true,
    "id": "9ed19cfe4e0dcad4e45f6fe90fe62875dd851378",
    "semantic_title": "optimal pricing schemes for identical items with time-sensitive buyers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25717": {
    "title": "Approval-Based Voting with Mixed Goods",
    "volume": "main",
    "abstract": "We consider a voting scenario in which the resource to be voted upon may consist of both indivisible and divisible goods. This generalizes both the well-studied model of multiwinner voting and the recently introduced model of cake sharing. Under approval votes, we propose two variants of the extended justified representation (EJR) notion from multiwinner voting, a stronger one called EJR for mixed goods (EJR-M) and a weaker one called EJR up to 1 (EJR-1). We extend three multiwinner voting rules to our setting—GreedyEJR, the method of equal shares (MES), and proportional approval voting (PAV)—and show that while all three generalizations satisfy EJR-1, only the first one provides EJR-M. In addition, we derive tight bounds on the proportionality degree implied by EJR-M and EJR-1, and investigate the proportionality degree of our proposed rules",
    "checked": true,
    "id": "4ddf61206194976246e3498899a3e56c0623f606",
    "semantic_title": "approval-based voting with mixed goods",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25718": {
    "title": "Utility Maximizer or Value Maximizer: Mechanism Design for Mixed Bidders in Online Advertising",
    "volume": "main",
    "abstract": "Digital advertising constitutes one of the main revenue sources for online platforms. In recent years, some advertisers tend to adopt auto-bidding tools to facilitate advertising performance optimization, making the classical utility maximizer model in auction theory not fit well. Some recent studies proposed a new model, called value maximizer, for auto-bidding advertisers with return-on-investment (ROI) constraints. However, the model of either utility maximizer or value maximizer could only characterize partial advertisers in real-world advertising platforms. In a mixed environment where utility maximizers and value maximizers coexist, the truthful ad auction design would be challenging since bidders could manipulate both their values and affiliated classes, leading to a multi-parameter mechanism design problem. In this work, we address this issue by proposing a payment rule which combines the corresponding ones in classical VCG and GSP mechanisms in a novel way. Based on this payment rule, we propose a truthful auction mechanism with an approximation ratio of 2 on social welfare, which is close to the lower bound of at least 5/4 that we also prove. The designed auction mechanism is a generalization of VCG for utility maximizers and GSP for value maximizers",
    "checked": true,
    "id": "ed2c8a88b17172d3169a5b86fc83e05807639b4b",
    "semantic_title": "utility maximizer or value maximizer: mechanism design for mixed bidders in online advertising",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25719": {
    "title": "Facility Location Games with Entrance Fees",
    "volume": "main",
    "abstract": "The facility location game is an extensively studied problem in mechanism design. In the classical model, the cost of each agent is her distance to the nearest facility. In this paper, we consider a novel model where each facility charges an entrance fee, which is a function of the facility's location. Thus, in our model, the cost of each agent is the sum of the distance to the facility and the entrance fee of the facility. The generalized model captures more real-life scenarios. In our model, the entrance fee function can be an arbitrary function, and the corresponding preferences of agents may not be single-peaked anymore: this makes the problem complex and requires new techniques in the analysis. We systematically study the model and design strategyproof mechanisms with nice approximation ratios and also complement these with nearly-tight impossibility results. Specifically, for one-facility and two-facility games, we provide upper and lower bounds for the approximation ratios given by deterministic and randomized mechanisms, with respect to the utilitarian and egalitarian objectives. Most of our bounds are tight, and these bounds are independent of the entrance fee functions. Our results also match the results of the classical model",
    "checked": true,
    "id": "3fc4bde395cfbcf0c3b3ddb0c47fd41d696a9d2e",
    "semantic_title": "facility location games with entrance fees",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25720": {
    "title": "Securing Lifelines: Safe Delivery of Critical Services in Areas with Volatile Security Situation via a Stackelberg Game Approach",
    "volume": "main",
    "abstract": "Vaccine delivery in under-resourced locations with security risks is not just challenging but also life threatening. The COVID pandemic and the need to vaccinate added even more urgency to this issue. Motivated by this problem, we propose a general framework to set-up limited temporary (vaccination) centers that balance physical security and desired (vaccine) service coverage with limited resources. We set-up the problem as a Stackelberg game between the centers operator (defender) and an adversary, where the set of centers is not fixed a priori but is part of the decision output. This results in a mixed combinatorial and continuous optimization problem. As part of our scalable approximation solution, we provide a fundamental contribution by identifying general duality conditions of switching max and min when both discrete and continuous variables are involved. Via detailed experiments, we show that the solution proposed is scalable in practice",
    "checked": true,
    "id": "9222a8fd12e782e07425db00b085a52824fa62d2",
    "semantic_title": "securing lifelines: safe delivery of critical services in areas with volatile security situation via a stackelberg game approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25721": {
    "title": "Differentially Private Fair Division",
    "volume": "main",
    "abstract": "Fairness and privacy are two important concerns in social decision-making processes such as resource allocation. We study privacy in the fair allocation of indivisible resources using the well-established framework of differential privacy. We present algorithms for approximate envy-freeness and proportionality when two instances are considered to be adjacent if they differ only on the utility of a single agent for a single item. On the other hand, we provide strong negative results for both fairness criteria when the adjacency notion allows the entire utility function of a single agent to change",
    "checked": true,
    "id": "3828d5639e3967d199a0cd6d8a5c2aa434a30ba1",
    "semantic_title": "differentially private fair division",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25722": {
    "title": "An Efficient Deep Reinforcement Learning Algorithm for Solving Imperfect Information Extensive-Form Games",
    "volume": "main",
    "abstract": "One of the most popular methods for learning Nash equilibrium (NE) in large-scale imperfect information extensive-form games (IIEFGs) is the neural variants of counterfactual regret minimization (CFR). CFR is a special case of Follow-The-Regularized-Leader (FTRL). At each iteration, the neural variants of CFR update the agent's strategy via the estimated counterfactual regrets. Then, they use neural networks to approximate the new strategy, which incurs an approximation error. These approximation errors will accumulate since the counterfactual regrets at iteration t are estimated using the agent's past approximated strategies. Such accumulated approximation error causes poor performance. To address this accumulated approximation error, we propose a novel FTRL algorithm called FTRL-ORW, which does not utilize the agent's past strategies to pick the next iteration strategy. More importantly, FTRL-ORW can update its strategy via the trajectories sampled from the game, which is suitable to solve large-scale IIEFGs since sampling multiple actions for each information set is too expensive in such games. However, it remains unclear which algorithm to use to compute the next iteration strategy for FTRL-ORW when only such sampled trajectories are revealed at iteration t. To address this problem and scale FTRL-ORW to large-scale games, we provide a model-free method called Deep FTRL-ORW, which computes the next iteration strategy using model-free Maximum Entropy Deep Reinforcement Learning. Experimental results on two-player zero-sum IIEFGs show that Deep FTRL-ORW significantly outperforms existing model-free neural methods and OS-MCCFR",
    "checked": true,
    "id": "7674101bb85b2fc228d7a6c8b4eace078a8ba381",
    "semantic_title": "an efficient deep reinforcement learning algorithm for solving imperfect information extensive-form games",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25723": {
    "title": "Fast and Interpretable Dynamics for Fisher Markets via Block-Coordinate Updates",
    "volume": "main",
    "abstract": "We consider the problem of large-scale Fisher market equilibrium computation through scalable first-order optimization methods. It is well-known that market equilibria can be captured using structured convex programs such as the Eisenberg-Gale and Shmyrev convex programs. Highly performant deterministic full-gradient first-order methods have been developed for these programs. In this paper, we develop new block-coordinate first-order methods for computing Fisher market equilibria, and show that these methods have interpretations as tâtonnement-style or proportional response-style dynamics where either buyers or items show up one at a time. We reformulate these convex programs and solve them using proximal block coordinate descent methods, a class of methods that update only a small number of coordinates of the decision variable in each iteration. Leveraging recent advances in the convergence analysis of these methods and structures of the equilibrium-capturing convex programs, we establish fast convergence rates of these methods",
    "checked": true,
    "id": "92841069ddd8ea9c3d9a8e46121430540382aa72",
    "semantic_title": "fast and interpretable dynamics for fisher markets via block-coordinate updates",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25724": {
    "title": "Ballot Length in Instant Runoff Voting",
    "volume": "main",
    "abstract": "Instant runoff voting (IRV) is an increasingly-popular alternative to traditional plurality voting in which voters submit rankings over the candidates rather than single votes. In practice, elections using IRV often restrict the ballot length, the number of candidates a voter is allowed to rank on their ballot. We theoretically and empirically analyze how ballot length can influence the outcome of an election, given fixed voter preferences. We show that there exist preference profiles over k candidates such that up to k-1 different candidates win at different ballot lengths. We derive exact lower bounds on the number of voters required for such profiles and provide a construction matching the lower bound for unrestricted voter preferences. Additionally, we characterize which sequences of winners are possible over ballot lengths and provide explicit profile constructions achieving any feasible winner sequence. We also examine how classic preference restrictions influence our results—for instance, single-peakedness makes k-1 different winners impossible but still allows at least Ω(√k). Finally, we analyze a collection of 168 real-world elections, where we truncate rankings to simulate shorter ballots. We find that shorter ballots could have changed the outcome in one quarter of these elections. Our results highlight ballot length as a consequential degree of freedom in the design of IRV elections",
    "checked": true,
    "id": "73a36bafe6953371e71d22e011b6aff726e6afab",
    "semantic_title": "ballot length in instant runoff voting",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25725": {
    "title": "Multi-Stage Facility Location Problems with Transient Agents",
    "volume": "main",
    "abstract": "We study various models for the one-dimensional multi-stage facility location problems with transient agents, where a transient agent arrives in some stage and stays for a number of consecutive stages. In the problems, we need to serve each agent in one of their stages by determining the location of the facility at each stage. In the first model, we assume there is no cost for moving the facility across the stages. We focus on optimal algorithms to minimize both the social cost objective, defined as the total distance of all agents to the facility over all stages, and the maximum cost objective, defined as the max distance of any agent to the facility over all stages. For each objective, we give a slice-wise polynomial (XP) algorithm (i.e., solvable in m^f(k) for some fixed parameter k and computable function f, where m is the input size) and show that there is a polynomial-time algorithm when a natural first-come-first-serve (FCFS) order of agent serving is enforced. We then consider the mechanism design problem, where the agents' locations and arrival stages are private, and design a group strategy-proof mechanism that achieves good approximation ratios for both objectives and settings with and without FCFS ordering. In the second model, we consider the facility's moving cost between adjacent stages under the social cost objective, which accounts for the total moving distance of the facility. Correspondingly, we design XP (and polynomial time) algorithms and a group strategy-proof mechanism for settings with or without the FCFS ordering",
    "checked": true,
    "id": "dcc51879ceb777063f697f1f5ae0050d273932c7",
    "semantic_title": "multi-stage facility location problems with transient agents",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25726": {
    "title": "Bayesian Optimization-Based Combinatorial Assignment",
    "volume": "main",
    "abstract": "We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian optimization-based combinatorial assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism to properly explore (and not just exploit) the bundle space during its preference elicitation phase. We run computational experiments in several spectrum auction domains to evaluate BOCA's performance. Our results show that BOCA achieves higher allocative efficiency than state-of-the-art approaches",
    "checked": true,
    "id": "1ec31c7e532b18a5c1724a34689baba35e050c0e",
    "semantic_title": "bayesian optimization-based combinatorial assignment",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25727": {
    "title": "Semi-random Impossibilities of Condorcet Criterion",
    "volume": "main",
    "abstract": "The Condorcet criterion (CC) is a classical and well-accepted criterion for voting. Unfortunately, it is incompatible with many other desiderata including participation (PAR), half-way monotonicity (HM), Maskin monotonicity (MM), and strategy-proofness (SP). Such incompatibilities are often known as impossibility theorems, and are proved by worst-case analysis. Previous work has investigated the likelihood for these impossibilities to occur under certain models, which are often criticized of being unrealistic. We strengthen previous work by proving the first set of semi-random impossibilities for voting rules to satisfy CC and the more general, group versions of the four desiderata: for any sufficiently large number of voters n, any size of the group 1<= B<= \\sqrt n, any voting rule r, and under a large class of semi-random models that include Impartial Culture, the likelihood for r to satisfy CC and PAR, CC and HM, CC and MM, or CC and SP is 1-\\Omega(B/\\sqrt n). This matches existing lower bounds for CC&PAR (B=1) and CC&SP and CC&HM (B<=\\sqrt n), showing that many commonly-studied voting rules are already asymptotically optimal in such cases",
    "checked": true,
    "id": "e642adacb243f5448c31072e9c68f05905840848",
    "semantic_title": "semi-random impossibilities of condorcet criterion",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25728": {
    "title": "Tournament Fixing Parameterized by Feedback Vertex Set Number Is FPT",
    "volume": "main",
    "abstract": "A knockout (or single-elimination) tournament is a format of a competition that is very popular in practice (particularly in sports, elections and decision making), and which has been extensively and intensively studied from a theoretical point of view for more than a decade. Particular attention has been devoted to the Tournament Fixing problem, where, roughly speaking, the objective is to determine whether we can conduct the knockout tournament in a way that makes our favorite player win. Here, part of the input is a tournament graph D that encodes the winner of each possible match. A sequence of papers has studied the parameterized complexity of Tournament Fixing with respect to the feedback arc set number (fas) of D Given that this parameter yielded tractability, it has been asked explicitly and repeatedly whether Tournament Fixing is FPT also with respect to the feedback vertex set number (fvs) of D. We answer this question positively. In fact, although fvs can be arbitrarily smaller than fas, we attain the same dependency on the parameter in the time complexity. So, additionally, our work subsumes the best known algorithm for Tournament Fixing with respect to as",
    "checked": true,
    "id": "058d26b18a3b230e81fd4259aa2c903b01d93f1c",
    "semantic_title": "tournament fixing parameterized by feedback vertex set number is fpt",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25729": {
    "title": "Truthful Mechanisms for Steiner Tree Problems",
    "volume": "main",
    "abstract": "Consider an undirected graph G=(V,E) model for a communication network, where each edge is owned by a selfish agent, who reports the cost for offering the use of her edge. Note that each edge agent may misreport her own cost for the use of the edge for her own benefit. In such a non-cooperative setting, we aim at designing an approximately truthful mechanism for establishing a Steiner tree, a minimum cost tree spanning over all the terminals. We present a truthful-in-expectation mechanism that achieves the approximation ratio ln 4 + ε ≈ 1.39, which matches the current best algorithmic ratio for STP",
    "checked": true,
    "id": "6b2b230df548eef82aa4ed60aa6aa0af29fc0d42",
    "semantic_title": "truthful mechanisms for steiner tree problems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25730": {
    "title": "Collusion-Proof and Sybil-Proof Reward Mechanisms for Query Incentive Networks",
    "volume": "main",
    "abstract": "This paper explores reward mechanisms for a query incentive network in which agents seek information from social networks. In a query tree issued by the task owner, each agent is rewarded by the owner for contributing to the solution, for instance, solving the task or inviting others to solve it. The reward mechanism determines the reward for each agent and motivates all agents to propagate and report their information truthfully. In particular, the reward cannot exceed the budget set by the task owner. However, our impossibility results demonstrate that a reward mechanism cannot simultaneously achieve Sybil-proof (agents benefit from manipulating multiple fake identities), collusion-proof (multiple agents pretend as a single agent to improve the reward), and other essential properties. In order to address these issues, we propose two novel reward mechanisms. The first mechanism achieves Sybil-proof and collusion-proof, respectively; the second mechanism sacrifices Sybil-proof to achieve the approximate versions of Sybil-proof and collusion-proof. Additionally, we show experimentally that our second reward mechanism outperforms the existing ones",
    "checked": true,
    "id": "8459e8869b782ded45ef762d728642d3919ae075",
    "semantic_title": "collusion-proof and sybil-proof reward mechanisms for query incentive networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25731": {
    "title": "Fisher Markets with Social Influence",
    "volume": "main",
    "abstract": "A Fisher market is an economic model of buyer and seller interactions in which each buyer's utility depends only on the bundle of goods she obtains. Many people's interests, however, are affected by their social interactions with others. In this paper, we introduce a generalization of Fisher markets, namely influence Fisher markets, which captures the impact of social influence on buyers' utilities. We show that competitive equilibria in influence Fisher markets correspond to generalized Nash equilibria in an associated pseudo-game, which implies the existence of competitive equilibria in all influence Fisher markets with continuous and concave utility functions. We then construct a monotone pseudo-game, whose variational equilibria and their duals together characterize competitive equilibria in influence Fisher markets with continuous, jointly concave, and homogeneous utility functions. This observation implies that competitive equilibria in these markets can be computed in polynomial time under standard smoothness assumptions on the utility functions. The dual of this second pseudo-game enables us to interpret the competitive equilibria of influence CCH Fisher markets as the solutions to a system of simultaneous Stackelberg games. Finally, we derive a novel first-order method that solves this Stackelberg system in polynomial time, prove that it is equivalent to computing competitive equilibrium prices via tâtonnement, and run experiments that confirm our theoretical results",
    "checked": true,
    "id": "189e54a3a2e519bfaa65e33b1d3a35fa0ba2122a",
    "semantic_title": "fisher markets with social influence",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25732": {
    "title": "Probably Approximate Shapley Fairness with Applications in Machine Learning",
    "volume": "main",
    "abstract": "The Shapley value (SV) is adopted in various scenarios in machine learning (ML), including data valuation, agent valuation, and feature attribution, as it satisfies their fairness requirements. However, as exact SVs are infeasible to compute in practice, SV estimates are approximated instead. This approximation step raises an important question: do the SV estimates preserve the fairness guarantees of exact SVs? We observe that the fairness guarantees of exact SVs are too restrictive for SV estimates. Thus, we generalise Shapley fairness to probably approximate Shapley fairness and propose fidelity score, a metric to measure the variation of SV estimates, that determines how probable the fairness guarantees hold. Our last theoretical contribution is a novel greedy active estimation (GAE) algorithm that will maximise the lowest fidelity score and achieve a better fairness guarantee than the de facto Monte-Carlo estimation. We empirically verify GAE outperforms several existing methods in guaranteeing fairness while remaining competitive in estimation accuracy in various ML scenarios using real-world datasets",
    "checked": true,
    "id": "a6a83c55042d4c48b4cd17b37b8a6991f69d51e6",
    "semantic_title": "probably approximate shapley fairness with applications in machine learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25733": {
    "title": "The Perils of Trial-and-Error Reward Design: Misdesign through Overfitting and Invalid Task Specifications",
    "volume": "main",
    "abstract": "In reinforcement learning (RL), a reward function that aligns exactly with a task's true performance metric is often necessarily sparse. For example, a true task metric might encode a reward of 1 upon success and 0 otherwise. The sparsity of these true task metrics can make them hard to learn from, so in practice they are often replaced with alternative dense reward functions. These dense reward functions are typically designed by experts through an ad hoc process of trial and error. In this process, experts manually search for a reward function that improves performance with respect to the task metric while also enabling an RL algorithm to learn faster. This process raises the question of whether the same reward function is optimal for all algorithms, i.e., whether the reward function can be overfit to a particular algorithm. In this paper, we study the consequences of this wide yet unexamined practice of trial-and-error reward design. We first conduct computational experiments that confirm that reward functions can be overfit to learning algorithms and their hyperparameters. We then conduct a controlled observation study which emulates expert practitioners' typical experiences of reward design, in which we similarly find evidence of reward function overfitting. We also find that experts' typical approach to reward design---of adopting a myopic strategy and weighing the relative goodness of each state-action pair---leads to misdesign through invalid task specifications, since RL algorithms use cumulative reward rather than rewards for individual state-action pairs as an optimization target. Code, data: github.com/serenabooth/reward-design-perils",
    "checked": true,
    "id": "688fc1e744877c3a68f306443042f016196ce98a",
    "semantic_title": "the perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25734": {
    "title": "The Value of AI Guidance in Human Examination of Synthetically-Generated Faces",
    "volume": "main",
    "abstract": "Face image synthesis has progressed beyond the point at which humans can effectively distinguish authentic faces from synthetically-generated ones. Recently developed synthetic face image detectors boast ``better-than-human'' discriminative ability, especially those guided by human perceptual intelligence during the model's training process. In this paper, we investigate whether these human-guided synthetic face detectors can assist non-expert human operators in the task of synthetic image detection when compared to models trained without human-guidance. We conducted a large-scale experiment with more than 1,560 subjects classifying whether an image shows an authentic or synthetically-generated face, and annotating regions supporting their decisions. In total, 56,015 annotations across 3,780 unique face images were collected. All subjects first examined samples without any AI support, followed by samples given (a) the AI's decision (``synthetic'' or ``authentic''), (b) class activation maps illustrating where the model deems salient for its decision, or (c) both the AI's decision and AI's saliency map. Synthetic faces were generated with six modern Generative Adversarial Networks. Interesting observations from this experiment include: (1) models trained with human-guidance, which are also more accurate in our experiments, offer better support to human examination of face images when compared to models trained traditionally using cross-entropy loss, (2) binary decisions presented to humans results in their better performance than when saliency maps are presented, (3) understanding the AI's accuracy helps humans to increase trust in a given model and thus increase their overall accuracy. This work demonstrates that although humans supported by machines achieve better-than-random accuracy of synthetic face detection, the approaches of supplying humans with AI support and of building trust are key factors determining high effectiveness of the human-AI tandem",
    "checked": true,
    "id": "f02ec8381a96bf00f691fdddc28bc6dfbb8a5780",
    "semantic_title": "the value of ai guidance in human examination of synthetically-generated faces",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25735": {
    "title": "Teaching to Learn: Sequential Teaching of Learners with Internal States",
    "volume": "main",
    "abstract": "In sequential machine teaching, a teacher's objective is to provide the optimal sequence of inputs to sequential learners in order to guide them towards the best model. However, this teaching objective considers a restricted class of learners with fixed inductive biases. In this paper, we extend the machine teaching framework to learners that can improve their inductive biases, represented as latent internal states, in order to generalize to new datasets. We introduce a novel framework in which learners' inductive biases may change with the teaching interaction, which affects the learning performance in future tasks. In order to teach such learners, we propose a multi-objective control approach that takes the future performance of the learner after teaching into account. This framework provides tools for modelling learners with internal states, humans and meta-learning algorithms alike. Furthermore, we distinguish manipulative teaching, which can be done by effectively hiding data and also used for indoctrination, from teaching to learn which aims to help the learner become better at learning from new datasets in the absence of a teacher. Our empirical results demonstrate that our framework is able to reduce the number of required tasks for online meta-learning, and increases independent learning performance of simulated human users in future tasks",
    "checked": true,
    "id": "1fa00fa4c018d90f2597d9e5b9a5fd2ee17896f9",
    "semantic_title": "teaching to learn: sequential teaching of learners with internal states",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25736": {
    "title": "Interactive Concept Bottleneck Models",
    "volume": "main",
    "abstract": "Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets",
    "checked": true,
    "id": "5cc7508c4168e8583a9971115117b552479c5f24",
    "semantic_title": "interactive concept bottleneck models",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25737": {
    "title": "Local Justice and Machine Learning: Modeling and Inferring Dynamic Ethical Preferences toward Allocations",
    "volume": "main",
    "abstract": "We consider a setting in which a social planner has to make a sequence of decisions to allocate scarce resources in a high-stakes domain. Our goal is to understand stakeholders' dynamic moral preferences toward such allocational policies. In particular, we evaluate the sensitivity of moral preferences to the history of allocations and their perceived future impact on various socially salient groups. We propose a mathematical model to capture and infer such dynamic moral preferences. We illustrate our model through small-scale human-subject experiments focused on the allocation of scarce medical resource distributions during a hypothetical viral epidemic. We observe that participants' preferences are indeed history- and impact-dependent. Additionally, our preliminary experimental results reveal intriguing patterns specific to medical resources---a topic that is particularly salient against the backdrop of the global covid-19 pandemic",
    "checked": true,
    "id": "6dc75e20e8146182e7076450841bbb8c2f70935a",
    "semantic_title": "local justice and machine learning: modeling and inferring dynamic ethical preferences toward allocations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25738": {
    "title": "Extracting Semantic-Dynamic Features for Long-Term Stable Brain Computer Interface",
    "volume": "main",
    "abstract": "Brain-computer Interface (BCI) builds a neural signal to the motor command pathway, which is a prerequisite for the realization of neural prosthetics. However, a long-term stable BCI suffers from the neural data drift across days while retraining the BCI decoder is expensive and restricts its application scenarios. Recent solutions of neural signal recalibration treat the continuous neural signals as discrete, which is less effective in temporal feature extraction. Inspired by the observation from biologists that low-dimensional dynamics could describe high-dimensional neural signals, we model the underlying neural dynamics and propose a semantic-dynamic feature that represents the semantics and dynamics in a shared feature space facilitating the BCI recalibration. Besides, we present the joint distribution alignment instead of the common used marginal alignment strategy, dealing with the various complex changes in neural data distribution. Our recalibration approach achieves state-of-the-art performance on the real neural data of two monkeys in both classification and regression tasks. Our approach is also evaluated on a simulated dataset, which indicates its robustness in dealing with various common causes of neural signal instability",
    "checked": true,
    "id": "ae324c30938b1c7a871c2de5104cd5a780e52d71",
    "semantic_title": "extracting semantic-dynamic features for long-term stable brain computer interface",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25739": {
    "title": "Moral Machine or Tyranny of the Majority?",
    "volume": "main",
    "abstract": "With artificial intelligence systems increasingly applied in consequential domains, researchers have begun to ask how AI systems ought to act in ethically charged situations where even humans lack consensus. In the Moral Machine project, researchers crowdsourced answers to \"Trolley Problems\" concerning autonomous vehicles. Subsequently, Noothigattu et al. (2018) proposed inferring linear functions that approximate each individual's preferences and aggregating these linear models by averaging parameters across the population. In this paper, we examine this averaging mechanism, focusing on fairness concerns and strategic effects. We investigate a simple setting where the population consists of two groups, the minority constitutes an α < 0.5 share of the population, and within-group preferences are homogeneous. Focusing on the fraction of contested cases where the minority group prevails, we make the following observations: (a) even when all parties report their preferences truthfully, the fraction of disputes where the minority prevails is less than proportionate in α; (b) the degree of sub-proportionality grows more severe as the level of disagreement between the groups increases; (c) when parties report preferences strategically, pure strategy equilibria do not always exist; and (d) whenever a pure strategy equilibrium exists, the majority group prevails 100% of the time. These findings raise concerns about stability and fairness of averaging as a mechanism for aggregating diverging voices. Finally, we discuss alternatives, including randomized dictatorship and median-based mechanisms",
    "checked": true,
    "id": "d655026a443d3de40ebacf61274e562ec05936ef",
    "semantic_title": "moral machine or tyranny of the majority?",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25740": {
    "title": "The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types",
    "volume": "main",
    "abstract": "When inferring reward functions from human behavior (be it demonstrations, comparisons, physical corrections, or e-stops), it has proven useful to model the human as making noisy-rational choices, with a \"rationality coefficient\" capturing how much noise or entropy we expect to see in the human behavior. Prior work typically sets the rationality level to a constant value, regardless of the type, or quality, of human feedback. However, in many settings, giving one type of feedback (e.g. a demonstration) may be much more difficult than a different type of feedback (e.g. answering a comparison query). Thus, we expect to see more or less noise depending on the type of human feedback. In this work, we advocate that grounding the rationality coefficient in real data for each feedback type, rather than assuming a default value, has a significant positive effect on reward learning. We test this in both simulated experiments and in a user study with real human feedback. We find that overestimating human rationality can have dire effects on reward learning accuracy and regret. We also find that fitting the rationality coefficient to human data enables better reward learning, even when the human deviates significantly from the noisy-rational choice model due to systematic biases. Further, we find that the rationality level affects the informativeness of each feedback type: surprisingly, demonstrations are not always the most informative---when the human acts very suboptimally, comparisons actually become more informative, even when the rationality level is the same for both. Ultimately, our results emphasize the importance and advantage of paying attention to the assumed human-rationality-level, especially when agents actively learn from multiple types of human feedback",
    "checked": true,
    "id": "e920f426eb32e64474b2a1176d97725f875dd82a",
    "semantic_title": "the effect of modeling human rationality level on learning rewards from multiple feedback types",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25741": {
    "title": "The Role of Heuristics and Biases during Complex Choices with an AI Teammate",
    "volume": "main",
    "abstract": "Behavioral scientists have classically documented aversion to algorithmic decision aids, from simple linear models to AI. Sentiment, however, is changing and possibly accelerating AI helper usage. AI assistance is, arguably, most valuable when humans must make complex choices. We argue that classic experimental methods used to study heuristics and biases are insufficient for studying complex choices made with AI helpers. We adapted an experimental paradigm designed for studying complex choices in such contexts. We show that framing and anchoring effects impact how people work with an AI helper and are predictive of choice outcomes. The evidence suggests that some participants, particularly those in a loss frame, put too much faith in the AI helper and experienced worse choice outcomes by doing so. The paradigm also generates computational modeling-friendly data allowing future studies of human-AI decision making",
    "checked": true,
    "id": "4006f4acbcd0f97b0f4763b1dc1404ad923cfabd",
    "semantic_title": "the role of heuristics and biases during complex choices with an ai teammate",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25742": {
    "title": "Learning to Defer with Limited Expert Predictions",
    "volume": "main",
    "abstract": "Recent research suggests that combining AI models with a human expert can exceed the performance of either alone. The combination of their capabilities is often realized by learning to defer algorithms that enable the AI to learn to decide whether to make a prediction for a particular instance or defer it to the human expert. However, to accurately learn which instances should be deferred to the human expert, a large number of expert predictions that accurately reflect the expert's capabilities are required—in addition to the ground truth labels needed to train the AI. This requirement shared by many learning to defer algorithms hinders their adoption in scenarios where the responsible expert regularly changes or where acquiring a sufficient number of expert predictions is costly. In this paper, we propose a three-step approach to reduce the number of expert predictions required to train learning to defer algorithms. It encompasses (1) the training of an embedding model with ground truth labels to generate feature representations that serve as a basis for (2) the training of an expertise predictor model to approximate the expert's capabilities. (3) The expertise predictor generates artificial expert predictions for instances not yet labeled by the expert, which are required by the learning to defer algorithms. We evaluate our approach on two public datasets. One with \"synthetically\" generated human experts and another from the medical domain containing real-world radiologists' predictions. Our experiments show that the approach allows the training of various learning to defer algorithms with a minimal number of human expert predictions. Furthermore, we demonstrate that even a small number of expert predictions per class is sufficient for these algorithms to exceed the performance the AI and the human expert can achieve individually",
    "checked": true,
    "id": "740ea8d58050d823ed4e6bdc89e7b136ada80b26",
    "semantic_title": "learning to defer with limited expert predictions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25743": {
    "title": "SWL-Adapt: An Unsupervised Domain Adaptation Model with Sample Weight Learning for Cross-User Wearable Human Activity Recognition",
    "volume": "main",
    "abstract": "In practice, Wearable Human Activity Recognition (WHAR) models usually face performance degradation on the new user due to user variance. Unsupervised domain adaptation (UDA) becomes the natural solution to cross-user WHAR under annotation scarcity. Existing UDA models usually align samples across domains without differentiation, which ignores the difference among samples. In this paper, we propose an unsupervised domain adaptation model with sample weight learning (SWL-Adapt) for cross-user WHAR. SWL-Adapt calculates sample weights according to the classification loss and domain discrimination loss of each sample with a parameterized network. We introduce the meta-optimization based update rule to learn this network end-to-end, which is guided by meta-classification loss on the selected pseudo-labeled target samples. Therefore, this network can fit a weighting function according to the cross-user WHAR task at hand, which is superior to existing sample differentiation rules fixed for special scenarios. Extensive experiments on three public WHAR datasets demonstrate that SWL-Adapt achieves the state-of-the-art performance on the cross-user WHAR task, outperforming the best baseline by an average of 3.1% and 5.3% in accuracy and macro F1 score, respectively",
    "checked": true,
    "id": "e5306c39f2d88c7d479fbd08ae630af7239b5e56",
    "semantic_title": "swl-adapt: an unsupervised domain adaptation model with sample weight learning for cross-user wearable human activity recognition",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25744": {
    "title": "Incentive-Boosted Federated Crowdsourcing",
    "volume": "main",
    "abstract": "Crowdsourcing is a favorable computing paradigm for processing computer-hard tasks by harnessing human intelligence. However, generic crowdsourcing systems may lead to privacy-leakage through the sharing of worker data. To tackle this problem, we propose a novel approach, called iFedCrowd (incentive-boosted Federated Crowdsourcing), to manage the privacy and quality of crowdsourcing projects. iFedCrowd allows participants to locally process sensitive data and only upload encrypted training models, and then aggregates the model parameters to build a shared server model to protect data privacy. To motivate workers to build a high-quality global model in an efficacy way, we introduce an incentive mechanism that encourages workers to constantly collect fresh data to train accurate client models and boosts the global model training. We model the incentive-based interaction between the crowdsourcing platform and participating workers as a Stackelberg game, in which each side maximizes its own profit. We derive the Nash Equilibrium of the game to find the optimal solutions for the two sides. Experimental results confirm that iFedCrowd can complete secure crowdsourcing projects with high quality and efficiency",
    "checked": true,
    "id": "f4368aa571b71165d19dc08ae1f589296207e144",
    "semantic_title": "incentive-boosted federated crowdsourcing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25745": {
    "title": "Towards Voice Reconstruction from EEG during Imagined Speech",
    "volume": "main",
    "abstract": "Translating imagined speech from human brain activity into voice is a challenging and absorbing research issue that can provide new means of human communication via brain signals. Efforts to reconstruct speech from brain activity have shown their potential using invasive measures of spoken speech data, but have faced challenges in reconstructing imagined speech. In this paper, we propose NeuroTalk, which converts non-invasive brain signals of imagined speech into the user's own voice. Our model was trained with spoken speech EEG which was generalized to adapt to the domain of imagined speech, thus allowing natural correspondence between the imagined speech and the voice as a ground truth. In our framework, an automatic speech recognition decoder contributed to decomposing the phonemes of the generated speech, demonstrating the potential of voice reconstruction from unseen words. Our results imply the potential of speech synthesis from human EEG signals, not only from spoken speech but also from the brain signals of imagined speech",
    "checked": true,
    "id": "c6b97adc9fc7a6619d43b269a9e3dc2321df8f03",
    "semantic_title": "towards voice reconstruction from eeg during imagined speech",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25746": {
    "title": "Evaluating and Improving Interactions with Hazy Oracles",
    "volume": "main",
    "abstract": "Many AI systems integrate sensor inputs, world knowledge, and human-provided information to perform inference. While such systems often treat the human input as flawless, humans are better thought of as hazy oracles whose input may be ambiguous or outside of the AI system's understanding. In such situations it makes sense for the AI system to defer its inference while it disambiguates the human-provided information by, for example, asking the human to rephrase the query. Though this approach has been considered in the past, current work is typically limited to application-specific methods and non-standardized human experiments. We instead introduce and formalize a general notion of deferred inference. Using this formulation, we then propose a novel evaluation centered around the Deferred Error Volume (DEV) metric, which explicitly considers the tradeoff between error reduction and the additional human effort required to achieve it. We demonstrate this new formalization and an innovative deferred inference method on the disparate tasks of Single-Target Video Object Tracking and Referring Expression Comprehension, ultimately reducing error by up to 48% without any change to the underlying model or its parameters",
    "checked": true,
    "id": "dcfcac9943a67748c8f5831272a6ad545b78bcb0",
    "semantic_title": "evaluating and improving interactions with hazy oracles",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25747": {
    "title": "Human-in-the-Loop Vehicle ReID",
    "volume": "main",
    "abstract": "Vehicle ReID has been an active topic in computer vision, with a substantial number of deep neural models proposed as end-to-end solutions. In this paper, we solve the problem from a new perspective and present an interesting variant called human-in-the-loop vehicle ReID to leverage interactive (and possibly wrong) human feedback signal for performance enhancement. Such human-machine cooperation mode is orthogonal to existing ReID models. To avoid incremental training overhead, we propose an Interaction ReID Network (IRIN) that can directly accept the feedback signal as an input and adjust the embedding of query image in an online fashion. IRIN is offline trained by simulating the human interaction process, with multiple optimization strategies to fully exploit the feedback signal. Experimental results show that even by interacting with flawed feedback generated by non-experts, IRIN still outperforms state-of-the-art ReID models by a considerable margin. If the feedback contains no false positive, IRIN boosts the mAP in Veri776 from 81.6% to 95.2% with only 5 rounds of interaction per query image",
    "checked": true,
    "id": "e8f5a0db120e6fa63e3555d6e84e4809cb0666de",
    "semantic_title": "human-in-the-loop vehicle reid",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25748": {
    "title": "Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach",
    "volume": "main",
    "abstract": "The increased integration of artificial intelligence (AI) technologies in human workflows has resulted in a new paradigm of AI-assisted decision making, in which an AI model provides decision recommendations while humans make the final decisions. To best support humans in decision making, it is critical to obtain a quantitative understanding of how humans interact with and rely on AI. Previous studies often model humans' reliance on AI as an analytical process, i.e., reliance decisions are made based on cost-benefit analysis. However, theoretical models in psychology suggest that the reliance decisions can often be driven by emotions like humans' trust in AI models. In this paper, we propose a hidden Markov model to capture the affective process underlying the human-AI interaction in AI-assisted decision making, by characterizing how decision makers adjust their trust in AI over time and make reliance decisions based on their trust. Evaluations on real human behavior data collected from human-subject experiments show that the proposed model outperforms various baselines in accurately predicting humans' reliance behavior in AI-assisted decision making. Based on the proposed model, we further provide insights into how humans' trust and reliance dynamics in AI-assisted decision making is influenced by contextual factors like decision stakes and their interaction experiences",
    "checked": true,
    "id": "c7fbdc22ac13f0e1944262379bc1b41d188cc0b5",
    "semantic_title": "modeling human trust and reliance in ai-assisted decision making: a markovian approach",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25749": {
    "title": "Learning Deep Hierarchical Features with Spatial Regularization for One-Class Facial Expression Recognition",
    "volume": "main",
    "abstract": "Existing methods on facial expression recognition (FER) are mainly trained in the setting when multi-class data is available. However, to detect the alien expressions that are absent during training, this type of methods cannot work. To address this problem, we develop a Hierarchical Spatial One Class Facial Expression Recognition Network (HS-OCFER) which can construct the decision boundary of a given expression class (called normal class) by training on only one-class data. Specifically, HS-OCFER consists of three novel components. First, hierarchical bottleneck modules are proposed to enrich the representation power of the model and extract detailed feature hierarchy from different levels. Second, multi-scale spatial regularization with facial geometric information is employed to guide the feature extraction towards emotional facial representations and prevent the model from overfitting extraneous disturbing factors. Third, compact intra-class variation is adopted to separate the normal class from alien classes in the decision space. Extensive evaluations on 4 typical FER datasets from both laboratory and wild scenarios show that our method consistently outperforms state-of-the-art One-Class Classification (OCC) approaches",
    "checked": true,
    "id": "02aa368f27c539415b555b2c01ce037e5ff6b2ec",
    "semantic_title": "learning deep hierarchical features with spatial regularization for one-class facial expression recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25750": {
    "title": "Frustratingly Easy Truth Discovery",
    "volume": "main",
    "abstract": "Truth discovery is a general name for a broad range of statistical methods aimed to extract the correct answers to questions, based on multiple answers coming from noisy sources. For example, workers in a crowdsourcing platform. In this paper, we consider an extremely simple heuristic for estimating workers' competence using average proximity to other workers. We prove that this estimates well the actual competence level and enables separating high and low quality workers in a wide spectrum of domains and statistical models. Under Gaussian noise, this simple estimate is the unique solution to the MLE with a constant regularization factor. Finally, weighing workers according to their average proximity in a crowdsourcing setting, results in substantial improvement over unweighted aggregation and other truth discovery algorithms in practice",
    "checked": false,
    "id": "3dc5c5e420a2bcdb75e98ca55a033b83bea36618",
    "semantic_title": "secure 5g positioning with truth discovery, attack detection, and tracing",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25751": {
    "title": "Beam Search Optimized Batch Bayesian Active Learning",
    "volume": "main",
    "abstract": "Active Learning is an essential method for label-efficient deep learning. As a Bayesian active learning method, Bayesian Active Learning by Disagreement (BALD) successfully selects the most representative samples by maximizing the mutual information between the model prediction and model parameters. However, when applied to a batch acquisition mode, like batch construction with greedy search, BALD suffers from poor performance, especially with noises of near-duplicate data. To address this shortcoming, we propose a diverse beam search optimized batch active learning method, which explores a graph for every batch construction by expanding the highest-scored samples of a predetermined number. To avoid near duplicate beam branches (very similar beams generated from the same root and similar samples), which is undesirable for lacking diverse representations in the feature space, we design a self-adapted constraint within candidate beams. The proposed method is able to acquire data that can better represent the distribution of the unlabeled pool, and at the same time, be significantly different from existing beams. We observe that the proposed method achieves higher batch performance than the baseline methods on three benchmark datasets",
    "checked": true,
    "id": "d1e41e2de8e79735b0f1302ac28544bdd450ac6e",
    "semantic_title": "beam search optimized batch bayesian active learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25752": {
    "title": "Multi-Scale Control Signal-Aware Transformer for Motion Synthesis without Phase",
    "volume": "main",
    "abstract": "Synthesizing controllable motion for a character using deep learning has been a promising approach due to its potential to learn a compact model without laborious feature engineering. To produce dynamic motion from weak control signals such as desired paths, existing methods often require auxiliary information such as phases for alleviating motion ambiguity, which limits their generalisation capability. As past poses often contain useful auxiliary hints, in this paper, we propose a task-agnostic deep learning method, namely Multi-scale Control Signal-aware Transformer (MCS-T), with an attention based encoder-decoder architecture to discover the auxiliary information implicitly for synthesizing controllable motion without explicitly requiring auxiliary information such as phase. Specifically, an encoder is devised to adaptively formulate the motion patterns of a character's past poses with multi-scale skeletons, and a decoder driven by control signals to further synthesize and predict the character's state by paying context-specialised attention to the encoded past motion patterns. As a result, it helps alleviate the issues of low responsiveness and slow transition which often happen in conventional methods not using auxiliary information. Both qualitative and quantitative experimental results on an existing biped locomotion dataset, which involves diverse types of motion transitions, demonstrate the effectiveness of our method. In particular, MCS-T is able to successfully generate motions comparable to those generated by the methods using auxiliary information",
    "checked": true,
    "id": "4c4ee8e1a92e0eda686c3bd80d25c5785c57ed28",
    "semantic_title": "multi-scale control signal-aware transformer for motion synthesis without phase",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25753": {
    "title": "SwiftAvatar: Efficient Auto-Creation of Parameterized Stylized Character on Arbitrary Avatar Engines",
    "volume": "main",
    "abstract": "The creation of a parameterized stylized character involves careful selection of numerous parameters, also known as the \"avatar vectors\" that can be interpreted by the avatar engine. Existing unsupervised avatar vector estimation methods that auto-create avatars for users, however, often fail to work because of the domain gap between realistic faces and stylized avatar images. To this end, we propose SwiftAvatar, a novel avatar auto-creation framework that is evidently superior to previous works. SwiftAvatar introduces dual-domain generators to create pairs of realistic faces and avatar images using shared latent codes. The latent codes can then be bridged with the avatar vectors as pairs, by performing GAN inversion on the avatar images rendered from the engine using avatar vectors. Through this way, we are able to synthesize paired data in high-quality as many as possible, consisting of avatar vectors and their corresponding realistic faces. We also propose semantic augmentation to improve the diversity of synthesis. Finally, a light-weight avatar vector estimator is trained on the synthetic pairs to implement efficient auto-creation. Our experiments demonstrate the effectiveness and efficiency of SwiftAvatar on two different avatar engines. The superiority and advantageous flexibility of SwiftAvatar are also verified in both subjective and objective evaluations",
    "checked": true,
    "id": "040c9807983131ad611b3a927e63b1267a37986d",
    "semantic_title": "swiftavatar: efficient auto-creation of parameterized stylized character on arbitrary avatar engines",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25754": {
    "title": "Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction",
    "volume": "main",
    "abstract": "Stochastic human motion prediction aims to forecast multiple plausible future motions given a single pose sequence from the past. Most previous works focus on designing elaborate losses to improve the accuracy, while the diversity is typically characterized by randomly sampling a set of latent variables from the latent prior, which is then decoded into possible motions. This joint training of sampling and decoding, however, suffers from posterior collapse as the learned latent variables tend to be ignored by a strong decoder, leading to limited diversity. Alternatively, inspired by the diffusion process in nonequilibrium thermodynamics, we propose MotionDiff, a diffusion probabilistic model to treat the kinematics of human joints as heated particles, which will diffuse from original states to a noise distribution. This process not only offers a natural way to obtain the \"whitened'' latents without any trainable parameters, but also introduces a new noise in each diffusion step, both of which facilitate more diverse motions. Human motion prediction is then regarded as the reverse diffusion process that converts the noise distribution into realistic future motions conditioned on the observed sequence. Specifically, MotionDiff consists of two parts: a spatial-temporal transformer-based diffusion network to generate diverse yet plausible motions, and a flexible refinement network to further enable geometric losses and align with the ground truth. Experimental results on two datasets demonstrate that our model yields the competitive performance in terms of both diversity and accuracy",
    "checked": true,
    "id": "34f622243804ba15e29f1a22a8898eb4cf33772d",
    "semantic_title": "human joint kinematics diffusion-refinement for stochastic motion prediction",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25755": {
    "title": "Collective Intelligence in Human-AI Teams: A Bayesian Theory of Mind Approach",
    "volume": "main",
    "abstract": "We develop a network of Bayesian agents that collectively model the mental states of teammates from the observed communication. Using a generative computational approach to cognition, we make two contributions. First, we show that our agent could generate interventions that improve the collective intelligence of a human-AI team beyond what humans alone would achieve. Second, we develop a real-time measure of human's theory of mind ability and test theories about human cognition. We use data collected from an online experiment in which 145 individuals in 29 human-only teams of five communicate through a chat-based system to solve a cognitive task. We find that humans (a) struggle to fully integrate information from teammates into their decisions, especially when communication load is high, and (b) have cognitive biases which lead them to underweight certain useful, but ambiguous, information. Our theory of mind ability measure predicts both individual- and team-level performance. Observing teams' first 25% of messages explains about 8% of the variation in final team performance, a 170% improvement compared to the current state of the art",
    "checked": true,
    "id": "3e6c44fa97a3871eb67467cfac40fb6cf56f104d",
    "semantic_title": "collective intelligence in human-ai teams: a bayesian theory of mind approach",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25756": {
    "title": "Learning to Select Pivotal Samples for Meta Re-weighting",
    "volume": "main",
    "abstract": "Sample re-weighting strategies provide a promising mechanism to deal with imperfect training data in machine learning, such as noisily labeled or class-imbalanced data. One such strategy involves formulating a bi-level optimization problem called the meta re-weighting problem, whose goal is to optimize performance on a small set of perfect pivotal samples, called meta samples. Many approaches have been proposed to efficiently solve this problem. However, all of them assume that a perfect meta sample set is already provided while we observe that the selections of meta sample set is performance-critical. In this paper, we study how to learn to identify such a meta sample set from a large, imperfect training set, that is subsequently cleaned and used to optimize performance in the meta re-weighting setting. We propose a learning framework which reduces the meta samples selection problem to a weighted K-means clustering problem through rigorously theoretical analysis. We propose two clustering methods within our learning framework, Representation-based clustering method (RBC) and Gradient-based clustering method (GBC), for balancing performance and computational efficiency. Empirical studies demonstrate the performance advantage of our methods over various baseline methods",
    "checked": true,
    "id": "5e5e1ef8e856596df92fb65c4459052109014398",
    "semantic_title": "learning to select pivotal samples for meta re-weighting",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25757": {
    "title": "Better Peer Grading through Bayesian Inference",
    "volume": "main",
    "abstract": "Peer grading systems aggregate noisy reports from multiple students to approximate a \"true\" grade as closely as possible. Most current systems either take the mean or median of reported grades; others aim to estimate students' grading accuracy under a probabilistic model. This paper extends the state of the art in the latter approach in three key ways: (1) recognizing that students can behave strategically (e.g., reporting grades close to the class average without doing the work); (2) appropriately handling censored data that arises from discrete-valued grading rubrics; and (3) using mixed integer programming to improve the interpretability of the grades assigned to students. We demonstrate how to make Bayesian inference practical in this model and evaluate our approach on both synthetic and real-world data obtained by using our implemented system in four large classes. These extensive experiments show that grade aggregation using our model accurately estimates true grades, students' likelihood of submitting uninformative grades, and the variation in their inherent grading error; we also characterize our models' robustness",
    "checked": true,
    "id": "c58cd28fbb8ad552b402582d32b068d7aa9452b7",
    "semantic_title": "better peer grading through bayesian inference",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25758": {
    "title": "Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination",
    "volume": "main",
    "abstract": "We study the problem of training a Reinforcement Learning (RL) agent that is collaborative with humans without using human data. Although such agents can be obtained through self-play training, they can suffer significantly from the distributional shift when paired with unencountered partners, such as humans. In this paper, we propose Maximum Entropy Population-based training (MEP) to mitigate such distributional shift. In MEP, agents in the population are trained with our derived Population Entropy bonus to promote the pairwise diversity between agents and the individual diversity of agents themselves. After obtaining this diversified population, a common best agent is trained by paring with agents in this population via prioritized sampling, where the prioritization is dynamically adjusted based on the training progress. We demonstrate the effectiveness of our method MEP, with comparison to Self-Play PPO (SP), Population-Based Training (PBT), Trajectory Diversity (TrajeDi), and Fictitious Co-Play (FCP) in both matrix game and Overcooked game environments, with partners being human proxy models and real humans. A supplementary video showing experimental results is available at https://youtu.be/Xh-FKD0AAKE",
    "checked": false,
    "id": "4270f2493dfd9ae26b9f7c707cf1398ddbbdc0a1",
    "semantic_title": "maximum entropy population based training for zero-shot human-ai coordination",
    "citation_count": 15
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25759": {
    "title": "A Set of Control Points Conditioned Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Predicting the trajectories of pedestrians in crowded conditions is an important task for applications like autonomous navigation systems. Previous studies have tackled this problem using two strategies. They (1) infer all future steps recursively, or (2) predict the potential destinations of pedestrians at once and interpolate the intermediate steps to arrive there. However, these strategies often suffer from the accumulated errors of the recursive inference, or restrictive assumptions about social relations in the intermediate path. In this paper, we present a graph convolutional network-based trajectory prediction. Firstly, we propose a control point prediction that divides the future path into three sections and infers the intermediate destinations of pedestrians to reduce the accumulated error. To do this, we construct multi-relational weighted graphs to account for their physical and complex social relations. We then introduce a trajectory refinement step based on a spatio-temporal and multi-relational graph. By considering the social interactions between neighbors, better prediction results are achievable. In experiments, the proposed network achieves state-of-the-art performance on various real-world trajectory prediction benchmarks",
    "checked": true,
    "id": "8c6eef90065c8c35e6051e80478c993b48a783e8",
    "semantic_title": "a set of control points conditioned pedestrian trajectory prediction",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25760": {
    "title": "Meta-Auxiliary Learning for Adaptive Human Pose Prediction",
    "volume": "main",
    "abstract": "Predicting high-fidelity future human poses, from a historically observed sequence, is crucial for intelligent robots to interact with humans. Deep end-to-end learning approaches, which typically train a generic pre-trained model on external datasets and then directly apply it to all test samples, emerge as the dominant solution to solve this issue. Despite encouraging progress, they remain non-optimal, as the unique properties (e.g., motion style, rhythm) of a specific sequence cannot be adapted. More generally, once encountering out-of-distributions, the predicted poses tend to be unreliable. Motivated by this observation, we propose a novel test-time adaptation framework that leverages two self-supervised auxiliary tasks to help the primary forecasting network adapt to the test sequence. In the testing phase, our model can adjust the model parameters by several gradient updates to improve the generation quality. However, due to catastrophic forgetting, both auxiliary tasks typically have a low ability to automatically present the desired positive incentives for the final prediction performance. For this reason, we also propose a meta-auxiliary learning scheme for better adaptation. Extensive experiments show that the proposed approach achieves higher accuracy and more realistic visualization",
    "checked": true,
    "id": "390a927e0c70bdc3e3a327dc8f18a0a90b9f82b9",
    "semantic_title": "meta-auxiliary learning for adaptive human pose prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25761": {
    "title": "Moving-Landmark Assisted Distributed Learning Based Decentralized Cooperative Localization (DL-DCL) with Fault Tolerance",
    "volume": "main",
    "abstract": "This paper considers the problem of cooperative localization of multiple robots under uncertainty, communicating over a partially connected, dynamic communication network and assisted by an agile landmark. Each robot owns an IMU and a relative pose sensing suite, which can get faulty due to system or environmental uncertainty, and therefore exhibit large bias in their estimation output. For the robots to localize accurately under sensor failure and system or environmental uncertainty, a novel Distributed Learning based Decentralized Cooperative Localization (DL-DCL) algorithm is proposed that involves real-time learning of an information fusion strategy by each robot for combining pose estimates from its own sensors as well as from those of its neighboring robots, and utilizing the moving landmark's pose information as a feedback to the learning process. Convergence analysis shows that the learning process converges exponentially under certain reasonable assumptions. Simulations involving sensor failures inducing around 40-60 times increase in the nominal bias show DL-DCL's estimation performance to be approximately 40% better than the well-known covariance-based estimate fusion methods. For the evaluation of DL-DCL's implementability and fault-tolerance capability in practice, a high-fidelity simulation is carried out in Gazebo with ROS2",
    "checked": true,
    "id": "0bf16ee9abecd5fba62b89f9fa29c0d2b6422157",
    "semantic_title": "moving-landmark assisted distributed learning based decentralized cooperative localization (dl-dcl) with fault tolerance",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25762": {
    "title": "Periodic Multi-Agent Path Planning",
    "volume": "main",
    "abstract": "Multi-agent path planning (MAPP) is the problem of planning collision-free trajectories from start to goal locations for a team of agents. This work explores a relatively unexplored setting of MAPP where streams of agents have to go through the starts and goals with high throughput. We tackle this problem by formulating a new variant of MAPP called periodic MAPP in which the timing of agent appearances is periodic. The objective with periodic MAPP is to find a periodic plan, a set of collision-free trajectories that the agent streams can use repeatedly over periods, with periods that are as small as possible. To meet this objective, we propose a solution method that is based on constraint relaxation and optimization. We show that the periodic plans once found can be used for a more practical case in which agents in a stream can appear at random times. We confirm the effectiveness of our method compared with baseline methods in terms of throughput in several scenarios that abstract autonomous intersection management tasks",
    "checked": true,
    "id": "94ccc03294726ad795c0379bf7059aa377ed3b22",
    "semantic_title": "periodic multi-agent path planning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25763": {
    "title": "Improving Robotic Tactile Localization Super-resolution via Spatiotemporal Continuity Learning and Overlapping Air Chambers",
    "volume": "main",
    "abstract": "Human hand has amazing super-resolution ability in sensing the force and position of contact and this ability can be strengthened by practice. Inspired by this, we propose a method for robotic tactile super-resolution enhancement by learning spatiotemporal continuity of contact position and a tactile sensor composed of overlapping air chambers. Each overlapping air chamber is constructed of soft material and seals the barometer inside to mimic adapting receptors of human skin. Each barometer obtains the global receptive field of the contact surface with the pressure propagation in the hyperelastic seal overlapping air chambers. Neural networks with causal convolution are employed to resolve the pressure data sampled by barometers and to predict the contact position. The temporal consistency of spatial position contributes to the accuracy and stability of positioning. We obtain an average super-resolution (SR) factor of over 2500 with only four physical sensing nodes on the rubber surface (0.1 mm in the best case on 38 × 26 mm²), which outperforms the state-of-the-art. The effect of time series length on the location prediction accuracy of causal convolution is quantitatively analyzed in this article. We show that robots can accomplish challenging tasks such as haptic trajectory following, adaptive grasping, and human-robot interaction with the tactile sensor. This research provides new insight into tactile super-resolution sensing and could be beneficial to various applications in the robotics field",
    "checked": true,
    "id": "58d73309807f7811722dda39d5867548337e0412",
    "semantic_title": "improving robotic tactile localization super-resolution via spatiotemporal continuity learning and overlapping air chambers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25764": {
    "title": "Co-imitation: Learning Design and Behaviour by Imitation",
    "volume": "main",
    "abstract": "The co-adaptation of robots has been a long-standing research endeavour with the goal of adapting both body and behaviour of a robot for a given task, inspired by the natural evolution of animals. Co-adaptation has the potential to eliminate costly manual hardware engineering as well as improve the performance of systems. The standard approach to co-adaptation is to use a reward function for optimizing behaviour and morphology. However, defining and constructing such reward functions is notoriously difficult and often a significant engineering effort. This paper introduces a new viewpoint on the co-adaptation problem, which we call co-imitation: finding a morphology and a policy that allow an imitator to closely match the behaviour of a demonstrator. To this end we propose a co-imitation methodology for adapting behaviour and morphology by matching state-distributions of the demonstrator. Specifically, we focus on the challenging scenario with mismatched state- and action-spaces between both agents. We find that co-imitation increases behaviour similarity across a variety of tasks and settings, and demonstrate co-imitation by transferring human walking, jogging and kicking skills onto a simulated humanoid",
    "checked": true,
    "id": "854a3a3212d33a9a05bd498874cfa3ddb0232535",
    "semantic_title": "co-imitation: learning design and behaviour by imitation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25765": {
    "title": "RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments",
    "volume": "main",
    "abstract": "Camera relocalization has various applications in autonomous driving. Previous camera pose regression models consider only ideal scenarios where there is little environmental perturbation. To deal with challenging driving environments that may have changing seasons, weather, illumination, and the presence of unstable objects, we propose RobustLoc, which derives its robustness against perturbations from neural differential equations. Our model uses a convolutional neural network to extract feature maps from multi-view images, a robust neural differential equation diffusion block module to diffuse information interactively, and a branched pose decoder with multi-layer training to estimate the vehicle poses. Experiments demonstrate that RobustLoc surpasses current state-of-the-art camera pose regression models and achieves robust performance in various environments. Our code is released at: https://github.com/sijieaaa/RobustLoc",
    "checked": true,
    "id": "08c1b87c94e741c09985d37872c1ae14c6c7c2e4",
    "semantic_title": "robustloc: robust camera pose regression in challenging driving environments",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25766": {
    "title": "Abstract Argumentation Framework with Conditional Preferences",
    "volume": "main",
    "abstract": "Dung's abstract Argumentation Framework (AF) has emerged as a central formalism in the area of knowledge representation and reasoning. Preferences in AF allow to represent the comparative strength of arguments in a simple yet expressive way. Preference-based AF (PAF) has been proposed to extend AF with preferences of the form a > b, whose intuitive meaning is that argument a is better than b. In this paper we generalize PAF by introducing conditional preferences of the form a > b \\leftarrow body that informally state that a is better than b whenever the condition expressed by body is true. The resulting framework, namely Conditional Preference-based AF (CPAF), extends the PAF semantics under three well-known preference criteria, i.e. democratic, elitist, and KTV. After introducing CPAF, we study the complexity of the verification problem (deciding whether a set of arguments is a ``best'' extension) as well as of the credulous and skeptical acceptance problems (deciding whether a given argument belongs to any or all ``best'' extensions, respectively) under multiple-status semantics (that is, complete, preferred, stable, and semi-stable semantics) for the above-mentioned preference criteria",
    "checked": true,
    "id": "ed43175c3bd2e197de2eb4558528c09017c23c17",
    "semantic_title": "abstract argumentation framework with conditional preferences",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25767": {
    "title": "Reactive Synthesis of Dominant Strategies",
    "volume": "main",
    "abstract": "We study the synthesis under environment specifications problem for LTL/LTLf which, in particular, generalizes FOND (strong) planning with these temporal goals. We consider the case where the agent cannot enforce its goal --- for which the argument for using best-effort strategies has been made --- and study the intermediate ground, between enforcing and best-effort strategies, of dominant strategies. Intuitively, such strategies achieve the goal against any environment for which it is achievable. We show that dominant strategies may exist when enforcing ones do not, while still sharing with the latter many desirable properties such as being interchangeable with each other, and being monotone with respect to tightening of environment specifications. We give necessary and sufficient conditions for the existence of dominant strategies, and show that deciding if they exist is 2EXPTIME-complete --- the same as for enforcing strategies. Finally, we give a uniform, optimal, game-theoretic algorithm for simultaneously solving the three synthesis problems of enforcing, dominant, and best-effort strategies",
    "checked": true,
    "id": "68d20218ac2adae05d799f9b2ea25d8598a1075f",
    "semantic_title": "reactive synthesis of dominant strategies",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25768": {
    "title": "Complexity of Safety and coSafety Fragments of Linear Temporal Logic",
    "volume": "main",
    "abstract": "Linear Temporal Logic (LTL) is the de-facto standard temporal logic for system specification, whose foundational properties have been studied for over five decades. Safety and cosafety properties of LTL define notable fragments of LTL, where a prefix of a trace suffices to establish whether a formula is true or not over that trace. In this paper, we study the complexity of the problems of satisfiability, validity, and realizability over infinite and finite traces for the safety and cosafety fragments of LTL. As for satisfiability and validity over infinite traces, we prove that the majority of the fragments have the same complexity as full LTL, that is, they are PSPACE-complete. The picture is radically different for realizability: we find fragments with the same expressive power whose complexity varies from 2EXPTIME-complete (as full LTL) to EXPTIME-complete. Notably, for all cosafety fragments, the complexity of the three problems does not change passing from infinite to finite traces, while for all safety fragments the complexity of satisfiability (resp., realizability) over finite traces drops to NP-complete (resp., Πᴾ₂- complete)",
    "checked": true,
    "id": "11057109ab2652f228d944be6f77c566e86a34e6",
    "semantic_title": "complexity of safety and cosafety fragments of linear temporal logic",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25769": {
    "title": "Automatically Verifying Expressive Epistemic Properties of Programs",
    "volume": "main",
    "abstract": "We propose a new approach to the verification of epistemic properties of programmes. First, we introduce the new ``program-epistemic'' logic L_PK, which is strictly richer and more general than similar formalisms appearing in the literature. To solve the verification problem in an efficient way, we introduce a translation from our language L_PK into first-order logic. Then, we show and prove correct a reduction from the model checking problem for program-epistemic formulas to the satisfiability of their first-order translation. Both our logic and our translation can handle richer specification w.r.t. the state of the art, allowing us to express the knowledge of agents about facts pertaining to programs (i.e., agents' knowledge before a program is executed as well as after is has been executed). Furthermore, we implement our translation in Haskell in a general way (i.e., independently of the programs in the logical statements), and we use existing SMT-solvers to check satisfaction of L_PK formulas on a benchmark example in the AI/agency field",
    "checked": true,
    "id": "5ba6cfee1efd0cddec2e744b12f0cca7e76ad3e8",
    "semantic_title": "automatically verifying expressive epistemic properties of programs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25770": {
    "title": "The Effect of Preferences in Abstract Argumentation under a Claim-Centric View",
    "volume": "main",
    "abstract": "In this paper, we study the effect of preferences in abstract argumentation under a claim-centric perspective. Recent work has revealed that semantical and computational properties can change when reasoning is performed on claim-level rather than on the argument-level, while under certain natural restrictions (arguments with the same claims have the same outgoing attacks) these properties are conserved. We now investigate these effects when, in addition, preferences have to be taken into account and consider four prominent reductions to handle preferences between arguments. As we shall see, these reductions give rise to different classes of claim-augmented argumentation frameworks, and behave differently in terms of semantic properties and computational complexity. This strengthens the view that the actual choice for handling preferences has to be taken with care",
    "checked": true,
    "id": "3971d26cf23d95487e333384a3cb58bfe26dbd32",
    "semantic_title": "the effect of preferences in abstract argumentation under a claim-centric view",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25771": {
    "title": "The Parameterized Complexity of Network Microaggregation",
    "volume": "main",
    "abstract": "Microaggregation is a classical statistical disclosure control technique which requires the input data to be partitioned into clusters while adhering to specified size constraints. We provide novel exact algorithms and lower bounds for the task of microaggregating a given network while considering both unrestricted and connected clusterings, and analyze these from the perspective of the parameterized complexity paradigm. Altogether, our results assemble a complete complexity-theoretic picture for the network microaggregation problem with respect to the most natural parameterizations of the problem, including input-specified parameters capturing the size and homogeneity of the clusters as well as the treewidth and vertex cover number of the network",
    "checked": true,
    "id": "35f9b5d2e666a7139be61e30a939885917e2a844",
    "semantic_title": "the parameterized complexity of network microaggregation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25772": {
    "title": "SMT Safety Verification of Ontology-Based Processes",
    "volume": "main",
    "abstract": "In the context of verification of data-aware processes, a formal approach based on satisfiability modulo theories (SMT) has been considered to verify parameterised safety properties. This approach requires a combination of model-theoretic notions and algorithmic techniques based on backward reachability. We introduce here Ontology-Based Processes, which are a variant of one of the most investigated models in this spectrum, namely simple artifact systems (SASs), where, instead of managing a database, we operate over a description logic (DL) ontology. We prove that when the DL is expressed in (a slight extension of) RDFS, it enjoys suitable model-theoretic properties, and that by relying on such DL we can define Ontology-Based Processes to which backward reachability can still be applied. Relying on these results we are able to show that in this novel setting, verification of safety properties is decidable in PSPACE",
    "checked": true,
    "id": "c191ecb6c77ad9c74d6aba0f68f628436ecc6b69",
    "semantic_title": "smt safety verification of ontology-based processes",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25773": {
    "title": "Epistemic Disjunctive Datalog for Querying Knowledge Bases",
    "volume": "main",
    "abstract": "The Datalog query language can express several powerful recursive properties, often crucial in real-world scenarios. While answering such queries is feasible over relational databases, the picture changes dramatically when data is enriched with intensional knowledge. It is indeed well-known that answering Datalog queries is undecidable already over lightweight knowledge bases (KBs) of the DL-Lite family. To overcome this issue, we propose a new query language based on Disjunctive Datalog rules combined with a modal epistemic operator. Rules in this language interact with the queried KB exclusively via the epistemic operator, thus extracting only the information true in every model of the KB. This form of interaction is crucial for not falling into undecidability. The contribution provided by this paper is threefold. First, we illustrate the syntax and the semantics of the novel query language. Second, we study the expressive power of different fragments of our new language and compare it with Disjunctive Datalog and its variants. Third, we outline the precise data complexity of answering queries in our new language over KBs expressed in various well-known formalisms",
    "checked": true,
    "id": "716ff41912617caddd2b4386a1071f5ad7249679",
    "semantic_title": "epistemic disjunctive datalog for querying knowledge bases",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25774": {
    "title": "Learning Logic Programs by Discovering Where Not to Search",
    "volume": "main",
    "abstract": "The goal of inductive logic programming (ILP) is to search for a hypothesis that generalises training examples and background knowledge (BK). To improve performance, we introduce an approach that, before searching for a hypothesis, first discovers \"where not to search\". We use given BK to discover constraints on hypotheses, such as that a number cannot be both even and odd. We use the constraints to bootstrap a constraint-driven ILP system. Our experiments on multiple domains (including program synthesis and inductive general game playing) show that our approach can (i) substantially reduce learning times by up to 97%, and (ii) can scale to domains with millions of facts",
    "checked": true,
    "id": "f7bcdc6e791173001b25acdd3f98b8cc44a7fd69",
    "semantic_title": "learning logic programs by discovering where not to search",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25775": {
    "title": "From Width-Based Model Checking to Width-Based Automated Theorem Proving",
    "volume": "main",
    "abstract": "In the field of parameterized complexity theory, the study of graph width measures has been intimately connected with the development of width-based model checking algorithms for combinatorial properties on graphs. In this work, we introduce a general framework to convert a large class of width-based model-checking algorithms into algorithms that can be used to test the validity of graph-theoretic conjectures on classes of graphs of bounded width. Our framework is modular and can be applied with respect to several well-studied width measures for graphs, including treewidth and cliquewidth. As a quantitative application of our framework, we prove analytically that for several long-standing graph-theoretic conjectures, there exists an algorithm that takes a number k as input and correctly determines in time double-exponential in a polynomial of k whether the conjecture is valid on all graphs of treewidth at most k. These upper bounds, which may be regarded as upper-bounds on the size of proofs/disproofs for these conjectures on the class of graphs of treewidth at most k, improve significantly on theoretical upper bounds obtained using previously available techniques",
    "checked": true,
    "id": "0b112e2d1f4876f58190322b6496cfa1011736e4",
    "semantic_title": "from width-based model checking to width-based automated theorem proving",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25776": {
    "title": "Model-Checking for Ability-Based Logics with Constrained Plans",
    "volume": "main",
    "abstract": "We investigate the complexity of the model-checking problem for a family of modal logics capturing the notion of \"knowing how\". We consider the most standard ability-based knowing how logic, for which we show that model-checking is PSpace-complete. By contrast, a multi-agent variant based on an uncertainty relation between plans in which uncertainty is encoded by a regular language, is shown to admit a PTime model-checking problem. We extend with budgets the above-mentioned ability-logics, as done for ATL-like logics. We show that for the former logic enriched with budgets, the complexity increases to at least ExpSpace-hardness, whereas for the latter, the PTime bound is preserved. Other variant logics are discussed along the paper",
    "checked": true,
    "id": "d0d9dee80c3177e67cd3e678a9c92ad6beac31b0",
    "semantic_title": "model-checking for ability-based logics with constrained plans",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25777": {
    "title": "A Structural Complexity Analysis of Synchronous Dynamical Systems",
    "volume": "main",
    "abstract": "Synchronous dynamical systems are well-established models that have been used to capture a range of phenomena in networks, including opinion diffusion, spread of disease and product adoption. We study the three most notable problems in synchronous dynamical systems: whether the system will transition to a target configuration from a starting configuration, whether the system will reach convergence from a starting configuration, and whether the system is guaranteed to converge from every possible starting configuration. While all three problems were known to be intractable in the classical sense, we initiate the study of their exact boundaries of tractability from the perspective of structural parameters of the network by making use of the more fine-grained parameterized complexity paradigm. As our first result, we consider treewidth - as the most prominent and ubiquitous structural parameter - and show that all three problems remain intractable even on instances of constant treewidth. We complement this negative finding with fixed-parameter algorithms for the former two problems parameterized by treedepth, a well-studied restriction of treewidth. While it is possible to rule out a similar algorithm for convergence guarantee under treedepth, we conclude with a fixed-parameter algorithm for this last problem when parameterized by treedepth and the maximum in-degree",
    "checked": true,
    "id": "91704981d91f327d3304eea20d75f318b969448e",
    "semantic_title": "a structural complexity analysis of synchronous dynamical systems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25778": {
    "title": "Evaluating Epistemic Logic Programs via Answer Set Programming with Quantifiers",
    "volume": "main",
    "abstract": "In this paper we introduce a simple way to evaluate epistemic logic programs by means of answer set programming with quantifiers, a recently proposed extension of answer set programming. The method can easily be adapted for most of the many semantics that were proposed for epistemic logic programs. We evaluate the proposed transformation on existing benchmarks using a recently proposed solver for answer set programming with quantifiers, which relies on QBF solvers",
    "checked": true,
    "id": "c27c2a50eb30d1b292b4ef1827532bd7c0d76106",
    "semantic_title": "evaluating epistemic logic programs via answer set programming with quantifiers",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25779": {
    "title": "Reachability Games Modulo Theories with a Bounded Safety Player",
    "volume": "main",
    "abstract": "Solving reachability games is a fundamental problem for the analysis, verification, and synthesis of reactive systems. We consider logical reachability games modulo theories (in short, GMTs), i.e., infinite-state games whose rules are defined by logical formulas over a multi-sorted first-order theory. Our games have an asymmetric constraint: the safety player has at most k possible moves from each game configuration, whereas the reachability player has no such limitation. Even though determining the winner of such a GMT is undecidable, it can be reduced to the well-studied problem of checking the satisfiability of a system of constrained Horn clauses (CHCs), for which many off-the-shelf solvers have been developed. Winning strategies for GMTs can also be computed by resorting to suitable CHC queries. We demonstrate that GMTs can model various relevant real-world games, and that our approach can effectively solve several problems from different domains, using Z3 as the backend CHC solver",
    "checked": true,
    "id": "8e6d4933293895540519b1c31645456da9f94e28",
    "semantic_title": "reachability games modulo theories with a bounded safety player",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25780": {
    "title": "Splitting Answer Set Programs with Respect to Intensionality Statements",
    "volume": "main",
    "abstract": "Splitting a logic program allows us to reduce the task of computing its stable models to similar tasks for its subprograms. This can be used to increase solving performance and to prove the correctness of programs. We generalize the conditions under which this technique is applicable, by considering not only dependencies between predicates but also their arguments and context. This allows splitting programs commonly used in practice to which previous results were not applicable",
    "checked": true,
    "id": "7d0725ae29843fdd48075d7c4f228440b377be45",
    "semantic_title": "splitting answer set programs with respect to intensionality statements",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25781": {
    "title": "Monitoring Arithmetic Temporal Properties on Finite Traces",
    "volume": "main",
    "abstract": "We study monitoring of linear-time arithmetic properties against finite traces generated by an unknown dynamic system. The monitoring state is determined by considering at once the trace prefix seen so far, and all its possible finite-length, future continuations. This makes monitoring at least as hard as satisfiability and validity. Traces consist of finite sequences of assignments of a fixed set of variables to numerical values. Properties are specified in a logic we call ALTLf, combining LTLf (LTL on finite traces) with linear arithmetic constraints that may carry lookahead, i.e., variables may be compared over multiple instants of the trace. While the monitoring problem for this setting is undecidable in general, we show decidability for (a) properties without lookahead, and (b) properties with lookahead that satisfy the abstract, semantic condition of finite summary, studied before in the context of model checking. We then single out concrete, practically relevant classes of constraints guaranteeing finite summary. Feasibility is witnessed by a prototype implementation",
    "checked": true,
    "id": "441f1b0d50a6e16de8794c1a3154405acfd40559",
    "semantic_title": "monitoring arithmetic temporal properties on finite traces",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25782": {
    "title": "Untangled: A Complete Dynamic Topological Logic",
    "volume": "main",
    "abstract": "Dynamical systems are general models of change or movement over time with a broad area of applicability to many branches of science, including computer science and AI. Dynamic topological logic (DTL) is a formal framework for symbolic reasoning about dynamical systems. DTL can express various liveness and reachability conditions on such systems, but has the drawback that the only known axiomatisation requires an extended language. In this paper, we consider dynamic topological logic restricted to the class of scattered spaces. Scattered spaces appear in the context of computational logic as they provide semantics for provability and enjoy definable fixed points. We exhibit the first sound and complete dynamic topological logic in the original language of DTL. In particular, we show that the version of DTL based on the class of scattered spaces is finitely axiomatisable, and that the natural axiomatisation is sound and complete",
    "checked": true,
    "id": "9bb88a92bf5b3e4bd9813c9a85e6e2d95916311f",
    "semantic_title": "untangled: a complete dynamic topological logic",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25783": {
    "title": "Inconsistent Cores for ASP: The Perks and Perils of Non-monotonicity",
    "volume": "main",
    "abstract": "Answer Set Programming (ASP) is a prominent modeling and solving framework. An inconsistent core (IC) of an ASP program is an inconsistent subset of rules. In the case of inconsistent programs, a smallest or subset-minimal IC contains crucial rules for the inconsistency. In this work, we study fnding minimal ICs of ASP programs and key fragments from a complexity-theoretic perspective. Interestingly, due to ASP's non-monotonic behavior, also consistent programs admit ICs. It turns out that there is an entire landscape of problems involving ICs with a diverse range of complexities up to the fourth level of the Polynomial Hierarchy. Deciding the existence of an IC is, already for tight programs, on the second level of the Polynomial Hierarchy. Furthermore, we give encodings for IC-related problems on the fragment of tight programs and illustrate feasibility on small instance sets",
    "checked": true,
    "id": "5cdd45ffb85a3b654d49983f3f8d98075c8aa83f",
    "semantic_title": "inconsistent cores for asp: the perks and perils of non-monotonicity",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25784": {
    "title": "General Acyclicity and Cyclicity Notions for the Disjunctive Skolem Chase",
    "volume": "main",
    "abstract": "The disjunctive skolem chase is a sound, complete, and potentially non-terminating procedure for solving boolean conjunctive query entailment over knowledge bases of disjunctive existential rules. We develop novel acyclicity and cyclicity notions for this procedure; that is, we develop sufficient conditions to determine chase termination and non-termination. Our empirical evaluation shows that our novel notions are significantly more general than existing criteria",
    "checked": true,
    "id": "56a6b99fe7d331e20786cbfedf2c96651539d59c",
    "semantic_title": "general acyclicity and cyclicity notions for the disjunctive skolem chase",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25785": {
    "title": "GANTEE: Generative Adversarial Network for Taxonomy Enterance Evaluation",
    "volume": "main",
    "abstract": "Taxonomy is formulated as directed acyclic graphs or trees of concepts that support many downstream tasks. Many new coming concepts need to be added to an existing taxonomy. The traditional taxonomy expansion task aims only at finding the best position for new coming concepts in the existing taxonomy. However, they have two drawbacks when being applied to the real-scenarios. The previous methods suffer from low-efficiency since they waste much time when most of the new coming concepts are indeed noisy concepts. They also suffer from low-effectiveness since they collect training samples only from the existing taxonomy, which limits the ability of the model to mine more hypernym-hyponym relationships among real concepts. This paper proposes a pluggable framework called Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE) to alleviate these drawbacks. A generative adversarial network is designed in this framework by discriminative models to alleviate the first drawback and the generative model to alleviate the second drawback. Two discriminators are used in GANTEE to provide long-term and short-term rewards, respectively. Moreover, to further improve the efficiency, pre-trained language models are used to retrieve the representation of the concepts quickly. The experiments on three real-world large-scale datasets with two different languages show that GANTEE improves the performance of the existing taxonomy expansion methods in both effectiveness and efficiency",
    "checked": true,
    "id": "25db8a16e39af2aea2bc2dea7aab1862b04a9b7c",
    "semantic_title": "gantee: generative adversarial network for taxonomy enterance evaluation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25786": {
    "title": "Finite Based Contraction and Expansion via Models",
    "volume": "main",
    "abstract": "We propose a new paradigm for Belief Change in which the new information is represented as sets of models, while the agent's body of knowledge is represented as a finite set of formulae, that is, a finite base. The focus on finiteness is crucial when we consider limited agents and reasoning algorithms. Moreover, having the input as arbitrary set of models is more general than the usual treatment of formulas as input. In this setting, we define new Belief Change operations akin to traditional expansion and contraction, and we identify the rationality postulates that emerge due to the finite representability requirement. We also analyse different logics concerning compatibility with our framework",
    "checked": true,
    "id": "53097257a9bb71b1af91da5d99c211311b0476da",
    "semantic_title": "finite based contraction and expansion via models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25787": {
    "title": "MAPS-KB: A Million-Scale Probabilistic Simile Knowledge Base",
    "volume": "main",
    "abstract": "The ability to understand and generate similes is an imperative step to realize human-level AI. However, there is still a considerable gap between machine intelligence and human cognition in similes, since deep models based on statistical distribution tend to favour high-frequency similes. Hence, a large-scale symbolic knowledge base of similes is required, as it contributes to the modeling of diverse yet unpopular similes while facilitating additional evaluation and reasoning. To bridge the gap, we propose a novel framework for large-scale simile knowledge base construction, as well as two probabilistic metrics which enable an improved understanding of simile phenomena in natural language. Overall, we construct MAPS-KB, a million-scale probabilistic simile knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB corpora. We conduct sufficient experiments to justify the effectiveness and necessity of the methods of our framework. We also apply MAPS-KB on three downstream tasks to achieve state-of-the-art performance, further demonstrating the value of MAPS-KB. Resources of MAPS-KB are publicly available at https://github.com/Abbey4799/MAPS-KB",
    "checked": true,
    "id": "83ab9f2fdb2d7445f8b72b732fe17f9e21603082",
    "semantic_title": "maps-kb: a million-scale probabilistic simile knowledge base",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25788": {
    "title": "Characterizing Structural Hardness of Logic Programs: What Makes Cycles and Reachability Hard for Treewidth?",
    "volume": "main",
    "abstract": "Answer Set Programming (ASP) is a problem modeling and solving framework for several problems in KR with growing industrial applications. Also for studies of computational complexity and deeper insights into the hardness and its sources, ASP has been attracting researchers for many years. These studies resulted in fruitful characterizations in terms of complexity classes, fine-grained insights in form of dichotomy-style results, as well as detailed parameterized complexity landscapes. Recently, this lead to a novel result establishing that for the measure treewidth, which captures structural density of a program, the evaluation of the well-known class of normal programs is expected to be slightly harder than deciding satisfiability (SAT). However, it is unclear how to utilize this structural power of ASP. This paper deals with a novel reduction from SAT to normal ASP that goes beyond well-known encodings: We explicitly utilize the structural power of ASP, whereby we sublinearly decrease the treewidth, which probably cannot be significantly improved. Then, compared to existing results, this characterizes hardness in a fine-grained way by establishing the required functional dependency of the dependency graph's cycle length (SCC size) on the treewidth",
    "checked": true,
    "id": "adbce6d521643cb25c30dc0ffb9d0d0080e7a92c",
    "semantic_title": "characterizing structural hardness of logic programs: what makes cycles and reachability hard for treewidth?",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25789": {
    "title": "Conditional Syntax Splitting for Non-monotonic Inference Operators",
    "volume": "main",
    "abstract": "Syntax splitting is a property of inductive inference operators that ensures we can restrict our attention to parts of the conditional belief base that share atoms with a given query. To apply syntax splitting, a conditional belief base needs to consist of syntactically disjoint conditionals. This requirement is often too strong in practice, as conditionals might share atoms. In this paper we introduce the concept of conditional syntax splitting, inspired by the notion of conditional independence as known from probability theory. We show that lexicographic inference and system W satisfy conditional syntax splitting, and connect conditional syntax splitting to several known properties from the literature on non-monotonic reasoning, including the drowning effect",
    "checked": true,
    "id": "e10984c1c2ac697c49222a118a4cadeffa93f57d",
    "semantic_title": "conditional syntax splitting for non-monotonic inference operators",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25790": {
    "title": "Relational Program Synthesis with Numerical Reasoning",
    "volume": "main",
    "abstract": "Learning programs with numerical values is fundamental to many AI applications, including bio-informatics and drug design. However, current program synthesis approaches struggle to learn programs with numerical values. An especially difficult problem is learning continuous values from multiple examples, such as intervals. To overcome this limitation, we introduce an inductive logic programming approach which combines relational learning with numerical reasoning. Our approach, which we call NumSynth, uses satisfiability modulo theories solvers to efficiently learn programs with numerical values. Our approach can identify numerical values in linear arithmetic fragments, such as real difference logic, and from infinite domains, such as real numbers or integers. Our experiments on four diverse domains, including game playing and program synthesis, show that our approach can (i) learn programs with numerical values from linear arithmetical reasoning, and (ii) outperform existing approaches in terms of predictive accuracies and learning times",
    "checked": true,
    "id": "c150adaab6e5887ef9ceae25214076468f42534e",
    "semantic_title": "relational program synthesis with numerical reasoning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25791": {
    "title": "Common Knowledge of Abstract Groups",
    "volume": "main",
    "abstract": "Epistemic logics typically talk about knowledge of individual agents or groups of explicitly listed agents. Often, however, one wishes to express knowledge of groups of agents specified by a given property, as in ‘it is common knowledge among economists'. We introduce such a logic of common knowledge, which we term abstract-group epistemic logic (AGEL). That is, AGEL features a common knowledge operator for groups of agents given by concepts in a separate agent logic that we keep generic, with one possible agent logic being ALC. We show that AGEL is EXPTIME-complete, with the lower bound established by reduction from standard group epistemic logic, and the upper bound by a satisfiability-preserving embedding into the full µ-calculus. Further main results include a finite model property (not enjoyed by the full µ-calculus) and a complete axiomatization",
    "checked": true,
    "id": "e7f497d35a06594fe2722946a0ed8539b3342817",
    "semantic_title": "common knowledge of abstract groups",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25792": {
    "title": "FASTDIAGP: An Algorithm for Parallelized Direct Diagnosis",
    "volume": "main",
    "abstract": "Constraint-based applications attempt to identify a solution that meets all defined user requirements. If the requirements are inconsistent with the underlying constraint set, algorithms that compute diagnoses for inconsistent constraints should be implemented to help users resolve the \"no solution could be found\" dilemma. FastDiag is a typical direct diagnosis algorithm that supports diagnosis calculation without pre-determining conflicts. However, this approach faces runtime performance issues, especially when analyzing complex and large-scale knowledge bases. In this paper, we propose a novel algorithm, so-called FastDiagP, which is based on the idea of speculative programming. This algorithm extends FastDiag by integrating a parallelization mechanism that anticipates and pre-calculates consistency checks requested by FastDiag. This mechanism helps to provide consistency checks with fast answers and boosts the algorithm's runtime performance. The performance improvements of our proposed algorithm have been shown through empirical results using the Linux-2.6.3.33 configuration knowledge base",
    "checked": true,
    "id": "349899ad280bcff77e39689df88e0fc7de852a2d",
    "semantic_title": "fastdiagp: an algorithm for parallelized direct diagnosis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25793": {
    "title": "Two Views of Constrained Differential Privacy: Belief Revision and Update",
    "volume": "main",
    "abstract": "In this paper, we provide two views of constrained differential private (DP) mechanisms. The first one is as belief revision. A constrained DP mechanism is obtained by standard probabilistic conditioning, and hence can be naturally implemented by Monte Carlo algorithms. The other is as belief update. A constrained DP is defined according to l2-distance minimization postprocessing or projection and hence can be naturally implemented by optimization algorithms. The main advantage of these two perspectives is that we can make full use of the machinery of belief revision and update to show basic properties for constrained differential privacy especially some important new composition properties. Within the framework established in this paper, constrained DP algorithms in the literature can be classified either as belief revision or belief update. At the end of the paper, we demonstrate their differences especially in utility on a couple of scenarios",
    "checked": true,
    "id": "bc06e5d3cf4ac73f5b6786ac296f9fec15026800",
    "semantic_title": "two views of constrained differential privacy: belief revision and update",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25794": {
    "title": "Copyright-Certified Distillation Dataset: Distilling One Million Coins into One Bitcoin with Your Private Key",
    "volume": "main",
    "abstract": "The rapid development of neural network dataset distillation in recent years has provided new ideas in many areas such as continuous learning, neural network architecture search and privacy preservation. Dataset distillation is a very effective method to distill large training datasets into small data, thus ensuring that the test accuracy of models trained on their synthesized small datasets matches that of models trained on the full dataset. Thus, dataset distillation itself is commercially valuable, not only for reducing training costs, but also for compressing storage costs and significantly reducing the training costs of deep learning. However, copyright protection for dataset distillation has not been proposed yet, so we propose the first method to protect intellectual property by embedding watermarks in the dataset distillation process. Our approach not only popularizes the dataset distillation technique, but also authenticates the ownership of the distilled dataset by the models trained on that distilled dataset",
    "checked": true,
    "id": "05cad00486c1ab88452d9b355477851e1bca9075",
    "semantic_title": "copyright-certified distillation dataset: distilling one million coins into one bitcoin with your private key",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25795": {
    "title": "DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing",
    "volume": "main",
    "abstract": "In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms baseline models on DH-KG, according to experimental results. Finally, we provide an example of how this technology can be used to treat hypertension. Our model and new datasets are publicly available",
    "checked": true,
    "id": "dc26b544e5bbdd149d6e52ab6e3fe155684829a2",
    "semantic_title": "dhge: dual-view hyper-relational knowledge graph embedding for link prediction and entity typing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25796": {
    "title": "Automated Verification of Propositional Agent Abstraction for Classical Planning via CTLK Model Checking",
    "volume": "main",
    "abstract": "Abstraction has long been an effective mechanism to help find a solution in classical planning. Agent abstraction, based on the situation calculus, is a promising explainable framework for agent planning, yet its automation is still far from being tackled. In this paper, we focus on a propositional version of agent abstraction designed for finite-state systems. We investigate the automated verification of the existence of propositional agent abstraction, given a finite-state system and a mapping indicating an abstraction for it. By formalizing sound, complete and deterministic properties of abstractions in a general framework, we show that the verification task can be reduced to the task of model checking against CTLK specifications. We implemented a prototype system, and validated the viability of our approach through experimentation on several domains from classical planning",
    "checked": true,
    "id": "072f7b74cf9bf19d6b498aed1aeb23ece57cc1e9",
    "semantic_title": "automated verification of propositional agent abstraction for classical planning via ctlk model checking",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25797": {
    "title": "Efficient Answer Enumeration in Description Logics with Functional Roles",
    "volume": "main",
    "abstract": "We study the enumeration of answers to ontology-mediated queries when the ontology is formulated in a description logic that supports functional roles and the query is a CQ. In particular, we show that enumeration is possible with linear preprocessing and constant delay when a certain extension of the CQ (pertaining to functional roles) is acyclic and free-connex acyclic. This holds both for complete answers and for partial answers. We provide matching lower bounds for the case where the query is self-join free",
    "checked": false,
    "id": "22eddbaef959961c58ec8b23ea5920c2a1fc75de",
    "semantic_title": "efficient answer enumeration in description logics with functional roles - extended version",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25798": {
    "title": "Distributed Spectrum-Based Fault Localization",
    "volume": "main",
    "abstract": "Spectrum-Based Fault Localization (SFL) is a popular approach for diagnosing faulty systems. SFL algorithms are inherently centralized, where observations are collected and analyzed by a single diagnoser. Applying SFL to diagnose distributed systems is challenging, especially when communication is costly and there are privacy concerns. We propose two SFL-based algorithms that are designed for distributed systems: one for diagnosing a single faulty component and one for diagnosing multiple faults. We analyze these algorithms theoretically and empirically. Our analysis shows that the distributed SFL algorithms we developed output identical diagnoses to centralized SFL while preserving privacy",
    "checked": true,
    "id": "28ac25147389c16bf5ea4d0fdaa3b1d7bd4f8cea",
    "semantic_title": "distributed spectrum-based fault localization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25799": {
    "title": "Multi-Level Wavelet Mapping Correlation for Statistical Dependence Measurement: Methodology and Performance",
    "volume": "main",
    "abstract": "We propose a new criterion for measuring dependence between two real variables, namely, Multi-level Wavelet Mapping Correlation (MWMC). MWMC can capture the nonlinear dependencies between variables by measuring their correlation under different levels of wavelet mappings. We show that the empirical estimate of MWMC converges exponentially to its population quantity. To support independence test better with MWMC, we further design a permutation test based on MWMC and prove that our test can not only control the type I error rate (the rate of false positives) well but also ensure that the type II error rate (the rate of false negatives) is upper bounded by O(1/n) (n is the sample size) with finite permutations. By extensive experiments on (conditional) independence tests and causal discovery, we show that our method outperforms existing independence test methods",
    "checked": true,
    "id": "61a6c57d88c71dffc1cd0b9ee9a424fa4f5a84b3",
    "semantic_title": "multi-level wavelet mapping correlation for statistical dependence measurement: methodology and performance",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25800": {
    "title": "Learning Interpretable Temporal Properties from Positive Examples Only",
    "volume": "main",
    "abstract": "We consider the problem of explaining the temporal behavior of black-box systems using human-interpretable models. Following recent research trends, we rely on the fundamental yet interpretable models of deterministic finite automata (DFAs) and linear temporal logic (LTL_f) formulas. In contrast to most existing works for learning DFAs and LTL_f formulas, we consider learning from only positive examples. Our motivation is that negative examples are generally difficult to observe, in particular, from black-box systems. To learn meaningful models from positive examples only, we design algorithms that rely on conciseness and language minimality of models as regularizers. Our learning algorithms are based on two approaches: a symbolic and a counterexample-guided one. The symbolic approach exploits an efficient encoding of language minimality as a constraint satisfaction problem, whereas the counterexample-guided one relies on generating suitable negative examples to guide the learning. Both approaches provide us with effective algorithms with minimality guarantees on the learned models. To assess the effectiveness of our algorithms, we evaluate them on a few practical case studies",
    "checked": true,
    "id": "713bf335895f27fa0f08a3366cb9f74ad8523d01",
    "semantic_title": "learning interpretable temporal properties from positive examples only",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25801": {
    "title": "Editing Boolean Classifiers: A Belief Change Perspective",
    "volume": "main",
    "abstract": "This paper is about editing Boolean classifiers, i.e., determining how a Boolean classifier should be modified when new pieces of evidence must be incorporated. Our main goal is to delineate what are the rational ways of making such edits. This goes through a number of rationality postulates inspired from those considered so far for belief revision. We give a representation theorem and present some families of edit operators satisfying the postulates",
    "checked": true,
    "id": "f7d7e2906b9de71b55a36e155e430c80771df3c0",
    "semantic_title": "editing boolean classifiers: a belief change perspective",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25802": {
    "title": "Implementing Bounded Revision via Lexicographic Revision and C-revision",
    "volume": "main",
    "abstract": "New information in the context of real life settings usually is accompanied by some kind of supplementary information that indicates context, reliability, or expertise of the information's source. Bounded Revision (BR) displays an iterated belief revision mechanism that takes as input a new information accompanied by a reference sentence acting as supplementary information, which specifies the depth with which the new input shall be integrated in the posterior belief state. The reference sentence specifies which worlds in the prior belief state are affected by the change mechanism. We show that Bounded Revision can be characterized by three simple, yet elegant postulates and corresponds to a special case of a lexicographic revision, which inherits all relevant features of BR. Furthermore, we present methodological implementations of BR including conditional revision with c-revisions, making it directly usable for conditional revision tools",
    "checked": true,
    "id": "37defded961bbaa40a5a2e4b2e7a4d64fc810761",
    "semantic_title": "implementing bounded revision via lexicographic revision and c-revision",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25803": {
    "title": "Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer",
    "volume": "main",
    "abstract": "Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases",
    "checked": true,
    "id": "4020f2736d49368fc38b3f0c59c6f8b90e29e355",
    "semantic_title": "multi-aspect explainable inductive relation prediction by sentence transformer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25804": {
    "title": "Learning to Break Symmetries for Efficient Optimization in Answer Set Programming",
    "volume": "main",
    "abstract": "The ability to efficiently solve hard combinatorial optimization problems is a key prerequisite to various applications of declarative programming paradigms. Symmetries in solution candidates pose a significant challenge to modern optimization algorithms since the enumeration of such candidates might substantially reduce their performance. This paper proposes a novel approach using Inductive Logic Programming (ILP) to lift symmetry-breaking constraints for optimization problems modeled in Answer Set Programming (ASP). Given an ASP encoding with optimization statements and a set of small representative instances, our method augments ground ASP programs with auxiliary normal rules enabling the identification of symmetries using existing tools, like SBASS. Then, the obtained symmetries are lifted to first-order constraints with ILP. We prove the correctness of our method and evaluate it on real-world optimization problems from the domain of automated configuration. Our experiments show significant improvements of optimization performance due to the learned first-order constraints",
    "checked": true,
    "id": "520d96e06afb9ab0ad9afc3f3a6c31b3fae3d6de",
    "semantic_title": "learning to break symmetries for efficient optimization in answer set programming",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25805": {
    "title": "On Undisputed Sets in Abstract Argumentation",
    "volume": "main",
    "abstract": "We introduce the notion of an undisputed set for abstract argumentation frameworks, which is a conflict-free set of arguments, such that its reduct contains no non-empty admissible set. We show that undisputed sets, and the stronger notion of strongly undisputed sets, provide a meaningful approach to weaken admissibility and deal with the problem of attacks from self-attacking arguments, in a similar manner as the recently introduced notion of weak admissibility. We investigate the properties of our new semantical notions and show certain relationships to classical semantics, in particular that undisputed sets are a generalisation of preferred extensions and strongly undisputed sets are a generalisation of stable extensions. We also investigate the computational complexity of standard reasoning tasks with these new notions and show that they lie on the second and third level of the polynomial hierarchy, respectively",
    "checked": true,
    "id": "a0d92572c348b42c3cdbc7b8ab2c44d0ba8b5a00",
    "semantic_title": "on undisputed sets in abstract argumentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25806": {
    "title": "Neurosymbolic Reasoning and Learning with Restricted Boltzmann Machines",
    "volume": "main",
    "abstract": "Knowledge representation and reasoning in neural networks has been a long-standing endeavour which has attracted much attention recently. The principled integration of reasoning and learning in neural networks is a main objective of the area of neurosymbolic Artificial Intelligence. In this paper, a neurosymbolic system is introduced that can represent any propositional logic formula. A proof of equivalence is presented showing that energy minimization in restricted Boltzmann machines corresponds to logical reasoning. We demonstrate the application of our approach empirically on logical reasoning and learning from data and knowledge. Experimental results show that reasoning can be performed effectively for a class of logical formulae. Learning from data and knowledge is also evaluated in comparison with learning of logic programs using neural networks. The results show that our approach can improve on state-of-the-art neurosymbolic systems. The theorems and empirical results presented in this paper are expected to reignite the research on the use of neural networks as massively-parallel models for logical reasoning and promote the principled integration of reasoning and learning in deep networks",
    "checked": true,
    "id": "3b1d4dd5f407f08295bc5d9865f5b48bff3d67df",
    "semantic_title": "neurosymbolic reasoning and learning with restricted boltzmann machines",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25807": {
    "title": "Materialisation-Based Reasoning in DatalogMTL with Bounded Intervals",
    "volume": "main",
    "abstract": "DatalogMTL is a powerful extension of Datalog with operators from metric temporal logic (MTL), which has received significant attention in recent years. In this paper, we investigate materialisation-based reasoning (a.k.a. forward chaining) in the context of DatalogMTL programs and datasets with bounded intervals, where partial representations of the canonical model are obtained through successive rounds of rule applications. Although materialisation does not naturally terminate in this setting, it is known that the structure of canonical models is ultimately periodic. Our first contribution in this paper is a detailed analysis of the periodic structure of canonical models; in particular, we formulate saturation conditions whose satisfaction by a partial materialisation implies an ability to recover the full canonical model via unfolding; this allows us to compute the actual periods describing the repeating parts of the canonical model as well as to establish concrete bounds on the number of rounds of rule applications required to achieve saturation. Based on these theoretical results, we propose a practical reasoning algorithm where saturation can be efficiently detected as materialisation progresses, and where the relevant periods used to evaluate entailment of queries via unfolding are efficiently computed. We have implemented our algorithm and our experiments suggest that our approach is both scalable and robust",
    "checked": true,
    "id": "d4a432b0cc0a5d488915053536af171eb4bd6055",
    "semantic_title": "materialisation-based reasoning in datalogmtl with bounded intervals",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25808": {
    "title": "Efficient Extraction of EL-Ontology Deductive Modules",
    "volume": "main",
    "abstract": "Because widely used real-world ontologies are often complex and large, one important challenge has emerged: designing tools for users to focus on sub-ontologies corresponding to their specific interests. To this end, various modules have been introduced to provide concise ontology views. This work concentrates on extracting deductive modules that preserve logical entailment over a given vocabulary. Existing deductive module proposals are either inefficient from a computing point of view or unsatisfactory from a quality point of view because the modules extracted are not concise enough. For example, minimal modules guarantee the most concise results, but their computation is highly time-consuming, while ⊥⊤∗-modules are easy to compute but usually contain many redundant items. To overcome computation cost and lack of quality, we propose to compute two kinds of deductive modules called pseudo-minimal modules and complete modules for EL-ontology. Our deductive module definitions rely on associating a tree representation with an ontology, and their computation is based on SAT encoding. Our experiments on real-world ontologies show that our pseudo-minimal modules are indeed minimal modules in almost all cases (98.9%), and computing pseudo-minimal modules is more efficient (99.79 times faster on average) than the state-of-the-art method Zoom for computing minimal modules. Also, our complete modules are more compact than ⊥⊤∗-modules, but their computation time remains comparable. Finally, note that our proposal applies to EL-ontologies while Zoom only works for EL-terminologies",
    "checked": true,
    "id": "6d45b678c6e09dcff3928d060e28e5d621d28272",
    "semantic_title": "efficient extraction of el-ontology deductive modules",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25809": {
    "title": "Visually Grounded Commonsense Knowledge Acquisition",
    "volume": "main",
    "abstract": "Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel contrastive attention mechanism. Comprehensive experimental results in held-out and human evaluation show that CLEVER can extract commonsense knowledge in promising quality, outperforming pre-trained language model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted commonsense scores show strong correlation with human judgment with a 0.78 Spearman coefficient. Moreover, the extracted commonsense can also be grounded into images with reasonable interpretability. The data and codes can be obtained at https://github.com/thunlp/CLEVER",
    "checked": true,
    "id": "03c4ecc2796ecde6a93562fdd149cea10157f805",
    "semantic_title": "visually grounded commonsense knowledge acquisition",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25810": {
    "title": "DNG: Taxonomy Expansion by Exploring the Intrinsic Directed Structure on Non-gaussian Space",
    "volume": "main",
    "abstract": "Taxonomy expansion is the process of incorporating a large number of additional nodes (i.e., ''queries'') into an existing taxonomy (i.e., ''seed''), with the most important step being the selection of appropriate positions for each query. Enormous efforts have been made by exploring the seed's structure. However, existing approaches are deficient in their mining of structural information in two ways: poor modeling of the hierarchical semantics and failure to capture directionality of the is-a relation. This paper seeks to address these issues by explicitly denoting each node as the combination of inherited feature (i.e., structural part) and incremental feature (i.e., supplementary part). Specifically, the inherited feature originates from ''parent'' nodes and is weighted by an inheritance factor. With this node representation, the hierarchy of semantics in taxonomies (i.e., the inheritance and accumulation of features from ''parent'' to ''child'') could be embodied. Additionally, based on this representation, the directionality of the is-a relation could be easily translated into the irreversible inheritance of features. Inspired by the Darmois-Skitovich Theorem, we implement this irreversibility by a non-Gaussian constraint on the supplementary feature. A log-likelihood learning objective is further utilized to optimize the proposed model (dubbed DNG), whereby the required non-Gaussianity is also theoretically ensured. Extensive experimental results on two real-world datasets verify the superiority of DNG relative to several strong baselines",
    "checked": true,
    "id": "5db9df997254ab379f195d7ad494e7ccbab4d91d",
    "semantic_title": "dng: taxonomy expansion by exploring the intrinsic directed structure on non-gaussian space",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25811": {
    "title": "Quality-Aware Self-Training on Differentiable Synthesis of Rare Relational Data",
    "volume": "main",
    "abstract": "Data scarcity is a very common real-world problem that poses a major challenge to data-driven analytics. Although a lot of data-balancing approaches have been proposed to mitigate this problem, they may drop some useful information or fall into the overfitting problem. Generative Adversarial Network (GAN) based data synthesis methods can alleviate such a problem but lack of quality control over the generated samples. Moreover, the latent associations between the attribute set and the class labels in a relational data cannot be easily captured by a vanilla GAN. In light of this, we introduce an end-to-end self-training scheme (namely, Quality-Aware Self-Training) for rare relational data synthesis, which generates labeled synthetic data via pseudo labeling on GAN-based synthesis. We design a semantic pseudo labeling module to first control the quality of the generated features/samples, then calibrate their semantic labels via a classifier committee consisting of multiple pre-trained shallow classifiers. The high-confident generated samples with calibrated pseudo labels are then fed into a semantic classification network as augmented samples for self-training. We conduct extensive experiments on 20 benchmark datasets of different domains, including 14 industrial datasets. The results show that our method significantly outperforms state-of-the-art methods, including two recent GAN-based data synthesis schemes. Codes are available at https://github.com/yaxinhou/QAST",
    "checked": true,
    "id": "bbf8add6afcfba8be55b92dc94dad68d34db6594",
    "semantic_title": "quality-aware self-training on differentiable synthesis of rare relational data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25812": {
    "title": "Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling",
    "volume": "main",
    "abstract": "Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stability, and locality as training objectives. Extensive experiments in different domains demonstrate that our method exhibits promising interpretability and competitive accuracy",
    "checked": true,
    "id": "10bbe48c262a4fcf666bf93855bfd3cafb9f71f7",
    "semantic_title": "learning to select prototypical parts for interpretable sequential data modeling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25813": {
    "title": "McOmet: Multimodal Fusion Transformer for Physical Audiovisual Commonsense Reasoning",
    "volume": "main",
    "abstract": "Physical commonsense reasoning is essential for building reliable and interpretable AI systems, which involves a general understanding of the physical properties and affordances of everyday objects, how these objects can be manipulated, and how they interact with others. It is fundamentally a multi-modal task, as physical properties are manifested through multiple modalities, including vision and acoustics. In this work, we present a unified framework, named Multimodal Commonsense Transformer (MCOMET), for physical audiovisual commonsense reasoning. MCOMET has two intriguing properties: i) it fully mines higher-ordered temporal relationships across modalities (e.g., pairs, triplets, and quadruplets); and ii) it restricts the cross-modal flow through the feature collection and propagation mechanism along with tight fusion bottlenecks, forcing the model to attend the most relevant parts in each modality and suppressing the dissemination of noisy information. We evaluate our model on a very recent public benchmark, PACS. Results show that MCOMET significantly outperforms a variety of strong baselines, revealing powerful multi-modal commonsense reasoning capabilities",
    "checked": true,
    "id": "4cf34a7c4b622f48047df066d8be23f8984fbf09",
    "semantic_title": "mcomet: multimodal fusion transformer for physical audiovisual commonsense reasoning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25814": {
    "title": "Approximating Full Conformal Prediction at Scale via Influence Functions",
    "volume": "main",
    "abstract": "Conformal prediction (CP) is a wrapper around traditional machine learning models, giving coverage guarantees under the sole assumption of exchangeability; in classification problems, a CP guarantees that the error rate is at most a chosen significance level, irrespective of whether the underlying model is misspecified. However, the prohibitive computational costs of full CP led researchers to design scalable alternatives, which alas do not attain the same guarantees or statistical power of full CP. In this paper, we use influence functions to efficiently approximate full CP. We prove that our method is a consistent approximation of full CP, and empirically show that the approximation error becomes smaller as the training set increases; e.g., for 1,000 training points the two methods output p-values that are <0.001 apart: a negligible error for any practical application. Our methods enable scaling full CP to large real-world datasets. We compare our full CP approximation (ACP) to mainstream CP alternatives, and observe that our method is computationally competitive whilst enjoying the statistical predictive power of full CP",
    "checked": true,
    "id": "b29b6315506a0db2cd39cb9faee82c2e8cc0105b",
    "semantic_title": "approximating full conformal prediction at scale via influence functions",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25815": {
    "title": "Efficient Distributed Inference of Deep Neural Networks via Restructuring and Pruning",
    "volume": "main",
    "abstract": "In this paper, we consider the parallel implementation of an already-trained deep model on multiple processing nodes (a.k.a. workers). Specifically, we investigate as to how a deep model should be divided into several parallel sub-models, each of which is executed efficiently by a worker. Since latency due to synchronization and data transfer among workers negatively impacts the performance of the parallel implementation, it is desirable to have minimum interdependency among parallel sub-models. To achieve this goal, we propose to rearrange the neurons in the neural network, partition them (without changing the general topology of the neural network), and modify the weights such that the interdependency among sub-models is minimized under the computations and communications constraints of the workers while minimizing its impact on the performance of the model. We propose RePurpose, a layer-wise model restructuring and pruning technique that guarantees the performance of the overall parallelized model. To efficiently apply RePurpose, we propose an approach based on L0 optimization and the Munkres assignment algorithm. We show that, compared to the existing methods, RePurpose significantly improves the efficiency of the distributed inference via parallel implementation, both in terms of communication and computational complexity",
    "checked": true,
    "id": "f4850283e7220666b04ee78d43bdc0db38032271",
    "semantic_title": "efficient distributed inference of deep neural networks via restructuring and pruning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25816": {
    "title": "Symbolic Metamodels for Interpreting Black-Boxes Using Primitive Functions",
    "volume": "main",
    "abstract": "One approach for interpreting black-box machine learning models is to find a global approximation of the model using simple interpretable functions, which is called a metamodel (a model of the model). Approximating the black-box with a metamodel can be used to 1) estimate instance-wise feature importance; 2) understand the functional form of the model; 3) analyze feature interactions. In this work, we propose a new method for finding interpretable metamodels. Our approach utilizes Kolmogorov superposition theorem, which expresses multivariate functions as a composition of univariate functions (our primitive parameterized functions). This composition can be represented in the form of a tree. Inspired by symbolic regression, we use a modified form of genetic programming to search over different tree configurations. Gradient descent (GD) is used to optimize the parameters of a given configuration. Our method is a novel memetic algorithm that uses GD not only for training numerical constants but also for the training of building blocks. Using several experiments, we show that our method outperforms recent metamodeling approaches suggested for interpreting black-boxes",
    "checked": true,
    "id": "b457b163f3e2ffd700918ed8e52d8c18c1115b21",
    "semantic_title": "symbolic metamodels for interpreting black-boxes using primitive functions",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25817": {
    "title": "Utilizing Prior Solutions for Reward Shaping and Composition in Entropy-Regularized Reinforcement Learning",
    "volume": "main",
    "abstract": "In reinforcement learning (RL), the ability to utilize prior knowledge from previously solved tasks can allow agents to quickly solve new problems. In some cases, these new problems may be approximately solved by composing the solutions of previously solved primitive tasks (task composition). Otherwise, prior knowledge can be used to adjust the reward function for a new problem, in a way that leaves the optimal policy unchanged but enables quicker learning (reward shaping). In this work, we develop a general framework for reward shaping and task composition in entropy-regularized RL. To do so, we derive an exact relation connecting the optimal soft value functions for two entropy-regularized RL problems with different reward functions and dynamics. We show how the derived relation leads to a general result for reward shaping in entropy-regularized RL. We then generalize this approach to derive an exact relation connecting optimal value functions for the composition of multiple tasks in entropy-regularized RL. We validate these theoretical contributions with experiments showing that reward shaping and task composition lead to faster learning in various settings",
    "checked": true,
    "id": "3098fd3d604086202f51aee3b5e6071639908be1",
    "semantic_title": "utilizing prior solutions for reward shaping and composition in entropy-regularized reinforcement learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25818": {
    "title": "Clustering What Matters: Optimal Approximation for Clustering with Outliers",
    "volume": "main",
    "abstract": "Clustering with outliers is one of the most fundamental problems in Computer Science. Given a set X of n points and two numbers k and m, the clustering with outliers aims to exclude m points from X, and partition the remaining points into k clusters that minimizes a certain cost function. In this paper, we give a general approach for solving clustering with outliers, which results in a fixed-parameter tractable (FPT) algorithm in k and m (i.e., an algorithm with running time of the form f(k, m) * poly(n) for some function f), that almost matches the approximation ratio for its outlier-free counterpart. As a corollary, we obtain FPT approximation algorithms with optimal approximation ratios for k-Median and k-Means with outliers in general and Euclidean metrics. We also exhibit more applications of our approach to other variants of the problem that impose additional constraints on the clustering, such as fairness or matroid constraints",
    "checked": true,
    "id": "443ed32448436d6710b875911c6944bde47a35cf",
    "semantic_title": "clustering what matters: optimal approximation for clustering with outliers",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25819": {
    "title": "Contrastive Classification and Representation Learning with Probabilistic Interpretation",
    "volume": "main",
    "abstract": "Cross entropy loss has served as the main objective function for classification-based tasks. Widely deployed for learning neural network classifiers, it shows both effectiveness and a probabilistic interpretation. Recently, after the success of self supervised contrastive representation learning methods, supervised contrastive methods have been proposed to learn representations and have shown superior and more robust performance, compared to solely training with cross entropy loss. However, cross entropy loss is still needed to train the final classification layer. In this work, we investigate the possibility of learning both the representation and the classifier using one objective function that combines the robustness of contrastive learning and the probabilistic interpretation of cross entropy loss. First, we revisit a previously proposed contrastive-based objective function that approximates cross entropy loss and present a simple extension to learn the classifier jointly. Second, we propose a new version of the supervised contrastive training that learns jointly the parameters of the classifier and the backbone of the network. We empirically show that these proposed objective functions demonstrate state-of-the-art performance and show a significant improvement over the standard cross entropy loss with more training stability and robustness in various challenging settings",
    "checked": true,
    "id": "5afecbef7a28dd25117bc00db3594a8e95968941",
    "semantic_title": "contrastive classification and representation learning with probabilistic interpretation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25820": {
    "title": "Simulating Network Paths with Recurrent Buffering Units",
    "volume": "main",
    "abstract": "Simulating physical network paths (e.g., Internet) is a cornerstone research problem in the emerging sub-field of AI-for-networking. We seek a model that generates end-to-end packet delay values in response to the time-varying load offered by a sender, which is typically a function of the previously output delays. The problem setting is unique, and renders the state-of-the-art text and time-series generative models inapplicable or ineffective. We formulate an ML problem at the intersection of dynamical systems, sequential decision making, and time-series modeling. We propose a novel grey-box approach to network simulation that embeds the semantics of physical network path in a new RNN-style model called Recurrent Buffering Unit, providing the interpretability of standard network simulator tools, the power of neural models, the efficiency of SGD-based techniques for learning, and yielding promising results on synthetic and real-world network traces",
    "checked": true,
    "id": "f2bc5f497daf770914737e1c2ad5ed5046a46bbe",
    "semantic_title": "simulating network paths with recurrent buffering units",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25821": {
    "title": "Fully Dynamic Online Selection through Online Contention Resolution Schemes",
    "volume": "main",
    "abstract": "We study fully dynamic online selection problems in an adversarial/stochastic setting that includes Bayesian online selection, prophet inequalities, posted price mechanisms, and stochastic probing problems subject to combinatorial constraints. In the classical ``incremental'' version of the problem, selected elements remain active until the end of the input sequence. On the other hand, in the fully dynamic version of the problem, elements stay active for a limited time interval, and then leave. This models, for example, the online matching of tasks to workers with task/worker-dependent working times, and sequential posted pricing of perishable goods. A successful approach to online selection problems in the adversarial setting is given by the notion of Online Contention Resolution Scheme (OCRS), that uses a priori information to formulate a linear relaxation of the underlying optimization problem, whose optimal fractional solution is rounded online for any adversarial order of the input sequence. Our main contribution is providing a general method for constructing an OCRS for fully dynamic online selection problems. Then, we show how to employ such OCRS to construct no-regret algorithms in a partial information model with semi-bandit feedback and adversarial inputs",
    "checked": true,
    "id": "a1a3675308d7b38e5b02d1b09031d84f4bf39c44",
    "semantic_title": "fully dynamic online selection through online contention resolution schemes",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25822": {
    "title": "Tree Learning: Optimal Sample Complexity and Algorithms",
    "volume": "main",
    "abstract": "We study the problem of learning a hierarchical tree representation of data from labeled samples, taken from an arbitrary (and possibly adversarial) distribution. Consider a collection of data tuples labeled according to their hierarchical structure. The smallest number of such tuples required in order to be able to accurately label subsequent tuples is of interest for data collection in machine learning. We present optimal sample complexity bounds for this problem in several learning settings, including (agnostic) PAC learning and online learning. Our results are based on tight bounds of the Natarajan and Littlestone dimensions of the associated problem. The corresponding tree classifiers can be constructed efficiently in near-linear time",
    "checked": true,
    "id": "2c4e7659c6d577ae7c1cac0d49daba5e28a7efce",
    "semantic_title": "tree learning: optimal sample complexity and algorithms",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25823": {
    "title": "Meta-Learning for Simple Regret Minimization",
    "volume": "main",
    "abstract": "We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d. from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over m bandit tasks with horizon n is mere O(m / √n). On the other hand, the meta simple regret of the frequentist algorithm is O(n√m + m/ √n). While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them empirically in several environments",
    "checked": true,
    "id": "4160cce636c0f5b40ff004a1489a9ad790e46e94",
    "semantic_title": "meta-learning for simple regret minimization",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25824": {
    "title": "Generalizing Downsampling from Regular Data to Graphs",
    "volume": "main",
    "abstract": "Downsampling produces coarsened, multi-resolution representations of data and it is used, for example, to produce lossy compression and visualization of large images, reduce computational costs, and boost deep neural representation learning. Unfortunately, due to their lack of a regular structure, there is still no consensus on how downsampling should apply to graphs and linked data. Indeed reductions in graph data are still needed for the goals described above, but reduction mechanisms do not have the same focus on preserving topological structures and properties, while allowing for resolution-tuning, as is the case in regular data downsampling. In this paper, we take a step in this direction, introducing a unifying interpretation of downsampling in regular and graph data. In particular, we define a graph coarsening mechanism which is a graph-structured counterpart of controllable equispaced coarsening mechanisms in regular data. We prove theoretical guarantees for distortion bounds on path lengths, as well as the ability to preserve key topological properties in the coarsened graphs. We leverage these concepts to define a graph pooling mechanism that we empirically assess in graph classification tasks, providing a greedy algorithm that allows efficient parallel implementation on GPUs, and showing that it compares favorably against pooling methods in literature",
    "checked": true,
    "id": "5de89665d72d63932c993fe5dec8dc0a9f21f9c1",
    "semantic_title": "generalizing downsampling from regular data to graphs",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25825": {
    "title": "PiCor: Multi-Task Deep Reinforcement Learning with Policy Correction",
    "volume": "main",
    "abstract": "Multi-task deep reinforcement learning (DRL) ambitiously aims to train a general agent that masters multiple tasks simultaneously. However, varying learning speeds of different tasks compounding with negative gradients interference makes policy learning inefficient. In this work, we propose PiCor, an efficient multi-task DRL framework that splits learning into policy optimization and policy correction phases. The policy optimization phase improves the policy by any DRL algothrim on the sampled single task without considering other tasks. The policy correction phase first constructs an adaptive adjusted performance constraint set. Then the intermediate policy learned by the first phase is constrained to the set, which controls the negative interference and balances the learning speeds across tasks. Empirically, we demonstrate that PiCor outperforms previous methods and significantly improves sample efficiency on simulated robotic manipulation and continuous control tasks. We additionally show that adaptive weight adjusting can further improve data efficiency and performance",
    "checked": true,
    "id": "ad17af39d0bfcc2498df27dc51de85adfac97704",
    "semantic_title": "picor: multi-task deep reinforcement learning with policy correction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25826": {
    "title": "Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm",
    "volume": "main",
    "abstract": "We consider the problem of constrained Markov decision process (CMDP) in continuous state actions spaces where the goal is to maximize the expected cumulative reward subject to some constraints. We propose a novel Conservative Natural Policy Gradient Primal Dual Algorithm (CNPGPD) to achieve zero constraint violation while achieving state of the art convergence results for the objective value function. For general policy parametrization, we prove convergence of value function to global optimal upto an approximation error due to restricted policy class. We improve the sample complexity of existing constrained NPGPD algorithm. To the best of our knowledge, this is the first work to establish zero constraint violation with Natural policy gradient style algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of proposed algorithm via experimental evaluations",
    "checked": true,
    "id": "72a305df18fe9010374504bce7c0ec485809b7c3",
    "semantic_title": "achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25827": {
    "title": "Optimal Sparse Recovery with Decision Stumps",
    "volume": "main",
    "abstract": "Decision trees are widely used for their low computational cost, good predictive performance, and ability to assess the importance of features. Though often used in practice for feature selection, the theoretical guarantees of these methods are not well understood. We here obtain a tight finite sample bound for the feature selection problem in linear regression using single-depth decision trees. We examine the statistical properties of these \"decision stumps\" for the recovery of the s active features from p total features, where s << p. Our analysis provides tight sample performance guarantees on high-dimensional sparse systems which align with the finite sample bound of O(s log p) as obtained by Lasso, improving upon previous bounds for both the median and optimal splitting criteria. Our results extend to the non-linear regime as well as arbitrary sub-Gaussian distributions, demonstrating that tree based methods attain strong feature selection properties under a wide variety of settings and further shedding light on the success of these methods in practice. As a byproduct of our analysis, we show that we can provably guarantee recovery even when the number of active features s is unknown. We further validate our theoretical results and proof methodology using computational experiments",
    "checked": true,
    "id": "9a4be3bc405aa5a003e44f1efd2715a1384abf22",
    "semantic_title": "optimal sparse recovery with decision stumps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25828": {
    "title": "Towards Efficient and Domain-Agnostic Evasion Attack with High-Dimensional Categorical Inputs",
    "volume": "main",
    "abstract": "Our work targets at searching feasible adversarial perturbation to attack a classifier with high-dimensional categorical inputs in a domain-agnostic setting. This is intrinsically a NP-hard knapsack problem where the exploration space becomes explosively larger as the feature dimension increases. Without the help of domain knowledge, solving this problem via heuristic method, such as Branch-and-Bound, suffers from exponential complexity, yet can bring arbitrarily bad attack results. We address the challenge via the lens of multi-armed bandit based combinatorial search. Our proposed method, namely FEAT, treats modifying each categorical feature as pulling an arm in multi-armed bandit programming. Our objective is to achieve highly efficient and effective attack using an Orthogonal Matching Pursuit (OMP)-enhanced Upper Confidence Bound (UCB) exploration strategy. Our theoretical analysis bounding the regret gap of FEAT guarantees its practical attack performance. In empirical analysis, we compare FEAT with other state-of-the-art domain-agnostic attack methods over various real-world categorical data sets of different applications. Substantial experimental observations confirm the expected efficiency and attack effectiveness of FEAT applied in different application scenarios. Our work further hints the applicability of FEAT for assessing the adversarial vulnerability of classification systems with high-dimensional categorical inputs",
    "checked": true,
    "id": "b1e1cc7d450895d4f947c86d123e3e560fa7eff9",
    "semantic_title": "towards efficient and domain-agnostic evasion attack with high-dimensional categorical inputs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25829": {
    "title": "Fairness and Welfare Quantification for Regret in Multi-Armed Bandits",
    "volume": "main",
    "abstract": "We extend the notion of regret with a welfarist perspective. Focussing on the classic multi-armed bandit (MAB) framework, the current work quantifies the performance of bandit algorithms by applying a fundamental welfare function, namely the Nash social welfare (NSW) function. This corresponds to equating algorithm's performance to the geometric mean of its expected rewards and leads us to the study of Nash regret, defined as the difference between the - a priori unknown - optimal mean (among the arms) and the algorithm's performance. Since NSW is known to satisfy fairness axioms, our approach complements the utilitarian considerations of average (cumulative) regret, wherein the algorithm is evaluated via the arithmetic mean of its expected rewards. This work develops an algorithm that, given the horizon of play T, achieves a Nash regret of O ( sqrt{(k log T)/T} ), here k denotes the number of arms in the MAB instance. Since, for any algorithm, the Nash regret is at least as much as its average regret (the AM-GM inequality), the known lower bound on average regret holds for Nash regret as well. Therefore, our Nash regret guarantee is essentially tight. In addition, we develop an anytime algorithm with a Nash regret guarantee of O( sqrt{(k log T)/T} log T )",
    "checked": true,
    "id": "e4470ac0408e958123758f424f85105efdf71fa3",
    "semantic_title": "fairness and welfare quantification for regret in multi-armed bandits",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25830": {
    "title": "Alternating Layered Variational Quantum Circuits Can Be Classically Optimized Efficiently Using Classical Shadows",
    "volume": "main",
    "abstract": "Variational quantum algorithms (VQAs) are the quantum analog of classical neural networks (NNs). A VQA consists of a parameterized quantum circuit (PQC) which is composed of multiple layers of ansatzes (simpler PQCs, which are an analogy of NN layers) that differ only in selections of parameters. Previous work has identified the alternating layered ansatz as potentially a new standard ansatz in near-term quantum computing. Indeed, shallow alternating layered VQAs are easy to implement and have been shown to be both trainable and expressive. In this work, we introduce a training algorithm with an exponential reduction in training cost of such VQAs. Moreover, our algorithm uses classical shadows of quantum input data, and can hence be run on a classical computer with rigorous performance guarantees. We demonstrate 2-3 orders of magnitude improvement in the training cost using our algorithm for the example problems of finding state preparation circuits and the quantum autoencoder",
    "checked": true,
    "id": "e0499cb8e5a94768e79b577c83fb464b5557d46f",
    "semantic_title": "alternating layered variational quantum circuits can be classically optimized efficiently using classical shadows",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25831": {
    "title": "Learnable Spectral Wavelets on Dynamic Graphs to Capture Global Interactions",
    "volume": "main",
    "abstract": "Learning on evolving(dynamic) graphs has caught the attention of researchers as static methods exhibit limited performance in this setting. The existing methods for dynamic graphs learn spatial features by local neighborhood aggregation, which essentially only captures the low pass signals and local interactions. In this work, we go beyond current approaches to incorporate global features for effectively learning representations of a dynamically evolving graph. We propose to do so by capturing the spectrum of the dynamic graph. Since static methods to learn the graph spectrum would not consider the history of the evolution of the spectrum as the graph evolves with time, we propose an approach to learn the graph wavelets to capture this evolving spectra. Further, we propose a framework that integrates the dynamically captured spectra in the form of these learnable wavelets into spatial features for incorporating local and global interactions. Experiments on eight standard datasets show that our method significantly outperforms related methods on various tasks for dynamic graphs",
    "checked": true,
    "id": "e0955cd4702c745d6a93bb5edef64ca143358467",
    "semantic_title": "learnable spectral wavelets on dynamic graphs to capture global interactions",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25832": {
    "title": "Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models",
    "volume": "main",
    "abstract": "We introduce equi-tuning, a novel fine-tuning method that transforms (potentially non-equivariant) pretrained models into group equivariant models while incurring minimum L_2 loss between the feature representations of the pretrained and the equivariant models. Large pretrained models can be equi-tuned for different groups to satisfy the needs of various downstream tasks. Equi-tuned models benefit from both group equivariance as an inductive bias and semantic priors from pretrained models. We provide applications of equi-tuning on three different tasks: image classification, compositional generalization in language, and fairness in natural language generation (NLG). We also provide a novel group-theoretic definition for fairness in NLG. The effectiveness of this definition is shown by testing it against a standard empirical method of fairness in NLG. We provide experimental results for equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and Densenet for image classification; RNNs, GRUs, and LSTMs for compositional generalization; and GPT2 for fairness in NLG. We test these models on benchmark datasets across all considered tasks to show the generality and effectiveness of the proposed method",
    "checked": true,
    "id": "45693b1fa3c1fcb89a67827fd63b45f0bacf0185",
    "semantic_title": "equi-tuning: group equivariant fine-tuning of pretrained models",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25833": {
    "title": "Sustaining Fairness via Incremental Learning",
    "volume": "main",
    "abstract": "Machine learning systems are often deployed for making critical decisions like credit lending, hiring, etc. While making decisions, such systems often encode the user's demographic information (like gender, age) in their intermediate representations. This can lead to decisions that are biased towards specific demographics. Prior work has focused on debiasing intermediate representations to ensure fair decisions. However, these approaches fail to remain fair with changes in the task or demographic distribution. To ensure fairness in the wild, it is important for a system to adapt to such changes as it accesses new data in an incremental fashion. In this work, we propose to address this issue by introducing the problem of learning fair representations in an incremental learning setting. To this end, we present Fairness-aware Incremental Representation Learning (FaIRL), a representation learning system that can sustain fairness while incrementally learning new tasks. FaIRL is able to achieve fairness and learn new tasks by controlling the rate-distortion function of the learned representations. Our empirical evaluations show that FaIRL is able to make fair decisions while achieving high performance on the target task, outperforming several baselines",
    "checked": true,
    "id": "e1583c318a4b7f97df7c6ab9a55878bd12788a8e",
    "semantic_title": "sustaining fairness via incremental learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25834": {
    "title": "Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling",
    "volume": "main",
    "abstract": "In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world (Wet Chicken), Pendulum, and Hopper. In these experiments, we setup an active learning framework and evaluate each model's capability at measuring aleatoric and epistemic uncertainty. The results show the advantages of using NF ensembles in capturing complicated aleatoric while maintaining accurate epistemic uncertainty estimates",
    "checked": true,
    "id": "2aeafb78c9fead00f0fe9c8f655ccf0b82bf0390",
    "semantic_title": "normalizing flow ensembles for rich aleatoric and epistemic uncertainty modeling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25835": {
    "title": "An Improved Algorithm for Online Min-Sum Set Cover",
    "volume": "main",
    "abstract": "We study a fundamental model of online preference aggregation, where an algorithm maintains an ordered list of n elements. An input is a stream of preferred sets R_1, R_2, ..., R_t, ... Upon seeing R_t and without knowledge of any future sets, an algorithm has to rerank elements (change the list ordering), so that at least one element of R_t is found near the list front. The incurred cost is a sum of the list update costs (the number of swaps of neighboring list elements) and access cost (the position of the first element of R_t on the list). This scenario occurs naturally in applications such as ordering items in an online shop using aggregated preferences of shop customers. The theoretical underpinning of this problem is known as Min-Sum Set Cover. Unlike previous work that mostly studied the performance of an online algorithm ALG in comparison to the static optimal solution (a single optimal list ordering), in this paper, we study an arguably harder variant where the benchmark is the provably stronger optimal dynamic solution OPT (that may also modify the list ordering). In terms of an online shop, this means that the aggregated preferences of its user base evolve with time. We construct a computationally efficient randomized algorithm whose competitive ratio (ALG-to-OPT cost ratio) is O(r^2) and prove the existence of a deterministic O(r^4)-competitive algorithm. Here, r is the maximum cardinality of sets R_t. This is the first algorithm whose ratio does not depend on n: the previously best algorithm for this problem was O(r^(3/2) * n^(1/2))-competitive and Ω(r) is a lower bound on the performance of any deterministic online algorithm",
    "checked": true,
    "id": "0317a6788b14e4adf34fdced4826c6ca556b0dae",
    "semantic_title": "an improved algorithm for online min-sum set cover",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25836": {
    "title": "AutoInit: Analytic Signal-Preserving Weight Initialization for Neural Networks",
    "volume": "main",
    "abstract": "Neural networks require careful weight initialization to prevent signals from exploding or vanishing. Existing initialization schemes solve this problem in specific cases by assuming that the network has a certain activation function or topology. It is difficult to derive such weight initialization strategies, and modern architectures therefore often use these same initialization schemes even though their assumptions do not hold. This paper introduces AutoInit, a weight initialization algorithm that automatically adapts to different neural network architectures. By analytically tracking the mean and variance of signals as they propagate through the network, AutoInit appropriately scales the weights at each layer to avoid exploding or vanishing signals. Experiments demonstrate that AutoInit improves performance of convolutional, residual, and transformer networks across a range of activation function, dropout, weight decay, learning rate, and normalizer settings, and does so more reliably than data-dependent initialization methods. This flexibility allows AutoInit to initialize models for everything from small tabular tasks to large datasets such as ImageNet. Such generality turns out particularly useful in neural architecture search and in activation function discovery. In these settings, AutoInit initializes each candidate appropriately, making performance evaluations more accurate. AutoInit thus serves as an automatic configuration tool that makes design of new neural network architectures more robust. The AutoInit package provides a wrapper around TensorFlow models and is available at https://github.com/cognizant-ai-labs/autoinit",
    "checked": true,
    "id": "f42e704bbb28331cf1eee2b09f41bcfc0815483e",
    "semantic_title": "autoinit: analytic signal-preserving weight initialization for neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25837": {
    "title": "A Parameterized Theory of PAC Learning",
    "volume": "main",
    "abstract": "Probably Approximately Correct (i.e., PAC) learning is a core concept of sample complexity theory, and efficient PAC learnability is often seen as a natural counterpart to the class P in classical computational complexity. But while the nascent theory of parameterized complexity has allowed us to push beyond the P-NP \"dichotomy\" in classical computational complexity and identify the exact boundaries of tractability for numerous problems, there is no analogue in the domain of sample complexity that could push beyond efficient PAC learnability. As our core contribution, we fill this gap by developing a theory of parameterized PAC learning which allows us to shed new light on several recent PAC learning results that incorporated elements of parameterized complexity. Within the theory, we identify not one but two notions of fixed-parameter learnability that both form distinct counterparts to the class FPT - the core concept at the center of the parameterized complexity paradigm - and develop the machinery required to exclude fixed-parameter learnability. We then showcase the applications of this theory to identify refined boundaries of tractability for CNF and DNF learning as well as for a range of learning problems on graphs",
    "checked": true,
    "id": "26b394a90119f8ee2a9820d92cc28780c600cca7",
    "semantic_title": "a parameterized theory of pac learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25838": {
    "title": "Fully-Dynamic Decision Trees",
    "volume": "main",
    "abstract": "We develop the first fully dynamic algorithm that maintains a decision tree over an arbitrary sequence of insertions and deletions of labeled examples. Given ε>0 our algorithm guarantees that, at every point in time, every node of the decision tree uses a split with Gini gain within an additive ε of the optimum. For real-valued features the algorithm has an amortized running time per insertion/deletion of O((d·log³n)/ε²), which improves to O((d·log²n)/ε) for binary or categorical features, while it uses space O(n·d), where n is the maximum number of examples at any point in time and d is the number of features. Our algorithm is nearly optimal, as we show that any algorithm with similar guarantees requires amortized running time Ω(d) and space Ω(n·d/polylog(nd)). We complement our theoretical results with an extensive experimental evaluation on real-world data, showing the effectiveness of our algorithm",
    "checked": true,
    "id": "4b94b06d0b0e12a84c83ee644d3627961715e197",
    "semantic_title": "fully-dynamic decision trees",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25839": {
    "title": "Scalable Theory-Driven Regularization of Scene Graph Generation Models",
    "volume": "main",
    "abstract": "Several techniques have recently aimed to improve the performance of deep learning models for Scene Graph Generation (SGG) by incorporating background knowledge. State-of-the-art techniques can be divided into two families: one where the background knowledge is incorporated into the model in a subsymbolic fashion, and another in which the background knowledge is maintained in symbolic form. Despite promising results, both families of techniques face several shortcomings: the first one requires ad-hoc, more complex neural architectures increasing the training or inference cost; the second one suffers from limited scalability w.r.t. the size of the background knowledge. Our work introduces a regularization technique for injecting symbolic background knowledge into neural SGG models that overcomes the limitations of prior art. Our technique is model-agnostic, does not incur any cost at inference time, and scales to previously unmanageable background knowledge sizes. We demonstrate that our technique can improve the accuracy of state-of-the-art SGG models, by up to 33%",
    "checked": true,
    "id": "f4411c40c4ab7133a1811704428377b43b178b77",
    "semantic_title": "scalable theory-driven regularization of scene graph generation models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25840": {
    "title": "Toward a Perspectivist Turn in Ground Truthing for Predictive Computing",
    "volume": "main",
    "abstract": "Most current Artificial Intelligence applications are based on supervised Machine Learning (ML), which ultimately grounds on data annotated by small teams of experts or large ensemble of volunteers. The annotation process is often performed in terms of a majority vote, however this has been proved to be often problematic by recent evaluation studies. In this article, we describe and advocate for a different paradigm, which we call perspectivism: this counters the removal of disagreement and, consequently, the assumption of correctness of traditionally aggregated gold-standard datasets, and proposes the adoption of methods that preserve divergence of opinions and integrate multiple perspectives in the ground truthing process of ML development. Drawing on previous works which inspired it, mainly from the crowdsourcing and multi-rater labeling settings, we survey the state-of-the-art and describe the potential of our proposal for not only the more subjective tasks (e.g. those related to human language) but also those tasks commonly understood as objective (e.g. medical decision making). We present the main benefits of adopting a perspectivist stance in ML, as well as possible disadvantages, and various ways in which such a stance can be implemented in practice. Finally, we share a set of recommendations and outline a research agenda to advance the perspectivist stance in ML",
    "checked": true,
    "id": "75802e1d48ac47d96808e2c9605a17ac1dd07345",
    "semantic_title": "toward a perspectivist turn in ground truthing for predictive computing",
    "citation_count": 38
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25841": {
    "title": "Semantic-Enhanced Image Clustering",
    "volume": "main",
    "abstract": "Image clustering is an important and open challenging task in computer vision. Although many methods have been proposed to solve the image clustering task, they only explore images and uncover clusters according to the image features, thus being unable to distinguish visually similar but semantically different images. In this paper, we propose to investigate the task of image clustering with the help of visual-language pre-training model. Different from the zero-shot setting, in which the class names are known, we only know the number of clusters in this setting. Therefore, how to map images to a proper semantic space and how to cluster images from both image and semantic spaces are two key problems. To solve the above problems, we propose a novel image clustering method guided by the visual-language pre-training model CLIP, named Semantic-Enhanced Image Clustering (SIC). In this new method, we propose a method to map the given images to a proper semantic space first and efficient methods to generate pseudo-labels according to the relationships between images and semantics. Finally, we propose to perform clustering with consistency learning in both image space and semantic space, in a self-supervised learning fashion. The theoretical result of convergence analysis shows that our proposed method can converge at a sublinear speed. Theoretical analysis of expectation risk also shows that we can reduce the expectation risk by improving neighborhood consistency, increasing prediction confidence, or reducing neighborhood imbalance. Experimental results on five benchmark datasets clearly show the superiority of our new method",
    "checked": true,
    "id": "ef95ed000d71c4b4d3dbddec448c258b210b1402",
    "semantic_title": "semantic-enhanced image clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25842": {
    "title": "RePreM: Representation Pre-training with Masked Model for Reinforcement Learning",
    "volume": "main",
    "abstract": "Inspired by the recent success of sequence modeling in RL and the use of masked language model for pre-training, we propose a masked model for pre-training in RL, RePreM (Representation Pre-training with Masked Model), which trains the encoder combined with transformer blocks to predict the masked states or actions in a trajectory. RePreM is simple but effective compared to existing representation pre-training methods in RL. It avoids algorithmic sophistication (such as data augmentation or estimating multiple models) with sequence modeling and generates a representation that captures long-term dynamics well. Empirically, we demonstrate the effectiveness of RePreM in various tasks, including dynamic prediction, transfer learning, and sample-efficient RL with both value-based and actor-critic methods. Moreover, we show that RePreM scales well with dataset size, dataset quality, and the scale of the encoder, which indicates its potential towards big RL models",
    "checked": true,
    "id": "1b6c1822cc273bc452eb0b3dfbe14de53dd10681",
    "semantic_title": "reprem: representation pre-training with masked model for reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25843": {
    "title": "FTM: A Frame-Level Timeline Modeling Method for Temporal Graph Representation Learning",
    "volume": "main",
    "abstract": "Learning representations for graph-structured data is essential for graph analytical tasks. While remarkable progress has been made on static graphs, researches on temporal graphs are still in its beginning stage. The bottleneck of the temporal graph representation learning approach is the neighborhood aggregation strategy, based on which graph attributes share and gather information explicitly. Existing neighborhood aggregation strategies fail to capture either the short-term features or the long-term features of temporal graph attributes, leading to unsatisfactory model performance and even poor robustness and domain generality of the representation learning method. To address this problem, we propose a Frame-level Timeline Modeling (FTM) method that helps to capture both short-term and long-term features and thus learns more informative representations on temporal graphs. In particular, we present a novel link-based framing technique to preserve the short-term features and then incorporate a timeline aggregator module to capture the intrinsic dynamics of graph evolution as long-term features. Our method can be easily assembled with most temporal GNNs. Extensive experiments on common datasets show that our method brings great improvements to the capability, robustness, and domain generality of backbone methods in downstream tasks. Our code can be found at https://github.com/yeeeqichen/FTM",
    "checked": true,
    "id": "c1a2d4fa243981f3e8af8b37b449a4488b810acd",
    "semantic_title": "ftm: a frame-level timeline modeling method for temporal graph representation learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25844": {
    "title": "Estimating Treatment Effects from Irregular Time Series Observations with Hidden Confounders",
    "volume": "main",
    "abstract": "Causal analysis for time series data, in particular estimating individualized treatment effect (ITE), is a key task in many real world applications, such as finance, retail, healthcare, etc. Real world time series, i.e., large-scale irregular or sparse and intermittent time series, raise significant challenges to existing work attempting to estimate treatment effects. Specifically, the existence of hidden confounders can lead to biased treatment estimates and complicate the causal inference process. In particular, anomaly hidden confounders which exceed the typical range can lead to high variance estimates. Moreover, in continuous time settings with irregular samples, it is challenging to directly handle the dynamics of causality. In this paper, we leverage recent advances in Lipschitz regularization and neural controlled differential equations (CDE) to develop an effective and scalable solution, namely LipCDE, to address the above challenges. LipCDE can directly model the dynamic causal relationships between historical data and outcomes with irregular samples by considering the boundary of hidden confounders given by Lipschitz constrained neural networks. Furthermore, we conduct extensive experiments on both synthetic and real world datasets to demonstrate the effectiveness and scalability of LipCDE",
    "checked": true,
    "id": "896e5cee54d50d7a1f981823b4627948610d72a5",
    "semantic_title": "estimating treatment effects from irregular time series observations with hidden confounders",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25845": {
    "title": "InParformer: Evolutionary Decomposition Transformers with Interactive Parallel Attention for Long-Term Time Series Forecasting",
    "volume": "main",
    "abstract": "Long-term time series forecasting (LTSF) provides substantial benefits for numerous real-world applications, whereas places essential demands on the model capacity to capture long-range dependencies. Recent Transformer-based models have significantly improved LTSF performance. It is worth noting that Transformer with the self-attention mechanism was originally proposed to model language sequences whose tokens (i.e., words) are discrete and highly semantic. However, unlike language sequences, most time series are sequential and continuous numeric points. Time steps with temporal redundancy are weakly semantic, and only leveraging time-domain tokens is hard to depict the overall properties of time series (e.g., the overall trend and periodic variations). To address these problems, we propose a novel Transformer-based forecasting model named InParformer with an Interactive Parallel Attention (InPar Attention) mechanism. The InPar Attention is proposed to learn long-range dependencies comprehensively in both frequency and time domains. To improve its learning capacity and efficiency, we further design several mechanisms, including query selection, key-value pair compression, and recombination. Moreover, InParformer is constructed with evolutionary seasonal-trend decomposition modules to enhance intricate temporal pattern extraction. Extensive experiments on six real-world benchmarks show that InParformer outperforms the state-of-the-art forecasting Transformers",
    "checked": true,
    "id": "4c0842b95cfaf5591790d9de1cccb4ddde155aa7",
    "semantic_title": "inparformer: evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25846": {
    "title": "Meta-Sketch: A Neural Data Structure for Estimating Item Frequencies of Data Streams",
    "volume": "main",
    "abstract": "To estimate item frequencies of data streams with limited space, sketches are widely used in real applications, including real-time web analytics, network monitoring, and self-driving. Sketches can be viewed as a model which maps the identifier of a stream item to the corresponding frequency domain. Starting from the premise, we envision a neural data structure, which we term the meta-sketch, to go beyond the basic structure of conventional sketches. The meta-sketch learns basic sketching abilities from meta-tasks constituted with synthetic datasets following Zipf distributions in the pre-training phase, and can be fast adapted to real (skewed) distributions in the adaption phase. Extensive experiments demonstrate the performance gains of the meta-sketch and offer insights into our proposals",
    "checked": true,
    "id": "6f279bb6ec61f3c5cafb4cd9c7c4e62c2768df3a",
    "semantic_title": "meta-sketch: a neural data structure for estimating item frequencies of data streams",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25847": {
    "title": "Unfooling Perturbation-Based Post Hoc Explainers",
    "volume": "main",
    "abstract": "Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our approach successfully detects whether a black box system adversarially conceals its decision-making process and mitigates the adversarial attack on real-world data for the prevalent explainers, LIME and SHAP. The code for this work is available at https://github.com/craymichael/unfooling",
    "checked": true,
    "id": "17101501b13b57f6627cf1d6f557375051e21afd",
    "semantic_title": "unfooling perturbation-based post hoc explainers",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25848": {
    "title": "Very Fast, Approximate Counterfactual Explanations for Decision Forests",
    "volume": "main",
    "abstract": "We consider finding a counterfactual explanation for a classification or regression forest, such as a random forest. This requires solving an optimization problem to find the closest input instance to a given instance for which the forest outputs a desired value. Finding an exact solution has a cost that is exponential on the number of leaves in the forest. We propose a simple but very effective approach: we constrain the optimization to input space regions populated by actual data points. The problem reduces to a form of nearest-neighbor search using a certain distance on a certain dataset. This has two advantages: first, the solution can be found very quickly, scaling to large forests and high-dimensional data, and enabling interactive use. Second, the solution found is more likely to be realistic in that it is guided towards high-density areas of input space",
    "checked": true,
    "id": "f7544bfd25ae5baefe62ce3973d368f1c4b2fa70",
    "semantic_title": "very fast, approximate counterfactual explanations for decision forests",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25849": {
    "title": "An Equivalence Analysis of Binary Quantification Methods",
    "volume": "main",
    "abstract": "Quantification (or prevalence estimation) algorithms aim at predicting the class distribution of unseen sets (or bags) of examples. These methods are useful for two main tasks: 1) quantification applications, for instance when we need to track the proportions of several groups of interest over time, and 2) domain adaptation problems, in which we usually need to adapt a previously trained classifier to a different --albeit related-- target distribution according to the estimated prevalences. This paper analyzes several binary quantification algorithms showing that not only do they share a common framework but are, in fact, equivalent. Inspired by this study, we propose a new method that extends one of the approaches analyzed. After an empirical evaluation of all these methods using synthetic and benchmark datasets, the paper concludes recommending three of them due to their precision, efficiency, and diversity",
    "checked": true,
    "id": "1a4131e72b4adfaf3290e27a29e904f0be341e6e",
    "semantic_title": "an equivalence analysis of binary quantification methods",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25850": {
    "title": "Soft Action Priors: Towards Robust Policy Transfer",
    "volume": "main",
    "abstract": "Despite success in many challenging problems, reinforcement learning (RL) is still confronted with sample inefficiency, which can be mitigated by introducing prior knowledge to agents. However, many transfer techniques in reinforcement learning make the limiting assumption that the teacher is an expert. In this paper, we use the action prior from the Reinforcement Learning as Inference framework - that is, a distribution over actions at each state which resembles a teacher policy, rather than a Bayesian prior - to recover state-of-the-art policy distillation techniques. Then, we propose a class of adaptive methods that can robustly exploit action priors by combining reward shaping and auxiliary regularization losses. In contrast to prior work, we develop algorithms for leveraging suboptimal action priors that may nevertheless impart valuable knowledge - which we call soft action priors. The proposed algorithms adapt by adjusting the strength of teacher feedback according to an estimate of the teacher's usefulness in each state. We perform tabular experiments, which show that the proposed methods achieve state-of-the-art performance, surpassing it when learning from suboptimal priors. Finally, we demonstrate the robustness of the adaptive algorithms in continuous action deep RL problems, in which adaptive algorithms considerably improved stability when compared to existing policy distillation methods",
    "checked": true,
    "id": "5815a68809dbedb72515cf930f6011761c8d8ea8",
    "semantic_title": "soft action priors: towards robust policy transfer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25851": {
    "title": "Invariant Representations with Stochastically Quantized Neural Networks",
    "volume": "main",
    "abstract": "Representation learning algorithms offer the opportunity to learn invariant representations of the input data with regard to nuisance factors. Many authors have leveraged such strategies to learn fair representations, i.e., vectors where information about sensitive attributes is removed. These methods are attractive as they may be interpreted as minimizing the mutual information between a neural layer's activations and a sensitive attribute. However, the theoretical grounding of such methods relies either on the computation of infinitely accurate adversaries or on minimizing a variational upper bound of a mutual information estimate. In this paper, we propose a methodology for direct computation of the mutual information between neurons in a layer and a sensitive attribute. We employ stochastically-activated binary neural networks, which lets us treat neurons as random variables. Our method is therefore able to minimize an upper bound on the mutual information between the neural representations and a sensitive attribute. We show that this method compares favorably with the state of the art in fair representation learning and that the learned representations display a higher level of invariance compared to full-precision neural networks",
    "checked": true,
    "id": "550f29904da2ccab4a3211349aa56cb4172a80d0",
    "semantic_title": "invariant representations with stochastically quantized neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25852": {
    "title": "Learning Pessimism for Reinforcement Learning",
    "volume": "main",
    "abstract": "Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks",
    "checked": true,
    "id": "4729d7b34c60de21fc5f7c9e0e9525936f644ca6",
    "semantic_title": "learning pessimism for reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25853": {
    "title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "Model-based approaches to reinforcement learning (MBRL) exhibit favorable performance in practice, but their theoretical guarantees in large spaces are mostly restricted to the setting when transition model is Gaussian or Lipschitz, and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a Bayesian coreset of only statistically significant past state-action pairs; and (iii) exhibits a sublinear Bayesian regret. To achieve these results, we adopt an approach based upon Stein's method, which, under a smoothness condition on the constructed posterior and target, allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). The aforementioned compression step is then computed in terms of greedily retaining only those samples which are more than a certain KSD away from the previous model estimate. Experimentally, we observe that this approach is competitive with several state-of-the-art RL methodologies, and can achieve up-to 50 percent reduction in wall clock time in some continuous control environments",
    "checked": true,
    "id": "98241cf085a52c5761278ea8eb6c51cffa3a9d38",
    "semantic_title": "posterior coreset construction with kernelized stein discrepancy for model-based reinforcement learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25854": {
    "title": "NHITS: Neural Hierarchical Interpolation for Time Series Forecasting",
    "volume": "main",
    "abstract": "Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting the task are the volatility of the predictions and their computational complexity. We introduce NHITS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, emphasizing components with different frequencies and scales while decomposing the input signal and synthesizing the forecast. We prove that the hierarchical interpolation technique can efficiently approximate arbitrarily long horizons in the presence of smoothness. Additionally, we conduct extensive large-scale dataset experiments from the long-horizon forecasting literature, demonstrating the advantages of our method over the state-of-the-art methods, where NHITS provides an average accuracy improvement of almost 20% over the latest Transformer architectures while reducing the computation time by an order of magnitude (50 times). Our code is available at https://github.com/Nixtla/neuralforecast",
    "checked": true,
    "id": "597811b2fa8f5155202a08226acde69efaf1eefb",
    "semantic_title": "nhits: neural hierarchical interpolation for time series forecasting",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25855": {
    "title": "Where Will Players Move Next? Dynamic Graphs and Hierarchical Fusion for Movement Forecasting in Badminton",
    "volume": "main",
    "abstract": "Sports analytics has captured increasing attention since analysis of the various data enables insights for training strategies, player evaluation, etc. In this paper, we focus on predicting what types of returning strokes will be made, and where players will move to based on previous strokes. As this problem has not been addressed to date, movement forecasting can be tackled through sequence-based and graph-based models by formulating as a sequence prediction task. However, existing sequence-based models neglect the effects of interactions between players, and graph-based models still suffer from multifaceted perspectives on the next movement. Moreover, there is no existing work on representing strategic relations among players' shot types and movements. To address these challenges, we first introduce the procedure of the Player Movements (PM) graph to exploit the structural movements of players with strategic relations. Based on the PM graph, we propose a novel Dynamic Graphs and Hierarchical Fusion for Movement Forecasting model (DyMF) with interaction style extractors to capture the mutual interactions of players themselves and between both players within a rally, and dynamic players' tactics across time. In addition, hierarchical fusion modules are designed to incorporate the style influence of both players and rally interactions. Extensive experiments show that our model empirically outperforms both sequence- and graph-based methods and demonstrate the practical usage of movement forecasting. Code is available at https://github.com/wywyWang/CoachAI-Projects/tree/main/Movement%20Forecasting",
    "checked": true,
    "id": "6b965a2f6173c1365a492ae6103206bed2be9820",
    "semantic_title": "where will players move next? dynamic graphs and hierarchical fusion for movement forecasting in badminton",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25856": {
    "title": "Graph Ordering Attention Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been successfully used in many problems involving graph-structured data, achieving state-of-the-art performance. GNNs typically employ a message-passing scheme, in which every node aggregates information from its neighbors using a permutation-invariant aggregation function. Standard well-examined choices such as the mean or sum aggregation functions have limited capabilities, as they are not able to capture interactions among neighbors. In this work, we formalize these interactions using an information-theoretic framework that notably includes synergistic information. Driven by this definition, we introduce the Graph Ordering Attention (GOAT) layer, a novel GNN component that captures interactions between nodes in a neighborhood. This is achieved by learning local node orderings via an attention mechanism and processing the ordered representations using a recurrent neural network aggregator. This design allows us to make use of a permutation-sensitive aggregator while maintaining the permutation-equivariance of the proposed GOAT layer. The GOAT model demonstrates its increased performance in modeling graph metrics that capture complex information, such as the betweenness centrality and the effective size of a node. In practical use-cases, its superior modeling capability is confirmed through its success in several real-world node classification benchmarks",
    "checked": true,
    "id": "509c7f6ec4ae7d33006f9c5e9c174f61a43df598",
    "semantic_title": "graph ordering attention networks",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25857": {
    "title": "Scalable and Globally Optimal Generalized L₁ K-center Clustering via Constraint Generation in Mixed Integer Linear Programming",
    "volume": "main",
    "abstract": "The k-center clustering algorithm, introduced over 35 years ago, is known to be robust to class imbalance prevalent in many clustering problems and has various applications such as data summarization, document clustering, and facility location determination. Unfortunately, existing k-center algorithms provide highly suboptimal solutions that can limit their practical application, reproducibility, and clustering quality. In this paper, we provide a novel scalable and globally optimal solution to a popular variant of the k-center problem known as generalized L_1 k-center clustering that uses L_1 distance and allows the selection of arbitrary vectors as cluster centers. We show that this clustering objective can be reduced to a mixed-integer linear program (MILP) that facilitates globally optimal clustering solutions. However, solving such a MILP may be intractable for large datasets; to remedy this, we present a scalable algorithm that leverages constraint generation to efficiently and provably converge to its global optimum. We further enhance outlier handling through a simple but elegant extension to our MILP objective. We first evaluate our algorithm on a variety of synthetic datasets to better understand its properties and then validate on 20 real benchmark datasets where we compare its performance to both traditional L_1 distance k-center and k-medians baselines. Our results demonstrate significant suboptimality of existing algorithms in comparison to our approach and further demonstrate that we can find optimal generalized L_1 k-center clustering solutions up to an unprecedented 1,000,000 data points",
    "checked": true,
    "id": "ed0ae2814794c9d86de2d2c04a2aee9cabcc01b5",
    "semantic_title": "scalable and globally optimal generalized l₁ k-center clustering via constraint generation in mixed integer linear programming",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25858": {
    "title": "Attribute and Structure Preserving Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph Contrastive Learning (GCL) has drawn much research interest due to its strong ability to capture both graph structure and node attribute information in a self-supervised manner. Current GCL methods usually adopt Graph Neural Networks (GNNs) as the base encoder, which typically relies on the homophily assumption of networks and overlooks node similarity in the attribute space. There are many scenarios where such assumption cannot be satisfied, or node similarity plays a crucial role. In order to design a more robust mechanism, we develop a novel attribute and structure preserving graph contrastive learning framework, named ASP, which comprehensively and efficiently preserves node attributes while exploiting graph structure. Specifically, we consider three different graph views in our framework, i.e., original view, attribute view, and global structure view. Then, we perform contrastive learning across three views in a joint fashion, mining comprehensive graph information. We validate the effectiveness of the proposed framework on various real-world networks with different levels of homophily. The results demonstrate the superior performance of our model over the representative baselines",
    "checked": true,
    "id": "8730e5e6aec81f191f162d9a621ffe18cabaac82",
    "semantic_title": "attribute and structure preserving graph contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25859": {
    "title": "On the Stability and Generalization of Triplet Learning",
    "volume": "main",
    "abstract": "Triplet learning, i.e. learning from triplet data, has attracted much attention in computer vision tasks with an extremely large number of categories, e.g., face recognition and person re-identification. Albeit with rapid progress in designing and applying triplet learning algorithms, there is a lacking study on the theoretical understanding of their generalization performance. To fill this gap, this paper investigates the generalization guarantees of triplet learning by leveraging the stability analysis. Specifically, we establish the first general high-probability generalization bound for the triplet learning algorithm satisfying the uniform stability, and then obtain the excess risk bounds of the order O(log(n)/(√n) ) for both stochastic gradient descent (SGD) and regularized risk minimization (RRM), where 2n is approximately equal to the number of training samples. Moreover, an optimistic generalization bound in expectation as fast as O(1/n) is derived for RRM in a low noise case via the on-average stability analysis. Finally, our results are applied to triplet metric learning to characterize its theoretical underpinning",
    "checked": true,
    "id": "c39f3752591181899f680644e8f6ce77cc5a8e3e",
    "semantic_title": "on the stability and generalization of triplet learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25860": {
    "title": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer",
    "volume": "main",
    "abstract": "Vision Transformers (ViT) have made many breakthroughs in computer vision tasks. However, considerable redundancy arises in the spatial dimension of an input image, leading to massive computational costs. Therefore, We propose a coarse-to-fine vision transformer (CF-ViT) to relieve computational burden while retaining performance in this paper. Our proposed CF-ViT is motivated by two important observations in modern ViT models: (1) The coarse-grained patch splitting can locate informative regions of an input image. (2) Most images can be well recognized by a ViT model in a small-length token sequence. Therefore, our CF-ViT implements network inference in a two-stage manner. At coarse inference stage, an input image is split into a small-length patch sequence for a computationally economical classification. If not well recognized, the informative patches are identified and further re-split in a fine-grained granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For example, without any compromise on performance, CF-ViT reduces 53% FLOPs of LV-ViT, and also achieves 2.01x throughput. Code of this project is at https://github.com/ChenMnZ/CF-V",
    "checked": true,
    "id": "ed00842931ebb0db2c634330a77c8dee6f77e547",
    "semantic_title": "cf-vit: a general coarse-to-fine method for vision transformer",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25861": {
    "title": "Context-Aware Safe Medication Recommendations with Molecular Graph and DDI Graph Embedding",
    "volume": "main",
    "abstract": "Molecular structures and Drug-Drug Interactions (DDI) are recognized as important knowledge to guide medication recommendation (MR) tasks, and medical concept embedding has been applied to boost their performance. Though promising performance has been achieved by leveraging Graph Neural Network (GNN) models to encode the molecular structures of medications or/and DDI, we observe that existing models are still defective: 1) to differentiate medications with similar molecules but different functionality; or/and 2) to properly capture the unintended reactions between drugs in the embedding space. To alleviate this limitation, we propose Carmen, a cautiously designed graph embedding-based MR framework. Carmen consists of four components, including patient representation learning, context information extraction, a context-aware GNN, and DDI encoding. Carmen incorporates the visit history into the representation learning of molecular graphs to distinguish molecules with similar topology but dissimilar activity. Its DDI encoding module is specially devised for the non-transitive interaction DDI graphs. The experiments on real-world datasets demonstrate that Carmen achieves remarkable performance improvement over state-of-the-art models and can improve the safety of recommended drugs with a proper DDI graph encoding",
    "checked": true,
    "id": "795cb7f144184139a0198b99083ecee62ff72991",
    "semantic_title": "context-aware safe medication recommendations with molecular graph and ddi graph embedding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25862": {
    "title": "Min-Max Submodular Ranking for Multiple Agents",
    "volume": "main",
    "abstract": "In the submodular ranking (SR) problem, the input consists of a set of submodular functions defined on a ground set of elements. The goal is to order elements for all the functions to have value above a certain threshold as soon on average as possible, assuming we choose one element per time. The problem is flexible enough to capture various applications in machine learning, including decision trees. This paper considers the min-max version of SR where multiple instances share the ground set. With the view of each instance being associated with an agent, the min-max problem is to order the common elements to minimize the maximum objective of all agents---thus, finding a fair solution for all agents. We give approximation algorithms for this problem and demonstrate their effectiveness in the application of finding a decision tree for multiple agents",
    "checked": true,
    "id": "6d45997fb96b394f4057118d2fb4caba2c9f3321",
    "semantic_title": "min-max submodular ranking for multiple agents",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25863": {
    "title": "Supervised Contrastive Few-Shot Learning for High-Frequency Time Series",
    "volume": "main",
    "abstract": "Significant progress has been made in representation learning, especially with recent success on self-supervised contrastive learning. However, for time series with less intuitive or semantic meaning, sampling bias may be inevitably encountered in unsupervised approaches. Although supervised contrastive learning has shown superior performance by leveraging label information, it may also suffer from class collapse. In this study, we consider a realistic scenario in industry with limited annotation information available. A supervised contrastive framework is developed for high-frequency time series representation and classification, wherein a novel variant of supervised contrastive loss is proposed to include multiple augmentations while induce spread within each class. Experiments on four mainstream public datasets as well as a series of sensitivity and ablation analyses demonstrate that the learned representations are effective and robust compared with the direct supervised learning and self-supervised learning, notably under the minimal few-shot situation",
    "checked": true,
    "id": "a2056f5affe509b50e41612057ca9cca143ef97a",
    "semantic_title": "supervised contrastive few-shot learning for high-frequency time series",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25864": {
    "title": "The Sufficiency of Off-Policyness and Soft Clipping: PPO Is Still Insufficient according to an Off-Policy Measure",
    "volume": "main",
    "abstract": "The popular Proximal Policy Optimization (PPO) algorithm approximates the solution in a clipped policy space. Does there exist better policies outside of this space? By using a novel surrogate objective that employs the sigmoid function (which provides an interesting way of exploration), we found that the answer is \"YES\", and the better policies are in fact located very far from the clipped space. We show that PPO is insufficient in \"off-policyness\", according to an off-policy metric called DEON. Our algorithm explores in a much larger policy space than PPO, and it maximizes the Conservative Policy Iteration (CPI) objective better than PPO during training. To the best of our knowledge, all current PPO methods have the clipping operation and optimize in the clipped policy space. Our method is the first of this kind, which advances the understanding of CPI optimization and policy gradient methods. Code is available at https://github.com/raincchio/P3O",
    "checked": true,
    "id": "3a5b68ef4736e235aa596eb6728f284258e11d18",
    "semantic_title": "the sufficiency of off-policyness and soft clipping: ppo is still insufficient according to an off-policy measure",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25865": {
    "title": "Global Convergence of Two-Timescale Actor-Critic for Solving Linear Quadratic Regulator",
    "volume": "main",
    "abstract": "The actor-critic (AC) reinforcement learning algorithms have been the powerhouse behind many challenging applications. Nevertheless, its convergence is fragile in general. To study its instability, existing works mostly consider the uncommon double-loop variant or basic models with finite state and action space. We investigate the more practical single-sample two-timescale AC for solving the canonical linear quadratic regulator (LQR) problem, where the actor and the critic update only once with a single sample in each iteration on an unbounded continuous state and action space. Existing analysis cannot conclude the convergence for such a challenging case. We develop a new analysis framework that allows establishing the global convergence to an epsilon-optimal solution with at most an order of epsilon to -2.5 sample complexity. To our knowledge, this is the first finite-time convergence analysis for the single sample two-timescale AC for solving LQR with global optimality. The sample complexity improves those of other variants by orders, which sheds light on the practical wisdom of single sample algorithms. We also further validate our theoretical findings via comprehensive simulation comparisons",
    "checked": true,
    "id": "4b20c81a39d1ccc41f749f59dbb632e632135d7a",
    "semantic_title": "global convergence of two-timescale actor-critic for solving linear quadratic regulator",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25866": {
    "title": "Topological Pooling on Graphs",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have demonstrated a significant success in various graph learning tasks, from graph classification to anomaly detection. There recently has emerged a number of approaches adopting a graph pooling operation within GNNs, with a goal to preserve graph attributive and structural features during the graph representation learning. However, most existing graph pooling operations suffer from the limitations of relying on node-wise neighbor weighting and embedding, which leads to insufficient encoding of rich topological structures and node attributes exhibited by real-world networks. By invoking the machinery of persistent homology and the concept of landmarks, we propose a novel topological pooling layer and witness complex-based topological embedding mechanism that allow us to systematically integrate hidden topological information at both local and global levels. Specifically, we design new learnable local and global topological representations Wit-TopoPool which allow us to simultaneously extract rich discriminative topological information from graphs. Experiments on 11 diverse benchmark datasets against 18 baseline models in conjunction with graph classification tasks indicate that Wit-TopoPool significantly outperforms all competitors across all datasets",
    "checked": true,
    "id": "77d14bbaece056f5a94231f46f02cca86e9bc085",
    "semantic_title": "topological pooling on graphs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25867": {
    "title": "Riemannian Local Mechanism for SPD Neural Networks",
    "volume": "main",
    "abstract": "The Symmetric Positive Definite (SPD) matrices have received wide attention for data representation in many scientific areas. Although there are many different attempts to develop effective deep architectures for data processing on the Riemannian manifold of SPD matrices, very few solutions explicitly mine the local geometrical information in deep SPD feature representations. Given the great success of local mechanisms in Euclidean methods, we argue that it is of utmost importance to ensure the preservation of local geometric information in the SPD networks. We first analyse the convolution operator commonly used for capturing local information in Euclidean deep networks from the perspective of a higher level of abstraction afforded by category theory. Based on this analysis, we define the local information in the SPD manifold and design a multi-scale submanifold block for mining local geometry. Experiments involving multiple visual tasks validate the effectiveness of our approach",
    "checked": true,
    "id": "fc57be2a8ce6f6ae81bc53a80e9256e4340a6ef5",
    "semantic_title": "riemannian local mechanism for spd neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25868": {
    "title": "TC-DWA:Text Clustering with Dual Word-Level Augmentation",
    "volume": "main",
    "abstract": "The pre-trained language models, e.g., ELMo and BERT, have recently achieved promising performance improvement in a wide range of NLP tasks, because they can output strong contextualized embedded features of words. Inspired by their great success, in this paper we target at fine-tuning them to effectively handle the text clustering task, i.e., a classic and fundamental challenge in machine learning. Accordingly, we propose a novel BERT-based method, namely Text Clustering with Dual Word-level Augmentation (TCDWA). To be specific, we formulate a self-training objective and enhance it with a dual word-level augmentation technique. First, we suppose that each text contains several most informative words, called anchor words, supporting the full text semantics. We use the embedded features of anchor words as augmented data, which are selected by ranking the norm-based attention weights of words. Second, we formulate an expectation form of word augmentation, which is equivalent to generating infinite augmented features, and further suggest a tractable approximation of Taylor expansion for efficient optimization. To evaluate the effectiveness of TCDWA, we conduct extensive experiments on several benchmark text datasets. The results demonstrate that TCDWA consistently outperforms the state-of-the-art baseline methods. Code available: https://github.com/BoCheng-96/TC-DWA",
    "checked": false,
    "id": "e79fbed9d087f8905e4043108bcf8591f621f21c",
    "semantic_title": "tc-dwa: text clustering with dual word-level augmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25869": {
    "title": "Causal Inference with Conditional Instruments Using Deep Generative Models",
    "volume": "main",
    "abstract": "The instrumental variable (IV) approach is a widely used way to estimate the causal effects of a treatment on an outcome of interest from observational data with latent confounders. A standard IV is expected to be related to the treatment variable and independent of all other variables in the system. However, it is challenging to search for a standard IV from data directly due to the strict conditions. The conditional IV (CIV) method has been proposed to allow a variable to be an instrument conditioning on a set of variables, allowing a wider choice of possible IVs and enabling broader practical applications of the IV approach. Nevertheless, there is not a data-driven method to discover a CIV and its conditioning set directly from data. To fill this gap, in this paper, we propose to learn the representations of the information of a CIV and its conditioning set from data with latent confounders for average causal effect estimation. By taking advantage of deep generative models, we develop a novel data-driven approach for simultaneously learning the representation of a CIV from measured variables and generating the representation of its conditioning set given measured variables. Extensive experiments on synthetic and real-world datasets show that our method outperforms the existing IV methods",
    "checked": true,
    "id": "4f79230f60316394b9cda5896d594c6f4b0c9e37",
    "semantic_title": "causal inference with conditional instruments using deep generative models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25870": {
    "title": "Wiener Graph Deconvolutional Network Improves Graph Self-Supervised Learning",
    "volume": "main",
    "abstract": "Graph self-supervised learning (SSL) has been vastly employed to learn representations from unlabeled graphs. Existing methods can be roughly divided into predictive learning and contrastive learning, where the latter one attracts more research attention with better empirical performance. We argue that, however, predictive models weaponed with powerful decoder could achieve comparable or even better representation power than contrastive models. In this work, we propose a Wiener Graph Deconvolutional Network (WGDN), an augmentation-adaptive decoder empowered by graph wiener filter to perform information reconstruction. Theoretical analysis proves the superior reconstruction ability of graph wiener filter. Extensive experimental results on various datasets demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "1497a56aa25a385e1d60e6a9c2fa60aa7bd17edd",
    "semantic_title": "wiener graph deconvolutional network improves graph self-supervised learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25871": {
    "title": "Partial-Label Regression",
    "volume": "main",
    "abstract": "Partial-label learning is a popular weakly supervised learning setting that allows each training example to be annotated with a set of candidate labels. Previous studies on partial-label learning only focused on the classification setting where candidate labels are all discrete, which cannot handle continuous labels with real values. In this paper, we provide the first attempt to investigate partial-label regression, where each training example is annotated with a set of real-valued candidate labels. To solve this problem, we first propose a simple baseline method that takes the average loss incurred by candidate labels as the predictive loss. The drawback of this method lies in that the loss incurred by the true label may be overwhelmed by other false labels. To overcome this drawback, we propose an identification method that takes the least loss incurred by candidate labels as the predictive loss. We further improve it by proposing a progressive identification method to differentiate candidate labels using progressively updated weights for incurred losses. We prove that the latter two methods are model-consistent and provide convergence analysis showing the optimal parametric convergence rate. Our proposed methods are theoretically grounded and can be compatible with any models, optimizers, and losses. Experiments validate the effectiveness of our proposed methods",
    "checked": true,
    "id": "2959a6a949de79fee19c6340e93db35096902a07",
    "semantic_title": "partial-label regression",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25872": {
    "title": "Offline Quantum Reinforcement Learning in a Conservative Manner",
    "volume": "main",
    "abstract": "Recently, to reap the quantum advantage, empowering reinforcement learning (RL) with quantum computing has attracted much attention, which is dubbed as quantum RL (QRL). However, current QRL algorithms employ an online learning scheme, i.e., the policy that is run on a quantum computer needs to interact with the environment to collect experiences, which could be expensive and dangerous for practical applications. In this paper, we aim to solve this problem in an offline learning manner. To be more specific, we develop the first offline quantum RL (offline QRL) algorithm named CQ2L (Conservative Quantum Q-learning), which learns from offline samples and does not require any interaction with the environment. CQ2L utilizes variational quantum circuits (VQCs), which are improved with data re-uploading and scaling parameters, to represent Q-value functions of agents. To suppress the overestimation of Q-values resulting from offline data, we first employ a double Q-learning framework to reduce the overestimation bias; then a penalty term that encourages generating conservative Q-values is designed. We conduct abundant experiments to demonstrate that the proposed method CQ2L can successfully solve offline QRL tasks that the online counterpart could not",
    "checked": true,
    "id": "d116ba398a85cd838618796a10638b9d56b58250",
    "semantic_title": "offline quantum reinforcement learning in a conservative manner",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25873": {
    "title": "Variational Wasserstein Barycenters with C-cyclical Monotonicity Regularization",
    "volume": "main",
    "abstract": "Wasserstein barycenter, built on the theory of Optimal Transport (OT), provides a powerful framework to aggregate probability distributions, and it has increasingly attracted great attention within the machine learning community. However, it is often intractable to precisely compute, especially for high dimensional and continuous settings. To alleviate this problem, we develop a novel regularization by using the fact that c-cyclical monotonicity is often necessary and sufficient conditions for optimality in OT problems, and incorporate it into the dual formulation of Wasserstein barycenters. For efficient computations, we adopt a variational distribution as the approximation of the true continuous barycenter, so as to frame the Wasserstein barycenters problem as an optimization problem with respect to variational parameters. Upon those ideas, we propose a novel end-to-end continuous approximation method, namely Variational Wasserstein Barycenters with c-Cyclical Monotonicity Regularization (VWB-CMR), given sample access to the input distributions. We show theoretical convergence analysis and demonstrate the superior performance of VWB-CMR on synthetic data and real applications of subset posterior aggregation",
    "checked": true,
    "id": "556bc791f59b3bc8834960d473d4226957000824",
    "semantic_title": "variational wasserstein barycenters with c-cyclical monotonicity regularization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25874": {
    "title": "MobileTL: On-Device Transfer Learning with Inverted Residual Blocks",
    "volume": "main",
    "abstract": "Transfer learning on edge is challenging due to on-device limited resources. Existing work addresses this issue by training a subset of parameters or adding model patches. Developed with inference in mind, Inverted Residual Blocks (IRBs) split a convolutional layer into depthwise and pointwise convolutions, leading to more stacking layers, e.g., convolution, normalization, and activation layers. Though they are efficient for inference, IRBs require that additional activation maps are stored in memory for training weights for convolution layers and scales for normalization layers. As a result, their high memory cost prohibits training IRBs on resource-limited edge devices, and making them unsuitable in the context of transfer learning. To address this issue, we present MobileTL, a memory and computationally efficient on-device transfer learning method for models built with IRBs. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backward pass. Also, MobileTL approximates the backward computation of the activation layer (e.g., Hard-Swish and ReLU6) as a signed function which enables storing a binary mask instead of activation maps for the backward pass. MobileTL fine-tunes a few top blocks (close to output) rather than propagating the gradient through the whole network to reduce the computation cost. Our method reduces memory usage by 46% and 53% for MobileNetV2 and V3 IRBs, respectively. For MobileNetV3, we observe a 36% reduction in floating-point operations (FLOPs) when fine-tuning 5 blocks, while only incurring a 0.6% accuracy reduction on CIFAR10. Extensive experiments on multiple datasets demonstrate that our method is Pareto-optimal (best accuracy under given hardware constraints) compared to prior work in transfer learning for edge devices",
    "checked": true,
    "id": "2ac22b58453c227a560f5ec4527be9f1857ca64c",
    "semantic_title": "mobiletl: on-device transfer learning with inverted residual blocks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25875": {
    "title": "Learning Optimal Features via Partial Invariance",
    "volume": "main",
    "abstract": "Learning models that are robust to distribution shifts is a key concern in the context of their real-life applicability. Invariant Risk Minimization (IRM) is a popular framework that aims to learn robust models from multiple environments. The success of IRM requires an important assumption: the underlying causal mechanisms/features remain invariant across environments. When not satisfied, we show that IRM can over-constrain the predictor and to remedy this, we propose a relaxation via partial invariance. In this work, we theoretically highlight the sub-optimality of IRM and then demonstrate how learning from a partition of training domains can help improve invariant models. Several experiments, conducted both in linear settings as well as with deep neural networks on tasks over both language and image data, allow us to verify our conclusions",
    "checked": true,
    "id": "26c61243f679b2cfb1edb4a8f95d1b44132c38f3",
    "semantic_title": "learning optimal features via partial invariance",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25876": {
    "title": "PrimeNet: Pre-training for Irregular Multivariate Time Series",
    "volume": "main",
    "abstract": "Real-world applications often involve irregular time series, for which the time intervals between successive observations are non-uniform. Irregularity across multiple features in a multi-variate time series further results in a different subset of features at any given time (i.e., asynchronicity). Existing pre-training schemes for time-series, however, often assume regularity of time series and make no special treatment of irregularity. We argue that such irregularity offers insight about domain property of the data—for example, frequency of hospital visits may signal patient health condition—that can guide representation learning. In this work, we propose PrimeNet to learn a self-supervised representation for irregular multivariate time-series. Specifically, we design a time sensitive contrastive learning and data reconstruction task to pre-train a model. Irregular time-series exhibits considerable variations in sampling density over time. Hence, our triplet generation strategy follows the density of the original data points, preserving its native irregularity. Moreover, the sampling density variation over time makes data reconstruction difficult for different regions. Therefore, we design a data masking technique that always masks a constant time duration to accommodate reconstruction for regions of different sampling density. We learn with these tasks using unlabeled data to build a pre-trained model and fine-tune on a downstream task with limited labeled data, in contrast with existing fully supervised approach for irregular time-series, requiring large amounts of labeled data. Experiment results show that PrimeNet significantly outperforms state-of-the-art methods on naturally irregular and asynchronous data from Healthcare and IoT applications for several downstream tasks, including classification, interpolation, and regression",
    "checked": true,
    "id": "70e5e9bd353d1b1e54b4eebd3a67b58026a13dff",
    "semantic_title": "primenet: pre-training for irregular multivariate time series",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25877": {
    "title": "Structured BFGS Method for Optimal Doubly Stochastic Matrix Approximation",
    "volume": "main",
    "abstract": "Doubly stochastic matrix plays an essential role in several areas such as statistics and machine learning. In this paper we consider the optimal approximation of a square matrix in the set of doubly stochastic matrices. A structured BFGS method is proposed to solve the dual of the primal problem. The resulting algorithm builds curvature information into the diagonal components of the true Hessian, so that it takes only additional linear cost to obtain the descent direction based on the gradient information without having to explicitly store the inverse Hessian approximation. The cost is substantially fewer than quadratic complexity of the classical BFGS algorithm. Meanwhile, a Newton-based line search method is presented for finding a suitable step size, which in practice uses the existing knowledge and takes only one iteration. The global convergence of our algorithm is established. We verify the advantages of our approach on both synthetic data and real data sets. The experimental results demonstrate that our algorithm outperforms the state-of-the-art solvers and enjoys outstanding scalability",
    "checked": true,
    "id": "593f9a54f6e53a0d1c1c19366521ca59e491d77c",
    "semantic_title": "structured bfgs method for optimal doubly stochastic matrix approximation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25878": {
    "title": "On the Complexity of PAC Learning in Hilbert Spaces",
    "volume": "main",
    "abstract": "We study the problem of binary classification from the point of view of learning convex polyhedra in Hilbert spaces, to which one can reduce any binary classification problem. The problem of learning convex polyhedra in finite-dimensional spaces is sufficiently well studied in the literature. We generalize this problem to that in a Hilbert space and propose an algorithm for learning a polyhedron which correctly classifies at least 1 − ε of the distribution, with a probability of at least 1 − δ, where ε and δ are given parameters. Also, as a corollary, we improve some previous bounds for polyhedral classification in finite-dimensional spaces",
    "checked": true,
    "id": "0138f1631efd6425f09f2457d667dbded41af7fa",
    "semantic_title": "on the complexity of pac learning in hilbert spaces",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25879": {
    "title": "Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks Using an Incompetent Teacher",
    "volume": "main",
    "abstract": "Machine unlearning has become an important area of research due to an increasing need for machine learning (ML) applications to comply with the emerging data privacy regulations. It facilitates the provision for removal of certain set or class of data from an already trained ML model without requiring retraining from scratch. Recently, several efforts have been put in to make unlearning to be effective and efficient. We propose a novel machine unlearning method by exploring the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain a model that doesn't contain any information about the forget data. We experimentally show that this method generalizes well, is fast and effective. Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate any unlearning method. Unlike the existing unlearning metrics, the ZRF score does not depend on the availability of the expensive retrained model. This makes it useful for analysis of the unlearned model after deployment as well. We present results of experiments conducted for random subset forgetting and class forgetting on various deep networks and across different application domains. Code is at: https://github.com/vikram2000b/bad-teaching- unlearning",
    "checked": true,
    "id": "425669c368004dc43bebaa1d4acdd46a6bcca171",
    "semantic_title": "can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25880": {
    "title": "Scalable Spatiotemporal Graph Neural Networks",
    "volume": "main",
    "abstract": "Neural forecasting of spatiotemporal time series drives both research and industrial innovation in several relevant application domains. Graph neural networks (GNNs) are often the core component of the forecasting architecture. However, in most spatiotemporal GNNs, the computational complexity scales up to a quadratic factor with the length of the sequence times the number of links in the graph, hence hindering the application of these models to large graphs and long temporal sequences. While methods to improve scalability have been proposed in the context of static graphs, few research efforts have been devoted to the spatiotemporal case. To fill this gap, we propose a scalable architecture that exploits an efficient encoding of both temporal and spatial dynamics. In particular, we use a randomized recurrent neural network to embed the history of the input time series into high-dimensional state representations encompassing multi-scale temporal dynamics. Such representations are then propagated along the spatial dimension using different powers of the graph adjacency matrix to generate node embeddings characterized by a rich pool of spatiotemporal features. The resulting node embeddings can be efficiently pre-computed in an unsupervised manner, before being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal representations to predictions. The training procedure can then be parallelized node-wise by sampling the node embeddings without breaking any dependency, thus enabling scalability to large networks. Empirical results on relevant datasets show that our approach achieves results competitive with the state of the art, while dramatically reducing the computational burden",
    "checked": true,
    "id": "5e60dc704e7933e2a3e83512f345bba0debfe3f3",
    "semantic_title": "scalable spatiotemporal graph neural networks",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25881": {
    "title": "Exploiting Multiple Abstractions in Episodic RL via Reward Shaping",
    "volume": "main",
    "abstract": "One major limitation to the applicability of Reinforcement Learning (RL) to many practical domains is the large number of samples required to learn an optimal policy. To address this problem and improve learning efficiency, we consider a linear hierarchy of abstraction layers of the Markov Decision Process (MDP) underlying the target domain. Each layer is an MDP representing a coarser model of the one immediately below in the hierarchy. In this work, we propose a novel form of Reward Shaping where the solution obtained at the abstract level is used to offer rewards to the more concrete MDP, in such a way that the abstract solution guides the learning in the more complex domain. In contrast with other works in Hierarchical RL, our technique has few requirements in the design of the abstract models and it is also tolerant to modeling errors, thus making the proposed approach practical. We formally analyze the relationship between the abstract models and the exploration heuristic induced in the lower-level domain. Moreover, we prove that the method guarantees optimal convergence and we demonstrate its effectiveness experimentally",
    "checked": true,
    "id": "07e496c302ed32f6da6f535eb4b0f91b58169726",
    "semantic_title": "exploiting multiple abstractions in episodic rl via reward shaping",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25882": {
    "title": "Tricking the Hashing Trick: A Tight Lower Bound on the Robustness of CountSketch to Adaptive Inputs",
    "volume": "main",
    "abstract": "CountSketch and Feature Hashing (the ``hashing trick'') are popular randomized dimensionality reduction methods that support recovery of l2 -heavy hitters and approximate inner products. When the inputs are not adaptive (do not depend on prior outputs), classic estimators applied to a sketch of size O(l / epsilon) are accurate for a number of queries that is exponential in l. When inputs are adaptive, however, an adversarial input can be constructed after O(l) queries with the classic estimator and the best known robust estimator only supports ~O(l^2) queries. In this work we show that this quadratic dependence is in a sense inherent: We design an attack that after O(l^2) queries produces an adversarial input vector whose sketch is highly biased. Our attack uses ``natural'' non-adaptive inputs (only the final adversarial input is chosen adaptively) and universally applies with any correct estimator, including one that is unknown to the attacker. In that, we expose inherent vulnerability of this fundamental method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25883": {
    "title": "Continuous Mixtures of Tractable Probabilistic Models",
    "volume": "main",
    "abstract": "Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large enough number of integration points the approximation becomes de-facto exact. Moreover, for a finite set of integration points, the integration method effectively compiles the continuous mixture into a standard PC. In experiments, we show that this simple scheme proves remarkably effective, as PCs learnt this way set new state of the art for tractable models on many standard density estimation benchmarks",
    "checked": true,
    "id": "b7e257197e70b5979992fe7c69a7746bae0e184d",
    "semantic_title": "continuous mixtures of tractable probabilistic models",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25884": {
    "title": "End-to-End Learning for Optimization via Constraint-Enforcing Approximators",
    "volume": "main",
    "abstract": "In many real-world applications, predictive methods are used to provide inputs for downstream optimization problems. It has been shown that using the downstream task-based objective to learn the intermediate predictive model is often better than using only intermediate task objectives, such as prediction error. The learning task in the former approach is referred to as end-to-end learning. The difficulty in end-to-end learning lies in differentiating through the optimization problem. Therefore, we propose a neural network architecture that can learn to approximately solve these optimization problems, particularly ensuring its output satisfies the feasibility constraints via alternate projections. We show these projections converge at a geometric rate to the exact projection. Our approach is more computationally efficient than existing methods as we do not need to solve the original optimization problem at each iteration. Furthermore, our approach can be applied to a wider range of optimization problems. We apply this to a shortest path problem for which the first stage forecasting problem is a computer vision task of predicting edge costs from terrain maps, a capacitated multi-product newsvendor problem, and a maximum matching problem. We show that this method out-performs existing approaches in terms of final task-based loss and training time",
    "checked": true,
    "id": "6b6e821d5841d5f2b688d610c3ca64ce4a3ee0c7",
    "semantic_title": "end-to-end learning for optimization via constraint-enforcing approximators",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25885": {
    "title": "Practical Parallel Algorithms for Submodular Maximization Subject to a Knapsack Constraint with Nearly Optimal Adaptivity",
    "volume": "main",
    "abstract": "Submodular maximization has wide applications in machine learning and data mining, where massive datasets have brought the great need for designing efficient and parallelizable algorithms. One measure of the parallelizability of a submodular maximization algorithm is its adaptivity complexity, which indicates the number of sequential rounds where a polynomial number of queries to the objective function can be executed in parallel. In this paper, we study the problem of non-monotone submodular maximization subject to a knapsack constraint, and propose the first combinatorial algorithm achieving an (8+epsilon)-approximation under O(log n) adaptive complexity, which is optimal up to a factor of O(loglog n). Moreover, under slightly larger adaptivity, we also propose approximation algorithms with nearly optimal query complexity of O(n), while achieving better approximation ratios. We show that our algorithms can also be applied to the special case of submodular maximization subject to a cardinality constraint, and achieve performance bounds comparable with those of state-of-the-art algorithms. Finally, the effectiveness of our approach is demonstrated by extensive experiments on real-world applications",
    "checked": true,
    "id": "5028562c146dd06b2a9aa8754496b86cb43de326",
    "semantic_title": "practical parallel algorithms for submodular maximization subject to a knapsack constraint with nearly optimal adaptivity",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25886": {
    "title": "Opposite Online Learning via Sequentially Integrated Stochastic Gradient Descent Estimators",
    "volume": "main",
    "abstract": "Stochastic gradient descent algorithm (SGD) has been popular in various fields of artificial intelligence as well as a prototype of online learning algorithms. This article proposes a novel and general framework of one-sided testing for streaming data based on SGD, which determines whether the unknown parameter is greater than a certain positive constant. We construct the online-updated test statistic sequentially by integrating the selected batch-specific estimator or its opposite, which is referred to opposite online learning. The batch-specific online estimators are chosen strategically according to the proposed sequential tactics designed by two-armed bandit process. Theoretical results prove the advantage of the strategy ensuring the distribution of test statistic to be optimal under the null hypothesis and also supply the theoretical evidence of power enhancement compared with classical test statistic. In application, the proposed method is appealing for statistical inference of one-sided testing because it is scalable for any model. Finally, the superior finite-sample performance is evaluated by simulation studies",
    "checked": true,
    "id": "ba45a6c7b542b0b8e8458ea845854f57eaf96dcc",
    "semantic_title": "opposite online learning via sequentially integrated stochastic gradient descent estimators",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25887": {
    "title": "Contrastive Learning with the Feature Reconstruction Amplifier",
    "volume": "main",
    "abstract": "Contrastive learning has emerged as one of the most promising self-supervised methods. It can efficiently learn the transferable representations of samples through the instance-level discrimination task. In general, the performance of the contrastive learning method can be further improved by projecting the transferable high-dimensional representations into the low-dimensional feature space. This is because the model can learn more abstract discriminative information. However, when low-dimensional features cannot provide sufficient discriminative information to the model (e.g., the samples are very similar to each other), the existing contrastive learning method will be limited to a great extent. Therefore, in this paper, we propose a general module called the Feature Reconstruction Amplifier (FRA) for adding additional high-dimensional feature information to the model. Specifically, FRA reconstructs the low-dimensional feature embeddings with Gaussian noise vectors and projects them to a high-dimensional reconstruction space. In this reconstruction space, we can add additional feature information through the designed loss. We have verified the effectiveness of the module itself through exhaustive ablation experiments. In addition, we perform linear evaluation and transfer learning on five common visual datasets, the experimental results demonstrate that our method is superior to recent advanced contrastive learning methods",
    "checked": true,
    "id": "53901562d0ff14da779f35b6431a4bd836e4bfa5",
    "semantic_title": "contrastive learning with the feature reconstruction amplifier",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25888": {
    "title": "Augmented Proximal Policy Optimization for Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "Safe reinforcement learning considers practical scenarios that maximize the return while satisfying safety constraints. Current algorithms, which suffer from training oscillations or approximation errors, still struggle to update the policy efficiently with precise constraint satisfaction. In this article, we propose Augmented Proximal Policy Optimization (APPO), which augments the Lagrangian function of the primal constrained problem via attaching a quadratic deviation term. The constructed multiplier-penalty function dampens cost oscillation for stable convergence while being equivalent to the primal constrained problem to precisely control safety costs. APPO alternately updates the policy and the Lagrangian multiplier via solving the constructed augmented primal-dual problem, which can be easily implemented by any first-order optimizer. We apply our APPO methods in diverse safety-constrained tasks, setting a new state of the art compared with a comprehensive list of safe RL baselines. Extensive experiments verify the merits of our method in easy implementation, stable convergence, and precise cost control",
    "checked": true,
    "id": "5ee4efa8f7454770df0c43ce4d6fb65dd085fca5",
    "semantic_title": "augmented proximal policy optimization for safe reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25889": {
    "title": "GradPU: Positive-Unlabeled Learning via Gradient Penalty and Positive Upweighting",
    "volume": "main",
    "abstract": "Positive-unlabeled learning is an essential problem in many real-world applications with only labeled positive and unlabeled data, especially when the negative samples are difficult to identify. Most existing positive-unlabeled learning methods will inevitably overfit the positive class to some extent due to the existence of unidentified positive samples. This paper first analyzes the overfitting problem and proposes to bound the generalization errors via Wasserstein distances. Based on that, we develop a simple yet effective positive-unlabeled learning method, GradPU, which consists of two key ingredients: A gradient-based regularizer that penalizes the gradient norms in the interpolated data region, which improves the generalization of positive class; An unnormalized upweighting mechanism that assigns larger weights to those positive samples that are hard, not-well-fitted and less frequently labeled. It enforces the training error of each positive sample to be small and increases the robustness to the labeling bias. We evaluate our proposed GradPU on three datasets: MNIST, FashionMNIST, and CIFAR10. The results demonstrate that GradPU achieves state-of-the-art performance on both unbiased and biased positive labeling scenarios",
    "checked": true,
    "id": "88e55907283a62d93d4f9b18ab5ecb6f7595f3c7",
    "semantic_title": "gradpu: positive-unlabeled learning via gradient penalty and positive upweighting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25890": {
    "title": "Semi-Supervised Deep Regression with Uncertainty Consistency and Variational Model Ensembling via Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Deep regression is an important problem with numerous applications. These range from computer vision tasks such as age estimation from photographs, to medical tasks such as ejection fraction estimation from echocardiograms for disease tracking. Semi-supervised approaches for deep regression are notably under-explored compared to classification and segmentation tasks, however. Unlike classification tasks, which rely on thresholding functions for generating class pseudo-labels, regression tasks use real number target predictions directly as pseudo-labels, making them more sensitive to prediction quality. In this work, we propose a novel approach to semi-supervised regression, namely Uncertainty-Consistent Variational Model Ensembling (UCVME), which improves training by generating high-quality pseudo-labels and uncertainty estimates for heteroscedastic regression. Given that aleatoric uncertainty is only dependent on input data by definition and should be equal for the same inputs, we present a novel uncertainty consistency loss for co-trained models. Our consistency loss significantly improves uncertainty estimates and allows higher quality pseudo-labels to be assigned greater importance under heteroscedastic regression. Furthermore, we introduce a novel variational model ensembling approach to reduce prediction noise and generate more robust pseudo-labels. We analytically show our method generates higher quality targets for unlabeled data and further improves training. Experiments show that our method outperforms state-of-the-art alternatives on different tasks and can be competitive with supervised methods that use full labels. Code is available at https://github.com/xmed-lab/UCVME",
    "checked": true,
    "id": "336d6ad9ddddbe6ed9a6520e01ba2e951a2f5650",
    "semantic_title": "semi-supervised deep regression with uncertainty consistency and variational model ensembling via bayesian neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25891": {
    "title": "Tackling Data Heterogeneity in Federated Learning with Class Prototypes",
    "volume": "main",
    "abstract": "Data heterogeneity across clients in federated learning (FL) settings is a widely acknowledged challenge. In response, personalized federated learning (PFL) emerged as a framework to curate local models for clients' tasks. In PFL, a common strategy is to develop local and global models jointly - the global model (for generalization) informs the local models, and the local models (for personalization) are aggregated to update the global model. A key observation is that if we can improve the generalization ability of local models, then we can improve the generalization of global models, which in turn builds better personalized models. In this work, we consider class imbalance, an overlooked type of data heterogeneity, in the classification setting. We propose FedNH, a novel method that improves the local models' performance for both personalization and generalization by combining the uniformity and semantics of class prototypes. FedNH initially distributes class prototypes uniformly in the latent space and smoothly infuses the class semantics into class prototypes. We show that imposing uniformity helps to combat prototype collapse while infusing class semantics improves local models. Extensive experiments were conducted on popular classification datasets under the cross-device setting. Our results demonstrate the effectiveness and stability of our method over recent works",
    "checked": true,
    "id": "61cbca5ac40ba71388c6d2235fa25a20014faec0",
    "semantic_title": "tackling data heterogeneity in federated learning with class prototypes",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25892": {
    "title": "CrysGNN: Distilling Pre-trained Knowledge to Enhance Property Prediction for Crystalline Materials",
    "volume": "main",
    "abstract": "In recent years, graph neural network (GNN) based approaches have emerged as a powerful technique to encode complex topological structure of crystal materials in an enriched repre- sentation space. These models are often supervised in nature and using the property-specific training data, learn relation- ship between crystal structure and different properties like formation energy, bandgap, bulk modulus, etc. Most of these methods require a huge amount of property-tagged data to train the system which may not be available for different prop- erties. However, there is an availability of a huge amount of crystal data with its chemical composition and structural bonds. To leverage these untapped data, this paper presents CrysGNN, a new pre-trained GNN framework for crystalline materials, which captures both node and graph level structural information of crystal graphs using a huge amount of unla- belled material data. Further, we extract distilled knowledge from CrysGNN and inject into different state of the art prop- erty predictors to enhance their property prediction accuracy. We conduct extensive experiments to show that with distilled knowledge from the pre-trained model, all the SOTA algo- rithms are able to outperform their own vanilla version with good margins. We also observe that the distillation process provides significant improvement over the conventional ap- proach of finetuning the pre-trained model. We will release the pre-trained model along with the large dataset of 800K crys- tal graph which we carefully curated; so that the pre-trained model can be plugged into any existing and upcoming models to enhance their prediction accuracy",
    "checked": false,
    "id": "4fc96ea53bc8c08bd184126869a22f039ca4dbfe",
    "semantic_title": "crysgnn : distilling pre-trained knowledge to enhance property prediction for crystalline materials",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25893": {
    "title": "Non-reversible Parallel Tempering for Deep Posterior Approximation",
    "volume": "main",
    "abstract": "Parallel tempering (PT), also known as replica exchange, is the go-to workhorse for simulations of multi-modal distributions. The key to the success of PT is to adopt efficient swap schemes. The popular deterministic even-odd (DEO) scheme exploits the non-reversibility property and has successfully reduced the communication cost from quadratic to linear given the sufficiently many chains. However, such an innovation largely disappears in big data due to the limited chains and few bias-corrected swaps. To handle this issue, we generalize the DEO scheme to promote non-reversibility and propose a few solutions to tackle the underlying bias caused by the geometric stopping time. Notably, in big data scenarios, we obtain a nearly linear communication cost based on the optimal window size. In addition, we also adopt stochastic gradient descent (SGD) with large and constant learning rates as exploration kernels. Such a user-friendly nature enables us to conduct approximation tasks for complex posteriors without much tuning costs",
    "checked": true,
    "id": "47340a5c393fdf682cb27f6c4ebd96984607e918",
    "semantic_title": "non-reversible parallel tempering for deep posterior approximation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25894": {
    "title": "Stability-Based Generalization Analysis of the Asynchronous Decentralized SGD",
    "volume": "main",
    "abstract": "The generalization ability often determines the success of machine learning algorithms in practice. Therefore, it is of great theoretical and practical importance to understand and bound the generalization error of machine learning algorithms. In this paper, we provide the first generalization results of the popular stochastic gradient descent (SGD) algorithm in the distributed asynchronous decentralized setting. Our analysis is based on the uniform stability tool, where stable means that the learned model does not change much in small variations of the training set. Under some mild assumptions, we perform a comprehensive generalizability analysis of the asynchronous decentralized SGD, including generalization error and excess generalization error bounds for the strongly convex, convex, and non-convex cases. Our theoretical results reveal the effects of the learning rate, training data size, training iterations, decentralized communication topology, and asynchronous delay on the generalization performance of the asynchronous decentralized SGD. We also study the optimization error regarding the objective function values and investigate how the initial point affects the excess generalization error. Finally, we conduct extensive experiments on MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets to validate the theoretical findings",
    "checked": true,
    "id": "a8059c10fb5c16aa1afb5f1263c4f4a1cd689595",
    "semantic_title": "stability-based generalization analysis of the asynchronous decentralized sgd",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25895": {
    "title": "Integer Subspace Differential Privacy",
    "volume": "main",
    "abstract": "We propose new differential privacy solutions for when external invariants and integer constraints are simultaneously enforced on the data product. These requirements arise in real world applications of private data curation, including the public release of the 2020 U.S. Decennial Census. They pose a great challenge to the production of provably private data products with adequate statistical usability. We propose integer subspace differential privacy to rigorously articulate the privacy guarantee when data products maintain both the invariants and integer characteristics, and demonstrate the composition and post-processing properties of our proposal. To address the challenge of sampling from a potentially highly restricted discrete space, we devise a pair of unbiased additive mechanisms, the generalized Laplace and the generalized Gaussian mechanisms, by solving the Diophantine equations as defined by the constraints. The proposed mechanisms have good accuracy, with errors exhibiting sub-exponential and sub-Gaussian tail probabilities respectively. To implement our proposal, we design an MCMC algorithm and supply empirical convergence assessment using estimated upper bounds on the total variation distance via L-lag coupling. We demonstrate the efficacy of our proposal with applications to a synthetic problem with intersecting invariants, a sensitive contingency table with known margins, and the 2010 Census county-level demonstration data with mandated fixed state population totals",
    "checked": true,
    "id": "eccd5376c514492ea39a40720a78139467db024c",
    "semantic_title": "integer subspace differential privacy",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25896": {
    "title": "Black-Box Adversarial Attack on Time Series Classification",
    "volume": "main",
    "abstract": "With the increasing use of deep neural network (DNN) in time series classification (TSC), recent work reveals the threat of adversarial attack, where the adversary can construct adversarial examples to cause model mistakes. However, existing researches on the adversarial attack of TSC typically adopt an unrealistic white-box setting with model details transparent to the adversary. In this work, we study a more rigorous black-box setting with attack detection applied, which restricts gradient access and requires the adversarial example to be also stealthy. Theoretical analyses reveal that the key lies in: estimating black-box gradient with diversity and non-convexity of TSC models resolved, and restricting the l0 norm of the perturbation to construct adversarial samples. Towards this end, we propose a new framework named BlackTreeS, which solves the hard optimization issue for adversarial example construction with two simple yet effective modules. In particular, we propose a tree search strategy to find influential positions in a sequence, and independently estimate the black-box gradients for these positions. Extensive experiments on three real-world TSC datasets and five DNN based models validate the effectiveness of BlackTreeS, e.g., it improves the attack success rate from 19.3% to 27.3%, and decreases the detection success rate from 90.9% to 6.8% for LSTM on the UWave dataset",
    "checked": true,
    "id": "5cf50e00f39010b3ab3f1f96de15223f5231c652",
    "semantic_title": "black-box adversarial attack on time series classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25897": {
    "title": "C-NTPP: Learning Cluster-Aware Neural Temporal Point Process",
    "volume": "main",
    "abstract": "Event sequences in continuous time space are ubiquitous across applications and have been intensively studied with both classic temporal point process (TPP) and its recent deep network variants. This work is motivated by an observation that many of event data exhibit inherent clustering patterns in terms of the sparse correlation among events, while such characteristics are seldom explicitly considered in existing neural TPP models whereby the history encoders are often embodied by RNNs or Transformers. In this work, we propose a c-NTPP (Cluster-Aware Neural Temporal Point Process) model, which leverages a sequential variational autoencoder framework to infer the latent cluster each event belongs to in the sequence. Specially, a novel event-clustered attention mechanism is devised to learn each cluster and then aggregate them together to obtain the final representation for each event. Extensive experiments show that c-NTPP achieves superior performance on both real-world and synthetic datasets, and it can also uncover the underlying clustering correlations",
    "checked": true,
    "id": "bf3aed71713f62e1e26971dbf36b920768ff4a71",
    "semantic_title": "c-ntpp: learning cluster-aware neural temporal point process",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25898": {
    "title": "Eliciting Structural and Semantic Global Knowledge in Unsupervised Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph Contrastive Learning (GCL) has recently drawn much research interest for learning generalizable node representations in a self-supervised manner. In general, the contrastive learning process in GCL is performed on top of the representations learned by a graph neural network (GNN) backbone, which transforms and propagates the node contextual information based on its local neighborhoods. However, nodes sharing similar characteristics may not always be geographically close, which poses a great challenge for unsupervised GCL efforts due to their inherent limitations in capturing such global graph knowledge. In this work, we address their inherent limitations by proposing a simple yet effective framework -- Simple Neural Networks with Structural and Semantic Contrastive Learning} (S^3-CL). Notably, by virtue of the proposed structural and semantic contrastive learning algorithms, even a simple neural network can learn expressive node representations that preserve valuable global structural and semantic patterns. Our experiments demonstrate that the node representations learned by S^3-CL) achieve superior performance on different downstream tasks compared with the state-of-the-art unsupervised GCL methods. Implementation and more experimental details are publicly available at https://github.com/kaize0409/S-3-CL",
    "checked": true,
    "id": "dcb17372a0233719d84aca856c6147a01e2ba34f",
    "semantic_title": "eliciting structural and semantic global knowledge in unsupervised graph contrastive learning",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25899": {
    "title": "Incremental Reinforcement Learning with Dual-Adaptive ε-Greedy Exploration",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has achieved impressive performance in various domains. However, most RL frameworks oversimplify the problem by assuming a fixed-yet-known environment and often have difficulty being generalized to real-world scenarios. In this paper, we address a new challenge with a more realistic setting, Incremental Reinforcement Learning, where the search space of the Markov Decision Process continually expands. While previous methods usually suffer from the lack of efficiency in exploring the unseen transitions, especially with increasing search space, we present a new exploration framework named Dual-Adaptive ϵ-greedy Exploration (DAE) to address the challenge of Incremental RL. Specifically, DAE employs a Meta Policy and an Explorer to avoid redundant computation on those sufficiently learned samples. Furthermore, we release a testbed based on a synthetic environment and the Atari benchmark to validate the effectiveness of any exploration algorithms under Incremental RL. Experimental results demonstrate that the proposed framework can efficiently learn the unseen transitions in new environments, leading to notable performance improvement, i.e., an average of more than 80%, over eight baselines examined",
    "checked": true,
    "id": "c1844cda42b3732a5576d05bb6e007eb1db00919",
    "semantic_title": "incremental reinforcement learning with dual-adaptive ε-greedy exploration",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25900": {
    "title": "Provably Efficient Primal-Dual Reinforcement Learning for CMDPs with Non-stationary Objectives and Constraints",
    "volume": "main",
    "abstract": "We consider primal-dual-based reinforcement learning (RL) in episodic constrained Markov decision processes (CMDPs) with non-stationary objectives and constraints, which plays a central role in ensuring the safety of RL in time-varying environments. In this problem, the reward/utility functions and the state transition functions are both allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain known variation budgets. Designing safe RL algorithms in time-varying environments is particularly challenging because of the need to integrate the constraint violation reduction, safe exploration, and adaptation to the non-stationarity. To this end, we identify two alternative conditions on the time-varying constraints under which we can guarantee the safety in the long run. We also propose the Periodically Restarted Optimistic Primal-Dual Proximal Policy Optimization (PROPD-PPO) algorithm that can coordinate with both two conditions. Furthermore, a dynamic regret bound and a constraint violation bound are established for the proposed algorithm in both the linear kernel CMDP function approximation setting and the tabular CMDP setting under two alternative conditions. This paper provides the first provably efficient algorithm for non-stationary CMDPs with safe exploration",
    "checked": true,
    "id": "09f0422754142a1a58182b8238f9cd1b242adab5",
    "semantic_title": "provably efficient primal-dual reinforcement learning for cmdps with non-stationary objectives and constraints",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25901": {
    "title": "Non-stationary Risk-Sensitive Reinforcement Learning: Near-Optimal Dynamic Regret, Adaptive Detection, and Separation Design",
    "volume": "main",
    "abstract": "We study risk-sensitive reinforcement learning (RL) based on an entropic risk measure in episodic non-stationary Markov decision processes (MDPs). Both the reward functions and the state transition kernels are unknown and allowed to vary arbitrarily over time with a budget on their cumulative variations. When this variation budget is known a prior, we propose two restart-based algorithms, namely Restart-RSMB and Restart-RSQ, and establish their dynamic regrets. Based on these results, we further present a meta-algorithm that does not require any prior knowledge of the variation budget and can adaptively detect the non-stationarity on the exponential value functions. A dynamic regret lower bound is then established for non-stationary risk-sensitive RL to certify the near-optimality of the proposed algorithms. Our results also show that the risk control and the handling of the non-stationarity can be separately designed in the algorithm if the variation budget is known a prior, while the non-stationary detection mechanism in the adaptive algorithm depends on the risk parameter. This work offers the first non-asymptotic theoretical analyses for the non-stationary risk-sensitive RL in the literature",
    "checked": true,
    "id": "25457e352e553a89a76ed7fddb8aa9687094614e",
    "semantic_title": "non-stationary risk-sensitive reinforcement learning: near-optimal dynamic regret, adaptive detection, and separation design",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25902": {
    "title": "SKDBERT: Compressing BERT via Stochastic Knowledge Distillation",
    "volume": "main",
    "abstract": "In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain compact BERT-style language model dubbed SKDBERT. In each distillation iteration, SKD samples a teacher model from a pre-defined teacher team, which consists of multiple teacher models with multi-level capacities, to transfer knowledge into student model in an one-to-one manner. Sampling distribution plays an important role in SKD. We heuristically present three types of sampling distributions to assign appropriate probabilities for multi-level teacher models. SKD has two advantages: 1) it can preserve the diversities of multi-level teacher models via stochastically sampling single teacher model in each distillation iteration, and 2) it can also improve the efficacy of knowledge distillation via multi-level teacher models when large capacity gap exists between the teacher model and the student model. Experimental results on GLUE benchmark show that SKDBERT reduces the size of a BERT model by 40% while retaining 99.5% performances of language understanding and being 100% faster",
    "checked": true,
    "id": "be6d5c3d95854d27edb911eb20c5e1c4af630828",
    "semantic_title": "skdbert: compressing bert via stochastic knowledge distillation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25903": {
    "title": "Model-Based Offline Reinforcement Learning with Local Misspecification",
    "volume": "main",
    "abstract": "We present a model-based offline reinforcement learning policy performance lower bound that explicitly captures dynamics model misspecification and distribution mismatch and we propose an empirical algorithm for optimal offline policy selection. Theoretically, we prove a novel safe policy improvement theorem by establishing pessimism approximations to the value function. Our key insight is to jointly consider selecting over dynamics models and policies: as long as a dynamics model can accurately represent the dynamics of the state-action pairs visited by a given policy, it is possible to approximate the value of that particular policy. We analyze our lower bound in the LQR setting and also show competitive performance to previous lower bounds on policy selection across a set of D4RL tasks",
    "checked": true,
    "id": "3aa1eafd0198ef0f49f79ff376ac60b170bb10e0",
    "semantic_title": "model-based offline reinforcement learning with local misspecification",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25904": {
    "title": "Can Label-Specific Features Help Partial-Label Learning?",
    "volume": "main",
    "abstract": "Partial label learning (PLL) aims to learn from inexact data annotations where each training example is associated with a coarse candidate label set. Due to its practicability, many PLL algorithms have been proposed in recent literature. Most prior PLL works attempt to identify the ground-truth labels from candidate sets and the classifier is trained afterward by fitting the features of examples and their exact ground-truth labels. From a different perspective, we propose to enrich the feature space and raise the question ``Can label-specific features help PLL?'' rather than learning from examples with identical features for all classes. Despite its benefits, previous label-specific feature approaches rely on ground-truth labels to split positive and negative examples of each class and then conduct clustering analysis, which is not directly applicable in PLL. To remedy this problem, we propose an uncertainty-aware confidence region to accommodate false positive labels. We first employ graph-based label enhancement to yield smooth pseudo-labels and facilitate the confidence region split. After acquiring label-specific features, a family of binary classifiers is induced. Extensive experiments on both synthesized and real-world datasets are conducted and the results show that our method consistently outperforms eight baselines. Our code is released at https://github.com/meteoseeker/UCL",
    "checked": true,
    "id": "635aaf49c94c29086a8bf67b4d7819d6c023226e",
    "semantic_title": "can label-specific features help partial-label learning?",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25905": {
    "title": "Interpreting Unfairness in Graph Neural Networks via Training Node Attribution",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have emerged as the leading paradigm for solving graph analytical problems in various real-world applications. Nevertheless, GNNs could potentially render biased predictions towards certain demographic subgroups. Understanding how the bias in predictions arises is critical, as it guides the design of GNN debiasing mechanisms. However, most existing works overwhelmingly focus on GNN debiasing, but fall short on explaining how such bias is induced. In this paper, we study a novel problem of interpreting GNN unfairness through attributing it to the influence of training nodes. Specifically, we propose a novel strategy named Probabilistic Distribution Disparity (PDD) to measure the bias exhibited in GNNs, and develop an algorithm to efficiently estimate the influence of each training node on such bias. We verify the validity of PDD and the effectiveness of influence estimation through experiments on real-world datasets. Finally, we also demonstrate how the proposed framework could be used for debiasing GNNs. Open-source code can be found at https://github.com/yushundong/BIND",
    "checked": true,
    "id": "ec568050b74edfdb7e463200daf42ba36663505c",
    "semantic_title": "interpreting unfairness in graph neural networks via training node attribution",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25906": {
    "title": "Robust and Fast Measure of Information via Low-Rank Representation",
    "volume": "main",
    "abstract": "The matrix-based Rényi's entropy allows us to directly quantify information measures from given data, without explicit estimation of the underlying probability distribution. This intriguing property makes it widely applied in statistical inference and machine learning tasks. However, this information theoretical quantity is not robust against noise in the data, and is computationally prohibitive in large-scale applications. To address these issues, we propose a novel measure of information, termed low-rank matrix-based Rényi's entropy, based on low-rank representations of infinitely divisible kernel matrices. The proposed entropy functional inherits the specialty of of the original definition to directly quantify information from data, but enjoys additional advantages including robustness and effective calculation. Specifically, our low-rank variant is more sensitive to informative perturbations induced by changes in underlying distributions, while being insensitive to uninformative ones caused by noises. Moreover, low-rank Rényi's entropy can be efficiently approximated by random projection and Lanczos iteration techniques, reducing the overall complexity from O(n³) to O(n²s) or even O(ns²), where n is the number of data samples and s ≪ n. We conduct large-scale experiments to evaluate the effectiveness of this new information measure, demonstrating superior results compared to matrix-based Rényi's entropy in terms of both performance and computational efficiency",
    "checked": true,
    "id": "83838422effa238682d462396a5a251fabb079b3",
    "semantic_title": "robust and fast measure of information via low-rank representation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25907": {
    "title": "Graph Anomaly Detection via Multi-Scale Contrastive Learning Networks with Augmented View",
    "volume": "main",
    "abstract": "Graph anomaly detection (GAD) is a vital task in graph-based machine learning and has been widely applied in many real-world applications. The primary goal of GAD is to capture anomalous nodes from graph datasets, which evidently deviate from the majority of nodes. Recent methods have paid attention to various scales of contrastive strategies for GAD, i.e., node-subgraph and node-node contrasts. However, they neglect the subgraph-subgraph comparison information which the normal and abnormal subgraph pairs behave differently in terms of embeddings and structures in GAD, resulting in sub-optimal task performance. In this paper, we fulfill the above idea in the proposed multi-view multi-scale contrastive learning framework with subgraph-subgraph contrast for the first practice. To be specific, we regard the original input graph as the first view and generate the second view by graph augmentation with edge modifications. With the guidance of maximizing the similarity of the subgraph pairs, the proposed subgraph-subgraph contrast contributes to more robust subgraph embeddings despite of the structure variation. Moreover, the introduced subgraph-subgraph contrast cooperates well with the widely-adopted node-subgraph and node-node contrastive counterparts for mutual GAD performance promotions. Besides, we also conduct sufficient experiments to investigate the impact of different graph augmentation approaches on detection performance. The comprehensive experimental results well demonstrate the superiority of our method compared with the state-of-the-art approaches and the effectiveness of the multi-view subgraph pair contrastive strategy for the GAD task. The source code is released at https://github.com/FelixDJC/GRADATE",
    "checked": true,
    "id": "ca590583797aad5bd5af7f2c8492a0b14da53810",
    "semantic_title": "graph anomaly detection via multi-scale contrastive learning networks with augmented view",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25908": {
    "title": "Diffeomorphic Information Neural Estimation",
    "volume": "main",
    "abstract": "Mutual Information (MI) and Conditional Mutual Information (CMI) are multi-purpose tools from information theory that are able to naturally measure the statistical dependencies between random variables, thus they are usually of central interest in several statistical and machine learning tasks, such as conditional independence testing and representation learning. However, estimating CMI, or even MI, is infamously challenging due the intractable formulation. In this study, we introduce DINE (Diffeomorphic Information Neural Estimator)–a novel approach for estimating CMI of continuous random variables, inspired by the invariance of CMI over diffeomorphic maps. We show that the variables of interest can be replaced with appropriate surrogates that follow simpler distributions, allowing the CMI to be efficiently evaluated via analytical solutions. Additionally, we demonstrate the quality of the proposed estimator in comparison with state-of-the-arts in three important tasks, including estimating MI, CMI, as well as its application in conditional independence testing. The empirical evaluations show that DINE consistently outperforms competitors in all tasks and is able to adapt very well to complex and high-dimensional relationships",
    "checked": true,
    "id": "799b8695fb63c0ea41d9d4d77959b234cf4b4cb3",
    "semantic_title": "diffeomorphic information neural estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25909": {
    "title": "Combining Slow and Fast: Complementary Filtering for Dynamics Learning",
    "volume": "main",
    "abstract": "Modeling an unknown dynamical system is crucial in order to predict the future behavior of the system. A standard approach is training recurrent models on measurement data. While these models typically provide exact short-term predictions, accumulating errors yield deteriorated long-term behavior. In contrast, models with reliable long-term predictions can often be obtained, either by training a robust but less detailed model, or by leveraging physics-based simulations. In both cases, inaccuracies in the models yield a lack of short-time details. Thus, different models with contrastive properties on different time horizons are available. This observation immediately raises the question: Can we obtain predictions that combine the best of both worlds? Inspired by sensor fusion tasks, we interpret the problem in the frequency domain and leverage classical methods from signal processing, in particular complementary filters. This filtering technique combines two signals by applying a high-pass filter to one signal, and low-pass filtering the other. Essentially, the high-pass filter extracts high-frequencies, whereas the low-pass filter extracts low frequencies. Applying this concept to dynamics model learning enables the construction of models that yield accurate long- and short-term predictions. Here, we propose two methods, one being purely learning-based and the other one being a hybrid model that requires an additional physics-based simulator",
    "checked": true,
    "id": "8300094620aa145281956869ed7930250af0ad38",
    "semantic_title": "combining slow and fast: complementary filtering for dynamics learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25910": {
    "title": "Popularizing Fairness: Group Fairness and Individual Welfare",
    "volume": "main",
    "abstract": "Group-fair learning methods typically seek to ensure that some measure of prediction efficacy for (often historically) disadvantaged minority groups is comparable to that for the majority of the population. When a principal seeks to adopt a group-fair approach to replace another, the principal may face opposition from those who feel they may be harmed by the switch, and this, in turn, may deter adoption. We propose that a potential mitigation to this concern is to ensure that a group-fair model is also popular, in the sense that, for a majority of the target population, it yields a preferred distribution over outcomes compared with the conventional model. In this paper, we show that state of the art fair learning approaches are often unpopular in this sense. We propose several efficient algorithms for postprocessing an existing group-fair learning scheme to improve its popularity while retaining fairness. Through extensive experiments, we demonstrate that the proposed postprocessing approaches are highly effective in practice",
    "checked": true,
    "id": "9953d04e41a4a63f8391032a39ac3ee0e3109610",
    "semantic_title": "popularizing fairness: group fairness and individual welfare",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25911": {
    "title": "FairFed: Enabling Group Fairness in Federated Learning",
    "volume": "main",
    "abstract": "Training ML models which are fair across different demographic groups is of critical importance due to the increased integration of ML in crucial decision-making scenarios such as healthcare and recruitment. Federated learning has been viewed as a promising solution for collaboratively training machine learning models among multiple parties while maintaining their local data privacy. However, federated learning also poses new challenges in mitigating the potential bias against certain populations (e.g., demographic groups), as this typically requires centralized access to the sensitive information (e.g., race, gender) of each datapoint. Motivated by the importance and challenges of group fairness in federated learning, in this work, we propose FairFed, a novel algorithm for fairness-aware aggregation to enhance group fairness in federated learning. Our proposed approach is server-side and agnostic to the applied local debiasing thus allowing for flexible use of different local debiasing methods across clients. We evaluate FairFed empirically versus common baselines for fair ML and federated learning and demonstrate that it provides fairer models, particularly under highly heterogeneous data distributions across clients. We also demonstrate the benefits of FairFed in scenarios involving naturally distributed real-life data collected from different geographical locations or departments within an organization",
    "checked": true,
    "id": "5b8cd1183840c5717d2b7cdec298d65d6ec69c32",
    "semantic_title": "fairfed: enabling group fairness in federated learning",
    "citation_count": 49
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25912": {
    "title": "Goal-Conditioned Generators of Deep Policies",
    "volume": "main",
    "abstract": "Goal-conditioned Reinforcement Learning (RL) aims at learning optimal policies, given goals encoded in special command inputs. Here we study goal-conditioned neural nets (NNs) that learn to generate deep NN policies in form of context-specific weight matrices, similar to Fast Weight Programmers and other methods from the 1990s. Using context commands of the form ``generate a policy that achieves a desired expected return,'' our NN generators combine powerful exploration of parameter space with generalization across commands to iteratively find better and better policies. A form of weight-sharing HyperNetworks and policy embeddings scales our method to generate deep NNs. Experiments show how a single learned policy generator can produce policies that achieve any return seen during training. Finally, we evaluate our algorithm on a set of continuous control tasks where it exhibits competitive performance. Our code is public",
    "checked": true,
    "id": "46af8550faab2e2b67fef7278eab0b581097e6ea",
    "semantic_title": "goal-conditioned generators of deep policies",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25913": {
    "title": "Directed Acyclic Graph Structure Learning from Dynamic Graphs",
    "volume": "main",
    "abstract": "Estimating the structure of directed acyclic graphs (DAGs) of features (variables) plays a vital role in revealing the latent data generation process and providing causal insights in various applications. Although there have been many studies on structure learning with various types of data, the structure learning on the dynamic graph has not been explored yet, and thus we study the learning problem of node feature generation mechanism on such ubiquitous dynamic graph data. In a dynamic graph, we propose to simultaneously estimate contemporaneous relationships and time-lagged interaction relationships between the node features. These two kinds of relationships form a DAG, which could effectively characterize the feature generation process in a concise way. To learn such a DAG, we cast the learning problem as a continuous score-based optimization problem, which consists of a differentiable score function to measure the validity of the learned DAGs and a smooth acyclicity constraint to ensure the acyclicity of the learned DAGs. These two components are translated into an unconstraint augmented Lagrangian objective which could be minimized by mature continuous optimization techniques. The resulting algorithm, named GraphNOTEARS, outperforms baselines on simulated data across a wide range of settings that may encounter in real-world applications. We also apply the proposed approach on two dynamic graphs constructed from the real-world Yelp dataset, demonstrating our method could learn the connections between node features, which conforms with the domain knowledge",
    "checked": true,
    "id": "a0fb1b670f62ac1de989e5f3c439ef5114196892",
    "semantic_title": "directed acyclic graph structure learning from dynamic graphs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25914": {
    "title": "Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting",
    "volume": "main",
    "abstract": "The distribution shift in Time Series Forecasting (TSF), indicating series distribution changes over time, largely hinders the performance of TSF models. Existing works towards distribution shift in time series are mostly limited in the quantification of distribution and, more importantly, overlook the potential shift between lookback and horizon windows. To address above challenges, we systematically summarize the distribution shift in TSF into two categories. Regarding lookback windows as input-space and horizon windows as output-space, there exist (i) intra-space shift, that the distribution within the input-space keeps shifted over time, and (ii) inter-space shift, that the distribution is shifted between input-space and output-space. Then we introduce, Dish-TS, a general neural paradigm for alleviating distribution shift in TSF. Specifically, for better distribution estimation, we propose the coefficient net (Conet), which can be any neural architectures, to map input sequences into learnable distribution coefficients. To relieve intra-space and inter-space shift, we organize Dish-TS as a Dual-Conet framework to separately learn the distribution of input- and output-space, which naturally captures the distribution difference of two spaces. In addition, we introduce a more effective training strategy for intractable Conet learning. Finally, we conduct extensive experiments on several datasets coupled with different state-of-the-art forecasting models. Experimental results show Dish-TS consistently boosts them with a more than 20% average improvement. Code is available at https://github.com/weifantt/Dish-TS",
    "checked": true,
    "id": "4366579b70e6c408e7fd621344bc869c1dda9d0f",
    "semantic_title": "dish-ts: a general paradigm for alleviating distribution shift in time series forecasting",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25915": {
    "title": "Learning Decomposed Spatial Relations for Multi-Variate Time-Series Modeling",
    "volume": "main",
    "abstract": "Modeling multi-variate time-series (MVTS) data is a long-standing research subject and has found wide applications. Recently, there is a surge of interest in modeling spatial relations between variables as graphs, i.e., first learning one static graph for each dataset and then exploiting the graph structure via graph neural networks. However, as spatial relations may differ substantially across samples, building one static graph for all the samples inherently limits flexibility and severely degrades the performance in practice. To address this issue, we propose a framework for fine-grained modeling and utilization of spatial correlation between variables. By analyzing the statistical properties of real-world datasets, a universal decomposition of spatial correlation graphs is first identified. Specifically, the hidden spatial relations can be decomposed into a prior part, which applies across all the samples, and a dynamic part, which varies between samples, and building different graphs is necessary to model these relations. To better coordinate the learning of the two relational graphs, we propose a min-max learning paradigm that not only regulates the common part of different dynamic graphs but also guarantees spatial distinguishability among samples. The experimental results show that our proposed model outperforms the state-of-the-art baseline methods on both time-series forecasting and time-series point prediction tasks",
    "checked": true,
    "id": "02657f902758c3b85146629fe4faa3feab0ea34f",
    "semantic_title": "learning decomposed spatial relations for multi-variate time-series modeling",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25916": {
    "title": "Wasserstein Graph Distance Based on L1–Approximated Tree Edit Distance between Weisfeiler–Lehman Subtrees",
    "volume": "main",
    "abstract": "The Weisfeiler-Lehman (WL) test is a widely used algorithm in graph machine learning, including graph kernels, graph metrics, and graph neural networks. However, it focuses only on the consistency of the graph, which means that it is unable to detect slight structural differences. Consequently, this limits its ability to capture structural information, which also limits the performance of existing models that rely on the WL test. This limitation is particularly severe for traditional metrics defined by the WL test, which cannot precisely capture slight structural differences. In this paper, we propose a novel graph metric called the Wasserstein WL Subtree (WWLS) distance to address this problem. Our approach leverages the WL subtree as structural information for node neighborhoods and defines node metrics using the L1-approximated tree edit distance (L1-TED) between WL subtrees of nodes. Subsequently, we combine the Wasserstein distance and the L1-TED to define the WWLS distance, which can capture slight structural differences that may be difficult to detect using conventional metrics. We demonstrate that the proposed WWLS distance outperforms baselines in both metric validation and graph classification experiments",
    "checked": false,
    "id": "a0ee7fda99777066554267a7c211aad9b8e1f4f2",
    "semantic_title": "wasserstein graph distance based on l1-approximated tree edit distance between weisfeiler-lehman subtrees",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25917": {
    "title": "Combinatorial Causal Bandits",
    "volume": "main",
    "abstract": "In combinatorial causal bandits (CCB), the learning agent chooses at most K variables in each round to intervene, collects feedback from the observed variables, with the goal of minimizing expected regret on the target variable Y. We study under the context of binary generalized linear models (BGLMs) with a succinct parametric representation of the causal models. We present the algorithm BGLM-OFU for Markovian BGLMs (i.e., no hidden variables) based on the maximum likelihood estimation method and give regret analysis for it. For the special case of linear models with hidden variables, we apply causal inference techniques such as the do calculus to convert the original model into a Markovian model, and then show that our BGLM-OFU algorithm and another algorithm based on the linear regression both solve such linear models with hidden variables. Our novelty includes (a) considering the combinatorial intervention action space and the general causal graph structures including ones with hidden variables, (b) integrating and adapting techniques from diverse studies such as generalized linear bandits and online influence maximization, and (c) avoiding unrealistic assumptions (such as knowing the joint distribution of the parents of Y under all interventions) and regret factors exponential to causal graph size in prior studies",
    "checked": true,
    "id": "3ab3e2e5a7ec0f0f994d468c017c10b5d9cf4c4c",
    "semantic_title": "combinatorial causal bandits",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25918": {
    "title": "Scalable Attributed-Graph Subspace Clustering",
    "volume": "main",
    "abstract": "Over recent years, graph convolutional networks emerged as powerful node clustering methods and have set state of the art results for this task. In this paper, we argue that some of these methods are unnecessarily complex and propose a node clustering model that is more scalable while being more effective. The proposed model uses Laplacian smoothing to learn an initial representation of the graph before applying an efficient self-expressive subspace clustering procedure. This is performed via learning a factored coefficient matrix. These factors are then embedded into a new feature space in such a way as to generate a valid affinity matrix (symmetric and non-negative) on which an implicit spectral clustering algorithm is performed. Experiments on several real-world attributed datasets demonstrate the cost-effective nature of our method with respect to the state of the art",
    "checked": true,
    "id": "b2d5c6b55d03dea1273462a0c8e5a8305da4173b",
    "semantic_title": "scalable attributed-graph subspace clustering",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25919": {
    "title": "SigMaNet: One Laplacian to Rule Them All",
    "volume": "main",
    "abstract": "This paper introduces SigMaNet, a generalized Graph Convolutional Network (GCN) capable of handling both undirected and directed graphs with weights not restricted in sign nor magnitude. The cornerstone of SigMaNet is the Sign-Magnetic Laplacian (LSM), a new Laplacian matrix that we introduce ex novo in this work. LSM allows us to bridge a gap in the current literature by extending the theory of spectral GCNs to (directed) graphs with both positive and negative weights. LSM exhibits several desirable properties not enjoyed by other Laplacian matrices on which several state-of-the-art architectures are based, among which encoding the edge direction and weight in a clear and natural way that is not negatively affected by the weight magnitude. LSM is also completely parameter-free, which is not the case of other Laplacian operators such as, e.g., the Magnetic Laplacian. The versatility and the performance of our proposed approach is amply demonstrated via computational experiments. Indeed, our results show that, for at least a metric, SigMaNet achieves the best performance in 15 out of 21 cases and either the first- or second-best performance in 21 cases out of 21, even when compared to architectures that are either more complex or that, due to being designed for a narrower class of graphs, should---but do not---achieve a better performance",
    "checked": true,
    "id": "5a5ebfb550a3a949f99afcdabddb4f063f0db13f",
    "semantic_title": "sigmanet: one laplacian to rule them all",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25920": {
    "title": "Optimal Decision Diagrams for Classification",
    "volume": "main",
    "abstract": "Decision diagrams for classification have some notable advantages over decision trees, as their internal connections can be determined at training time and their width is not bound to grow exponentially with their depth. Accordingly, decision diagrams are usually less prone to data fragmentation in internal nodes. However, the inherent complexity of training these classifiers acted as a long-standing barrier to their widespread adoption. In this context, we study the training of optimal decision diagrams (ODDs) from a mathematical programming perspective. We introduce a novel mixed-integer linear programming model for training and demonstrate its applicability for many datasets of practical importance. Further, we show how this model can be easily extended for fairness, parsimony, and stability notions. We present numerical analyses showing that our model allows training ODDs in short computational times, and that ODDs achieve better accuracy than optimal decision trees, while allowing for improved stability without significant accuracy losses",
    "checked": true,
    "id": "4da0660eb65e50c8e71d57110e118a9c70b3a2d0",
    "semantic_title": "optimal decision diagrams for classification",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25921": {
    "title": "Estimating Average Causal Effects from Patient Trajectories",
    "volume": "main",
    "abstract": "In medical practice, treatments are selected based on the expected causal effects on patient outcomes. Here, the gold standard for estimating causal effects are randomized controlled trials; however, such trials are costly and sometimes even unethical. Instead, medical practice is increasingly interested in estimating causal effects among patient (sub)groups from electronic health records, that is, observational data. In this paper, we aim at estimating the average causal effect (ACE) from observational data (patient trajectories) that are collected over time. For this, we propose DeepACE: an end-to-end deep learning model. DeepACE leverages the iterative G-computation formula to adjust for the bias induced by time-varying confounders. Moreover, we develop a novel sequential targeting procedure which ensures that DeepACE has favorable theoretical properties, i.e., is doubly robust and asymptotically efficient. To the best of our knowledge, this is the first work that proposes an end-to-end deep learning model tailored for estimating time-varying ACEs. We compare DeepACE in an extensive number of experiments, confirming that it achieves state-of-the-art performance. We further provide a case study for patients suffering from low back pain to demonstrate that DeepACE generates important and meaningful findings for clinical practice. Our work enables practitioners to develop effective treatment recommendations based on population effects",
    "checked": true,
    "id": "fae41e2dded7f20f110a57c61dd0d1dbdd4c6a24",
    "semantic_title": "estimating average causal effects from patient trajectories",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25922": {
    "title": "Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation",
    "volume": "main",
    "abstract": "Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets",
    "checked": true,
    "id": "165503c48e553a5559190ce74cda823f4e166b54",
    "semantic_title": "decorate the newcomers: visual domain prompt for continual test time adaptation",
    "citation_count": 14
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25923": {
    "title": "EffConv: Efficient Learning of Kernel Sizes for Convolution Layers of CNNs",
    "volume": "main",
    "abstract": "Determining kernel sizes of a CNN model is a crucial and non-trivial design choice and significantly impacts its performance. The majority of kernel size design methods rely on complex heuristic tricks or leverage neural architecture search that requires extreme computational resources. Thus, learning kernel sizes, using methods such as modeling kernels as a combination of basis functions, jointly with the model weights has been proposed as a workaround. However, previous methods cannot achieve satisfactory results or are inefficient for large-scale datasets. To fill this gap, we design a novel efficient kernel size learning method in which a size predictor model learns to predict optimal kernel sizes for a classifier given a desired number of parameters. It does so in collaboration with a kernel predictor model that predicts the weights of the kernels - given kernel sizes predicted by the size predictor - to minimize the training objective, and both models are trained end-to-end. Our method only needs a small fraction of the training epochs of the original CNN to train these two models and find proper kernel sizes for it. Thus, it offers an efficient and effective solution for the kernel size learning problem. Our extensive experiments on MNIST, CIFAR-10, STL-10, and ImageNet-32 demonstrate that our method can achieve the best training time vs. accuracy trade-off compared to previous kernel size learning methods and significantly outperform them on challenging datasets such as STL-10 and ImageNet-32. Our implementations are available at https://github.com/Alii-Ganjj/EffConv",
    "checked": true,
    "id": "302dc87c74a8cb0c8491f1c29483693eb283df7d",
    "semantic_title": "effconv: efficient learning of kernel sizes for convolution layers of cnns",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25924": {
    "title": "Fast Counterfactual Inference for History-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "Incorporating sequence-to-sequence models into history-based Reinforcement Learning (RL) provides a general way to extend RL to partially-observable tasks. This method compresses history spaces according to the correlations between historical observations and the rewards. However, they do not adjust for the confounding correlations caused by data sampling and assign high beliefs to uninformative historical observations, leading to limited compression of history spaces. Counterfactual Inference (CI), which estimates causal effects by single-variable intervention, is a promising way to adjust for confounding. However, it is computationally infeasible to directly apply the single-variable intervention to a huge number of historical observations. This paper proposes to perform CI on observation sub-spaces instead of single observations and develop a coarse-to-fine CI algorithm, called Tree-based History Counterfactual Inference (T-HCI), to reduce the number of interventions exponentially. We show that T-HCI is computationally feasible in practice and brings significant sample efficiency gains in various challenging partially-observable tasks, including Maze, BabyAI, and robot manipulation tasks",
    "checked": true,
    "id": "80f2036ab4e76c86f69a63e77e000833853c2722",
    "semantic_title": "fast counterfactual inference for history-based reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25925": {
    "title": "Robust Causal Graph Representation Learning against Confounding Effects",
    "volume": "main",
    "abstract": "The prevailing graph neural network models have achieved significant progress in graph representation learning. However, in this paper, we uncover an ever-overlooked phenomenon: the pre-trained graph representation learning model tested with full graphs underperforms the model tested with well-pruned graphs. This observation reveals that there exist confounders in graphs, which may interfere with the model learning semantic information, and current graph representation learning methods have not eliminated their influence. To tackle this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to learn robust graph representations against confounding effects. RCGRL introduces an active approach to generate instrumental variables under unconditional moment restrictions, which empowers the graph representation learning model to eliminate confounders, thereby capturing discriminative information that is causally related to downstream predictions. We offer theorems and proofs to guarantee the theoretical effectiveness of the proposed approach. Empirically, we conduct extensive experiments on a synthetic dataset and multiple benchmark datasets. Experimental results demonstrate the effectiveness and generalization ability of RCGRL. Our codes are available at https://github.com/hang53/RCGRL",
    "checked": true,
    "id": "8a4c8b331abc0d5522fc5262595ff7d597c8a93b",
    "semantic_title": "robust causal graph representation learning against confounding effects",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25926": {
    "title": "Towards Decision-Friendly AUC: Learning Multi-Classifier with AUCµ",
    "volume": "main",
    "abstract": "Area Under the ROC Curve (AUC) is a widely used ranking metric in imbalanced learning due to its insensitivity to label distributions. As a well-known multiclass extension of AUC, Multiclass AUC (MAUC, a.k.a. M-metric) measures the average AUC of multiple binary classifiers. In this paper, we argue that simply optimizing MAUC is far from enough for imbalanced multi-classification. More precisely, MAUC only focuses on learning scoring functions via ranking optimization, while leaving the decision process unconsidered. Therefore, scoring functions being able to make good decisions might suffer from low performance in terms of MAUC. To overcome this issue, we turn to explore AUCµ, another multiclass variant of AUC, which further takes the decision process into consideration. Motivated by this fact, we propose a surrogate risk optimization framework to improve model performance from the perspective of AUCµ. Practically, we propose a two-stage training framework for multi-classification, where at the first stage a scoring function is learned maximizing AUCµ, and at the second stage we seek for a decision function to improve the F1-metric via our proposed soft F1. Theoretically, we first provide sufficient conditions that optimizing the surrogate losses could lead to the Bayes optimal scoring function. Afterward, we show that the proposed surrogate risk enjoys a generalization bound in order of O(1/√N). Experimental results on four benchmark datasets demonstrate the effectiveness of our proposed method in both AUCµ and F1-metric",
    "checked": true,
    "id": "31cdc5c5f6994192f0c93587dc3521428c8931a1",
    "semantic_title": "towards decision-friendly auc: learning multi-classifier with aucµ",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25927": {
    "title": "Long-Tail Cross Modal Hashing",
    "volume": "main",
    "abstract": "Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced data, while imbalanced data with long-tail distribution is more general in real-world. Several long-tail hashing methods have been proposed but they can not adapt for multi-modal data, due to the complex interplay between labels and individuality and commonality information of multi-modal data. Furthermore, CMH methods mostly mine the commonality of multi-modal data to learn hash codes, which may override tail labels encoded by the individuality of respective modalities. In this paper, we propose LtCMH (Long-tail CMH) to handle imbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the individuality and commonality of different modalities by minimizing the dependency between the individuality of respective modalities and by enhancing the commonality of these modalities. Then it dynamically combines the individuality and commonality with direct features extracted from respective modalities to create meta features that enrich the representation of tail labels, and binaries meta features to generate hash codes. LtCMH significantly outperforms state-of-the-art baselines on long-tail datasets and holds a better (or comparable) performance on datasets with balanced labels",
    "checked": true,
    "id": "181f5132f3a353c5bd320de91429bab04b6dbb2d",
    "semantic_title": "long-tail cross modal hashing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25928": {
    "title": "Handling Missing Data via Max-Entropy Regularized Graph Autoencoder",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are popular weapons for modeling relational data. Existing GNNs are not specified for attribute-incomplete graphs, making missing attribute imputation a burning issue. Until recently, many works notice that GNNs are coupled with spectral concentration, which means the spectrum obtained by GNNs concentrates on a local part in spectral domain, e.g., low-frequency due to oversmoothing issue. As a consequence, GNNs may be seriously flawed for reconstructing graph attributes as graph spectral concentration tends to cause a low imputation precision. In this work, we present a regularized graph autoencoder for graph attribute imputation, named MEGAE, which aims at mitigating spectral concentration problem by maximizing the graph spectral entropy. Notably, we first present the method for estimating graph spectral entropy without the eigen-decomposition of Laplacian matrix and provide the theoretical upper error bound. A maximum entropy regularization then acts in the latent space, which directly increases the graph spectral entropy. Extensive experiments show that MEGAE outperforms all the other state-of-the-art imputation methods on a variety of benchmark datasets",
    "checked": true,
    "id": "cf0d2eb36235a15a4ad8e2b8d4cf363ff9a530b8",
    "semantic_title": "handling missing data via max-entropy regularized graph autoencoder",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25929": {
    "title": "Reinforced Approximate Exploratory Data Analysis",
    "volume": "main",
    "abstract": "Exploratory data analytics (EDA) is a sequential decision making process where analysts choose subsequent queries that might lead to some interesting insights based on the previous queries and corresponding results. Data processing systems often execute the queries on samples to produce results with low latency. Different downsampling strategy preserves different statistics of the data and have different magnitude of latency reductions. The optimum choice of sampling strategy often depends on the particular context of the analysis flow and the hidden intent of the analyst. In this paper, we are the first to consider the impact of sampling in interactive data exploration settings as they introduce approximation errors. We propose a Deep Reinforcement Learning (DRL) based framework which can optimize the sample selection in order to keep the analysis and insight generation flow intact. Evaluations with real datasets show that our technique can preserve the original insight generation flow while improving the interaction latency, compared to baseline methods",
    "checked": true,
    "id": "52055da99bb0fdad22f8c11b96ca69f0d3d7f9e1",
    "semantic_title": "reinforced approximate exploratory data analysis",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25930": {
    "title": "Learning Program Synthesis for Integer Sequences from Scratch",
    "volume": "main",
    "abstract": "We present a self-learning approach for synthesizing programs from integer sequences. Our method relies on a tree search guided by a learned policy. Our system is tested on the On-Line Encyclopedia of Integer Sequences. There, it discovers, on its own, solutions for 27987 sequences starting from basic operators and without human-written training examples",
    "checked": true,
    "id": "7658f56be043f2d883eaeaef40998d79cf15a26f",
    "semantic_title": "learning program synthesis for integer sequences from scratch",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25931": {
    "title": "Semi-transductive Learning for Generalized Zero-Shot Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "Sketch-based image retrieval (SBIR) is an attractive research area where freehand sketches are used as queries to retrieve relevant images. Existing solutions have advanced the task to the challenging zero-shot setting (ZS-SBIR), where the trained models are tested on new classes without seen data. However, they are prone to overfitting under a realistic scenario when the test data includes both seen and unseen classes. In this paper, we study generalized ZS-SBIR (GZS-SBIR) and propose a novel semi-transductive learning paradigm. Transductive learning is performed on the image modality to explore the potential data distribution within unseen classes, and zero-shot learning is performed on the sketch modality sharing the learned knowledge through a semi-heterogeneous architecture. A hybrid metric learning strategy is proposed to establish semantics-aware ranking property and calibrate the joint embedding space. Extensive experiments are conducted on two large-scale benchmarks and four evaluation metrics. The results show that our method is superior over the state-of-the-art competitors in the challenging GZS-SBIR task",
    "checked": true,
    "id": "db7236a9c1d344fda071d003b49816e2d414b768",
    "semantic_title": "semi-transductive learning for generalized zero-shot sketch-based image retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25932": {
    "title": "Multi-Classifier Adversarial Optimization for Active Learning",
    "volume": "main",
    "abstract": "Active learning (AL) aims to find a better trade-off between labeling costs and model performance by consciously selecting more informative samples to label. Recently, adversarial approaches have emerged as effective solutions. Most of them leverage generative adversarial networks to align feature distributions of labeled and unlabeled data, upon which discriminators are trained to better distinguish between them. However, these methods fail to consider the relationship between unlabeled samples and decision boundaries, and their training processes are often complex and unstable. To this end, this paper proposes a novel adversarial AL method, namely multi-classifier adversarial optimization for active learning (MAOAL). MAOAL employs task-specific decision boundaries for data alignment while selecting the most informative samples to label. To fulfill this, we introduce a novel classifier class confusion (C3) metric, which represents the classifier discrepancy as the inter-class correlation of classifier outputs. Without any additional hyper-parameters, the C3 metric further reduces the negative impacts of ambiguous samples in the process of distribution alignment and sample selection. More concretely, the network is trained adversarially by adding two auxiliary classifiers, reducing the distribution bias of labeled and unlabeled samples by minimizing the C3 loss between classifiers, while learning tighter decision boundaries and highlighting hard samples by maximizing the C3 loss. Finally, the unlabeled samples with the highest C3 loss are selected to label. Extensive experiments demonstrate the superiority of our approach over state-of-the-art AL methods in terms of image classification and object detection",
    "checked": true,
    "id": "fc8f1fd3f350d1c243a8e6cc373d247b3f07ae40",
    "semantic_title": "multi-classifier adversarial optimization for active learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25933": {
    "title": "Differentially Private Heatmaps",
    "volume": "main",
    "abstract": "We consider the task of producing heatmaps from users' aggregated data while protecting their privacy. We give a differentially private (DP) algorithm for this task and demonstrate its advantages over previous algorithms on real-world datasets. Our core algorithmic primitive is a DP procedure that takes in a set of distributions and produces an output that is close in Earth Mover's Distance (EMD) to the average of the inputs. We prove theoretical bounds on the error of our algorithm under a certain sparsity assumption and that these are essentially optimal",
    "checked": true,
    "id": "a381bd5c4976f4a8aef5c2a884a1246d592265c7",
    "semantic_title": "differentially private heatmaps",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25934": {
    "title": "DiFA: Differentiable Feature Acquisition",
    "volume": "main",
    "abstract": "Feature acquisition in predictive modeling is an important task in many practical applications. For example, in patient health prediction, we do not fully observe their personal features and need to dynamically select features to acquire. Our goal is to acquire a small subset of features that maximize prediction performance. Recently, some works reformulated feature acquisition as a Markov decision process and applied reinforcement learning (RL) algorithms, where the reward reflects both prediction performance and feature acquisition cost. However, RL algorithms only use zeroth-order information on the reward, which leads to slow empirical convergence, especially when there are many actions (number of features) to consider. For predictive modeling, it is possible to use first-order information on the reward, i.e., gradients, since we are often given an already collected dataset. Therefore, we propose differentiable feature acquisition (DiFA), which uses a differentiable representation of the feature selection policy to enable gradients to flow from the prediction loss to the policy parameters. We conduct extensive experiments on various real-world datasets and show that DiFA significantly outperforms existing feature acquisition methods when the number of features is large",
    "checked": true,
    "id": "c18c34012cb678a0fd82c848283258bbf6c2f5bf",
    "semantic_title": "difa: differentiable feature acquisition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25935": {
    "title": "Local Intrinsic Dimensional Entropy",
    "volume": "main",
    "abstract": "Most entropy measures depend on the spread of the probability distribution over the sample space |X|, and the maximum entropy achievable scales proportionately with the sample space cardinality |X|. For a finite |X|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimensionality. We find that ID-Entropy satisfies many desirable properties and can be extended to conditional entropy, joint entropy and mutual-information variants. ID-Entropy also yields new information bottleneck principles and also links to causality. In the context of deep learning, for feedforward architectures, we show, theoretically and empirically, that the ID-Entropy of a hidden layer directly controls the generalization gap for both classifiers and auto-encoders, when the target function is Lipschitz continuous. Our work primarily shows that, for continuous spaces, taking a structural rather than a statistical approach yields entropy measures which preserve intrinsic data dimensionality, while being relevant for studying various architectures",
    "checked": true,
    "id": "5a4dc0635e88c7fe615aa38f7bcf1ef309d7ae16",
    "semantic_title": "local intrinsic dimensional entropy",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25936": {
    "title": "Improving Uncertainty Quantification of Deep Classifiers via Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis",
    "volume": "main",
    "abstract": "Safe deployment of deep neural networks in high-stake real-world applications require theoretically sound uncertainty quantification. Conformal prediction (CP) is a principled framework for uncertainty quantification of deep models in the form of prediction set for classification tasks with a user-specified coverage (i.e., true class label is contained with high probability). This paper proposes a novel algorithm referred to as Neighborhood Conformal Prediction (NCP) to improve the efficiency of uncertainty quantification from CP for deep classifiers (i.e., reduce prediction set size). The key idea behind NCP is to use the learned representation of the neural network to identify k nearest-neighbor calibration examples for a given testing input and assign them importance weights proportional to their distance to create adaptive prediction sets. We theoretically show that if the learned data representation of the neural network satisfies some mild conditions, NCP will produce smaller prediction sets than traditional CP algorithms. Our comprehensive experiments on CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural networks strongly demonstrate that NCP leads to significant reduction in prediction set size over prior CP methods",
    "checked": true,
    "id": "c545b21e71c7c51bb4cc1d94bedbf6d6fdd57639",
    "semantic_title": "improving uncertainty quantification of deep classifiers via neighborhood conformal prediction: novel algorithm and theoretical analysis",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25937": {
    "title": "Adaptive Hierarchy-Branch Fusion for Online Knowledge Distillation",
    "volume": "main",
    "abstract": "Online Knowledge Distillation (OKD) is designed to alleviate the dilemma that the high-capacity pre-trained teacher model is not available. However, the existing methods mostly focus on improving the ensemble prediction accuracy from multiple students (a.k.a. branches), which often overlook the homogenization problem that makes student model saturate quickly and hurts the performance. We assume that the intrinsic bottleneck of the homogenization problem comes from the identical branch architecture and coarse ensemble strategy. We propose a novel Adaptive Hierarchy-Branch Fusion framework for Online Knowledge Distillation, termed AHBF-OKD, which designs hierarchical branches and adaptive hierarchy-branch fusion module to boost the model diversity and aggregate complementary knowledge. Specifically, we first introduce hierarchical branch architectures to construct diverse peers by increasing the depth of branches monotonously on the basis of target branch. To effectively transfer knowledge from the most complex branch to the simplest target branch, we propose an adaptive hierarchy-branch fusion module to create hierarchical teacher assistants recursively, which regards the target branch as the smallest teacher assistant. During the training, the teacher assistant from the previous hierarchy is explicitly distilled by the teacher assistant and the branch from the current hierarchy. Thus, the important scores to different branches are effectively and adaptively allocated to reduce the branch homogenization. Extensive experiments demonstrate the effectiveness of AHBF-OKD on different datasets, including CIFAR-10/100 and ImageNet 2012. For example, on ImageNet 2012, the distilled ResNet-18 achieves Top-1 error of 29.28\\%, which significantly outperforms the state-of-the-art methods. The source code is available at https://github.com/linruigong965/AHBF",
    "checked": true,
    "id": "cbd86a65d38affa84976dbb968d89c58668c3bee",
    "semantic_title": "adaptive hierarchy-branch fusion for online knowledge distillation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25938": {
    "title": "Deep Latent Regularity Network for Modeling Stochastic Partial Differential Equations",
    "volume": "main",
    "abstract": "Stochastic partial differential equations (SPDEs) are crucial for modelling dynamics with randomness in many areas including economics, physics, and atmospheric sciences. Recently, using deep learning approaches to learn the PDE solution for accelerating PDE simulation becomes increasingly popular. However, SPDEs have two unique properties that require new design on the models. First, the model to approximate the solution of SPDE should be generalizable over both initial conditions and the random sampled forcing term. Second, the random forcing terms usually have poor regularity whose statistics may diverge (e.g., the space-time white noise). To deal with the problems, in this work, we design a deep neural network called \\emph{Deep Latent Regularity Net} (DLR-Net). DLR-Net includes a regularity feature block as the main component, which maps the initial condition and the random forcing term to a set of regularity features. The processing of regularity features is inspired by regularity structure theory and the features provably compose a set of basis to represent the SPDE solution. The regularity features are then fed into a small backbone neural operator to get the output. We conduct experiments on various SPDEs including the dynamic $\\Phi^4_1$ model and the stochastic 2D Navier-Stokes equation to predict their solutions, and the results demonstrate that the proposed DLR-Net can achieve SOTA accuracy compared with the baselines. Moreover, the inference time is over 20 times faster than the traditional numerical solver and is comparable with the baseline deep learning models",
    "checked": true,
    "id": "3da11f998d7387aeced2945064f90f6af7074d1b",
    "semantic_title": "deep latent regularity network for modeling stochastic partial differential equations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25939": {
    "title": "Dynamic Representation Learning with Temporal Point Processes for Higher-Order Interaction Forecasting",
    "volume": "main",
    "abstract": "The explosion of digital information and the growing involvement of people in social networks led to enormous research activity to develop methods that can extract meaningful information from interaction data. Commonly, interactions are represented by edges in a network or a graph, which implicitly assumes that the interactions are pairwise and static. However, real-world interactions deviate from these assumptions: (i) interactions can be multi-way, involving more than two nodes or individuals (e.g., family relationships, protein interactions), and (ii) interactions can change over a period of time (e.g., change of opinions and friendship status). While pairwise interactions have been studied in a dynamic network setting and multi-way interactions have been studied using hypergraphs in static networks, there exists no method, at present, that can predict multi-way interactions or hyperedges in dynamic settings. Existing related methods cannot answer temporal queries like what type of interaction will occur next and when it will occur. This paper proposes a temporal point process model for hyperedge prediction to address these problems. Our proposed model uses dynamic representation learning techniques for nodes in a neural point process framework to forecast hyperedges. We present several experimental results and set benchmark results. As far as our knowledge, this is the first work that uses the temporal point process to forecast hyperedges in dynamic networks",
    "checked": true,
    "id": "6eea9f9fb4f802c509f1cd698ff42f0e67ebf338",
    "semantic_title": "dynamic representation learning with temporal point processes for higher-order interaction forecasting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25940": {
    "title": "An Adaptive Layer to Leverage Both Domain and Task Specific Information from Scarce Data",
    "volume": "main",
    "abstract": "Many companies make use of customer service chats to help the customer and try to solve their problem. However, customer service data is confidential and as such, cannot easily be shared in the research community. This also implies that these data are rarely labeled, making it difficult to take advantage of it with machine learning methods. In this paper we present the first work on a customer's problem status prediction and identification of problematic conversations. Given very small subsets of labeled textual conversations and unlabeled ones, we propose a semi-supervised framework dedicated to customer service data leveraging speaker role information to adapt the model to the domain and the task using a two-step process. Our framework, Task-Adaptive Fine-tuning, goes from predicting customer satisfaction to identifying the status of the customer's problem, with the latter being the main objective of the multi-task setting. It outperforms recent inductive semi-supervised approaches on this novel task while only considering a relatively low number of parameters to train on during the final target task. We believe it can not only serve models dedicated to customer service but also to any other application making use of confidential conversational data where labeled sets are rare. Source code is available at https://github.com/gguibon/taft",
    "checked": true,
    "id": "91691c66df6bde3d134137dcdc61580503fe7ab9",
    "semantic_title": "an adaptive layer to leverage both domain and task specific information from scarce data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25941": {
    "title": "Interpolating Graph Pair to Regularize Graph Classification",
    "volume": "main",
    "abstract": "We present a simple and yet effective interpolation-based regularization technique, aiming to improve the generalization of Graph Neural Networks (GNNs) on supervised graph classification. We leverage Mixup, an effective regularizer for vision, where random sample pairs and their labels are interpolated to create synthetic images for training. Unlike images with grid-like coordinates, graphs have arbitrary structure and topology, which can be very sensitive to any modification that alters the graph's semantic meanings. This posts two unanswered questions for Mixup-like regularization schemes: Can we directly mix up a pair of graph inputs? If so, how well does such mixing strategy regularize the learning of GNNs? To answer these two questions, we propose ifMixup, which first adds dummy nodes to make two graphs have the same input size and then simultaneously performs linear interpolation between the aligned node feature vectors and the aligned edge representations of the two graphs. We empirically show that such simple mixing schema can effectively regularize the classification learning, resulting in superior predictive accuracy to popular graph augmentation and GNN methods",
    "checked": true,
    "id": "def2c67f039009752915913b3bfddf586dad11cc",
    "semantic_title": "interpolating graph pair to regularize graph classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25942": {
    "title": "Graph Knows Unknowns: Reformulate Zero-Shot Learning as Sample-Level Graph Recognition",
    "volume": "main",
    "abstract": "Zero-shot learning (ZSL) is an extreme case of transfer learning that aims to recognize samples (e.g., images) of unseen classes relying on a train-set covering only seen classes and a set of auxiliary knowledge (e.g., semantic descriptors). Existing methods usually resort to constructing a visual-to-semantics mapping based on features extracted from each whole sample. However, since the visual and semantic spaces are inherently independent and may exist in different manifolds, these methods may easily suffer from the domain bias problem due to the knowledge transfer from seen to unseen classes. Unlike existing works, this paper investigates the fine-grained ZSL from a novel perspective of sample-level graph. Specifically, we decompose an input into several fine-grained elements and construct a graph structure per sample to measure and utilize element-granularity relations within each sample. Taking advantage of recently developed graph neural networks (GNNs), we formulate the ZSL problem to a graph-to-semantics mapping task, which can better exploit element-semantics correlation and local sub-structural information in samples. Experimental results on the widely used benchmark datasets demonstrate that the proposed method can mitigate the domain bias problem and achieve competitive performance against other representative methods",
    "checked": true,
    "id": "217e4ffcffa4107a38ced923086006b73e9399a4",
    "semantic_title": "graph knows unknowns: reformulate zero-shot learning as sample-level graph recognition",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25943": {
    "title": "Self-Supervised Bidirectional Learning for Graph Matching",
    "volume": "main",
    "abstract": "Deep learning methods have demonstrated promising performance on the NP-hard Graph Matching (GM) problems. However, the state-of-the-art methods usually require the ground-truth labels, which may take extensive human efforts or be impractical to collect. In this paper, we present a robust self-supervised bidirectional learning method (IA-SSGM) to tackle GM in an unsupervised manner. It involves an affinity learning component and a classic GM solver. Specifically, we adopt the Hungarian solver to generate pseudo correspondence labels for the simple probabilistic relaxation of the affinity matrix. In addition, a bidirectional recycling consistency module is proposed to generate pseudo samples by recycling the pseudo correspondence back to permute the input. It imposes a consistency constraint between the pseudo affinity and the original one, which is theoretically supported to help reduce the matching error. Our method further develops a graph contrastive learning jointly with the affinity learning to enhance its robustness against the noise and outliers in real applications. Experiments deliver superior performance over the previous state-of-the-arts on five real-world benchmarks, especially under the more difficult outlier scenarios, demon- strating the effectiveness of our method",
    "checked": true,
    "id": "fcb7161eb81b82d533d410532ddffc871ab61830",
    "semantic_title": "self-supervised bidirectional learning for graph matching",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25944": {
    "title": "Boosting Graph Neural Networks via Adaptive Knowledge Distillation",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have shown remarkable performance on diverse graph mining tasks. While sharing the same message passing framework, our study shows that different GNNs learn distinct knowledge from the same graph. This implies potential performance improvement by distilling the complementary knowledge from multiple models. However, knowledge distillation (KD) transfers knowledge from high-capacity teachers to a lightweight student, which deviates from our scenario: GNNs are often shallow. To transfer knowledge effectively, we need to tackle two challenges: how to transfer knowledge from compact teachers to a student with the same capacity; and, how to exploit student GNN's own learning ability. In this paper, we propose a novel adaptive KD framework, called BGNN, which sequentially transfers knowledge from multiple GNNs into a student GNN. We also introduce an adaptive temperature module and a weight boosting module. These modules guide the student to the appropriate knowledge for effective learning. Extensive experiments have demonstrated the effectiveness of BGNN. In particular, we achieve up to 3.05% improvement for node classification and 6.35% improvement for graph classification over vanilla GNNs",
    "checked": true,
    "id": "402125f4732ef1ec5974ac7513a63bf5b8414611",
    "semantic_title": "boosting graph neural networks via adaptive knowledge distillation",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25945": {
    "title": "Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) has been used to efficiently solve vision-based control tasks in high-dimensional image observations. Although recent MBRL algorithms perform well in trained observations, they fail when faced with visual distractions in observations. These task-irrelevant distractions (e.g., clouds, shadows, and light) may be constantly present in real-world scenarios. In this study, we propose a novel self-supervised method, Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and world model with dual contrastive learning which efficiently captures task-relevant features among multi-view data augmentations. We also introduce a recurrent state inverse dynamics model that helps the world model to better understand the temporal structure. The proposed methods can enhance the robustness of the world model against visual distractions. To evaluate the generalization performance, we first train Dr. G on simple backgrounds and then test it on complex natural video backgrounds in the DeepMind Control suite, and the randomizing environments in Robosuite. Dr. G yields a performance improvement of 117% and 14% over prior works, respectively. Our code is open-sourced and available at https://github.com/JeongsooHa/DrG.git",
    "checked": true,
    "id": "dbe429e7edf790b26c0f117027dc2cddb8ab9815",
    "semantic_title": "dream to generalize: zero-shot model-based reinforcement learning for unseen visual distractions",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25946": {
    "title": "Discriminability and Transferability Estimation: A Bayesian Source Importance Estimation Approach for Multi-Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "Source free domain adaptation (SFDA) transfers a single-source model to the unlabeled target domain without accessing the source data. With the intelligence development of various fields, a zoo of source models is more commonly available, arising in a new setting called multi-source-free domain adaptation (MSFDA). We find that the critical inborn challenge of MSFDA is how to estimate the importance (contribution) of each source model. In this paper, we shed new Bayesian light on the fact that the posterior probability of source importance connects to discriminability and transferability. We propose Discriminability And Transferability Estimation (DATE), a universal solution for source importance estimation. Specifically, a proxy discriminability perception module equips with habitat uncertainty and density to evaluate each sample's surrounding environment. A source-similarity transferability perception module quantifies the data distribution similarity and encourages the transferability to be reasonably distributed with a domain diversity loss. Extensive experiments show that DATE can precisely and objectively estimate the source importance and outperform prior arts by non-trivial margins. Moreover, experiments demonstrate that DATE can take the most popular SFDA networks as backbones and make them become advanced MSFDA solutions",
    "checked": true,
    "id": "1d25e3ff3c28f40bf65b467486d0862476f4e28b",
    "semantic_title": "discriminability and transferability estimation: a bayesian source importance estimation approach for multi-source-free domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25947": {
    "title": "Astromorphic Self-Repair of Neuromorphic Hardware Systems",
    "volume": "main",
    "abstract": "While neuromorphic computing architectures based on Spiking Neural Networks (SNNs) are increasingly gaining interest as a pathway toward bio-plausible machine learning, attention is still focused on computational units like the neuron and synapse. Shifting from this neuro-synaptic perspective, this paper attempts to explore the self-repair role of glial cells, in particular, astrocytes. The work investigates stronger correlations with astrocyte computational neuroscience models to develop macro-models with a higher degree of bio-fidelity that accurately captures the dynamic behavior of the self-repair process. Hardware-software co-design analysis reveals that bio-morphic astrocytic regulation has the potential to self-repair hardware realistic faults in neuromorphic hardware systems with significantly better accuracy and repair convergence for unsupervised learning tasks on the MNIST and F-MNIST datasets. Our implementation source code and trained models are available at https://github.com/NeuroCompLab-psu/Astromorphic_Self_Repair",
    "checked": true,
    "id": "7a2d9d1e7e63561f3fe035e24f758f063233a54e",
    "semantic_title": "astromorphic self-repair of neuromorphic hardware systems",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25948": {
    "title": "Estimating Regression Predictive Distributions with Sample Networks",
    "volume": "main",
    "abstract": "Estimating the uncertainty in deep neural network predictions is crucial for many real-world applications. A common approach to model uncertainty is to choose a parametric distribution and fit the data to it using maximum likelihood estimation. The chosen parametric form can be a poor fit to the data-generating distribution, resulting in unreliable uncertainty estimates. In this work, we propose SampleNet, a flexible and scalable architecture for modeling uncertainty that avoids specifying a parametric form on the output distribution. SampleNets do so by defining an empirical distribution using samples that are learned with the Energy Score and regularized with the Sinkhorn Divergence. SampleNets are shown to be able to well-fit a wide range of distributions and to outperform baselines on large-scale real-world regression tasks",
    "checked": true,
    "id": "e3c78bda7118e5b887c960c47a0dd476fe782b54",
    "semantic_title": "estimating regression predictive distributions with sample networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25949": {
    "title": "NAS-LID: Efficient Neural Architecture Search with Local Intrinsic Dimension",
    "volume": "main",
    "abstract": "One-shot neural architecture search (NAS) substantially improves the search efficiency by training one supernet to estimate the performance of every possible child architecture (i.e., subnet). However, the inconsistency of characteristics among subnets incurs serious interference in the optimization, resulting in poor performance ranking correlation of subnets. Subsequent explorations decompose supernet weights via a particular criterion, e.g., gradient matching, to reduce the interference; yet they suffer from huge computational cost and low space separability. In this work, we propose a lightweight and effective local intrinsic dimension (LID)-based method NAS-LID. NAS-LID evaluates the geometrical properties of architectures by calculating the low-cost LID features layer-by-layer, and the similarity characterized by LID enjoys better separability compared with gradients, which thus effectively reduces the interference among subnets. Extensive experiments on NASBench-201 indicate that NAS-LID achieves superior performance with better efficiency. Specifically, compared to the gradient-driven method, NAS-LID can save up to 86% of GPU memory overhead when searching on NASBench-201. We also demonstrate the effectiveness of NAS-LID on ProxylessNAS and OFA spaces. Source code:https://github.com/marsggbo/NAS-LID",
    "checked": true,
    "id": "94f9a6982512e7a06b1cab477f2b06ec12bd86c7",
    "semantic_title": "nas-lid: efficient neural architecture search with local intrinsic dimension",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25950": {
    "title": "Safeguarded Learned Convex Optimization",
    "volume": "main",
    "abstract": "Applications abound in which optimization problems must be repeatedly solved, each time with new (but similar) data. Analytic optimization algorithms can be hand-designed to provably solve these problems in an iterative fashion. On one hand, data-driven algorithms can \"learn to optimize\" (L2O) with much fewer iterations and similar cost per iteration as general-purpose optimization algorithms. On the other hand, unfortunately, many L2O algorithms lack converge guarantees. To fuse the advantages of these approaches, we present a Safe-L2O framework. Safe-L2O updates incorporate a safeguard to guarantee convergence for convex problems with proximal and/or gradient oracles. The safeguard is simple and computationally cheap to implement, and it is activated only when the data-driven L2O updates would perform poorly or appear to diverge. This yields the numerical benefits of employing machine learning to create rapid L2O algorithms while still guaranteeing convergence. Our numerical examples show convergence of Safe-L2O algorithms, even when the provided data is not from the distribution of training data",
    "checked": false,
    "id": "1162d0066feb34587fb759334b62cc09cac47574",
    "semantic_title": "learner-private convex optimization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25951": {
    "title": "Improving Long-Horizon Imitation through Instruction Prediction",
    "volume": "main",
    "abstract": "Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be found at \\url{https://github.com/jhejna/instruction-prediction}",
    "checked": true,
    "id": "300bf71c102fe349a082482f791ba15795fe69cb",
    "semantic_title": "improving long-horizon imitation through instruction prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25952": {
    "title": "Self-Supervised Learning for Anomalous Channel Detection in EEG Graphs: Application to Seizure Analysis",
    "volume": "main",
    "abstract": "Electroencephalogram (EEG) signals are effective tools towards seizure analysis where one of the most important challenges is accurate detection of seizure events and brain regions in which seizure happens or initiates. However, all existing machine learning-based algorithms for seizure analysis require access to the labeled seizure data while acquiring labeled data is very labor intensive, expensive, as well as clinicians dependent given the subjective nature of the visual qualitative interpretation of EEG signals. In this paper, we propose to detect seizure channels and clips in a self-supervised manner where no access to the seizure data is needed. The proposed method considers local structural and contextual information embedded in EEG graphs by employing positive and negative sub-graphs. We train our method through minimizing contrastive and generative losses. The employ of local EEG sub-graphs makes the algorithm an appropriate choice when accessing to the all EEG channels is impossible due to complications such as skull fractures. We conduct an extensive set of experiments on the largest seizure dataset and demonstrate that our proposed framework outperforms the state-of-the-art methods in the EEG-based seizure study. The proposed method is the only study that requires no access to the seizure data in its training phase, yet establishes a new state-of-the-art to the field, and outperforms all related supervised methods",
    "checked": true,
    "id": "29a467cc939db1607061377fa551285481b8fa4c",
    "semantic_title": "self-supervised learning for anomalous channel detection in eeg graphs: application to seizure analysis",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25953": {
    "title": "Improving Pareto Front Learning via Multi-Sample Hypernetworks",
    "volume": "main",
    "abstract": "Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning tasks show that the proposed framework significantly outperforms the baselines in producing the trade-off Pareto front",
    "checked": true,
    "id": "0c7f2a3ead7688c44b45fb48aa26bf60e6364199",
    "semantic_title": "improving pareto front learning via multi-sample hypernetworks",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25954": {
    "title": "Towards Better Visualizing the Decision Basis of Networks via Unfold and Conquer Attribution Guidance",
    "volume": "main",
    "abstract": "Revealing the transparency of Deep Neural Networks (DNNs) has been widely studied to describe the decision mechanisms of network inner structures. In this paper, we propose a novel post-hoc framework, Unfold and Conquer Attribution Guidance (UCAG), which enhances the explainability of the network decision by spatially scrutinizing the input features with respect to the model confidence. Addressing the phenomenon of missing detailed descriptions, UCAG sequentially complies with the confidence of slices of the image, leading to providing an abundant and clear interpretation. Therefore, it is possible to enhance the representation ability of explanation by preserving the detailed descriptions of assistant input features, which are commonly overwhelmed by the main meaningful regions. We conduct numerous evaluations to validate the performance in several metrics: i) deletion and insertion, ii) (energy-based) pointing games, and iii) positive and negative density maps. Experimental results, including qualitative comparisons, demonstrate that our method outperforms the existing methods with the nature of clear and detailed explanations and applicability",
    "checked": true,
    "id": "ab28e10316c060589a2d4b02500b537e02900526",
    "semantic_title": "towards better visualizing the decision basis of networks via unfold and conquer attribution guidance",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25955": {
    "title": "Federated Robustness Propagation: Sharing Adversarial Robustness in Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) emerges as a popular distributed learning schema that learns a model from a set of participating users without sharing raw data. One major challenge of FL comes with heterogeneous users, who may have distributionally different (or non-iid) data and varying computation resources. As federated users would use the model for prediction, they often demand the trained model to be robust against malicious attackers at test time. Whereas adversarial training (AT) provides a sound solution for centralized learning, extending its usage for federated users has imposed significant challenges, as many users may have very limited training data and tight computational budgets, to afford the data-hungry and costly AT. In this paper, we study a novel FL strategy: propagating adversarial robustness from rich-resource users that can afford AT, to those with poor resources that cannot afford it, during federated learning. We show that existing FL techniques cannot be effectively integrated with the strategy to propagate robustness among non-iid users and propose an efficient propagation approach by the proper use of batch-normalization. We demonstrate the rationality and effectiveness of our method through extensive experiments. Especially, the proposed method is shown to grant federated models remarkable robustness even when only a small portion of users afford AT during learning. Source code can be accessed at https://github.com/illidanlab/FedRBN",
    "checked": true,
    "id": "c7dd25b1e9bce4d9bbb4aa8fb053d0c68da17a33",
    "semantic_title": "federated robustness propagation: sharing adversarial robustness in heterogeneous federated learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25956": {
    "title": "Compressed Decentralized Learning of Conditional Mean Embedding Operators in Reproducing Kernel Hilbert Spaces",
    "volume": "main",
    "abstract": "Conditional mean embedding (CME) operators encode conditional probability densities within Reproducing Kernel Hilbert Space (RKHS). In this paper, we present a decentralized algorithm for a collection of agents to cooperatively approximate CME over a network. Communication constraints limit the agents from sending all data to their neighbors; we only allow sparse representations of covariance operators to be exchanged among agents, compositions of which defines CME. Using a coherence-based compression scheme, we present a consensus-type algorithm that preserves the average of the approximations of the covariance operators across the network. We theoretically prove that the iterative dynamics in RKHS is stable. We then empirically study our algorithm to estimate CMEs to learn spectra of Koopman operators for Markovian dynamical systems and to execute approximate value iteration for Markov decision processes (MDPs)",
    "checked": true,
    "id": "d243063362a4b7fd481565d876040dddfef1597d",
    "semantic_title": "compressed decentralized learning of conditional mean embedding operators in reproducing kernel hilbert spaces",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25957": {
    "title": "RLEKF: An Optimizer for Deep Potential with Ab Initio Accuracy",
    "volume": "main",
    "abstract": "It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the O(N^2) computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our alphaDynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (x11.67) and wall-clock time (x1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck",
    "checked": true,
    "id": "aaa3c21e1532a73835239e6d75a7951f0cf7e51a",
    "semantic_title": "rlekf: an optimizer for deep potential with ab initio accuracy",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25958": {
    "title": "Background-Mixed Augmentation for Weakly Supervised Change Detection",
    "volume": "main",
    "abstract": "Change detection (CD) is to decouple object changes (i.e., object missing or appearing) from background changes (i.e., environment variations) like light and season variations in two images captured in the same scene over a long time span, presenting critical applications in disaster management, urban development, etc. In particular, the endless patterns of background changes require detectors to have a high generalization against unseen environment variations, making this task significantly challenging. Recent deep learning-based methods develop novel network architectures or optimization strategies with paired-training examples, which do not handle the generalization issue explicitly and require huge manual pixel-level annotation efforts. In this work, for the first attempt in the CD community, we study the generalization issue of CD from the perspective of data augmentation and develop a novel weakly supervised training algorithm that only needs image-level labels. Different from general augmentation techniques for classification, we propose the background-mixed augmentation that is specifically designed for change detection by augmenting examples under the guidance of a set of background changing images and letting deep CD models see diverse environment variations. Moreover, we propose the augmented & real data consistency loss that encourages the generalization increase significantly. Our method as a general framework can enhance a wide range of existing deep learning-based detectors. We conduct extensive experiments in two public datasets and enhance four state-of-the-art methods, demonstrating the advantages of our method. We release the code at https://github.com/tsingqguo/bgmix",
    "checked": true,
    "id": "0e4d8a2985ee177159ff2525aabd7415054b3374",
    "semantic_title": "background-mixed augmentation for weakly supervised change detection",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25959": {
    "title": "Enabling Knowledge Refinement upon New Concepts in Abductive Learning",
    "volume": "main",
    "abstract": "Recently there are great efforts on leveraging machine learning and logical reasoning. Many approaches start from a given knowledge base, and then try to utilize the knowledge to help machine learning. In real practice, however, the given knowledge base can often be incomplete or even noisy, and thus, it is crucial to develop the ability of knowledge refinement or enhancement. This paper proposes to enable the Abductive learning (ABL) paradigm to have the ability of knowledge refinement/enhancement. In particular, we focus on the problem that, in contrast to closed-environment tasks where a fixed set of symbols are enough to represent the concepts in the domain, in open-environment tasks new concepts may emerge. Ignoring those new concepts can lead to significant performance decay, whereas it is challenging to identify new concepts and add them to the existing knowledge base with potential conflicts resolved. We propose the ABL_nc approach which exploits machine learning in ABL to identify new concepts from data, exploits knowledge graph to match them with entities, and refines existing knowledge base to resolve conflicts. The refined/enhanced knowledge base can then be used in the next loop of ABL and help improve the performance of machine learning. Experiments on three neuro-symbolic learning tasks verified the effectiveness of the proposed approach",
    "checked": true,
    "id": "970f85cd760979fc5cd70c78420481a2fc675e24",
    "semantic_title": "enabling knowledge refinement upon new concepts in abductive learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25960": {
    "title": "Self-Supervised Graph Attention Networks for Deep Weighted Multi-View Clustering",
    "volume": "main",
    "abstract": "As one of the most important research topics in the unsupervised learning field, Multi-View Clustering (MVC) has been widely studied in the past decade and numerous MVC methods have been developed. Among these methods, the recently emerged Graph Neural Networks (GNN) shine a light on modeling both topological structure and node attributes in the form of graphs, to guide unified embedding learning and clustering. However, the effectiveness of existing GNN-based MVC methods is still limited due to the insufficient consideration in utilizing the self-supervised information and graph information, which can be reflected from the following two aspects: 1) most of these models merely use the self-supervised information to guide the feature learning and fail to realize that such information can be also applied in graph learning and sample weighting; 2) the usage of graph information is generally limited to the feature aggregation in these models, yet it also provides valuable evidence in detecting noisy samples. To this end, in this paper we propose Self-Supervised Graph Attention Networks for Deep Weighted Multi-View Clustering (SGDMC), which promotes the performance of GNN-based deep MVC models by making full use of the self-supervised information and graph information. Specifically, a novel attention-allocating approach that considers both the similarity of node attributes and the self-supervised information is developed to comprehensively evaluate the relevance among different nodes. Meanwhile, to alleviate the negative impact caused by noisy samples and the discrepancy of cluster structures, we further design a sample-weighting strategy based on the attention graph as well as the discrepancy between the global pseudo-labels and the local cluster assignment. Experimental results on multiple real-world datasets demonstrate the effectiveness of our method over existing approaches",
    "checked": true,
    "id": "399c35dcde2eac017b875af1a21916a9ecb10434",
    "semantic_title": "self-supervised graph attention networks for deep weighted multi-view clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25961": {
    "title": "Reward-Biased Maximum Likelihood Estimation for Neural Contextual Bandits: A Distributional Learning Perspective",
    "volume": "main",
    "abstract": "Reward-biased maximum likelihood estimation (RBMLE) is a classic principle in the adaptive control literature for tackling explore-exploit trade-offs. This paper studies the neural contextual bandit problem from a distributional perspective and proposes NeuralRBMLE, which leverages the likelihood of surrogate parametric distributions to learn the unknown reward distributions and thereafter adapts the RBMLE principle to achieve efficient exploration by properly adding a reward-bias term. NeuralRBMLE leverages the representation power of neural networks and directly encodes exploratory behavior in the parameter space, without constructing confidence intervals of the estimated rewards. We propose two variants of NeuralRBMLE algorithms: The first variant directly obtains the RBMLE estimator by gradient ascent, and the second variant simplifies RBMLE to a simple index policy through an approximation. We show that both algorithms achieve order-optimality. Through extensive experiments, we demonstrate that the NeuralRBMLE algorithms achieve comparable or better empirical regrets than the state-of-the-art methods on real-world datasets with non-linear reward functions",
    "checked": true,
    "id": "cbf2ca54338251f762a5c548dd50554eeee9df97",
    "semantic_title": "reward-biased maximum likelihood estimation for neural contextual bandits: a distributional learning perspective",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25962": {
    "title": "Learning Noise-Induced Reward Functions for Surpassing Demonstrations in Imitation Learning",
    "volume": "main",
    "abstract": "Imitation learning (IL) has recently shown impressive performance in training a reinforcement learning agent with human demonstrations, eliminating the difficulty of designing elaborate reward functions in complex environments. However, most IL methods work under the assumption of the optimality of the demonstrations and thus cannot learn policies to surpass the demonstrators. Some methods have been investigated to obtain better-than-demonstration (BD) performance with inner human feedback or preference labels. In this paper, we propose a method to learn rewards from suboptimal demonstrations via a weighted preference learning technique (LERP). Specifically, we first formulate the suboptimality of demonstrations as the inaccurate estimation of rewards. The inaccuracy is modeled with a reward noise random variable following the Gumbel distribution. Moreover, we derive an upper bound of the expected return with different noise coefficients and propose a theorem to surpass the demonstrations. Unlike existing literature, our analysis does not depend on the linear reward constraint. Consequently, we develop a BD model with a weighted preference learning technique. Experimental results on continuous control and high-dimensional discrete control tasks show the superiority of our LERP method over other state-of-the-art BD methods",
    "checked": true,
    "id": "6e9f395ccc11296e291b137d31cd882b726258e5",
    "semantic_title": "learning noise-induced reward functions for surpassing demonstrations in imitation learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25963": {
    "title": "XClusters: Explainability-First Clustering",
    "volume": "main",
    "abstract": "We study the problem of explainability-first clustering where explainability becomes a first-class citizen for clustering. Previous clustering approaches use decision trees for explanation, but only after the clustering is completed. In contrast, our approach is to perform clustering and decision tree training holistically where the decision tree's performance and size also influence the clustering results. We assume the attributes for clustering and explaining are distinct, although this is not necessary. We observe that our problem is a monotonic optimization where the objective function is a difference of monotonic functions. We then propose an efficient branch-and-bound algorithm for finding the best parameters that lead to a balance of clustering accuracy and decision tree explainability. Our experiments show that our method can improve the explainability of any clustering that fits in our framework",
    "checked": true,
    "id": "42ce38afc731d63dfd6029eab11be492683dbfc7",
    "semantic_title": "xclusters: explainability-first clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25964": {
    "title": "Model-Based Reinforcement Learning with Multinomial Logistic Function Approximation",
    "volume": "main",
    "abstract": "We study model-based reinforcement learning (RL) for episodic Markov decision processes (MDP) whose transition probability is parametrized by an unknown transition core with features of state and action. Despite much recent progress in analyzing algorithms in the linear MDP setting, the understanding of more general transition models is very restrictive. In this paper, we propose a provably efficient RL algorithm for the MDP whose state transition is given by a multinomial logistic model. We show that our proposed algorithm based on the upper confidence bounds achieves O(d√(H^3 T)) regret bound where d is the dimension of the transition core, H is the horizon, and T is the total number of steps. To the best of our knowledge, this is the first model-based RL algorithm with multinomial logistic function approximation with provable guarantees. We also comprehensively evaluate our proposed algorithm numerically and show that it consistently outperforms the existing methods, hence achieving both provable efficiency and practical superior performance",
    "checked": true,
    "id": "139e16319cc2179853c440e9540c41200e3ddd3c",
    "semantic_title": "model-based reinforcement learning with multinomial logistic function approximation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25965": {
    "title": "Fast Regularized Discrete Optimal Transport with Group-Sparse Regularizers",
    "volume": "main",
    "abstract": "Regularized discrete optimal transport (OT) is a powerful tool to measure the distance between two discrete distributions that have been constructed from data samples on two different domains. While it has a wide range of applications in machine learning, in some cases the sampled data from only one of the domains will have class labels such as unsupervised domain adaptation. In this kind of problem setting, a group-sparse regularizer is frequently leveraged as a regularization term to handle class labels. In particular, it can preserve the label structure on the data samples by corresponding the data samples with the same class label to one group-sparse regularization term. As a result, we can measure the distance while utilizing label information by solving the regularized optimization problem with gradient-based algorithms. However, the gradient computation is expensive when the number of classes or data samples is large because the number of regularization terms and their respective sizes also turn out to be large. This paper proposes fast discrete OT with group-sparse regularizers. Our method is based on two ideas. The first is to safely skip the computations of the gradients that must be zero. The second is to efficiently extract the gradients that are expected to be nonzero. Our method is guaranteed to return the same value of the objective function as that of the original approach. Experiments demonstrate that our method is up to 8.6 times faster than the original method without degrading accuracy",
    "checked": true,
    "id": "172fb6a4ac8d0f0f4a9e8617c7babc6a16887d1f",
    "semantic_title": "fast regularized discrete optimal transport with group-sparse regularizers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25966": {
    "title": "Neural Representations Reveal Distinct Modes of Class Fitting in Residual Convolutional Networks",
    "volume": "main",
    "abstract": "We leverage probabilistic models of neural representations to investigate how residual networks fit classes. To this end, we estimate class-conditional density models for representations learned by deep ResNets. We then use these models to characterize distributions of representations across learned classes. Surprisingly, we find that classes in the investigated models are not fitted in a uniform way. On the contrary: we uncover two groups of classes that are fitted with markedly different distributions of representations. These distinct modes of class-fitting are evident only in the deeper layers of the investigated models, indicating that they are not related to low-level image features. We show that the uncovered structure in neural representations correlate with memorization of training examples and adversarial robustness. Finally, we compare class-conditional distributions of neural representations between memorized and typical examples. This allows us to uncover where in the network structure class labels arise for memorized and standard inputs",
    "checked": true,
    "id": "6ea9c6d13a8797257ee59429697bf31a41d1b9d2",
    "semantic_title": "neural representations reveal distinct modes of class fitting in residual convolutional networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25967": {
    "title": "Audio-Visual Contrastive Learning with Temporal Self-Supervision",
    "volume": "main",
    "abstract": "We propose a self-supervised learning approach for videos that learns representations of both the RGB frames and the accompanying audio without human supervision. In contrast to images that capture the static scene appearance, videos also contain sound and temporal scene dynamics. To leverage the temporal and aural dimension inherent to videos, our method extends temporal self-supervision to the audio-visual setting and integrates it with multi-modal contrastive objectives. As temporal self-supervision, we pose playback speed and direction recognition in both modalities and propose intra- and inter-modal temporal ordering tasks. Furthermore, we design a novel contrastive objective in which the usual pairs are supplemented with additional sample-dependent positives and negatives sampled from the evolving feature space. In our model, we apply such losses among video clips and between videos and their temporally corresponding audio clips. We verify our model design in extensive ablation experiments and evaluate the video and audio representations in transfer experiments to action recognition and retrieval on UCF101 and HMBD51, audio classification on ESC50, and robust video fingerprinting on VGG-Sound, with state-of-the-art results",
    "checked": true,
    "id": "e475885c185b803ee025bc26c220c5736a5cfd5c",
    "semantic_title": "audio-visual contrastive learning with temporal self-supervision",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25968": {
    "title": "Confidence-Aware Training of Smoothed Classifiers for Certified Robustness",
    "volume": "main",
    "abstract": "Any classifier can be \"smoothed out\" under Gaussian noise to build a new classifier that is provably robust to l2-adversarial perturbations, viz., by averaging its predictions over the noise via randomized smoothing. Under the smoothed classifiers, the fundamental trade-off between accuracy and (adversarial) robustness has been well evidenced in the literature: i.e., increasing the robustness of a classifier for an input can be at the expense of decreased accuracy for some other inputs. In this paper, we propose a simple training method leveraging this trade-off to obtain robust smoothed classifiers, in particular, through a sample-wise control of robustness over the training samples. We make this control feasible by using \"accuracy under Gaussian noise\" as an easy-to-compute proxy of adversarial robustness for an input. Specifically, we differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. Our experiments show that the proposed method, despite its simplicity, consistently exhibits improved certified robustness upon state-of-the-art training methods. Somewhat surprisingly, we find these improvements persist even for other notions of robustness, e.g., to various types of common corruptions. Code is available at https://github.com/alinlab/smoothing-catrs",
    "checked": true,
    "id": "d95ec70652b3afcfe263bce6e3ba2ccefff2381d",
    "semantic_title": "confidence-aware training of smoothed classifiers for certified robustness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25969": {
    "title": "Learnable Path in Neural Controlled Differential Equations",
    "volume": "main",
    "abstract": "Neural controlled differential equations (NCDEs), which are continuous analogues to recurrent neural networks (RNNs), are a specialized model in (irregular) time-series processing. In comparison with similar models, e.g., neural ordinary differential equations (NODEs), the key distinctive characteristics of NCDEs are i) the adoption of the continuous path created by an interpolation algorithm from each raw discrete time-series sample and ii) the adoption of the Riemann--Stieltjes integral. It is the continuous path which makes NCDEs be analogues to continuous RNNs. However, NCDEs use existing interpolation algorithms to create the path, which is unclear whether they can create an optimal path. To this end, we present a method to generate another latent path (rather than relying on existing interpolation algorithms), which is identical to learning an appropriate interpolation method. We design an encoder-decoder module based on NCDEs and NODEs, and a special training method for it. Our method shows the best performance in both time-series classification and forecasting",
    "checked": true,
    "id": "f5bd94c41926c585157765b31943925cbe1b59ec",
    "semantic_title": "learnable path in neural controlled differential equations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25970": {
    "title": "DrugOOD: Out-of-Distribution Dataset Curator and Benchmark for AI-Aided Drug Discovery – a Focus on Affinity Prediction Problems with Noise Annotations",
    "volume": "main",
    "abstract": "AI-aided drug discovery (AIDD) is gaining popularity due to its potential to make the search for new pharmaceuticals faster, less expensive, and more effective. Despite its extensive use in numerous fields (e.g., ADMET prediction, virtual screening), little research has been conducted on the out-of-distribution (OOD) learning problem with noise. We present DrugOOD, a systematic OOD dataset curator and benchmark for AIDD. Particularly, we focus on the drug-target binding affinity prediction problem, which involves both macromolecule (protein target) and small-molecule (drug compound). DrugOOD offers an automated dataset curator with user-friendly customization scripts, rich domain annotations aligned with biochemistry knowledge, realistic noise level annotations, and rigorous benchmarking of SOTA OOD algorithms, as opposed to only providing fixed datasets. Since the molecular data is often modeled as irregular graphs using graph neural network (GNN) backbones, DrugOOD also serves as a valuable testbed for graph OOD learning problems. Extensive empirical studies have revealed a significant performance gap between in-distribution and out-of-distribution experiments, emphasizing the need for the development of more effective schemes that permit OOD generalization under noise for AIDD",
    "checked": false,
    "id": "dcb21a5a0bc8b6d1bcfff10659e192a95ea20773",
    "semantic_title": "drugood: out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery - a focus on affinity prediction problems with noise annotations",
    "citation_count": 33
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25971": {
    "title": "MNER-QG: An End-to-End MRC Framework for Multimodal Named Entity Recognition with Query Grounding",
    "volume": "main",
    "abstract": "Multimodal named entity recognition (MNER) is a critical step in information extraction, which aims to detect entity spans and classify them to corresponding entity types given a sentence-image pair. Existing methods either (1) obtain named entities with coarse-grained visual clues from attention mechanisms, or (2) first detect fine-grained visual regions with toolkits and then recognize named entities. However, they suffer from improper alignment between entity types and visual regions or error propagation in the two-stage manner, which finally imports irrelevant visual information into texts. In this paper, we propose a novel end-to-end framework named MNER-QG that can simultaneously perform MRC-based multimodal named entity recognition and query grounding. Specifically, with the assistance of queries, MNER-QG can provide prior knowledge of entity types and visual regions, and further enhance representations of both text and image. To conduct the query grounding task, we provide manual annotations and weak supervisions that are obtained via training a highly flexible visual grounding model with transfer learning. We conduct extensive experiments on two public MNER datasets, Twitter2015 and Twitter2017. Experimental results show that MNER-QG outperforms the current state-of-the-art models on the MNER task, and also improves the query grounding performance",
    "checked": true,
    "id": "27937956a426872feace4b7c89cf892dbd75451d",
    "semantic_title": "mner-qg: an end-to-end mrc framework for multimodal named entity recognition with query grounding",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25972": {
    "title": "Learning from Training Dynamics: Identifying Mislabeled Data beyond Manually Designed Features",
    "volume": "main",
    "abstract": "While mislabeled or ambiguously-labeled samples in the training set could negatively affect the performance of deep models, diagnosing the dataset and identifying mislabeled samples helps to improve the generalization power. Training dynamics, i.e., the traces left by iterations of optimization algorithms, have recently been proved to be effective to localize mislabeled samples with hand-crafted features. In this paper, beyond manually designed features, we introduce a novel learning-based solution, leveraging a noise detector, instanced by an LSTM network, which learns to predict whether a sample was mislabeled using the raw training dynamics as input. Specifically, the proposed method trains the noise detector in a supervised manner using the dataset with synthesized label noises and can adapt to various datasets (either naturally or synthesized label-noised) without retraining. We conduct extensive experiments to evaluate the proposed method. We train the noise detector based on the synthesized label-noised CIFAR dataset and test such noise detector on Tiny ImageNet, CUB-200, Caltech-256, WebVision and Clothing1M. Results show that the proposed method precisely detects mislabeled samples on various datasets without further adaptation, and outperforms state-of-the-art methods. Besides, more experiments demonstrate that the mislabel identification can guide a label correction, namely data debugging, providing orthogonal improvements of algorithm-centric state-of-the-art techniques from the data aspect",
    "checked": true,
    "id": "0858317aa3ebce7ebaae01be87f28925e49e51d8",
    "semantic_title": "learning from training dynamics: identifying mislabeled data beyond manually designed features",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25973": {
    "title": "Online Tuning for Offline Decentralized Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning could learn effective policies from a fixed dataset, which is promising for real-world applications. However, in offline decentralized multi-agent reinforcement learning, due to the discrepancy between the behavior policy and learned policy, the transition dynamics in offline experiences do not accord with the transition dynamics in online execution, which creates severe errors in value estimates, leading to uncoordinated low-performing policies. One way to overcome this problem is to bridge offline training and online tuning. However, considering both deployment efficiency and sample efficiency, we could only collect very limited online experiences, making it insufficient to use merely online data for updating the agent policy. To utilize both offline and online experiences to tune the policies of agents, we introduce online transition correction (OTC) to implicitly correct the offline transition dynamics by modifying sampling probabilities. We design two types of distances, i.e., embedding-based and value-based distance, to measure the similarity between transitions, and further propose an adaptive rank-based prioritization to sample transitions according to the transition similarity. OTC is simple yet effective to increase data efficiency and improve agent policies in online tuning. Empirically, OTC outperforms baselines in a variety of tasks",
    "checked": true,
    "id": "2ab82ae37b7d27e79f59e624fa9419682c7cf8b8",
    "semantic_title": "online tuning for offline decentralized multi-agent reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25974": {
    "title": "Robust Domain Adaptation for Machine Reading Comprehension",
    "volume": "main",
    "abstract": "Most domain adaptation methods for machine reading comprehension (MRC) use a pre-trained question-answer (QA) construction model to generate pseudo QA pairs for MRC transfer. Such a process will inevitably introduce mismatched pairs (i.e., Noisy Correspondence) due to i) the unavailable QA pairs in target documents, and ii) the domain shift during applying the QA construction model to the target domain. Undoubtedly, the noisy correspondence will degenerate the performance of MRC, which however is neglected by existing works. To solve such an untouched problem, we propose to construct QA pairs by additionally using the dialogue related to the documents, as well as a new domain adaptation method for MRC. Specifically, we propose Robust Domain Adaptation for Machine Reading Comprehension (RMRC) method which consists of an answer extractor (AE), a question selector (QS), and an MRC model. Specifically, RMRC filters out the irrelevant answers by estimating the correlation to the document via the AE, and extracts the questions by fusing the candidate questions in multiple rounds of dialogue chats via the QS. With the extracted QA pairs, MRC is fine-tuned and provides the feedback to optimize the QS through a novel reinforced self-training method. Thanks to the optimization of the QS, our method will greatly alleviate the noisy correspondence problem caused by the domain shift. To the best of our knowledge, this could be the first study to reveal the influence of noisy correspondence in domain adaptation MRC models and show a feasible solution to achieve the robustness against the mismatched pairs. Extensive experiments on three datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "d0a78f091fdfb5c77ab286400fac125ac8f46927",
    "semantic_title": "robust domain adaptation for machine reading comprehension",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25975": {
    "title": "Multi-View MOOC Quality Evaluation via Information-Aware Graph Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we study the problem of MOOC quality evaluation that is essential for improving the course materials, promoting students' learning efficiency, and benefiting user services. While achieving promising performances, current works still suffer from the complicated interactions and relationships of entities in MOOC platforms. To tackle the challenges, we formulate the problem as a course representation learning task based, and develop an Information-aware Graph Representation Learning(IaGRL) for multi-view MOOC quality evaluation. Specifically, We first build a MOOC Heterogeneous Network (HIN) to represent the interactions and relationships among entities in MOOC platforms. And then we decompose the MOOC HIN into multiple single-relation graphs based on meta-paths to depict multi-view semantics of courses. The course representation learning can be further converted to a multi-view graph representation task. Different from traditional graph representation learning, the learned course representations are expected to match the following three types of validity: (1) the agreement on expressiveness between the raw course portfolio and the learned course representations; (2) the consistency between the representations in each view and the unified representations; (3) the alignment between the course and MOOC platform representations. Therefore, we propose to exploit mutual information for preserving the validity of course representations. We conduct extensive experiments over real-world MOOC datasets to demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "1997376d6922b84ca0f372cbb0b9a48596ea497d",
    "semantic_title": "multi-view mooc quality evaluation via information-aware graph representation learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25976": {
    "title": "Spatio-Temporal Meta-Graph Learning for Traffic Forecasting",
    "volume": "main",
    "abstract": "Traffic forecasting as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the spatio-temporal heterogeneity and non-stationarity implied in the traffic stream, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (i.e., METR-LA and PEMS-BAY) and a new large-scale traffic speed dataset called EXPY-TKY that covers 1843 expressway road links in Tokyo. Our model outperformed the state-of-the-arts on all three datasets. Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle the road links and time slots with different patterns and be robustly adaptive to any anomalous traffic situations. Codes and datasets are available at https://github.com/deepkashiwa20/MegaCRN",
    "checked": true,
    "id": "12dd3ea6cd7041aef237f63c03a3e90fdc16897b",
    "semantic_title": "spatio-temporal meta-graph learning for traffic forecasting",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25977": {
    "title": "Complement Sparsification: Low-Overhead Model Pruning for Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse models with the weights pruned from the global model to capture the local trends. For improved model performance, these two types of complementary sparse models are aggregated into a dense model in each round, which is subsequently pruned in an iterative process. CS requires little computation overhead on the top of vanilla FL for both the server and the clients. We demonstrate that CS is an approximation of vanilla FL and, thus, its models perform well. We evaluate CS experimentally with two popular FL benchmark datasets. CS achieves substantial reduction in bidirectional communication, while achieving performance comparable with vanilla FL. In addition, CS outperforms baseline pruning mechanisms for FL",
    "checked": true,
    "id": "4a8da6b76b6d4396e407973d236c7b7d282d7259",
    "semantic_title": "complement sparsification: low-overhead model pruning for federated learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25978": {
    "title": "Energy-Motivated Equivariant Pretraining for 3D Molecular Graphs",
    "volume": "main",
    "abstract": "Pretraining molecular representation models without labels is fundamental to various applications. Conventional methods mainly process 2D molecular graphs and focus solely on 2D tasks, making their pretrained models incapable of characterizing 3D geometry and thus defective for downstream 3D tasks. In this work, we tackle 3D molecular pretraining in a complete and novel sense. In particular, we first propose to adopt an equivariant energy-based model as the backbone for pretraining, which enjoys the merits of fulfilling the symmetry of 3D space. Then we develop a node-level pretraining loss for force prediction, where we further exploit the Riemann-Gaussian distribution to ensure the loss to be E(3)-invariant, enabling more robustness. Moreover, a graph-level noise scale prediction task is also leveraged to further promote the eventual performance. We evaluate our model pretrained from a large-scale 3D dataset GEOM-QM9 on two challenging 3D benchmarks: MD17 and QM9. Experimental results demonstrate the efficacy of our method against current state-of-the-art pretraining approaches, and verify the validity of our design for each proposed component. Code is available at https://github.com/jiaor17/3D-EMGP",
    "checked": true,
    "id": "70de410aece38bf0d5d12fbf03815c80676be742",
    "semantic_title": "energy-motivated equivariant pretraining for 3d molecular graphs",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25979": {
    "title": "Local-Global Defense against Unsupervised Adversarial Attacks on Graphs",
    "volume": "main",
    "abstract": "Unsupervised pre-training algorithms for graph representation learning are vulnerable to adversarial attacks, such as first-order perturbations on graphs, which will have an impact on particular downstream applications. Designing an effective representation learning strategy against white-box attacks remains a crucial open topic. Prior research attempts to improve representation robustness by maximizing mutual information between the representation and the perturbed graph, which is sub-optimal because it does not adapt its defense techniques to the severity of the attack. To address this issue, we propose an unsupervised defense method that combines local and global defense to improve the robustness of representation. Note that we put forward the Perturbed Edges Harmfulness (PEH) metric to determine the riskiness of the attack. Thus, when the edges are attacked, the model can automatically identify the risk of attack. We present a method of attention-based protection against high-risk attacks that penalizes attention coefficients of perturbed edges to encoders. Extensive experiments demonstrate that our strategies can enhance the robustness of representation against various adversarial attacks on three benchmark graphs",
    "checked": true,
    "id": "2235f0df7efa6571007c33c3a5f3ea4286be1b9a",
    "semantic_title": "local-global defense against unsupervised adversarial attacks on graphs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25980": {
    "title": "Trafformer: Unify Time and Space in Traffic Prediction",
    "volume": "main",
    "abstract": "Traffic prediction is an important component of the intelligent transportation system. Existing deep learning methods encode temporal information and spatial information separately or iteratively. However, the spatial and temporal information is highly correlated in a traffic network, so existing methods may not learn the complex spatial-temporal dependencies hidden in the traffic network due to the decomposed model design. To overcome this limitation, we propose a new model named Trafformer, which unifies spatial and temporal information in one transformer-style model. Trafformer enables every node at every timestamp interact with every other node in every other timestamp in just one step in the spatial-temporal correlation matrix. This design enables Trafformer to catch complex spatial-temporal dependencies. Following the same design principle, we use the generative style decoder to predict multiple timestamps in only one forward operation instead of the iterative style decoder in Transformer. Furthermore, to reduce the complexity brought about by the huge spatial-temporal self-attention matrix, we also propose two variants of Trafformer to further improve the training and inference speed without losing much effectivity. Extensive experiments on two traffic datasets demonstrate that Trafformer outperforms existing methods and provides a promising future direction for the spatial-temporal traffic prediction problem",
    "checked": true,
    "id": "f98ce3517924c03ef82528fb05958934cb97d8d8",
    "semantic_title": "trafformer: unify time and space in traffic prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25981": {
    "title": "On Solution Functions of Optimization: Universal Approximation and Covering Number Bounds",
    "volume": "main",
    "abstract": "We study the expressibility and learnability of solution functions of convex optimization and their multi-layer architectural extension. The main results are: (1) the class of solution functions of linear programming (LP) and quadratic programming (QP) is a universal approximant for the smooth model class or some restricted Sobolev space, and we characterize the rate-distortion, (2) the approximation power is investigated through a viewpoint of regression error, where information about the target function is provided in terms of data observations, (3) compositionality in the form of deep architecture with optimization as a layer is shown to reconstruct some basic functions used in numerical analysis without error, which implies that (4) a substantial reduction in rate-distortion can be achieved with a universal network architecture, and (5) we discuss the statistical bounds of empirical covering numbers for LP/QP, as well as a generic optimization problem (possibly nonconvex) by exploiting tame geometry. Our results provide the **first rigorous analysis of the approximation and learning-theoretic properties of solution functions** with implications for algorithmic design and performance guarantees",
    "checked": true,
    "id": "f1959e2743e7ff857957252dd3af836e0790253a",
    "semantic_title": "on solution functions of optimization: universal approximation and covering number bounds",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25982": {
    "title": "Pointerformer: Deep Reinforced Multi-Pointer Transformer for the Traveling Salesman Problem",
    "volume": "main",
    "abstract": "Traveling Salesman Problem (TSP), as a classic routing optimization problem originally arising in the domain of transportation and logistics, has become a critical task in broader domains, such as manufacturing and biology. Recently, Deep Reinforcement Learning (DRL) has been increasingly employed to solve TSP due to its high inference efficiency. Nevertheless, most of existing end-to-end DRL algorithms only perform well on small TSP instances and can hardly generalize to large scale because of the drastically soaring memory consumption and computation time along with the enlarging problem scale. In this paper, we propose a novel end-to-end DRL approach, referred to as Pointerformer, based on multi-pointer Transformer. Particularly, Pointerformer adopts both reversible residual network in the encoder and multi-pointer network in the decoder to effectively contain memory consumption of the encoder-decoder architecture. To further improve the performance of TSP solutions, Pointerformer employs a feature augmentation method to explore the symmetries of TSP at both training and inference stages as well as an enhanced context embedding approach to include more comprehensive context information in the query. Extensive experiments on a randomly generated benchmark and a public benchmark have shown that, while achieving comparative results on most small-scale TSP instances as state-of-the-art DRL approaches do, Pointerformer can also well generalize to large-scale TSPs",
    "checked": true,
    "id": "647369ee45ecead812991f860caa8a96c6617063",
    "semantic_title": "pointerformer: deep reinforced multi-pointer transformer for the traveling salesman problem",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25983": {
    "title": "Knowledge-Constrained Answer Generation for Open-Ended Video Question Answering",
    "volume": "main",
    "abstract": "Open-ended Video question answering (open-ended VideoQA) aims to understand video content and question semantics to generate the correct answers. Most of the best performing models define the problem as a discriminative task of multi-label classification. In real-world scenarios, however, it is difficult to define a candidate set that includes all possible answers. In this paper, we propose a Knowledge-constrained Generative VideoQA Algorithm (KcGA) with an encoder-decoder pipeline, which enables out-of-domain answer generation through an adaptive external knowledge module and a multi-stream information control mechanism. We use ClipBERT to extract the video-question features, extract framewise object-level external knowledge from a commonsense knowledge base and compute the contextual-aware episode memory units via an attention based GRU to form the external knowledge features, and exploit multi-stream information control mechanism to fuse video-question and external knowledge features such that the semantic complementation and alignment are well achieved. We evaluate our model on two open-ended benchmark datasets to demonstrate that we can effectively and robustly generate high-quality answers without restrictions of training data",
    "checked": true,
    "id": "e0b19fd70c0a45355767473b671e0c2f4774d460",
    "semantic_title": "knowledge-constrained answer generation for open-ended video question answering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25984": {
    "title": "POEM: Polarization of Embeddings for Domain-Invariant Representations",
    "volume": "main",
    "abstract": "Handling out-of-distribution samples is a long-lasting challenge for deep visual models. In particular, domain generalization (DG) is one of the most relevant tasks that aims to train a model with a generalization capability on novel domains. Most existing DG approaches share the same philosophy to minimize the discrepancy between domains by finding the domain-invariant representations. On the contrary, our proposed method called POEM acquires a strong DG capability by learning domain-invariant and domain-specific representations and polarizing them. Specifically, POEM co-trains category-classifying and domain-classifying embeddings while regularizing them to be orthogonal via minimizing the cosine-similarity between their features, i.e., the polarization of embeddings. The clear separation of embeddings suppresses domain-specific features in the domain-invariant embeddings. The concept of POEM shows a unique direction to enhance the domain robustness of representations that brings considerable and consistent performance gains when combined with existing DG methods. Extensive simulation results in popular DG benchmarks with the PACS, VLCS, OfficeHome, TerraInc, and DomainNet datasets show that POEM indeed facilitates the category-classifying embedding to be more domain-invariant",
    "checked": true,
    "id": "36747690b3dc56fc4479cddc1a9992c2c75fbb58",
    "semantic_title": "poem: polarization of embeddings for domain-invariant representations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25985": {
    "title": "An Efficient Algorithm for Fair Multi-Agent Multi-Armed Bandit with Low Regret",
    "volume": "main",
    "abstract": "Recently a multi-agent variant of the classical multi-armed bandit was proposed to tackle fairness issues in online learning. Inspired by a long line of work in social choice and economics, the goal is to optimize the Nash social welfare instead of the total utility. Unfortunately previous algorithms either are not efficient or achieve sub-optimal regret in terms of the number of rounds. We propose a new efficient algorithm with lower regret than even previous inefficient ones. We also complement our efficient algorithm with an inefficient approach with regret that matches the lower bound for one agent. The experimental findings confirm the effectiveness of our efficient algorithm compared to the previous approaches",
    "checked": true,
    "id": "7f36e9584a69c7cc38a0267f40c2a8e4645074b5",
    "semantic_title": "an efficient algorithm for fair multi-agent multi-armed bandit with low regret",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25986": {
    "title": "Towards More Robust Interpretation via Local Gradient Alignment",
    "volume": "main",
    "abstract": "Neural network interpretation methods, particularly feature attribution methods, are known to be fragile with respect to adversarial input perturbations. To address this, several methods for enhancing the local smoothness of the gradient while training have been proposed for attaining robust feature attributions. However, the lack of considering the normalization of the attributions, which is essential in their visualizations, has been an obstacle to understanding and improving the robustness of feature attribution methods. In this paper, we provide new insights by taking such normalization into account. First, we show that for every non-negative homogeneous neural network, a naive l2-robust criterion for gradients is not normalization invariant, which means that two functions with the same normalized gradient can have different values. Second, we formulate a normalization invariant cosine distance-based criterion and derive its upper bound, which gives insight for why simply minimizing the Hessian norm at the input, as has been done in previous work, is not sufficient for attaining robust feature attribution. Finally, we propose to combine both l2 and cosine distance-based criteria as regularization terms to leverage the advantages of both in aligning the local gradient. As a result, we experimentally show that models trained with our method produce much more robust interpretations on CIFAR-10 and ImageNet-100 without significantly hurting the accuracy, compared to the recent baselines. To the best of our knowledge, this is the first work to verify the robustness of interpretation on a larger-scale dataset beyond CIFAR-10, thanks to the computational efficiency of our method",
    "checked": true,
    "id": "a062076a5ff4af5f8e80905d51441779d2891cd7",
    "semantic_title": "towards more robust interpretation via local gradient alignment",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25987": {
    "title": "Identifying Selection Bias from Observational Data",
    "volume": "main",
    "abstract": "Access to a representative sample from the population is an assumption that underpins all of machine learning. Selection effects can cause observations to instead come from a subpopulation, by which our inferences may be subject to bias. It is therefore important to know whether or not a sample is affected by selection effects. We study under which conditions we can identify selection bias and give results for both parametric and non-parametric families of distributions. Based on these results we develop two practical methods to determine whether or not an observed sample comes from a distribution subject to selection bias. Through extensive evaluation on synthetic and real world data we verify that our methods beat the state of the art both in detecting as well as characterizing selection bias",
    "checked": true,
    "id": "ca2820b5b7a877619744d3862401ad6e28478731",
    "semantic_title": "identifying selection bias from observational data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25988": {
    "title": "PIXEL: Physics-Informed Cell Representations for Fast and Accurate PDE Solvers",
    "volume": "main",
    "abstract": "With the increases in computational power and advances in machine learning, data-driven learning-based methods have gained significant attention in solving PDEs. Physics-informed neural networks (PINNs) have recently emerged and succeeded in various forward and inverse PDE problems thanks to their excellent properties, such as flexibility, mesh-free solutions, and unsupervised training. However, their slower convergence speed and relatively inaccurate solutions often limit their broader applicability in many science and engineering domains. This paper proposes a new kind of data-driven PDEs solver, physics-informed cell representations (PIXEL), elegantly combining classical numerical methods and learning-based approaches. We adopt a grid structure from the numerical methods to improve accuracy and convergence speed and overcome the spectral bias presented in PINNs. Moreover, the proposed method enjoys the same benefits in PINNs, e.g., using the same optimization frameworks to solve both forward and inverse PDE problems and readily enforcing PDE constraints with modern automatic differentiation techniques. We provide experimental results on various challenging PDEs that the original PINNs have struggled with and show that PIXEL achieves fast convergence speed and high accuracy. Project page: https://namgyukang.github.io/PIXEL/",
    "checked": true,
    "id": "162ce23964f2aef2378297da1b20e08a8d77d000",
    "semantic_title": "pixel: physics-informed cell representations for fast and accurate pde solvers",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25989": {
    "title": "On the Sample Complexity of Vanilla Model-Based Offline Reinforcement Learning with Dependent Samples",
    "volume": "main",
    "abstract": "Offline reinforcement learning (offline RL) considers problems where learning is performed using only previously collected samples and is helpful for the settings in which collecting new data is costly or risky. In model-based offline RL, the learner performs estimation (or optimization) using a model constructed according to the empirical transition frequencies. We analyze the sample complexity of vanilla model-based offline RL with dependent samples in the infinite-horizon discounted-reward setting. In our setting, the samples obey the dynamics of the Markov decision process and, consequently, may have interdependencies. Under no assumption of independent samples, we provide a high-probability, polynomial sample complexity bound for vanilla model-based off-policy evaluation that requires partial or uniform coverage. We extend this result to the off-policy optimization under uniform coverage. As a comparison to the model-based approach, we analyze the sample complexity of off-policy evaluation with vanilla importance sampling in the infinite-horizon setting. Finally, we provide an estimator that outperforms the sample-mean estimator for almost deterministic dynamics that are prevalent in reinforcement learning",
    "checked": true,
    "id": "d96495c78401e645ee85e38ee042e6e3fc76bad3",
    "semantic_title": "on the sample complexity of vanilla model-based offline reinforcement learning with dependent samples",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25990": {
    "title": "Communication-Efficient Collaborative Best Arm Identification",
    "volume": "main",
    "abstract": "We investigate top-m arm identification, a basic problem in bandit theory, in a multi-agent learning model in which agents collaborate to learn an objective function. We are interested in designing collaborative learning algorithms that achieve maximum speedup (compared to single-agent learning algorithms) using minimum communication cost, as communication is frequently the bottleneck in multi-agent learning. We give both algorithmic and impossibility results, and conduct a set of experiments to demonstrate the effectiveness of our algorithms",
    "checked": true,
    "id": "dc178a8b1926588ffc179c5bd53edc95b5dcc0bf",
    "semantic_title": "communication-efficient collaborative best arm identification",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25991": {
    "title": "Variable-Based Calibration for Machine Learning Classifiers",
    "volume": "main",
    "abstract": "The deployment of machine learning classifiers in high-stakes domains requires well-calibrated confidence scores for model predictions. In this paper we introduce the notion of variable-based calibration to characterize calibration properties of a model with respect to a variable of interest, generalizing traditional score-based metrics such as expected calibration error (ECE). In particular, we find that models with near-perfect ECE can exhibit significant miscalibration as a function of features of the data. We demonstrate this phenomenon both theoretically and in practice on multiple well-known datasets, and show that it can persist after the application of existing calibration methods. To mitigate this issue, we propose strategies for detection, visualization, and quantification of variable-based calibration error. We then examine the limitations of current score-based calibration methods and explore potential modifications. Finally, we discuss the implications of these findings, emphasizing that an understanding of calibration beyond simple aggregate measures is crucial for endeavors such as fairness and model interpretability",
    "checked": true,
    "id": "45847e00891099b2fcdc7fb6d4d29852c5b30258",
    "semantic_title": "variable-based calibration for machine learning classifiers",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25992": {
    "title": "Design Amortization for Bayesian Optimal Experimental Design",
    "volume": "main",
    "abstract": "Bayesian optimal experimental design is a sub-field of statistics focused on developing methods to make efficient use of experimental resources. Any potential design is evaluated in terms of a utility function, such as the (theoretically well-justified) expected information gain (EIG); unfortunately however, under most circumstances the EIG is intractable to evaluate. In this work we build off of successful variational approaches, which optimize a parameterized variational model with respect to bounds on the EIG. Past work focused on learning a new variational model from scratch for each new design considered. Here we present a novel neural architecture that allows experimenters to optimize a single variational model that can estimate the EIG for potentially infinitely many designs. To further improve computational efficiency, we also propose to train the variational model on a significantly cheaper-to-evaluate lower bound, and show empirically that the resulting model provides an excellent guide for more accurate, but expensive to evaluate bounds on the EIG. We demonstrate the effectiveness of our technique on generalized linear models, a class of statistical models that is widely used in the analysis of controlled experiments. Experiments show that our method is able to greatly improve accuracy over existing approximation strategies, and achieve these results with far better sample efficiency",
    "checked": true,
    "id": "660195b1bc56862f1a781044527080fbe486032d",
    "semantic_title": "design amortization for bayesian optimal experimental design",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25993": {
    "title": "On Error and Compression Rates for Prototype Rules",
    "volume": "main",
    "abstract": "We study the close interplay between error and compression in the non-parametric multiclass classification setting in terms of prototype learning rules. We focus in particular on a recently proposed compression-based learning rule termed OptiNet. Beyond its computational merits, this rule has been recently shown to be universally consistent in any metric instance space that admits a universally consistent rule---the first learning algorithm known to enjoy this property. However, its error and compression rates have been left open. Here we derive such rates in the case where instances reside in Euclidean space under commonly posed smoothness and tail conditions on the data distribution. We first show that OptiNet achieves non-trivial compression rates while enjoying near minimax-optimal error rates. We then proceed to study a novel general compression scheme for further compressing prototype rules that locally adapts to the noise level without sacrificing accuracy. Applying it to OptiNet, we show that under a geometric margin condition further gain in the compression rate is achieved. Experimental results comparing the performance of the various methods are presented",
    "checked": true,
    "id": "17422b058269f3b66a32b267a69b50e226796d84",
    "semantic_title": "on error and compression rates for prototype rules",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25994": {
    "title": "CertiFair: A Framework for Certified Global Fairness of Neural Networks",
    "volume": "main",
    "abstract": "We consider the problem of whether a Neural Network (NN) model satisfies global individual fairness. Individual Fairness (defined in (Dwork et al. 2012)) suggests that similar individuals with respect to a certain task are to be treated similarly by the decision model. In this work, we have two main objectives. The first is to construct a verifier which checks whether the fairness property holds for a given NN in a classification task or provides a counterexample if it is violated, i.e., the model is fair if all similar individuals are classified the same, and unfair if a pair of similar individuals are classified differently. To that end, we construct a sound and complete verifier that verifies global individual fairness properties of ReLU NN classifiers using distance-based similarity metrics. The second objective of this paper is to provide a method for training provably fair NN classifiers from unfair (biased) data. We propose a fairness loss that can be used during training to enforce fair outcomes for similar individuals. We then provide provable bounds on the fairness of the resulting NN. We run experiments on commonly used fairness datasets that are publicly available and we show that global individual fairness can be improved by 96 % without a significant drop in test accuracy",
    "checked": true,
    "id": "3a287e857f9a002c2d626228ecea6d375035ffd8",
    "semantic_title": "certifair: a framework for certified global fairness of neural networks",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25995": {
    "title": "Key Feature Replacement of In-Distribution Samples for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection can be used in deep learning-based applications to reject outlier samples from being unreliably classified by deep neural networks. Learning to classify between OOD and in-distribution samples is difficult because data comprising the former is extremely diverse. It has been observed that an auxiliary OOD dataset is most effective in training a ``rejection'' network when its samples are semantically similar to in-distribution images. We first deduce that OOD images are perceived by a deep neural network to be semantically similar to in-distribution samples when they share a common background, as deep networks are observed to incorrectly classify such images with high confidence. We then propose a simple yet effective Key In-distribution feature Replacement BY inpainting (KIRBY) procedure that constructs a surrogate OOD dataset by replacing class-discriminative features of in-distribution samples with marginal background features. The procedure can be implemented using off-the-shelf vision algorithms, where each step within the algorithm is shown to make the surrogate data increasingly similar to in-distribution data. Design choices in each step are studied extensively, and an exhaustive comparison with state-of-the-art algorithms demonstrates KIRBY's competitiveness on various benchmarks",
    "checked": true,
    "id": "ee507a5776a2b3c548f712f601ec5e515bd4f8d7",
    "semantic_title": "key feature replacement of in-distribution samples for out-of-distribution detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25996": {
    "title": "FLAME: Free-Form Language-Based Motion Synthesis & Editing",
    "volume": "main",
    "abstract": "Text-based motion generation models are drawing a surge of interest for their potential for automating the motion-making process in the game, animation, or robot industries. In this paper, we propose a diffusion-based motion synthesis and editing model named FLAME. Inspired by the recent successes in diffusion models, we integrate diffusion-based generative models into the motion domain. FLAME can generate high-fidelity motions well aligned with the given text. Also, it can edit the parts of the motion, both frame-wise and joint-wise, without any fine-tuning. FLAME involves a new transformer-based architecture we devise to better handle motion data, which is found to be crucial to manage variable-length motions and well attend to free-form text. In experiments, we show that FLAME achieves state-of-the-art generation performances on three text-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that FLAME's editing capability can be extended to other tasks such as motion prediction or motion in-betweening, which have been previously covered by dedicated models",
    "checked": true,
    "id": "c17a983b11381fabb53f28066f76d4b2dc5a6a17",
    "semantic_title": "flame: free-form language-based motion synthesis & editing",
    "citation_count": 36
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25997": {
    "title": "Inverse-Reference Priors for Fisher Regularization of Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Recent studies have shown that the generalization ability of deep neural networks (DNNs) is closely related to the Fisher information matrix (FIM) calculated during the early training phase. Several methods have been proposed to regularize the FIM for increased generalization of DNNs. However, they cannot be used directly for Bayesian neural networks (BNNs) because the variable parameters of BNNs make it difficult to calculate the FIM. To address this problem, we achieve regularization of the FIM of BNNs by specifying a new suitable prior distribution called the inverse-reference (IR) prior. To regularize the FIM, the IR prior is derived as the inverse of the reference prior that imposes minimal prior knowledge on the parameters and maximizes the trace of the FIM. We demonstrate that the IR prior can enhance the generalization ability of BNNs for large-scale data over previously used priors while providing adequate uncertainty quantifications using various benchmark image datasets and BNN structures",
    "checked": true,
    "id": "b2caa2a9444341fc539283e86c5e68e7658e6d2b",
    "semantic_title": "inverse-reference priors for fisher regularization of bayesian neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25998": {
    "title": "Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video",
    "volume": "main",
    "abstract": "Forced alignment refers to a technology that time-aligns a given transcription with a corresponding speech. However, as the forced alignment technologies have developed using speech audio, they might fail in alignment when the input speech audio is noise-corrupted or is not accessible. We focus on that there is another component that the speech can be inferred from, the speech video (i.e., talking face video). Since the drawbacks of audio-based forced alignment can be complemented using the visual information when the audio signal is under poor condition, we try to develop a novel video-based forced alignment method. However, different from audio forced alignment, it is challenging to develop a reliable visual forced alignment technology for the following two reasons: 1) Visual Speech Recognition (VSR) has a much lower performance compared to audio-based Automatic Speech Recognition (ASR), and 2) the translation from text to video is not reliable, so the method typically used for building audio forced alignment cannot be utilized in developing visual forced alignment. In order to alleviate these challenges, in this paper, we propose a new method that is appropriate for visual forced alignment, namely Deep Visual Forced Alignment (DVFA). The proposed DVFA can align the input transcription (i.e., sentence) with the talking face video without accessing the speech audio. Moreover, by augmenting the alignment task with anomaly case detection, DVFA can detect mismatches between the input transcription and the input video while performing the alignment. Therefore, we can robustly align the text with the talking face video even if there exist error words in the text. Through extensive experiments, we show the effectiveness of the proposed DVFA not only in the alignment task but also in interpreting the outputs of VSR models",
    "checked": true,
    "id": "38b933b515db9cdb834aaecd7f5a1c24e7be6af6",
    "semantic_title": "deep visual forced alignment: learning to align transcription with talking face video",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/25999": {
    "title": "Better Generalized Few-Shot Learning Even without Base Data",
    "volume": "main",
    "abstract": "This paper introduces and studies zero-base generalized few-shot learning (zero-base GFSL), which is an extreme yet practical version of few-shot learning problem. Motivated by the cases where base data is not available due to privacy or ethical issues, the goal of zero-base GFSL is to newly incorporate the knowledge of few samples of novel classes into a pretrained model without any samples of base classes. According to our analysis, we discover the fact that both mean and variance of the weight distribution of novel classes are not properly established, compared to those of base classes. The existing GFSL methods attempt to make the weight norms balanced, which we find help only the variance part, but discard the importance of mean of weights particularly for novel classes, leading to the limited performance in the GFSL problem even with base data. In this paper, we overcome this limitation by proposing a simple yet effective normalization method that can effectively control both mean and variance of the weight distribution of novel classes without using any base samples and thereby achieve a satisfactory performance on both novel and base classes. Our experimental results somewhat surprisingly show that the proposed zero-base GFSL method that does not utilize any base samples even outperforms the existing GFSL methods that make the best use of base data. Our implementation is available at: https://github.com/bigdata-inha/Zero-Base-GFSL",
    "checked": true,
    "id": "9255f7866199fadec7de87323ce27ca4c5603b6c",
    "semantic_title": "better generalized few-shot learning even without base data",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26000": {
    "title": "Learning Topology-Specific Experts for Molecular Property Prediction",
    "volume": "main",
    "abstract": "Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision: topological semantics induced by GNNs and molecular scaffolds, respectively. Extensive experiments demonstrate that TopExpert has boosted the performance for molecular property prediction and also achieved better generalization for new molecules with unseen scaffolds than baselines. The code is available at https://github.com/kimsu55/ToxExpert",
    "checked": true,
    "id": "1604c2854370b9c6c9558b360c566f058ef33e7f",
    "semantic_title": "learning topology-specific experts for molecular property prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26001": {
    "title": "Double Doubly Robust Thompson Sampling for Generalized Linear Contextual Bandits",
    "volume": "main",
    "abstract": "We propose a novel algorithm for generalized linear contextual bandits (GLBs) with a regret bound sublinear to the time horizon, the minimum eigenvalue of the covariance of contexts and a lower bound of the variance of rewards. In several identified cases, our result is the first regret bound for generalized linear bandits (GLBs) achieving the regret bound sublinear to the dimension of contexts without discarding the observed rewards. Previous approaches achieve the regret bound sublinear to the dimension of contexts by discarding the observed rewards, whereas our algorithm achieves the bound incorporating contexts from all arms in our double doubly robust (DDR) estimator. The DDR estimator is a subclass of doubly robust estimator but with a tighter error bound. We also provide a logarithmic cumulative regret bound under a probabilistic margin condition. This is the first regret bound under the margin condition for linear models or GLMs when contexts are different for all arms but coefficients are common. We conduct empirical studies using synthetic data and real examples, demonstrating the effectiveness of our algorithm",
    "checked": true,
    "id": "e97740f9ed9cfc7d27c95f3bd7aceb5691aff4be",
    "semantic_title": "double doubly robust thompson sampling for generalized linear contextual bandits",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26002": {
    "title": "Exploring Temporal Information Dynamics in Spiking Neural Networks",
    "volume": "main",
    "abstract": "Most existing Spiking Neural Network (SNN) works state that SNNs may utilize temporal information dynamics of spikes. However, an explicit analysis of temporal information dynamics is still missing. In this paper, we ask several important questions for providing a fundamental understanding of SNNs: What are temporal information dynamics inside SNNs? How can we measure the temporal information dynamics? How do the temporal information dynamics affect the overall learning performance? To answer these questions, we estimate the Fisher Information of the weights to measure the distribution of temporal information during training in an empirical manner. Surprisingly, as training goes on, Fisher information starts to concentrate in the early timesteps. After training, we observe that information becomes highly concentrated in earlier few timesteps, a phenomenon we refer to as temporal information concentration. We observe that the temporal information concentration phenomenon is a common learning feature of SNNs by conducting extensive experiments on various configurations such as architecture, dataset, optimization strategy, time constant, and timesteps. Furthermore, to reveal how temporal information concentration affects the performance of SNNs, we design a loss function to change the trend of temporal information. We find that temporal information concentration is crucial to building a robust SNN but has little effect on classification accuracy. Finally, we propose an efficient iterative pruning method based on our observation on temporal information concentration. Code is available at https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks",
    "checked": true,
    "id": "d7a33032b9cc1401c60ca856fa39c03b838836c1",
    "semantic_title": "exploring temporal information dynamics in spiking neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26003": {
    "title": "FastAMI – a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics",
    "volume": "main",
    "abstract": "Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach",
    "checked": false,
    "id": "54b2c1035b4da593e62f6658a6e064a56c743c16",
    "semantic_title": "fastami - a monte carlo approach to the adjustment for chance in clustering comparison metrics",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26004": {
    "title": "A Gift from Label Smoothing: Robust Training with Adaptive Label Smoothing via Auxiliary Classifier under Label Noise",
    "volume": "main",
    "abstract": "As deep neural networks can easily overfit noisy labels, robust training in the presence of noisy labels is becoming an important challenge in modern deep learning. While existing methods address this problem in various directions, they still produce unpredictable sub-optimal results since they rely on the posterior information estimated by the feature extractor corrupted by noisy labels. Lipschitz regularization successfully alleviates this problem by training a robust feature extractor, but it requires longer training time and expensive computations. Motivated by this, we propose a simple yet effective method, called ALASCA, which efficiently provides a robust feature extractor under label noise. ALASCA integrates two key ingredients: (1) adaptive label smoothing based on our theoretical analysis that label smoothing implicitly induces Lipschitz regularization, and (2) auxiliary classifiers that enable practical application of intermediate Lipschitz regularization with negligible computations. We conduct wide-ranging experiments for ALASCA and combine our proposed method with previous noise-robust methods on several synthetic and real-world datasets. Experimental results show that our framework consistently improves the robustness of feature extractors and the performance of existing baselines with efficiency",
    "checked": true,
    "id": "18048fcf06e2f131d93dbc1a6c3708f0f188a344",
    "semantic_title": "a gift from label smoothing: robust training with adaptive label smoothing via auxiliary classifier under label noise",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26005": {
    "title": "Grouping Matrix Based Graph Pooling with Adaptive Number of Clusters",
    "volume": "main",
    "abstract": "Graph pooling is a crucial operation for encoding hierarchical structures within graphs. Most existing graph pooling approaches formulate the problem as a node clustering task which effectively captures the graph topology. Conventional methods ask users to specify an appropriate number of clusters as a hyperparameter, then assuming that all input graphs share the same number of clusters. In inductive settings where the number of clusters could vary, however, the model should be able to represent this variation in its pooling layers in order to learn suitable clusters. Thus we propose GMPool, a novel differentiable graph pooling architecture that automatically determines the appropriate number of clusters based on the input data. The main intuition involves a grouping matrix defined as a quadratic form of the pooling operator, which induces use of binary classification probabilities of pairwise combinations of nodes. GMPool obtains the pooling operator by first computing the grouping matrix, then decomposing it. Extensive evaluations on molecular property prediction tasks demonstrate that our method outperforms conventional methods",
    "checked": false,
    "id": "c7bbe399aad2da39ba8f988fec83129d60aa5d52",
    "semantic_title": "grouping-matrix based graph pooling with adaptive number of clusters",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26006": {
    "title": "The Influence of Dimensions on the Complexity of Computing Decision Trees",
    "volume": "main",
    "abstract": "A decision tree recursively splits a feature space \\mathbb{R}^d and then assigns class labels based on the resulting partition. Decision trees have been part of the basic machine-learning toolkit for decades. A large body of work considers heuristic algorithms that compute a decision tree from training data, usually aiming to minimize in particular the size of the resulting tree. In contrast, little is known about the complexity of the underlying computational problem of computing a minimum-size tree for the given training data. We study this problem with respect to the number d of dimensions of the feature space \\mathbb{R}^d, which contains n training examples. We show that it can be solved in O(n^(2d + 1)) time, but under reasonable complexity-theoretic assumptions it is not possible to achieve f(d) * n^o(d / log d) running time. The problem is solvable in (dR)^O(dR) * n^(1+o(1)) time, if there are exactly two classes and R is an upper bound on the number of tree leaves labeled with the first class",
    "checked": true,
    "id": "65d6bee49cf1d2ebd40014dd53e96c7ab9842ee3",
    "semantic_title": "the influence of dimensions on the complexity of computing decision trees",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26007": {
    "title": "Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs",
    "volume": "main",
    "abstract": "Simulations that produce three-dimensional data are ubiquitous in science, ranging from fluid flows to plasma physics. We propose a similarity model based on entropy, which allows for the creation of physically meaningful ground truth distances for the similarity assessment of scalar and vectorial data, produced from transport and motion-based simulations. Utilizing two data acquisition methods derived from this model, we create collections of fields from numerical PDE solvers and existing simulation data repositories. Furthermore, a multiscale CNN architecture that computes a volumetric similarity metric (VolSiM) is proposed. To the best of our knowledge this is the first learning method inherently designed to address the challenges arising for the similarity assessment of high-dimensional simulation data. Additionally, the tradeoff between a large batch size and an accurate correlation computation for correlation-based loss functions is investigated, and the metric's invariance with respect to rotation and scale operations is analyzed. Finally, the robustness and generalization of VolSiM is evaluated on a large range of test data, as well as a particularly challenging turbulence case study, that is close to potential real-world applications",
    "checked": true,
    "id": "a6eb9ecef16437bc76efbe0b016cea3bbdbf8f87",
    "semantic_title": "learning similarity metrics for volumetric simulations with multiscale cnns",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26008": {
    "title": "Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) have recently obtained success in many applications, but their intensive computation and heavy memory usage at both training and inference time limit their generalization. Previous compression algorithms usually start from the pre-trained dense models and only focus on efficient inference, while time-consuming training is still unavoidable. In contrast, this paper points out that the million-scale training data is redundant, which is the fundamental reason for the tedious training. To address the issue, this paper aims to introduce sparsity into data and proposes an end-to-end efficient training framework from three sparse perspectives, dubbed Tri-Level E-ViT. Specifically, we leverage a hierarchical data redundancy reduction scheme, by exploring the sparsity under three levels: number of training examples in the dataset, number of patches (tokens) in each example, and number of connections between tokens that lie in attention weights. With extensive experiments, we demonstrate that our proposed technique can noticeably accelerate training for various ViT architectures while maintaining accuracy. Remarkably, under certain ratios, we are able to improve the ViT accuracy rather than compromising it. For example, we can achieve 15.2% speedup with 72.6% (+0.4) Top-1 accuracy on Deit-T, and 15.7% speedup with 79.9% (+0.1) Top-1 accuracy on Deit-S. This proves the existence of data redundancy in ViT. Our code is released at https://github.com/ZLKong/Tri-Level-ViT",
    "checked": true,
    "id": "a92d2c468835571f3302ce9299fdf37b36c2e367",
    "semantic_title": "peeling the onion: hierarchical reduction of data redundancy for efficient vision transformer training",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26009": {
    "title": "Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness",
    "volume": "main",
    "abstract": "Learning from raw high dimensional data via interaction with a given environment has been effectively achieved through the utilization of deep neural networks. Yet the observed degradation in policy performance caused by imperceptible worst-case policy dependent translations along high sensitivity directions (i.e. adversarial perturbations) raises concerns on the robustness of deep reinforcement learning policies. In our paper, we show that these high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box setting. Furthermore, we show that vanilla training techniques intriguingly result in learning more robust policies compared to the policies learnt via the state-of-the-art adversarial training techniques. We believe our work lays out intriguing properties of the deep reinforcement learning policy manifold and our results can help to build robust and generalizable deep reinforcement learning policies",
    "checked": true,
    "id": "6143e809cd2e9c7a18c3bc8819419fd3b02fbcf2",
    "semantic_title": "adversarial robust deep reinforcement learning requires redefining robustness",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26010": {
    "title": "Almost Cost-Free Communication in Federated Best Arm Identification",
    "volume": "main",
    "abstract": "We study the problem of best arm identification in a federated learning multi-armed bandit setup with a central server and multiple clients. Each client is associated with a multi-armed bandit in which each arm yields i.i.d. rewards following a Gaussian distribution with an unknown mean and known variance. The set of arms is assumed to be the same at all the clients. We define two notions of best arm local and global. The local best arm at a client is the arm with the largest mean among the arms local to the client, whereas the global best arm is the arm with the largest average mean across all the clients. We assume that each client can only observe the rewards from its local arms and thereby estimate its local best arm. The clients communicate with a central server on uplinks that entail a cost of C>=0 units per usage per uplink. The global best arm is estimated at the server. The goal is to identify the local best arms and the global best arm with minimal total cost, defined as the sum of the total number of arm selections at all the clients and the total communication cost, subject to an upper bound on the error probability. We propose a novel algorithm FedElim that is based on successive elimination and communicates only in exponential time steps and obtain a high probability instance-dependent upper bound on its total cost. The key takeaway from our paper is that for any C>=0 and error probabilities sufficiently small, the total number of arm selections (resp. the total cost) under FedElim is at most 2 (resp. 3) times the maximum total number of arm selections under its variant that communicates in every time step. Additionally, we show that the latter is optimal in expectation up to a constant factor, thereby demonstrating that communication is almost cost-free in FedElim. We numerically validate the efficacy of FedElim on two synthetic datasets and the MovieLens dataset",
    "checked": true,
    "id": "52382817395bb0c6b7c07e1d41dcb2ce7e30ea1a",
    "semantic_title": "almost cost-free communication in federated best arm identification",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26011": {
    "title": "UEQMS: UMAP Embedded Quick Mean Shift Algorithm for High Dimensional Clustering",
    "volume": "main",
    "abstract": "The mean shift algorithm is a simple yet very effective clustering method widely used for image and video segmentation as well as other exploratory data analysis applications. Recently, a new algorithm called MeanShift++ (MS++) for low-dimensional clustering was proposed with a speedup of 4000 times over the vanilla mean shift. In this work, starting with a first-of-its-kind theoretical analysis of MS++, we extend its reach to high-dimensional data clustering by integrating the Uniform Manifold Approximation and Projection (UMAP) based dimensionality reduction in the same framework. Analytically, we show that MS++ can indeed converge to a non-critical point. Subsequently, we suggest modifications to MS++ to improve its convergence characteristics. In addition, we propose a way to further speed up MS++ by avoiding the execution of the MS++ iterations for every data point. By incorporating UMAP with modified MS++, we design a faster algorithm, named UMAP embedded quick mean shift (UEQMS), for partitioning data with a relatively large number of recorded features. Through extensive experiments, we showcase the efficacy of UEQMS over other state-of-the-art algorithms in terms of accuracy and runtime",
    "checked": true,
    "id": "507217edca49186856896dcf69d6bfbb4eef6d3d",
    "semantic_title": "ueqms: umap embedded quick mean shift algorithm for high dimensional clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26012": {
    "title": "The Effect of Diversity in Meta-Learning",
    "volume": "main",
    "abstract": "Recent studies show that task distribution plays a vital role in the meta-learner's performance. Conventional wisdom is that task diversity should improve the performance of meta-learning. In this work, we find evidence to the contrary; (i) our experiments draw into question the efficacy of our learned models: similar manifolds can be learned with a subset of the data (lower task diversity). This finding questions the advantage of providing more data to the model, and (ii) adding diversity to the task distribution (higher task diversity) sometimes hinders the model and does not lead to a significant improvement in performance as previously believed. To strengthen our findings, we provide both empirical and theoretical evidence",
    "checked": true,
    "id": "277e80c67e9dcc8572d73ff0cec5c22530df5f8a",
    "semantic_title": "the effect of diversity in meta-learning",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26013": {
    "title": "Gradient Estimation for Binary Latent Variables via Gradient Variance Clipping",
    "volume": "main",
    "abstract": "Gradient estimation is often necessary for fitting generative models with discrete latent variables, in contexts such as reinforcement learning and variational autoencoder (VAE) training. The DisARM estimator achieves state of the art gradient variance for Bernoulli latent variable models in many contexts. However, DisARM and other estimators have potentially exploding variance near the boundary of the parameter space, where solutions tend to lie. To ameliorate this issue, we propose a new gradient estimator bitflip-1 that is lower variance at the boundaries of the parameter space. As bitflip-1 has complementary properties to existing estimators, we introduce an aggregated estimator, unbiased gradient variance clipping (UGC) that uses either a bitflip-1 or a DisARM gradient update for each coordinate. We theoretically prove that UGC has uniformly lower variance than DisARM. Empirically, we observe that UGC achieves the optimal value of the optimization objectives in toy experiments, discrete VAE training, and in a best subset selection problem",
    "checked": true,
    "id": "cc95439462f741ac55ecf8f728aa7c67a41ac6fe",
    "semantic_title": "gradient estimation for binary latent variables via gradient variance clipping",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26014": {
    "title": "LoNe Sampler: Graph Node Embeddings by Coordinated Local Neighborhood Sampling",
    "volume": "main",
    "abstract": "Local graph neighborhood sampling is a fundamental computational problem that is at the heart of algorithms for node representation learning. Several works have presented algorithms for learning discrete node embeddings where graph nodes are represented by discrete features such as attributes of neighborhood nodes. Discrete embeddings offer several advantages compared to continuous word2vec-like node embeddings: ease of computation, scalability, and interpretability. We present LoNe Sampler, a suite of algorithms for generating discrete node embeddings by Local Neighborhood Sampling, and address two shortcomings of previous work. First, our algorithms have rigorously understood theoretical properties. Second, we show how to generate approximate explicit vector maps that avoid the expensive computation of a Gram matrix for the training of a kernel model. Experiments on benchmark datasets confirm the theoretical findings and demonstrate the advantages of the proposed methods",
    "checked": true,
    "id": "56f676e55a306303863551beda729a6570aa3e30",
    "semantic_title": "lone sampler: graph node embeddings by coordinated local neighborhood sampling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26015": {
    "title": "WLD-Reg: A Data-Dependent Within-Layer Diversity Regularizer",
    "volume": "main",
    "abstract": "Neural networks are composed of multiple layers arranged in a hierarchical structure jointly trained with a gradient-based optimization, where the errors are back-propagated from the last layer back to the first one. At each optimization step, neurons at a given layer receive feedback from neurons belonging to higher layers of the hierarchy. In this paper, we propose to complement this traditional 'between-layer' feedback with additional 'within-layer' feedback to encourage the diversity of the activations within the same layer. To this end, we measure the pairwise similarity between the outputs of the neurons and use it to model the layer's overall diversity. We present an extensive empirical study confirming that the proposed approach enhances the performance of several state-of-the-art neural network models in multiple tasks. The code is publically available at https://github.com/firasl/AAAI-23-WLD-Reg",
    "checked": true,
    "id": "86d12d692995538259706c383841349433712c72",
    "semantic_title": "wld-reg: a data-dependent within-layer diversity regularizer",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26016": {
    "title": "SpatialFormer: Semantic and Target Aware Attentions for Few-Shot Learning",
    "volume": "main",
    "abstract": "Recent Few-Shot Learning (FSL) methods put emphasis on generating a discriminative embedding features to precisely measure the similarity between support and query sets. Current CNN-based cross-attention approaches generate discriminative representations via enhancing the mutually semantic similar regions of support and query pairs. However, it suffers from two problems: CNN structure produces inaccurate attention map based on local features, and mutually similar backgrounds cause distraction. To alleviate these problems, we design a novel SpatialFormer structure to generate more accurate attention regions based on global features. Different from the traditional Transformer modeling intrinsic instance-level similarity which causes accuracy degradation in FSL, our SpatialFormer explores the semantic-level similarity between pair inputs to boost the performance. Then we derive two specific attention modules, named SpatialFormer Semantic Attention (SFSA) and SpatialFormer Target Attention (SFTA), to enhance the target object regions while reduce the background distraction. Particularly, SFSA highlights the regions with same semantic information between pair features, and SFTA finds potential foreground object regions of novel feature that are similar to base categories. Extensive experiments show that our methods are effective and achieve new state-of-the-art results on few-shot classification benchmarks",
    "checked": true,
    "id": "74277b50e285dfaa0adbed61627c10f7ea168997",
    "semantic_title": "spatialformer: semantic and target aware attentions for few-shot learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26017": {
    "title": "A Data Source for Reasoning Embodied Agents",
    "volume": "main",
    "abstract": "Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reasoning models and database representations. Code to generate the data and train the models will be released at github.com/facebookresearch/neuralmemory",
    "checked": true,
    "id": "5c828011a508611df4d58cced9cc48d049dc4eb9",
    "semantic_title": "a data source for reasoning embodied agents",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26018": {
    "title": "Generalization Bounds for Inductive Matrix Completion in Low-Noise Settings",
    "volume": "main",
    "abstract": "We study inductive matrix completion (matrix completion with side information) under an i.i.d. subgaussian noise assumption at a low noise regime, with uniform sampling of the entries. We obtain for the first time generalization bounds with the following three properties: (1) they scale like the standard deviation of the noise and in particular approach zero in the exact recovery case; (2) even in the presence of noise, they converge to zero when the sample size approaches infinity; and (3) for a fixed dimension of the side information, they only have a logarithmic dependence on the size of the matrix. Differently from many works in approximate recovery, we present results both for bounded Lipschitz losses and for the absolute loss, with the latter relying on Talagrand-type inequalities. The proofs create a bridge between two approaches to the theoretical analysis of matrix completion, since they consist in a combination of techniques from both the exact recovery literature and the approximate recovery literature",
    "checked": true,
    "id": "e42259cf0ba53f6c072cadf52d471e943e021387",
    "semantic_title": "generalization bounds for inductive matrix completion in low-noise settings",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26019": {
    "title": "I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs",
    "volume": "main",
    "abstract": "Although machine learning on hypergraphs has attracted considerable attention, most of the works have focused on (semi-)supervised learning, which may cause heavy labeling costs and poor generalization. Recently, contrastive learning has emerged as a successful unsupervised representation learning method. Despite the prosperous development of contrastive learning in other domains, contrastive learning on hypergraphs remains little explored. In this paper, we propose TriCL (Tri-directional Contrastive Learning), a general framework for contrastive learning on hypergraphs. Its main idea is tri-directional contrast, and specifically, it aims to maximize in two augmented views the agreement (a) between the same node, (b) between the same group of nodes, and (c) between each group and its members. Together with simple but surprisingly effective data augmentation and negative sampling schemes, these three forms of contrast enable TriCL to capture both node- and group-level structural information in node embeddings. Our extensive experiments using 14 baseline approaches, 10 datasets, and two tasks demonstrate the effectiveness of TriCL, and most noticeably, TriCL almost consistently outperforms not just unsupervised competitors but also (semi-)supervised competitors mostly by significant margins for node classification. The code and datasets are available at https://github.com/wooner49/TriCL",
    "checked": true,
    "id": "84c790e54149b74a0ebfaabedb40a90d56d28950",
    "semantic_title": "i'm me, we're us, and i'm us: tri-directional contrastive learning on hypergraphs",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26020": {
    "title": "Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment",
    "volume": "main",
    "abstract": "As deep learning models become popular, there is a lot of need for deploying them to diverse device environments. Because it is costly to develop and optimize a neural network for every single environment, there is a line of research to search neural networks for multiple target environments efficiently. However, existing works for such a situation still suffer from requiring many GPUs and expensive costs. Motivated by this, we propose a novel neural network optimization framework named Bespoke for low-cost deployment. Our framework searches for a lightweight model by replacing parts of an original model with randomly selected alternatives, each of which comes from a pretrained neural network or the original model. In the practical sense, Bespoke has two significant merits. One is that it requires near zero cost for designing the search space of neural networks. The other merit is that it exploits the sub-networks of public pretrained neural networks, so the total cost is minimal compared to the existing works. We conduct experiments exploring Bespoke's the merits, and the results show that it finds efficient models for multiple targets with meager cost",
    "checked": true,
    "id": "eb36afb336dff204652ef5dc39c9ce32353e48cb",
    "semantic_title": "bespoke: a block-level neural network optimization framework for low-cost deployment",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26021": {
    "title": "Time-Aware Random Walk Diffusion to Improve Dynamic Graph Learning",
    "volume": "main",
    "abstract": "How can we augment a dynamic graph for improving the performance of dynamic graph neural networks? Graph augmentation has been widely utilized to boost the learning performance of GNN-based models. However, most existing approaches only enhance spatial structure within an input static graph by transforming the graph, and do not consider dynamics caused by time such as temporal locality, i.e., recent edges are more influential than earlier ones, which remains challenging for dynamic graph augmentation. In this work, we propose TiaRa (Time-aware Random Walk Diffusion), a novel diffusion-based method for augmenting a dynamic graph represented as a discrete-time sequence of graph snapshots. For this purpose, we first design a time-aware random walk proximity so that a surfer can walk along the time dimension as well as edges, resulting in spatially and temporally localized scores. We then derive our diffusion matrices based on the time-aware random walk, and show they become enhanced adjacency matrices that both spatial and temporal localities are augmented. Throughout extensive experiments, we demonstrate that TiaRa effectively augments a given dynamic graph, and leads to significant improvements in dynamic GNN models for various graph datasets and tasks",
    "checked": true,
    "id": "98293695c9f356ca1a5d667ff41d0287aef9a868",
    "semantic_title": "time-aware random walk diffusion to improve dynamic graph learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26022": {
    "title": "Demystifying Randomly Initialized Networks for Evaluating Generative Models",
    "volume": "main",
    "abstract": "Evaluation of generative models is mostly based on the comparison between the estimated distribution and the ground truth distribution in a certain feature space. To embed samples into informative features, previous works often use convolutional neural networks optimized for classification, which is criticized by recent studies. Therefore, various feature spaces have been explored to discover alternatives. Among them, a surprising approach is to use a randomly initialized neural network for feature embedding. However, the fundamental basis to employ the random features has not been sufficiently justified. In this paper, we rigorously investigate the feature space of models with random weights in comparison to that of trained models. Furthermore, we provide an empirical evidence to choose networks for random features to obtain consistent and reliable results. Our results indicate that the features from random networks can evaluate generative models well similarly to those from trained networks, and furthermore, the two types of features can be used together in a complementary way",
    "checked": true,
    "id": "67260931df4bda9bfd816da76dfbf0aafdaf47b0",
    "semantic_title": "demystifying randomly initialized networks for evaluating generative models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26023": {
    "title": "Layer-Wise Adaptive Model Aggregation for Scalable Federated Learning",
    "volume": "main",
    "abstract": "In Federated Learning (FL), a common approach for aggregating local solutions across clients is periodic full model averaging. It is, however, known that different layers of neural networks can have a different degree of model discrepancy across the clients. The conventional full aggregation scheme does not consider such a difference and synchronizes the whole model parameters at once, resulting in inefficient network bandwidth consumption. Aggregating the parameters that are similar across the clients does not make meaningful training progress while increasing the communication cost. We propose FedLAMA, a layer-wise adaptive model aggregation scheme for scalable FL. FedLAMA adjusts the aggregation interval in a layer-wise manner, jointly considering the model discrepancy and the communication cost. This fine-grained aggregation strategy enables to reduce the communication cost without significantly harming the model accuracy. Our extensive empirical study shows that, as the aggregation interval increases, FedLAMA shows a remarkably smaller accuracy drop than the periodic full aggregation, while achieving comparable communication efficiency",
    "checked": true,
    "id": "5f9a5d01f2ad2d044703dc50d63c233c5f91fe9a",
    "semantic_title": "layer-wise adaptive model aggregation for scalable federated learning",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26024": {
    "title": "Goal-Conditioned Q-learning as Knowledge Distillation",
    "volume": "main",
    "abstract": "Many applications of reinforcement learning can be formalized as goal-conditioned environments, where, in each episode, there is a \"goal\" that affects the rewards obtained during that episode but does not affect the dynamics. Various techniques have been proposed to improve performance in goal-conditioned environments, such as automatic curriculum generation and goal relabeling. In this work, we explore a connection between off-policy reinforcement learning in goal-conditioned settings and knowledge distillation. In particular: the current Q-value function and the target Q-value estimate are both functions of the goal, and we would like to train the Q-value function to match its target for all goals. We therefore apply Gradient-Based Attention Transfer (Zagoruyko and Komodakis 2017), a knowledge distillation technique, to the Q-function update. We empirically show that this can improve the performance of goal-conditioned off-policy reinforcement learning when the space of goals is high-dimensional. We also show that this technique can be adapted to allow for efficient learning in the case of multiple simultaneous sparse goals, where the agent can attain a reward by achieving any one of a large set of objectives, all specified at test time. Finally, to provide theoretical support, we give examples of classes of environments where (under some assumptions) standard off-policy algorithms such as DDPG require at least O(d^2) replay buffer transitions to learn an optimal policy, while our proposed technique requires only O(d) transitions, where d is the dimensionality of the goal and state space. Code and appendix are available at https://github.com/alevine0/ReenGAGE",
    "checked": true,
    "id": "713f4ac842a83650c63e1e43b748fcb5a83bd22d",
    "semantic_title": "goal-conditioned q-learning as knowledge distillation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26025": {
    "title": "Optimism in Face of a Context:Regret Guarantees for Stochastic Contextual MDP",
    "volume": "main",
    "abstract": "We present regret minimization algorithms for stochastic contextual MDPs under minimum reachability assumption, using an access to an offline least square regression oracle. We analyze three different settings: where the dynamics is known, where the dynamics is unknown but independent of the context and the most challenging setting where the dynamics is unknown and context-dependent. For the latter, our algorithm obtains regret bound (up to poly-logarithmic factors) of order (H+1/pₘᵢₙ)H|S|³ᐟ²(|A|Tlog(max{|?|,|?|} /?))¹ᐟ² with probability 1−?, where ? and ? are finite and realizable function classes used to approximate the dynamics and rewards respectively, pₘᵢₙ is the minimum reachability parameter, S is the set of states, A the set of actions, H the horizon, and T the number of episodes. To our knowledge, our approach is the first optimistic approach applied to contextual MDPs with general function approximation (i.e., without additional knowledge regarding the function class, such as it being linear and etc.). We present a lower bound of ?((TH|S||A|ln|?| /ln|A| )¹ᐟ² ), on the expected regret which holds even in the case of known dynamics. Lastly, we discuss an extension of our results to CMDPs without minimum reachability, that obtains order of T³ᐟ⁴ regret",
    "checked": false,
    "id": "3c9ee60cf7e32e08b6f177ac1d104c3ea74c3c76",
    "semantic_title": "optimism in face of a context: regret guarantees for stochastic contextual mdp",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26026": {
    "title": "Differentiable Meta Multigraph Search with Partial Message Propagation on Heterogeneous Information Networks",
    "volume": "main",
    "abstract": "Heterogeneous information networks (HINs) are widely employed for describing real-world data with intricate entities and relationships. To automatically utilize their semantic information, graph neural architecture search has recently been developed for various tasks of HINs. Existing works, on the other hand, show weaknesses in instability and inflexibility. To address these issues, we propose a novel method called Partial Message Meta Multigraph search (PMMM) to automatically optimize the neural architecture design on HINs. Specifically, to learn how graph neural networks (GNNs) propagate messages along various types of edges, PMMM adopts an efficient differentiable framework to search for a meaningful meta multigraph, which can capture more flexible and complex semantic relations than a meta graph. The differentiable search typically suffers from performance instability, so we further propose a stable algorithm called partial message search to ensure that the searched meta multigraph consistently surpasses the manually designed meta-structures, i.e., meta-paths. Extensive experiments on six benchmark datasets over two representative tasks, including node classification and recommendation, demonstrate the effectiveness of the proposed method. Our approach outperforms the state-of-the-art heterogeneous GNNs, finds out meaningful meta multigraphs, and is significantly more stable. Our code is available at https://github.com/JHL-HUST/PMMM",
    "checked": true,
    "id": "c42b9e01b1c908a059220f436db2db8dd9d07f11",
    "semantic_title": "differentiable meta multigraph search with partial message propagation on heterogeneous information networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26027": {
    "title": "Learning Adversarially Robust Sparse Networks via Weight Reparameterization",
    "volume": "main",
    "abstract": "Although increasing model size can enhance the adversarial robustness of deep neural networks, in resource-constrained environments, there exist critical sparsity constraints. While the recent robust pruning technologies show promising direction to obtain adversarially robust sparse networks, they perform poorly with high sparsity. In this work, we bridge this performance gap by reparameterizing network parameters to simultaneously learn the sparse structure and the robustness. Specifically, we introduce Twin-Rep, which reparameterizes original weights into the product of two factors during training and performs pruning on the reparameterized weights to satisfy the target sparsity constraint. Twin-Rep implicitly adds the sparsity constraint without changing the robust training objective, thus can enhance robustness under high sparsity. We also introduce another variant of weight reparameterization for better channel pruning. When inferring, we restore the original weight structure to obtain compact and robust networks. Extensive experiments on diverse datasets demonstrate that our method achieves state-of-the-art results, outperforming the current sparse robust training method and robustness-aware pruning method. Our code is available at https://github.com/UCAS-LCH/Twin-Rep",
    "checked": true,
    "id": "72bd3ee3bc2628128e369bb1babd1c776c4f26ed",
    "semantic_title": "learning adversarially robust sparse networks via weight reparameterization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26028": {
    "title": "ACE: Cooperative Multi-Agent Q-learning with Bidirectional Action-Dependency",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning (MARL) suffers from the non-stationarity problem, which is the ever-changing targets at every iteration when multiple agents update their policies at the same time. Starting from first principle, in this paper, we manage to solve the non-stationarity problem by proposing bidirectional action-dependent Q-learning (ACE). Central to the development of ACE is the sequential decision making process wherein only one agent is allowed to take action at one time. Within this process, each agent maximizes its value function given the actions taken by the preceding agents at the inference stage. In the learning phase, each agent minimizes the TD error that is dependent on how the subsequent agents have reacted to their chosen action. Given the design of bidirectional dependency, ACE effectively turns a multi-agent MDP into a single-agent MDP. We implement the ACE framework by identifying the proper network representation to formulate the action dependency, so that the sequential decision process is computed implicitly in one forward pass. To validate ACE, we compare it with strong baselines on two MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the state-of-the-art algorithms on Google Research Football and StarCraft Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE achieves 100% success rate on almost all the hard and super hard maps. We further study extensive research problems regarding ACE, including extension, generalization and practicability",
    "checked": true,
    "id": "48e556c2c67b7d05ea16ec0354bcc0372c927fbb",
    "semantic_title": "ace: cooperative multi-agent q-learning with bidirectional action-dependency",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26029": {
    "title": "When Online Learning Meets ODE: Learning without Forgetting on Variable Feature Space",
    "volume": "main",
    "abstract": "Machine learning systems that built upon varying feature space are ubiquitous across the world. When the set of practical or virtual features changes, the online learning approach can adjust the learned model accordingly rather than re-training from scratch and has been an attractive area of research. Despite its importance, most studies for algorithms that are capable of handling online features have no ensurance of stationarity point convergence, while the accuracy guaranteed methods are still limited to some simple cases such as L_1 or L_2 norms with square loss. To address this challenging problem, we develop an efficient Dynamic Feature Learning System (DFLS) to perform online learning on the unfixed feature set for more general statistical models and demonstrate how DFLS opens up many new applications. We are the first to achieve accurate & reliable feature-wise online learning for a broad class of models like logistic regression, spline interpolation, group Lasso and Poisson regression. By utilizing DFLS, the updated model is theoretically the same as the model trained from scratch using the entire new feature space. Specifically, we reparameterize the feature-varying procedure and devise the corresponding ordinary differential equation (ODE) system to compute the optimal solutions of the new model status. Simulation studies reveal that the proposed DFLS can substantially ease the computational cost without forgetting",
    "checked": true,
    "id": "42ee78b8ae50b905ba4d0b3127f609f100e7e689",
    "semantic_title": "when online learning meets ode: learning without forgetting on variable feature space",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26030": {
    "title": "FanoutNet: A Neuralized PCB Fanout Automation Method Using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "In modern electronic manufacturing processes, multi-layer Printed Circuit Board (PCB) routing requires connecting more than hundreds of nets with perplexing topology under complex routing constraints and highly limited resources, so that takes intense effort and time of human engineers. PCB fanout as a pre-design of PCB routing has been proved to be an ideal technique to reduce the complexity of PCB routing by pre-allocating resources and pre-routing. However, current PCB fanout design heavily relies on the experience of human engineers, and there is no existing solution for PCB fanout automation in industry, which limits the quality of PCB routing automation. To address the problem, we propose a neuralized PCB fanout method by deep reinforcement learning. To the best of our knowledge, we are the first in the literature to propose the automation method for PCB fanout. We combine with Convolution Neural Network (CNN) and attention-based network to train our fanout policy model and value model. The models learn representations of PCB layout and netlist to make decisions and evaluations in place of human engineers. We employ Proximal Policy Optimization (PPO) to update the parameters of the models. In addition, we apply our PCB fanout method to a PCB router to improve the quality of PCB routing. Extensive experimental results on real-world industrial PCB benchmarks demonstrate that our approach achieves 100% routability in all industrial cases and improves wire length by an average of 6.8%, which makes a significant improvement compared with the state-of-the-art methods",
    "checked": true,
    "id": "7cfb571902b6dca6badfcdbe843add81f9fbba77",
    "semantic_title": "fanoutnet: a neuralized pcb fanout automation method using deep reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26031": {
    "title": "Causal Recurrent Variational Autoencoder for Medical Time Series Generation",
    "volume": "main",
    "abstract": "We propose causal recurrent variational autoencoder (CR-VAE), a novel generative model that is able to learn a Granger causal graph from a multivariate time series x and incorporates the underlying causal mechanism into its data generation process. Distinct to the classical recurrent VAEs, our CR-VAE uses a multi-head decoder, in which the p-th head is responsible for generating the p-th dimension of x (i.e., x^p). By imposing a sparsity-inducing penalty on the weights (of the decoder) and encouraging specific sets of weights to be zero, our CR-VAE learns a sparse adjacency matrix that encodes causal relations between all pairs of variables. Thanks to this causal matrix, our decoder strictly obeys the underlying principles of Granger causality, thereby making the data generating process transparent. We develop a two-stage approach to train the overall objective. Empirically, we evaluate the behavior of our model in synthetic data and two real-world human brain datasets involving, respectively, the electroencephalography (EEG) signals and the functional magnetic resonance imaging (fMRI) data. Our model consistently outperforms state-of-the-art time series generative models both qualitatively and quantitatively. Moreover, it also discovers a faithful causal graph with similar or improved accuracy over existing Granger causality-based causal inference methods. Code of CR-VAE is publicly available at https://github.com/hongmingli1995/CR-VAE",
    "checked": true,
    "id": "22c670c365abefe17d1f99bba584f71d7762944f",
    "semantic_title": "causal recurrent variational autoencoder for medical time series generation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26032": {
    "title": "Dual Mutual Information Constraints for Discriminative Clustering",
    "volume": "main",
    "abstract": "Deep clustering is a fundamental task in machine learning and data mining that aims at learning clustering-oriented feature representations. In previous studies, most of deep clustering methods follow the idea of self-supervised representation learning by maximizing the consistency of all similar instance pairs while ignoring the effect of feature redundancy on clustering performance. In this paper, to address the above issue, we design a dual mutual information constrained clustering method named DMICC which is based on deep contrastive clustering architecture, in which the dual mutual information constraints are particularly employed with solid theoretical guarantees and experimental validations. Specifically, at the feature level, we reduce the redundancy among features by minimizing the mutual information across all the dimensionalities to encourage the neural network to extract more discriminative features. At the instance level, we maximize the mutual information of the similar instance pairs to obtain more unbiased and robust representations. The dual mutual information constraints happen simultaneously and thus complement each other to jointly optimize better features that are suitable for the clustering task. We also prove that our adopted mutual information constraints are superior in feature extraction, and the proposed dual mutual information constraints are clearly bounded and thus solvable. Extensive experiments on five benchmark datasets show that our proposed approach outperforms most other clustering algorithms. The code is available at https://github.com/Li-Hyn/DMICC",
    "checked": true,
    "id": "2fe34c646b8cefda49aba1b9eb36689126ea5902",
    "semantic_title": "dual mutual information constraints for discriminative clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26033": {
    "title": "AdaBoost.C2: Boosting Classifiers Chains for Multi-Label Classification",
    "volume": "main",
    "abstract": "During the last decades, multi-label classification (MLC) has attracted the attention of more and more researchers due to its wide real-world applications. Many boosting methods for MLC have been proposed and achieved great successes. However, these methods only extend existing boosting frameworks to MLC and take loss functions in multi-label version to guide the iteration. These loss functions generally give a comprehensive evaluation on the label set entirety, and thus the characteristics of different labels are ignored. In this paper, we propose a multi-path AdaBoost framework specific to MLC, where each boosting path is established for distinct label and the combination of them is able to provide a maximum optimization to Hamming Loss. In each iteration, classifiers chain is taken as the base classifier to strengthen the connection between multiple AdaBoost paths and exploit the label correlation. Extensive experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "ce63df31e8108d012be79f05d144314820fb5ed3",
    "semantic_title": "adaboost.c2: boosting classifiers chains for multi-label classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26034": {
    "title": "Scaling Up Dynamic Graph Representation Learning via Spiking Neural Networks",
    "volume": "main",
    "abstract": "Recent years have seen a surge in research on dynamic graph representation learning, which aims to model temporal graphs that are dynamic and evolving constantly over time. However, current work typically models graph dynamics with recurrent neural networks (RNNs), making them suffer seriously from computation and memory overheads on large temporal graphs. So far, scalability of dynamic graph representation learning on large temporal graphs remains one of the major challenges. In this paper, we present a scalable framework, namely SpikeNet, to efficiently capture the temporal and structural patterns of temporal graphs. We explore a new direction in that we can capture the evolving dynamics of temporal graphs with spiking neural networks (SNNs) instead of RNNs. As a low-power alternative to RNNs, SNNs explicitly model graph dynamics as spike trains of neuron populations and enable spike-based propagation in an efficient way. Experiments on three large real-world temporal graph datasets demonstrate that SpikeNet outperforms strong baselines on the temporal node classification task with lower computational costs. Particularly, SpikeNet generalizes to a large temporal graph (2.7M nodes and 13.9M edges) with significantly fewer parameters and computation overheads",
    "checked": true,
    "id": "5f68069bd99ae7413e6db72d2a2906f732a66916",
    "semantic_title": "scaling up dynamic graph representation learning via spiking neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26035": {
    "title": "Improved Kernel Alignment Regret Bound for Online Kernel Learning",
    "volume": "main",
    "abstract": "In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of O((A_TT ln T)^{1/4}) at a computational complexity (space and per-round time) of O((A_TT ln T)^{1/2}), where A_T is called kernel alignment. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of O((A_T)^{1/2}) at a computational complexity of O((ln T)^2). Otherwise, our algorithm enjoys a regret of O((A_TT)^{1/4}) at a computational complexity of O((A_TT)^{1/2}). We extend our algorithm to batch learning and obtain a O(T^{-1}(E[A_T])^{1/2}) excess risk bound which improves the previous O(T^{-1/2}) bound",
    "checked": true,
    "id": "123bf0bf0185f67e006cd4dc309efe81a7290c88",
    "semantic_title": "improved kernel alignment regret bound for online kernel learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26036": {
    "title": "VBLC: Visibility Boosting and Logit-Constraint Learning for Domain Adaptive Semantic Segmentation under Adverse Conditions",
    "volume": "main",
    "abstract": "Generalizing models trained on normal visual conditions to target domains under adverse conditions is demanding in the practical systems. One prevalent solution is to bridge the domain gap between clear- and adverse-condition images to make satisfactory prediction on the target. However, previous methods often reckon on additional reference images of the same scenes taken from normal conditions, which are quite tough to collect in reality. Furthermore, most of them mainly focus on individual adverse condition such as nighttime or foggy, weakening the model versatility when encountering other adverse weathers. To overcome the above limitations, we propose a novel framework, Visibility Boosting and Logit-Constraint learning (VBLC), tailored for superior normal-toadverse adaptation. VBLC explores the potential of getting rid of reference images and resolving the mixture of adverse conditions simultaneously. In detail, we first propose the visibility boost module to dynamically improve target images via certain priors in the image level. Then, we figure out the overconfident drawback in the conventional cross-entropy loss for self-training method and devise the logit-constraint learning, which enforces a constraint on logit outputs during training to mitigate this pain point. To the best of our knowledge, this is a new perspective for tackling such a challenging task. Extensive experiments on two normal-to-adverse domain adaptation benchmarks, i.e., Cityscapes to ACDC and Cityscapes to FoggyCityscapes + RainCityscapes, verify the effectiveness of VBLC, where it establishes the new state of the art. Code is available at https://github.com/BIT-DA/VBLC",
    "checked": true,
    "id": "1e515aa55d1609c19fac4b8f640811ec801bee40",
    "semantic_title": "vblc: visibility boosting and logit-constraint learning for domain adaptive semantic segmentation under adverse conditions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26037": {
    "title": "Understanding the Generalization Performance of Spectral Clustering Algorithms",
    "volume": "main",
    "abstract": "The theoretical analysis of spectral clustering is mainly devoted to consistency, while there is little research on its generalization performance. In this paper, we study the excess risk bounds of the popular spectral clustering algorithms: relaxed RatioCut and relaxed NCut. Our analysis follows the two practical steps of spectral clustering algorithms: continuous solution and discrete solution. Firstly, we provide the convergence rate of the excess risk bounds between the empirical continuous optimal solution and the population-level continuous optimal solution. Secondly, we show the fundamental quantity influencing the excess risk between the empirical discrete optimal solution and the population-level discrete optimal solution. At the empirical level, algorithms can be designed to reduce this quantity. Based on our theoretical analysis, we propose two novel algorithms that can penalize this quantity and, additionally, can cluster the out-of-sample data without re-eigendecomposition on the overall samples. Numerical experiments on toy and real datasets confirm the effectiveness of our proposed algorithms",
    "checked": true,
    "id": "d54913c6d5ec12d23352d1fbde5de848d65be1da",
    "semantic_title": "understanding the generalization performance of spectral clustering algorithms",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26038": {
    "title": "Restructuring Graph for Higher Homophily via Adaptive Spectral Clustering",
    "volume": "main",
    "abstract": "While a growing body of literature has been studying new Graph Neural Networks (GNNs) that work on both homophilic and heterophilic graphs, little has been done on adapting classical GNNs to less-homophilic graphs. Although the ability to handle less-homophilic graphs is restricted, classical GNNs still stand out in several nice properties such as efficiency, simplicity, and explainability. In this work, we propose a novel graph restructuring method that can be integrated into any type of GNNs, including classical GNNs, to leverage the benefits of existing GNNs while alleviating their limitations. Our contribution is threefold: a) learning the weight of pseudo-eigenvectors for an adaptive spectral clustering that aligns well with known node labels, b) proposing a new density-aware homophilic metric that is robust to label imbalance, and c) reconstructing the adjacency matrix based on the result of adaptive spectral clustering to maximize the homophilic scores. The experimental results show that our graph restructuring method can significantly boost the performance of six classical GNNs by an average of 25% on less-homophilic graphs. The boosted performance is comparable to state-of-the-art methods",
    "checked": true,
    "id": "a9b816d65b92a07791ea8c359f2a2e0cc5746de4",
    "semantic_title": "restructuring graph for higher homophily via adaptive spectral clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26039": {
    "title": "Nearest-Neighbor Sampling Based Conditional Independence Testing",
    "volume": "main",
    "abstract": "The conditional randomization test (CRT) was recently proposed to test whether two random variables X and Y are conditionally independent given random variables Z. The CRT assumes that the conditional distribution of X given Z is known under the null hypothesis and then it is compared to the distribution of the observed samples of the original data. The aim of this paper is to develop a novel alternative of CRT by using nearest-neighbor sampling without assuming the exact form of the distribution of X given Z. Specifically, we utilize the computationally efficient 1-nearest-neighbor to approximate the conditional distribution that encodes the null hypothesis. Then, theoretically, we show that the distribution of the generated samples is very close to the true conditional distribution in terms of total variation distance. Furthermore, we take the classifier-based conditional mutual information estimator as our test statistic. The test statistic as an empirical fundamental information theoretic quantity is able to well capture the conditional-dependence feature. We show that our proposed test is computationally very fast, while controlling type I and II errors quite well. Finally, we demonstrate the efficiency of our proposed test in both synthetic and real data analyses",
    "checked": true,
    "id": "c7e27ec1d6edcfcd87c3c9793320ec27b65fb72a",
    "semantic_title": "nearest-neighbor sampling based conditional independence testing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26040": {
    "title": "Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network",
    "volume": "main",
    "abstract": "Heterogeneous graph neural networks (HGNs) are prominent approaches to node classification tasks on heterogeneous graphs. Despite the superior performance, insights about the predictions made from HGNs are obscure to humans. Existing explainability techniques are mainly proposed for GNNs on homogeneous graphs. They focus on highlighting salient graph objects to the predictions whereas the problem of how these objects affect the predictions remains unsolved. Given heterogeneous graphs with complex structures and rich semantics, it is imperative that salient objects can be accompanied with their influence paths to the predictions, unveiling the reasoning process of HGNs. In this paper, we develop xPath, a new framework that provides fine-grained explanations for black-box HGNs specifying a cause node with its influence path to the target node. In xPath, we differentiate the influence of a node on the prediction w.r.t. every individual influence path, and measure the influence by perturbing graph structure via a novel graph rewiring algorithm. Furthermore, we introduce a greedy search algorithm to find the most influential fine-grained explanations efficiently. Empirical results on various HGNs and heterogeneous graphs show that xPath yields faithful explanations efficiently, outperforming the adaptations of advanced GNN explanation approaches",
    "checked": true,
    "id": "80d07f8cf49d0fe88bedde8a578a51fc81d0ed23",
    "semantic_title": "towards fine-grained explainability for heterogeneous graph neural network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26041": {
    "title": "Metric Nearness Made Practical",
    "volume": "main",
    "abstract": "Given a square matrix with noisy dissimilarity measures between pairs of data samples, the metric nearness model computes the best approximation of the matrix from a set of valid distance metrics. Despite its wide applications in machine learning and data processing tasks, the model faces non-trivial computational requirements in seeking the solution due to the large number of metric constraints associated with the feasible region. Our work designed a practical approach in two stages to tackle the challenge and improve the model's scalability and applicability. The first stage computes a fast yet high-quality approximate solution from a set of isometrically embeddable metrics, further improved by an effective heuristic. The second stage refines the approximate solution with the Halpern-Lions-Wittmann-Bauschke projection algorithm, which converges quickly to the optimal solution. In empirical evaluations, the proposed approach runs at least an order of magnitude faster than the state-of-the-art solutions, with significantly improved scalability, complete conformity to constraints, less memory consumption, and other desirable features in real applications",
    "checked": true,
    "id": "2f91785af642140181f9063fe4b86eae33251c0e",
    "semantic_title": "metric nearness made practical",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26042": {
    "title": "Predictive Exit: Prediction of Fine-Grained Early Exits for Computation- and Energy-Efficient Inference",
    "volume": "main",
    "abstract": "By adding exiting layers to the deep learning networks, early exit can terminate the inference earlier with accurate results. However, the passive decision-making of whether to exit or continue the next layer has to go through every pre-placed exiting layer until it exits. In addition, it is hard to adjust the configurations of the computing platforms alongside the inference proceeds. By incorporating a low-cost prediction engine, we propose a Predictive Exit framework for computation- and energy-efficient deep learning applications. Predictive Exit can forecast where the network will exit (i.e., establish the number of remaining layers to finish the inference), which effectively reduces the network computation cost by exiting on time without running every pre-placed exiting layer. Moreover, according to the number of remaining layers, proper computing configurations (i.e., frequency and voltage) are selected to execute the network to further save energy. Extensive experimental results demonstrate that Predictive Exit achieves up to 96.2% computation reduction and 72.9% energy-saving compared with classic deep learning networks; and 12.8% computation reduction and 37.6% energy-saving compared with the early exit under state-of-the-art exiting strategies, given the same inference accuracy and latency",
    "checked": true,
    "id": "949b0036c619bda8d3171716b20209b277211d0b",
    "semantic_title": "predictive exit: prediction of fine-grained early exits for computation- and energy-efficient inference",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26043": {
    "title": "Learning with Partial Labels from Semi-supervised Perspective",
    "volume": "main",
    "abstract": "Partial Label (PL) learning refers to the task of learning from the partially labeled data, where each training instance is ambiguously equipped with a set of candidate labels but only one is valid. Advances in the recent deep PL learning literature have shown that the deep learning paradigms, e.g., self-training, contrastive learning, or class activate values, can achieve promising performance. Inspired by the impressive success of deep Semi-Supervised (SS) learning, we transform the PL learning problem into the SS learning problem, and propose a novel PL learning method, namely Partial Label learning with Semi-supervised Perspective (PLSP). Specifically, we first form the pseudo-labeled dataset by selecting a small number of reliable pseudo-labeled instances with high-confidence prediction scores and treating the remaining instances as pseudo-unlabeled ones. Then we design a SS learning objective, consisting of a supervised loss for pseudo-labeled instances and a semantic consistency regularization for pseudo-unlabeled instances. We further introduce a complementary regularization for those non-candidate labels to constrain the model predictions on them to be as small as possible. Empirical results demonstrate that PLSP significantly outperforms the existing PL baseline methods, especially on high ambiguity levels. Code available: https://github.com/changchunli/PLSP",
    "checked": true,
    "id": "6f46720a181758da276eb71a38e99b338cec5924",
    "semantic_title": "learning with partial labels from semi-supervised perspective",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26044": {
    "title": "Learning Compact Features via In-Training Representation Alignment",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) for supervised learning can be viewed as a pipeline of the feature extractor (i.e., last hidden layer) and a linear classifier (i.e., output layer) that are trained jointly with stochastic gradient descent (SGD) on the loss function (e.g., cross-entropy). In each epoch, the true gradient of the loss function is estimated using a mini-batch sampled from the training set and model parameters are then updated with the mini-batch gradients. Although the latter provides an unbiased estimation of the former, they are subject to substantial variances derived from the size and number of sampled mini-batches, leading to noisy and jumpy updates. To stabilize such undesirable variance in estimating the true gradients, we propose In-Training Representation Alignment (ITRA) that explicitly aligns feature distributions of two different mini-batches with a matching loss in the SGD training process. We also provide a rigorous analysis of the desirable effects of the matching loss on feature representation learning: (1) extracting compact feature representation; (2) reducing over-adaption on mini-batches via an adaptively weighting mechanism; and (3) accommodating to multi-modalities. Finally, we conduct large-scale experiments on both image and text classifications to demonstrate its superior performance to the strong baselines",
    "checked": true,
    "id": "fef6bb50c6dce8d4b5fdcaab89400b9776bc00df",
    "semantic_title": "learning compact features via in-training representation alignment",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26045": {
    "title": "An Extreme-Adaptive Time Series Prediction Model Based on Probability-Enhanced LSTM Neural Networks",
    "volume": "main",
    "abstract": "Forecasting time series with extreme events has been a challenging and prevalent research topic, especially when the time series data are affected by complicated uncertain factors, such as is the case in hydrologic prediction. Diverse traditional and deep learning models have been applied to discover the nonlinear relationships and recognize the complex patterns in these types of data. However, existing methods usually ignore the negative influence of imbalanced data, or severe events, on model training. Moreover, methods are usually evaluated on a small number of generally well-behaved time series, which does not show their ability to generalize. To tackle these issues, we propose a novel probability-enhanced neural network model, called NEC+, which concurrently learns extreme and normal prediction functions and a way to choose among them via selective back propagation. We evaluate the proposed model on the difficult 3-day ahead hourly water level prediction task applied to 9 reservoirs in California. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art baselines and exhibits superior generalization ability on data with diverse distributions",
    "checked": true,
    "id": "1023544eb078faa4100c6ac10abc2aa010dd6255",
    "semantic_title": "an extreme-adaptive time series prediction model based on probability-enhanced lstm neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26046": {
    "title": "Implicit Stochastic Gradient Descent for Training Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "Physics-informed neural networks (PINNs) have effectively been demonstrated in solving forward and inverse differential equation problems, but they are still trapped in training failures when the target functions to be approximated exhibit high-frequency or multi-scale features. In this paper, we propose to employ implicit stochastic gradient descent (ISGD) method to train PINNs for improving the stability of training process. We heuristically analyze how ISGD overcome stiffness in the gradient flow dynamics of PINNs, especially for problems with multi-scale solutions. We theoretically prove that for two-layer fully connected neural networks with large hidden nodes, randomly initialized ISGD converges to a globally optimal solution for the quadratic loss function. Empirical results demonstrate that ISGD works well in practice and compares favorably to other gradient-based optimization methods such as SGD and Adam, while can also effectively address the numerical stiffness in training dynamics via gradient descent",
    "checked": true,
    "id": "491b39c1ee81411873de6f1a1f41842195de000d",
    "semantic_title": "implicit stochastic gradient descent for training physics-informed neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26047": {
    "title": "Provable Pathways: Learning Multiple Tasks over Multiple Paths",
    "volume": "main",
    "abstract": "Constructing useful representations across a large number of tasks is a key requirement for sample-efficient intelligent systems. A traditional idea in multitask learning (MTL) is building a shared representation across tasks which can then be adapted to new tasks by tuning last layers. A desirable refinement of using a shared one-fits-all representation is to construct task-specific representations. To this end, recent PathNet/muNet architectures represent individual tasks as pathways within a larger supernet. The subnetworks induced by pathways can be viewed as task-specific representations that are composition of modules within supernet's computation graph. This work explores the pathways proposal from the lens of statistical learning: We first develop novel generalization bounds for empirical risk minimization problems learning multiple tasks over multiple paths (Multipath MTL). In conjunction, we formalize the benefits of resulting multipath representation when adapting to new downstream tasks. Our bounds are expressed in terms of Gaussian complexity, lead to tangible guarantees for the class of linear representations, and provide novel insights into the quality and benefits of a multipath representation. When computation graph is a tree, Multipath MTL hierarchically clusters the tasks and builds cluster-specific representations. We provide further discussion and experiments for hierarchical MTL and rigorously identify the conditions under which Multipath MTL is provably superior to traditional MTL approaches with shallow supernets",
    "checked": true,
    "id": "9b1b0bb968d25b0573f424d7acf5e5b4e7b9fde9",
    "semantic_title": "provable pathways: learning multiple tasks over multiple paths",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26048": {
    "title": "Towards Inference Efficient Deep Ensemble Learning",
    "volume": "main",
    "abstract": "Ensemble methods can deliver surprising performance gains but also bring significantly higher computational costs, e.g., can be up to 2048X in large-scale ensemble tasks. However, we found that the majority of computations in ensemble methods are redundant. For instance, over 77% of samples in CIFAR-100 dataset can be correctly classified with only a single ResNet-18 model, which indicates that only around 23% of the samples need an ensemble of extra models. To this end, we propose an inference efficient ensemble learning method, to simultaneously optimize for effectiveness and efficiency in ensemble learning. More specifically, we regard ensemble of models as a sequential inference process and learn the optimal halting event for inference on a specific sample. At each timestep of the inference process, a common selector judges if the current ensemble has reached ensemble effectiveness and halt further inference, otherwise filters this challenging sample for the subsequent models to conduct more powerful ensemble. Both the base models and common selector are jointly optimized to dynamically adjust ensemble inference for different samples with various hardness, through the novel optimization goals including sequential ensemble boosting and computation saving. The experiments with different backbones on real-world datasets illustrate our method can bring up to 56% inference cost reduction while maintaining comparable performance to full ensemble, achieving significantly better ensemble utility than other baselines. Code and supplemental materials are available at https://seqml.github.io/irene",
    "checked": true,
    "id": "8e9931c561dbba7227baba58f7b487a5e2b5c676",
    "semantic_title": "towards inference efficient deep ensemble learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26049": {
    "title": "SplitNet: A Reinforcement Learning Based Sequence Splitting Method for the MinMax Multiple Travelling Salesman Problem",
    "volume": "main",
    "abstract": "MinMax Multiple Travelling Salesman Problem (mTSP) is an important class of combinatorial optimization problems with many practical applications, of which the goal is to minimize the longest tour of all vehicles. Due to its high computational complexity, existing methods for solving this problem cannot obtain a solution of satisfactory quality with fast speed, especially when the scale of the problem is large. In this paper, we propose a learning-based method named SplitNet to transform the single TSP solutions into the MinMax mTSP solutions of the same instances. Specifically, we generate single TSP solution sequences and split them into mTSP subsequences using an attention-based model trained by reinforcement learning. We also design a decision region for the splitting policy, which significantly reduces the policy action space on instances of various scales and thus improves the generalization ability of SplitNet. The experimental results show that SplitNet generalizes well and outperforms existing learning-based baselines and Google OR-Tools on widely-used random datasets of different scales and public datasets with fast solving speed",
    "checked": true,
    "id": "ac02c425ce7663042a0052845459f07413900bce",
    "semantic_title": "splitnet: a reinforcement learning based sequence splitting method for the minmax multiple travelling salesman problem",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26050": {
    "title": "Stepdown SLOPE for Controlled Feature Selection",
    "volume": "main",
    "abstract": "Sorted L-One Penalized Estimation (SLOPE) has shown the nice theoretical property as well as empirical behavior recently on the false discovery rate (FDR) control of high-dimensional feature selection by adaptively imposing the non-increasing sequence of tuning parameters on the sorted L1 penalties. This paper goes beyond the previous concern limited to the FDR control by considering the stepdown-based SLOPE in order to control the probability of k or more false rejections (k-FWER) and the false discovery proportion (FDP). Two new SLOPEs, called k-SLOPE and F-SLOPE, are proposed to realize k-FWER and FDP control respectively, where the stepdown procedure is injected into the SLOPE scheme. For the proposed stepdown SLOPEs, we establish their theoretical guarantees on controlling k-FWER and FDP under the orthogonal design setting, and also provide an intuitive guideline for the choice of regularization parameter sequence in much general setting. Empirical evaluations on simulated data validate the effectiveness of our approaches on controlled feature selection and support our theoretical findings",
    "checked": true,
    "id": "f13fa189356fdc601e52b8a579f27447590258be",
    "semantic_title": "stepdown slope for controlled feature selection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26051": {
    "title": "Positive Distribution Pollution: Rethinking Positive Unlabeled Learning from a Unified Perspective",
    "volume": "main",
    "abstract": "Positive Unlabeled (PU) learning, which has a wide range of applications, is becoming increasingly prevalent. However, it suffers from problems such as data imbalance, selection bias, and prior agnostic in real scenarios. Existing studies focus on addressing part of these problems, which fail to provide a unified perspective to understand these problems. In this paper, we first rethink these problems by analyzing a typical PU scenario and come up with an insightful point of view that all these problems are inherently connected to one problem, i.e., positive distribution pollution, which refers to the inaccuracy in estimating positive data distribution under very little labeled data. Then, inspired by this insight, we devise a variational model named CoVPU, which addresses all three problems in a unified perspective by targeting the positive distribution pollution problem. CoVPU not only accurately separates the positive data from the unlabeled data based on discrete normalizing flows, but also effectively approximates the positive distribution based on our derived unbiased rebalanced risk estimator and supervises the approximation based on a novel prior-free variational loss. Rigorous theoretical analysis proves the convergence of CoVPU to an optimal Bayesian classifier. Extensive experiments demonstrate the superiority of CoVPU over the state-of-the-art PU learning methods under these problems",
    "checked": true,
    "id": "7a488cc69575d623c1bf808209d166bc87e50d97",
    "semantic_title": "positive distribution pollution: rethinking positive unlabeled learning from a unified perspective",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26052": {
    "title": "Policy-Independent Behavioral Metric-Based Representation for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Behavioral metrics can calculate the distance between states or state-action pairs from the rewards and transitions difference. By virtue of their capability to filter out task-irrelevant information in theory, using them to shape a state embedding space becomes a new trend of representation learning for deep reinforcement learning (RL), especially when there are explicit distracting factors in observation backgrounds. However, due to the tight coupling between the metric and the RL policy, such metric-based methods may result in less informative embedding spaces which can weaken their aid to the baseline RL algorithm and even consume more samples to learn. We resolve this by proposing a new behavioral metric. It decouples the learning of RL policy and metric owing to its independence on RL policy. We theoretically justify its scalability to continuous state and action spaces and design a practical way to incorporate it into an RL procedure as a representation learning target. We evaluate our approach on DeepMind control tasks with default and distracting backgrounds. By statistically reliable evaluation protocols, our experiments demonstrate our approach is superior to previous metric-based methods in terms of sample efficiency and asymptotic performance in both backgrounds",
    "checked": true,
    "id": "192e9ef2b836c17022cfdf66dd793c27753db488",
    "semantic_title": "policy-independent behavioral metric-based representation for deep reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26053": {
    "title": "Geometry-Aware Network for Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "Measuring and alleviating the discrepancies between the synthetic (source) and real scene (target) data is the core issue for domain adaptive semantic segmentation. Though recent works have introduced depth information in the source domain to reinforce the geometric and semantic knowledge transfer, they cannot extract the intrinsic 3D information of objects, including positions and shapes, merely based on 2D estimated depth. In this work, we propose a novel Geometry-Aware Network for Domain Adaptation (GANDA), leveraging more compact 3D geometric point cloud representations to shrink the domain gaps. In particular, we first utilize the auxiliary depth supervision from the source domain to obtain the depth prediction in the target domain to accomplish structure-texture disentanglement. Beyond depth estimation, we explicitly exploit 3D topology on the point clouds generated from RGB-D images for further coordinate-color disentanglement and pseudo-labels refinement in the target domain. Moreover, to improve the 2D classifier in the target domain, we perform domain-invariant geometric adaptation from source to target and unify the 2D semantic and 3D geometric segmentation results in two domains. Note that our GANDA is plug-and-play in any existing UDA framework. Qualitative and quantitative results demonstrate that our model outperforms state-of-the-arts on GTA5->Cityscapes and SYNTHIA->Cityscapes",
    "checked": true,
    "id": "d339a2889db2190044fbda01808b60a39a75f4f2",
    "semantic_title": "geometry-aware network for domain adaptive semantic segmentation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26054": {
    "title": "Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria",
    "volume": "main",
    "abstract": "Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in the measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our analytical findings. While fairness criteria are primarily chosen under normative considerations in practice, our results show that naively applying a fairness constraint can lead to not only a loss in utility for the decision maker, but more severe unfairness when data bias exists. Thus, understanding how fairness criteria react to different forms of data bias presents a critical guideline for choosing among existing fairness criteria, or for proposing new criteria, when available datasets may be biased",
    "checked": true,
    "id": "a5df174ce3ed582867e1c5d0bae159827d759456",
    "semantic_title": "social bias meets data bias: the impacts of labeling and measurement errors on fairness criteria",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26055": {
    "title": "On the Expressive Flexibility of Self-Attention Matrices",
    "volume": "main",
    "abstract": "Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. We ask the following question: Can self-attention matrix approximate arbitrary patterns? How small is the query dimension d required for such approximation? Our first result shows that the task of deciding whether approximation of a given pattern is possible or not is NP-hard for a fixed d greater than one. In practice, self-attention matrix typically exhibits two properties: it is sparse, and it changes dynamically depending on the input to the module. Motivated by this observation, we show that the self-attention matrix can provably approximate sparse matrices. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, d grows only logarithmically with the sequence length n",
    "checked": true,
    "id": "32fa352ee110fd9f80c9d62282611b4a444f5300",
    "semantic_title": "on the expressive flexibility of self-attention matrices",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26056": {
    "title": "Wasserstein Actor-Critic: Directed Exploration via Optimism for Continuous-Actions Control",
    "volume": "main",
    "abstract": "Uncertainty quantification has been extensively used as a means to achieve efficient directed exploration in Reinforcement Learning (RL). However, state-of-the-art methods for continuous actions still suffer from high sample complexity requirements. Indeed, they either completely lack strategies for propagating the epistemic uncertainty throughout the updates, or they mix it with aleatoric uncertainty while learning the full return distribution (e.g., distributional RL). In this paper, we propose Wasserstein Actor-Critic (WAC), an actor-critic architecture inspired by the recent Wasserstein Q-Learning (WQL), that employs approximate Q-posteriors to represent the epistemic uncertainty and Wasserstein barycenters for uncertainty propagation across the state-action space. WAC enforces exploration in a principled way by guiding the policy learning process with the optimization of an upper bound of the Q-value estimates. Furthermore, we study some peculiar issues that arise when using function approximation, coupled with the uncertainty estimation, and propose a regularized loss for the uncertainty estimation. Finally, we evaluate our algorithm on standard MujoCo tasks as well as suite of continuous-actions domains, where exploration is crucial, in comparison with state-of-the-art baselines. Additional details and results can be found in the supplementary material with our Arxiv preprint",
    "checked": true,
    "id": "d26ea31d844d51f11f0703dfae54fdfe9c0d1092",
    "semantic_title": "wasserstein actor-critic: directed exploration via optimism for continuous-actions control",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26057": {
    "title": "Dual Label-Guided Graph Refinement for Multi-View Graph Clustering",
    "volume": "main",
    "abstract": "With the increase of multi-view graph data, multi-view graph clustering (MVGC) that can discover the hidden clusters without label supervision has attracted growing attention from researchers. Existing MVGC methods are often sensitive to the given graphs, especially influenced by the low quality graphs, i.e., they tend to be limited by the homophily assumption. However, the widespread real-world data hardly satisfy the homophily assumption. This gap limits the performance of existing MVGC methods on low homophilous graphs. To mitigate this limitation, our motivation is to extract high-level view-common information which is used to refine each view's graph, and reduce the influence of non-homophilous edges. To this end, we propose dual label-guided graph refinement for multi-view graph clustering (DuaLGR), to alleviate the vulnerability in facing low homophilous graphs. Specifically, DuaLGR consists of two modules named dual label-guided graph refinement module and graph encoder module. The first module is designed to extract the soft label from node features and graphs, and then learn a refinement matrix. In cooperation with the pseudo label from the second module, these graphs are refined and aggregated adaptively with different orders. Subsequently, a consensus graph can be generated in the guidance of the pseudo label. Finally, the graph encoder module encodes the consensus graph along with node features to produce the high-level pseudo label for iteratively clustering. The experimental results show the superior performance on coping with low homophilous graph data. The source code for DuaLGR is available at https://github.com/YwL-zhufeng/DuaLGR",
    "checked": true,
    "id": "89fdbaa025a339558438160fbcca2e0812abfb56",
    "semantic_title": "dual label-guided graph refinement for multi-view graph clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26058": {
    "title": "Metric Residual Network for Sample Efficient Goal-Conditioned Reinforcement Learning",
    "volume": "main",
    "abstract": "Goal-conditioned reinforcement learning (GCRL) has a wide range of potential real-world applications, including manipulation and navigation problems in robotics. Especially in such robotics tasks, sample efficiency is of the utmost importance for GCRL since, by default, the agent is only rewarded when it reaches its goal. While several methods have been proposed to improve the sample efficiency of GCRL, one relatively under-studied approach is the design of neural architectures to support sample efficiency. In this work, we introduce a novel neural architecture for GCRL that achieves significantly better sample efficiency than the commonly-used monolithic network architecture. The key insight is that the optimal action-value function must satisfy the triangle inequality in a specific sense. Furthermore, we introduce the metric residual network (MRN) that deliberately decomposes the action-value function into the negated summation of a metric plus a residual asymmetric component. MRN provably approximates any optimal action-value function, thus making it a fitting neural architecture for GCRL. We conduct comprehensive experiments across 12 standard benchmark environments in GCRL. The empirical results demonstrate that MRN uniformly outperforms other state-of-the-art GCRL neural architectures in terms of sample efficiency. The code is available at https://github.com/Cranial-XIX/metric-residual-network",
    "checked": false,
    "id": "58a7ecdc94d6e59454c08c9c3e132a9e6ca00094",
    "semantic_title": "metric residual networks for sample efficient goal-conditioned reinforcement learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26059": {
    "title": "DICNet: Deep Instance-Level Contrastive Network for Double Incomplete Multi-View Multi-Label Classification",
    "volume": "main",
    "abstract": "In recent years, multi-view multi-label learning has aroused extensive research enthusiasm. However, multi-view multi-label data in the real world is commonly incomplete due to the uncertain factors of data collection and manual annotation, which means that not only multi-view features are often missing, and label completeness is also difficult to be satisfied. To deal with the double incomplete multi-view multi-label classification problem, we propose a deep instance-level contrastive network, namely DICNet. Different from conventional methods, our DICNet focuses on leveraging deep neural network to exploit the high-level semantic representations of samples rather than shallow-level features. First, we utilize the stacked autoencoders to build an end-to-end multi-view feature extraction framework to learn the view-specific representations of samples. Furthermore, in order to improve the consensus representation ability, we introduce an incomplete instance-level contrastive learning scheme to guide the encoders to better extract the consensus information of multiple views and use a multi-view weighted fusion module to enhance the discrimination of semantic features. Overall, our DICNet is adept in capturing consistent discriminative representations of multi-view multi-label data and avoiding the negative effects of missing views and missing labels. Extensive experiments performed on five datasets validate that our method outperforms other state-of-the-art methods",
    "checked": true,
    "id": "e0e3ccfcf9fc94b99536561ba6dd2c9dd49c96bb",
    "semantic_title": "dicnet: deep instance-level contrastive network for double incomplete multi-view multi-label classification",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26060": {
    "title": "Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View- and Category-Aware Transformers",
    "volume": "main",
    "abstract": "As we all know, multi-view data is more expressive than single-view data and multi-label annotation enjoys richer supervision information than single-label, which makes multi-view multi-label learning widely applicable for various pattern recognition tasks. In this complex representation learning problem, three main challenges can be characterized as follows: i) How to learn consistent representations of samples across all views? ii) How to exploit and utilize category correlations of multi-label to guide inference? iii) How to avoid the negative impact resulting from the incompleteness of views or labels? To cope with these problems, we propose a general multi-view multi-label learning framework named label-guided masked view- and category-aware transformers in this paper. First, we design two transformer-style based modules for cross-view features aggregation and multi-label classification, respectively. The former aggregates information from different views in the process of extracting view-specific features, and the latter learns subcategory embedding to improve classification performance. Second, considering the imbalance of expressive power among views, an adaptively weighted view fusion module is proposed to obtain view-consistent embedding features. Third, we impose a label manifold constraint in sample-level representation learning to maximize the utilization of supervised information. Last but not least, all the modules are designed under the premise of incomplete views and labels, which makes our method adaptable to arbitrary multi-view and multi-label data. Extensive experiments on five datasets confirm that our method has clear advantages over other state-of-the-art methods",
    "checked": true,
    "id": "867b2f2a1b041a4157cf3b49e52ff1e03c1aca04",
    "semantic_title": "incomplete multi-view multi-label learning via label-guided masked view- and category-aware transformers",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26061": {
    "title": "Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization for Heterogeneous Representational Coarseness",
    "volume": "main",
    "abstract": "Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. It has been theoretically and empirically shown that discretization of representations leads to improved generalization, including in reinforcement learning where discretization can be used to bottleneck multi-agent communication to promote agent specialization and robustness. The discretization tightness of most VQ-based methods is defined by the number of discrete codes in the representation vector and the codebook size, which are fixed as hyperparameters. In this work, we propose learning to dynamically select discretization tightness conditioned on inputs, based on the hypothesis that data naturally contains variations in complexity that call for different levels of representational coarseness which is observed in many heterogeneous data sets. We show that dynamically varying tightness in communication bottlenecks can improve model performance on visual reasoning and reinforcement learning tasks with heterogeneity in representations",
    "checked": false,
    "id": "677f0dbaeddbfce9a7afb5efb6703798e2c99e0f",
    "semantic_title": "adaptive discrete communication bottlenecks with dynamic vector quantization",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26062": {
    "title": "Combating Mode Collapse via Offline Manifold Entropy Estimation",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) have shown compelling results in various tasks and applications in recent years. However, mode collapse remains a critical problem in GANs. In this paper, we propose a novel training pipeline to address the mode collapse issue of GANs. Different from existing methods, we propose to generalize the discriminator as feature embedding and maximize the entropy of distributions in the embedding space learned by the discriminator. Specifically, two regularization terms, i.e., Deep Local Linear Embedding (DLLE) and Deep Isometric feature Mapping (DIsoMap), are introduced to encourage the discriminator to learn the structural information embedded in the data, such that the embedding space learned by the discriminator can be well-formed. Based on the well-learned embedding space supported by the discriminator, a non-parametric entropy estimator is designed to efficiently maximize the entropy of embedding vectors, playing as an approximation of maximizing the entropy of the generated distribution. By improving the discriminator and maximizing the distance of the most similar samples in the embedding space, our pipeline effectively reduces the mode collapse without sacrificing the quality of generated samples. Extensive experimental results show the effectiveness of our method which outperforms the GAN baseline, MaF-GAN on CelebA (9.13 vs. 12.43 in FID) and surpasses the recent state-of-the-art energy-based model on the ANIMEFACE dataset (2.80 vs. 2.26 in Inception score)",
    "checked": true,
    "id": "c6cc2fc16008a57fc35887d55c84ddef0ad68955",
    "semantic_title": "combating mode collapse via offline manifold entropy estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26063": {
    "title": "Robust Representation Learning by Clustering with Bisimulation Metrics for Visual Reinforcement Learning with Distractions",
    "volume": "main",
    "abstract": "Recent work has shown that representation learning plays a critical role in sample-efficient reinforcement learning (RL) from pixels. Unfortunately, in real-world scenarios, representation learning is usually fragile to task-irrelevant distractions such as variations in background or viewpoint. To tackle this problem, we propose a novel clustering-based approach, namely Clustering with Bisimulation Metrics (CBM), which learns robust representations by grouping visual observations in the latent space. Specifically, CBM alternates between two steps: (1) grouping observations by measuring their bisimulation distances to the learned prototypes; (2) learning a set of prototypes according to the current cluster assignments. Computing cluster assignments with bisimulation metrics enables CBM to capture task-relevant information, as bisimulation metrics quantify the behavioral similarity between observations. Moreover, CBM encourages the consistency of representations within each group, which facilitates filtering out task-irrelevant information and thus induces robust representations against distractions. An appealing feature is that CBM can achieve sample-efficient representation learning even if multiple distractions exist simultaneously. Experiments demonstrate that CBM significantly improves the sample efficiency of popular visual RL algorithms and achieves state-of-the-art performance on both multiple and single distraction settings. The code is available at https://github.com/MIRALab-USTC/RL-CBM",
    "checked": true,
    "id": "5fda3e94df50485dd7caa6940fa7435ec9bfe80c",
    "semantic_title": "robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26064": {
    "title": "Reliable Robustness Evaluation via Automatically Constructed Attack Ensembles",
    "volume": "main",
    "abstract": "Attack Ensemble (AE), which combines multiple attacks together, provides a reliable way to evaluate adversarial robustness. In practice, AEs are often constructed and tuned by human experts, which however tends to be sub-optimal and time-consuming. In this work, we present AutoAE, a conceptually simple approach for automatically constructing AEs. In brief, AutoAE repeatedly adds the attack and its iteration steps to the ensemble that maximizes ensemble improvement per additional iteration consumed. We show theoretically that AutoAE yields AEs provably within a constant factor of the optimal for a given defense. We then use AutoAE to construct two AEs for l∞ and l2 attacks, and apply them without any tuning or adaptation to 45 top adversarial defenses on the RobustBench leaderboard. In all except one cases we achieve equal or better (often the latter) robustness evaluation than existing AEs, and notably, in 29 cases we achieve better robustness evaluation than the best known one. Such performance of AutoAE shows itself as a reliable evaluation protocol for adversarial robustness, which further indicates the huge potential of automatic AE construction. Code is available at https://github.com/LeegerPENG/AutoAE",
    "checked": true,
    "id": "117d6de151477f7a1134b117abae09f39ac3a390",
    "semantic_title": "reliable robustness evaluation via automatically constructed attack ensembles",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26065": {
    "title": "Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks",
    "volume": "main",
    "abstract": "Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications",
    "checked": true,
    "id": "5251da262f05511dfcd48e527ddf2d781e1c33a5",
    "semantic_title": "enhancing the antidote: improved pointwise certifications against poisoning attacks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26066": {
    "title": "Safe Multi-View Deep Classification",
    "volume": "main",
    "abstract": "Multi-view deep classification expects to obtain better classification performance than using a single view. However, due to the uncertainty and inconsistency of data sources, adding data views does not necessarily lead to the performance improvements in multi-view classification. How to avoid worsening classification performance when adding views is crucial for multi-view deep learning but rarely studied. To tackle this limitation, in this paper, we reformulate the multi-view classification problem from the perspective of safe learning and thereby propose a Safe Multi-view Deep Classification (SMDC) method, which can guarantee that the classification performance does not deteriorate when fusing multiple views. In the SMDC method, we dynamically integrate multiple views and estimate the inherent uncertainties among multiple views with different root causes based on evidence theory. Through minimizing the uncertainties, SMDC promotes the evidences from data views for correct classification, and in the meantime excludes the incorrect evidences to produce the safe multi-view classification results. Furthermore, we theoretically prove that in the safe multi-view classification, adding data views will certainly not increase the empirical risk of classification. The experiments on various kinds of multi-view datasets validate that the proposed SMDC method can achieve precise and safe classification results",
    "checked": true,
    "id": "4a23f3a7989e4b0b74151f5baaa5555ae917c799",
    "semantic_title": "safe multi-view deep classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26067": {
    "title": "Tensor Compressive Sensing Fused Low-Rankness and Local-Smoothness",
    "volume": "main",
    "abstract": "A plethora of previous studies indicates that making full use of multifarious intrinsic properties of primordial data is a valid pathway to recover original images from their degraded observations. Typically, both low-rankness and local-smoothness broadly exist in real-world tensor data such as hyperspectral images and videos. Modeling based on both properties has received a great deal of attention, whereas most studies concentrate on experimental performance, and theoretical investigations are still lacking. In this paper, we study the tensor compressive sensing problem based on the tensor correlated total variation, which is a new regularizer used to simultaneously capture both properties existing in the same dataset. The new regularizer has the outstanding advantage of not using a trade-off parameter to balance the two properties. The obtained theories provide a robust recovery guarantee, where the error bound shows that our model certainly benefits from both properties in ground-truth data adaptively. Moreover, based on the ADMM update procedure, we design an algorithm with a global convergence guarantee to solve this model. At last, we carry out experiments to apply our model to hyperspectral image and video restoration problems. The experimental results show that our method is prominently better than many other competing ones. Our code and Supplementary Material are available at https://github.com/fsliuxl/cs-tctv",
    "checked": true,
    "id": "db6d22c63e8ec9ea4746c5a6ed1ccf7d1ebe1135",
    "semantic_title": "tensor compressive sensing fused low-rankness and local-smoothness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26068": {
    "title": "Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain",
    "volume": "main",
    "abstract": "Linking computational natural language processing (NLP) models and neural responses to language in the human brain on the one hand facilitates the effort towards disentangling the neural representations underpinning language perception, on the other hand provides neurolinguistics evidence to evaluate and improve NLP models. Mappings of an NLP model's representations of and the brain activities evoked by linguistic input are typically deployed to reveal this symbiosis. However, two critical problems limit its advancement: 1) The model's representations (artificial neurons, ANs) rely on layer-level embeddings and thus lack fine-granularity; 2) The brain activities (biological neurons, BNs) are limited to neural recordings of isolated cortical unit (i.e., voxel/region) and thus lack integrations and interactions among brain functions. To address those problems, in this study, we 1) define ANs with fine-granularity in transformer-based NLP models (BERT in this study) and measure their temporal activations to input text sequences; 2) define BNs as functional brain networks (FBNs) extracted from functional magnetic resonance imaging (fMRI) data to capture functional interactions in the brain; 3) couple ANs and BNs by maximizing the synchronization of their temporal activations. Our experimental results demonstrate 1) The activations of ANs and BNs are significantly synchronized; 2) the ANs carry meaningful linguistic/semantic information and anchor to their BN signatures; 3) the anchored BNs are interpretable in a neurolinguistic context. Overall, our study introduces a novel, general, and effective framework to link transformer-based NLP models and neural activities in response to language and may provide novel insights for future studies such as brain-inspired evaluation and development of NLP models",
    "checked": true,
    "id": "990a61871d6dc1aa474cf334542fb9fbfd32f996",
    "semantic_title": "coupling artificial neurons in bert and biological neurons in the human brain",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26069": {
    "title": "EASAL: Entity-Aware Subsequence-Based Active Learning for Named Entity Recognition",
    "volume": "main",
    "abstract": "Active learning is a critical technique for reducing labelling load by selecting the most informative data. Most previous works applied active learning on Named Entity Recognition (token-level task) similar to the text classification (sentence-level task). They failed to consider the heterogeneity of uncertainty within each sentence and required access to the entire sentence for the annotator when labelling. To overcome the mentioned limitations, in this paper, we allow the active learning algorithm to query subsequences within sentences and propose an Entity-Aware Subsequences-based Active Learning (EASAL) that utilizes an effective Head-Tail pointer to query one entity-aware subsequence for each sentence based on BERT. For other tokens outside this subsequence, we randomly select 30% of these tokens to be pseudo-labelled for training together where the model directly predicts their pseudo-labels. Experimental results on both news and biomedical datasets demonstrate the effectiveness of our proposed method. The code is released at https://github.com/lylylylylyly/EASAL",
    "checked": true,
    "id": "a1306e40b8d0c89150967c6b63b4bf4cbf67c8a3",
    "semantic_title": "easal: entity-aware subsequence-based active learning for named entity recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26070": {
    "title": "Online Hyperparameter Optimization for Class-Incremental Learning",
    "volume": "main",
    "abstract": "Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings—where typically the training-from-half (TFH) setting needs more stability, but the training-from-scratch (TFS) needs more plasticity. To this end, we design an online learning method that can adaptively optimize the tradeoff without knowing the setting as a priori. Specifically, we first introduce the key hyperparameters that influence the tradeoff, e.g., knowledge distillation (KD) loss weights, learning rates, and classifier types. Then, we formulate the hyperparameter optimization process as an online Markov Decision Process (MDP) problem and propose a specific algorithm to solve it. We apply local estimated rewards and a classic bandit algorithm Exp3 to address the issues when applying online MDP methods to the CIL protocol. Our method consistently improves top-performing CIL methods in both TFH and TFS settings, e.g., boosting the average accuracy of TFH and TFS by 2.2 percentage points on ImageNet-Full, compared to the state-of-the-art. Code is provided at https://class-il.mpi-inf.mpg.de/online/",
    "checked": true,
    "id": "ecc0cd4141d0600b04b313dd8e3c12626a8ffc71",
    "semantic_title": "online hyperparameter optimization for class-incremental learning",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26071": {
    "title": "Hard Sample Aware Network for Contrastive Deep Graph Clustering",
    "volume": "main",
    "abstract": "Contrastive deep graph clustering, which aims to divide nodes into disjoint groups via contrastive mechanisms, is a challenging research spot. Among the recent works, hard sample mining-based algorithms have achieved great attention for their promising performance. However, we find that the existing hard sample mining methods have two problems as follows. 1) In the hardness measurement, the important structural information is overlooked for similarity calculation, degrading the representativeness of the selected hard negative samples. 2) Previous works merely focus on the hard negative sample pairs while neglecting the hard positive sample pairs. Nevertheless, samples within the same cluster but with low similarity should also be carefully learned. To solve the problems, we propose a novel contrastive deep graph clustering method dubbed Hard Sample Aware Network (HSAN) by introducing a comprehensive similarity measure criterion and a general dynamic sample weighing strategy. Concretely, in our algorithm, the similarities between samples are calculated by considering both the attribute embeddings and the structure embeddings, better revealing sample relationships and assisting hardness measurement. Moreover, under the guidance of the carefully collected high-confidence clustering information, our proposed weight modulating function will first recognize the positive and negative samples and then dynamically up-weight the hard sample pairs while down-weighting the easy ones. In this way, our method can mine not only the hard negative samples but also the hard positive sample, thus improving the discriminative capability of the samples further. Extensive experiments and analyses demonstrate the superiority and effectiveness of our proposed method. The source code of HSAN is shared at https://github.com/yueliu1999/HSAN and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github",
    "checked": true,
    "id": "6d663f1b9ec6e700c7e781e81651186d5208b6ff",
    "semantic_title": "hard sample aware network for contrastive deep graph clustering",
    "citation_count": 17
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26072": {
    "title": "Temporal-Frequency Co-training for Time Series Semi-supervised Learning",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) has been actively studied due to its ability to alleviate the reliance of deep learning models on labeled data. Although existing SSL methods based on pseudo-labeling strategies have made great progress, they rarely consider time-series data's intrinsic properties (e.g., temporal dependence). Learning representations by mining the inherent properties of time series has recently gained much attention. Nonetheless, how to utilize feature representations to design SSL paradigms for time series has not been explored. To this end, we propose a Time Series SSL framework via Temporal-Frequency Co-training (TS-TFC), leveraging the complementary information from two distinct views for unlabeled data learning. In particular, TS-TFC employs time-domain and frequency-domain views to train two deep neural networks simultaneously, and each view's pseudo-labels generated by label propagation in the representation space are adopted to guide the training of the other view's classifier. To enhance the discriminative of representations between categories, we propose a temporal-frequency supervised contrastive learning module, which integrates the learning difficulty of categories to improve the quality of pseudo-labels. Through co-training the pseudo-labels obtained from temporal-frequency representations, the complementary information in the two distinct views is exploited to enable the model to better learn the distribution of categories. Extensive experiments on 106 UCR datasets show that TS-TFC outperforms state-of-the-art methods, demonstrating the effectiveness and robustness of our proposed model",
    "checked": true,
    "id": "131819cd0495e02bc2e62be869d07a8c2316a71d",
    "semantic_title": "temporal-frequency co-training for time series semi-supervised learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26073": {
    "title": "Q-functionals for Value-Based Continuous Control",
    "volume": "main",
    "abstract": "We present Q-functionals, an alternative architecture for continuous control deep reinforcement learning. Instead of returning a single value for a state-action pair, our network transforms a state into a function that can be rapidly evaluated in parallel for many actions, allowing us to efficiently choose high-value actions through sampling. This contrasts with the typical architecture of off-policy continuous control, where a policy network is trained for the sole purpose of selecting actions from the Q-function. We represent our action-dependent Q-function as a weighted sum of basis functions (Fourier, Polynomial, etc) over the action space, where the weights are state-dependent and output by the Q-functional network. Fast sampling makes practical a variety of techniques that require Monte-Carlo integration over Q-functions, and enables action-selection strategies besides simple value-maximization. We characterize our framework, describe various implementations of Q-functionals, and demonstrate strong performance on a suite of continuous control tasks",
    "checked": true,
    "id": "e49fbfba9a959e408ef5dfa5b58f3320ac04735a",
    "semantic_title": "q-functionals for value-based continuous control",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26074": {
    "title": "A Coreset Learning Reality Check",
    "volume": "main",
    "abstract": "Subsampling algorithms are a natural approach to reduce data size before fitting models on massive datasets. In recent years, several works have proposed methods for subsampling rows from a data matrix while maintaining relevant information for classification. While these works are supported by theory and limited experiments, to date there has not been a comprehensive evaluation of these methods. In our work, we directly compare multiple methods for logistic regression drawn from the coreset and optimal subsampling literature and discover inconsistencies in their effectiveness. In many cases, methods do not outperform simple uniform subsampling",
    "checked": true,
    "id": "fb3d8ae9e0ac948b2d7dd42293ce508093d160c8",
    "semantic_title": "a coreset learning reality check",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26075": {
    "title": "Centerless Multi-View K-means Based on the Adjacency Matrix",
    "volume": "main",
    "abstract": "Although K-Means clustering has been widely studied due to its simplicity, these methods still have the following fatal drawbacks. Firstly, they need to initialize the cluster centers, which causes unstable clustering performance. Secondly, they have poor performance on non-Gaussian datasets. Inspired by the affinity matrix, we propose a novel multi-view K-Means based on the adjacency matrix. It maps the affinity matrix to the distance matrix according to the principle that every sample has a small distance from the points in its neighborhood and a large distance from the points outside of the neighborhood. Moreover, this method well exploits the complementary information embedded in different views by minimizing the tensor Schatten p-norm regularize on the third-order tensor which consists of cluster assignment matrices of different views. Additionally, this method avoids initializing cluster centroids to obtain stable performance. And there is no need to compute the means of clusters so that our model is not sensitive to outliers. Experiment on a toy dataset shows the excellent performance on non-Gaussian datasets. And other experiments on several benchmark datasets demonstrate the superiority of our proposed method",
    "checked": true,
    "id": "576072821acc99b7576b5239b57cae5e63de9404",
    "semantic_title": "centerless multi-view k-means based on the adjacency matrix",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26076": {
    "title": "PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor",
    "volume": "main",
    "abstract": "Time-consuming performance evaluation is the bottleneck of traditional Neural Architecture Search (NAS) methods. Predictor-based NAS can speed up performance evaluation by directly predicting performance, rather than training a large number of sub-models and then validating their performance. Most predictor-based NAS approaches use a proxy dataset to train model-based predictors efficiently but suffer from performance degradation and generalization problems. We attribute these problems to the poor abilities of existing predictors to character the sub-models' structure, specifically the topology information extraction and the node feature representation of the input graph data. To address these problems, we propose a Transformer-like NAS predictor PINAT, consisting of a Permutation INvariance Augmentation module serving as both token embedding layer and self-attention head, as well as a Laplacian matrix to be the positional encoding. Our design produces more representative features of the encoded architecture and outperforms state-of-the-art NAS predictors on six search spaces: NAS-Bench-101, NAS-Bench-201, DARTS, ProxylessNAS, PPI, and ModelNet. The code is available at https://github.com/ShunLu91/PINAT",
    "checked": true,
    "id": "3930a0285877386fca94e60c4c69305e220fd4ee",
    "semantic_title": "pinat: a permutation invariance augmented transformer for nas predictor",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26077": {
    "title": "Multi-View Domain Adaptive Object Detection on Camera Networks",
    "volume": "main",
    "abstract": "In this paper, we study a new domain adaptation setting on camera networks, namely Multi-View Domain Adaptive Object Detection (MVDA-OD), in which labeled source data is unavailable in the target adaptation process and target data is captured from multiple overlapping cameras. In such a challenging context, existing methods including adversarial training and self-training fall short due to multi-domain data shift and the lack of source data. To tackle this problem, we propose a novel training framework consisting of two stages. First, we pre-train the backbone using self-supervised learning, in which a multi-view association is developed to construct an effective pretext task. Second, we fine-tune the detection head using robust self-training, where a tracking-based single-view augmentation is introduced to achieve weak-hard consistency learning. By doing so, an object detection model can take advantage of informative samples generated by multi-view association and single-view augmentation to learn discriminative backbones as well as robust detection classifiers. Experiments on two real-world multi-camera datasets demonstrate significant advantages of our approach over the state-of-the-art domain adaptive object detection methods",
    "checked": true,
    "id": "fd1b60b1c2c142af4919f4b13b792f89b50cc134",
    "semantic_title": "multi-view domain adaptive object detection on camera networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26078": {
    "title": "Generative Label Enhancement with Gaussian Mixture and Partial Ranking",
    "volume": "main",
    "abstract": "Label distribution learning (LDL) is an effective learning paradigm for dealing with label ambiguity. When applying LDL, the datasets annotated with label distributions (i.e., the real-valued vectors like the probability distribution) are typically required. Unfortunately, most existing datasets only contain the logical labels, and manual annotating with label distributions is costly. To address this problem, we treat the label distribution as a latent vector and infer its posterior by variational Bayes. Specifically, we propose a generative label enhancement model to encode the process of generating feature vectors and logical label vectors from label distributions in a principled way. In terms of features, we assume that the feature vector is generated by a Gaussian mixture dominated by the label distribution, which captures the one-to-many relationship from the label distribution to the feature vector and thus reduces the feature generation error. In terms of logical labels, we design a probability distribution to generate the logical label vector from a label distribution, which captures partial label ranking in the logical label vector and thus provides a more accurate guidance for inferring the label distribution. Besides, to approximate the posterior of the label distribution, we design a inference model, and derive the variational learning objective. Finally, extensive experiments on real-world datasets validate our proposal",
    "checked": true,
    "id": "066425bb5cc653f0548b30dfcde1cd15f6b254fb",
    "semantic_title": "generative label enhancement with gaussian mixture and partial ranking",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26079": {
    "title": "Crowd-Level Abnormal Behavior Detection via Multi-Scale Motion Consistency Learning",
    "volume": "main",
    "abstract": "Detecting abnormal crowd motion emerging from complex interactions of individuals is paramount to ensure the safety of crowds. Crowd-level abnormal behaviors (CABs), e.g., counter flow and crowd turbulence, are proven to be the crucial causes of many crowd disasters. In the recent decade, video anomaly detection (VAD) techniques have achieved remarkable success in detecting individual-level abnormal behaviors (e.g., sudden running, fighting and stealing), but research on VAD for CABs is rather limited. Unlike individual-level anomaly, CABs usually do not exhibit salient difference from the normal behaviors when observed locally, and the scale of CABs could vary from one scenario to another. In this paper, we present a systematic study to tackle the important problem of VAD for CABs with a novel crowd motion learning framework, multi-scale motion consistency network (MSMC-Net). MSMC-Net first captures the spatial and temporal crowd motion consistency information in a graph representation. Then, it simultaneously trains multiple feature graphs constructed at different scales to capture rich crowd patterns. An attention network is used to adaptively fuse the multi-scale features for better CAB detection. For the empirical study, we consider three large-scale crowd event datasets, UMN, Hajj and Love Parade. Experimental results show that MSMC-Net could substantially improve the state-of-the-art performance on all the datasets",
    "checked": true,
    "id": "d31a54553648a1001ebd63b734e5b644c10878f7",
    "semantic_title": "crowd-level abnormal behavior detection via multi-scale motion consistency learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26080": {
    "title": "MVCINN: Multi-View Diabetic Retinopathy Detection Using a Deep Cross-Interaction Neural Network",
    "volume": "main",
    "abstract": "Diabetic retinopathy (DR) is the main cause of irreversible blindness for working-age adults. The previous models for DR detection have difficulties in clinical application. The main reason is that most of the previous methods only use single-view data, and the single field of view (FOV) only accounts for about 13% of the FOV of the retina, resulting in the loss of most lesion features. To alleviate this problem, we propose a multi-view model for DR detection, which takes full advantage of multi-view images covering almost all of the retinal field. To be specific, we design a Cross-Interaction Self-Attention based Module (CISAM) that interfuses local features extracted from convolutional blocks with long-range global features learned from transformer blocks. Furthermore, considering the pathological association in different views, we use the feature jigsaw to assemble and learn the features of multiple views. Extensive experiments on the latest public multi-view MFIDDR dataset with 34,452 images demonstrate the superiority of our method, which performs favorably against state-of-the-art models. To the best of our knowledge, this work is the first study on the public large-scale multi-view fundus images dataset for DR detection",
    "checked": true,
    "id": "852ce0da50b02fd07ab65de6a630cd2111cca543",
    "semantic_title": "mvcinn: multi-view diabetic retinopathy detection using a deep cross-interaction neural network",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26081": {
    "title": "Local Explanations for Reinforcement Learning",
    "volume": "main",
    "abstract": "Many works in explainable AI have focused on explaining black-box classification models. Explaining deep reinforcement learning (RL) policies in a manner that could be understood by domain users has received much less attention. In this paper, we propose a novel perspective to understanding RL policies based on identifying important states from automatically learned meta-states. The key conceptual difference between our approach and many previous ones is that we form meta-states based on locality governed by the expert policy dynamics rather than based on similarity of actions, and that we do not assume any particular knowledge of the underlying topology of the state space. Theoretically, we show that our algorithm to find meta-states converges and the objective that selects important states from each meta-state is submodular leading to efficient high quality greedy selection. Experiments on four domains (four rooms, door-key, minipacman, and pong) and a carefully conducted user study illustrate that our perspective leads to better understanding of the policy. We conjecture that this is a result of our meta-states being more intuitive in that the corresponding important states are strong indicators of tractable intermediate goals that are easier for humans to interpret and follow",
    "checked": true,
    "id": "60f47eb09931260b43812602c636134e9f69f4f7",
    "semantic_title": "local explanations for reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26082": {
    "title": "Compositional Prototypical Networks for Few-Shot Classification",
    "volume": "main",
    "abstract": "It is assumed that pre-training provides the feature extractor with strong class transferability and that high novel class generalization can be achieved by simply reusing the transferable feature extractor. In this work, our motivation is to explicitly learn some fine-grained and transferable meta-knowledge so that feature reusability can be further improved. Concretely, inspired by the fact that humans can use learned concepts or components to help them recognize novel classes, we propose Compositional Prototypical Networks (CPN) to learn a transferable prototype for each human-annotated attribute, which we call a component prototype. We empirically demonstrate that the learned component prototypes have good class transferability and can be reused to construct compositional prototypes for novel classes. Then a learnable weight generator is utilized to adaptively fuse the compositional and visual prototypes. Extensive experiments demonstrate that our method can achieve state-of-the-art results on different datasets and settings. The performance gains are especially remarkable in the 5-way 1-shot setting. The code is available at https://github.com/fikry102/CPN",
    "checked": true,
    "id": "ed3e8dc12806a03b183e0f5f54a4d1467d12d6fc",
    "semantic_title": "compositional prototypical networks for few-shot classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26083": {
    "title": "Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning",
    "volume": "main",
    "abstract": "Are Federated Learning (FL) systems free from backdoor poisoning with the arsenal of various defense strategies deployed? This is an intriguing problem with significant practical implications regarding the utility of FL services. Despite the recent flourish of poisoning-resilient FL methods, our study shows that carefully tuning the collusion between malicious participants can minimize the trigger-induced bias of the poisoned local model from the poison-free one, which plays the key role in delivering stealthy backdoor attacks and circumventing a wide spectrum of state-of-the-art defense methods in FL. In our work, we instantiate the attack strategy by proposing a distributed backdoor attack method, namely Cerberus Poisoning (CerP). It jointly tunes the backdoor trigger and controls the poisoned model changes on each malicious participant to achieve a stealthy yet successful backdoor attack against a wide spectrum of defensive mechanisms of federated learning techniques. Our extensive study on 3 large-scale benchmark datasets and 13 mainstream defensive mechanisms confirms that Cerberus Poisoning raises a significantly severe threat to the integrity and security of federated learning practices, regardless of the flourish of robust Federated Learning methods",
    "checked": true,
    "id": "2824ad1c27459d43e3482a6ec56579fb3e751233",
    "semantic_title": "poisoning with cerberus: stealthy and colluded backdoor attack against federated learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26084": {
    "title": "OMPQ: Orthogonal Mixed Precision Quantization",
    "volume": "main",
    "abstract": "To bridge the ever-increasing gap between deep neural networks' complexity and hardware capability, network quantization has attracted more and more research attention. The latest trend of mixed precision quantization takes advantage of hardware's multiple bit-width arithmetic operations to unleash the full potential of network quantization. However, existing approaches rely heavily on an extremely time-consuming search process and various relaxations when seeking the optimal bit configuration. To address this issue, we propose to optimize a proxy metric of network orthogonality that can be efficiently solved with linear programming, which proves to be highly correlated with quantized model accuracy and bit-width. Our approach significantly reduces the search time and the required data amount by orders of magnitude, but without a compromise on quantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on ResNet-18 with 6.7Mb parameters, which does not require any searching iterations. Given the high efficiency and low data dependency of our algorithm, we use it for the post-training quantization, which achieves 71.27% Top-1 accuracy on MobileNetV2 with only 1.5Mb parameters",
    "checked": true,
    "id": "ca9ef03e0ff38ea65a7efb8bc1e5907e87a79c15",
    "semantic_title": "ompq: orthogonal mixed precision quantization",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26085": {
    "title": "Recovering the Graph Underlying Networked Dynamical Systems under Partial Observability: A Deep Learning Approach",
    "volume": "main",
    "abstract": "We study the problem of graph structure identification, i.e., of recovering the graph of dependencies among time series. We model these time series data as components of the state of linear stochastic networked dynamical systems. We assume partial observability, where the state evolution of only a subset of nodes comprising the network is observed. We propose a new feature-based paradigm: to each pair of nodes, we compute a feature vector from the observed time series. We prove that these features are linearly separable, i.e., there exists a hyperplane that separates the cluster of features associated with connected pairs of nodes from those of disconnected pairs. This renders the features amenable to train a variety of classifiers to perform causal inference. In particular, we use these features to train Convolutional Neural Networks (CNNs). The resulting causal inference mechanism outperforms state-of-the-art counterparts w.r.t. sample-complexity. The trained CNNs generalize well over structurally distinct networks (dense or sparse) and noise-level profiles. Remarkably, they also generalize well to real-world networks while trained over a synthetic network -- namely, a particular realization of a random graph",
    "checked": true,
    "id": "663bcc25755bdb3ca8ead4e5a0f5bb099cae1c4f",
    "semantic_title": "recovering the graph underlying networked dynamical systems under partial observability: a deep learning approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26086": {
    "title": "LIMIP: Lifelong Learning to Solve Mixed Integer Programs",
    "volume": "main",
    "abstract": "Mixed Integer programs (MIPs) are typically solved by the Branch-and-Bound algorithm. Recently, Learning to imitate fast approximations of the expert strong branching heuristic has gained attention due to its success in reducing the running time for solving MIPs. However, existing learning-to-branch methods assume that the entire training data is available in a single session of training. This assumption is often not true, and if the training data is supplied in continual fashion over time, existing techniques suffer from catastrophic forgetting. In this work, we study the hitherto unexplored paradigm of Lifelong Learning to Branch on Mixed Integer Programs. To mitigate catastrophic forgetting, we propose LIMIP, which is powered by the idea of modeling an MIP instance in the form of a bipartite graph, which we map to an embedding space using a bipartite Graph Attention Network. This rich embedding space avoids catastrophic forgetting through the application of knowledge distillation and elastic weight consolidation, wherein we learn the parameters key towards retaining efficacy and are therefore protected from significant drift. We evaluate LIMIP on a series of NP-hard problems and establish that in comparison to existing baselines, LIMIP is up to 50% better when confronted with lifelong learning",
    "checked": true,
    "id": "a68d81e903e0a5768b9fdfaf6c5400ecc3c95be3",
    "semantic_title": "limip: lifelong learning to solve mixed integer programs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26087": {
    "title": "Proximal Stochastic Recursive Momentum Methods for Nonconvex Composite Decentralized Optimization",
    "volume": "main",
    "abstract": "Consider a network of N decentralized computing agents collaboratively solving a nonconvex stochastic composite problem. In this work, we propose a single-loop algorithm, called DEEPSTORM, that achieves optimal sample complexity for this setting. Unlike double-loop algorithms that require a large batch size to compute the (stochastic) gradient once in a while, DEEPSTORM uses a small batch size, creating advantages in occasions such as streaming data and online learning. This is the first method achieving optimal sample complexity for decentralized nonconvex stochastic composite problems, requiring O(1) batch size. We conduct convergence analysis for DEEPSTORM with both constant and diminishing step sizes. Additionally, under proper initialization and a small enough desired solution error, we show that DEEPSTORM with a constant step size achieves a network-independent sample complexity, with an additional linear speed-up with respect to N over centralized methods. All codes are made available at https://github.com/gmancino/DEEPSTORM",
    "checked": true,
    "id": "0b40fe0a80748daa6096db08648d2f20c7272383",
    "semantic_title": "proximal stochastic recursive momentum methods for nonconvex composite decentralized optimization",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26088": {
    "title": "Online Reinforcement Learning with Uncertain Episode Lengths",
    "volume": "main",
    "abstract": "Existing episodic reinforcement algorithms assume that the length of an episode is fixed across time and known a priori. In this paper, we consider a general framework of episodic reinforcement learning when the length of each episode is drawn from a distribution. We first establish that this problem is equivalent to online reinforcement learning with general discounting where the learner is trying to optimize the expected discounted sum of rewards over an infinite horizon, but where the discounting function is not necessarily geometric. We show that minimizing regret with this new general discounting is equivalent to minimizing regret with uncertain episode lengths. We then design a reinforcement learning algorithm that minimizes regret with general discounting but acts for the setting with uncertain episode lengths. We instantiate our general bound for different types of discounting, including geometric and polynomial discounting. We also show that we can obtain similar regret bounds even when the uncertainty over the episode lengths is unknown, by estimating the unknown distribution over time. Finally, we compare our learning algorithms with existing value-iteration based episodic RL algorithms on a grid-world environment",
    "checked": true,
    "id": "a533ceb0f5c291c06a595728609b4f419a395c89",
    "semantic_title": "online reinforcement learning with uncertain episode lengths",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26089": {
    "title": "Tight Performance Guarantees of Imitator Policies with Continuous Actions",
    "volume": "main",
    "abstract": "Behavioral Cloning (BC) aims at learning a policy that mimics the behavior demonstrated by an expert. The current theoretical understanding of BC is limited to the case of finite actions. In this paper, we study BC with the goal of providing theoretical guarantees on the performance of the imitator policy in the case of continuous actions. We start by deriving a novel bound on the performance gap based on Wasserstein distance, applicable for continuous-action experts, holding under the assumption that the value function is Lipschitz continuous. Since this latter condition is hardy fulfilled in practice, even for Lipschitz Markov Decision Processes and policies, we propose a relaxed setting, proving that value function is always H\\\"older continuous. This result is of independent interest and allows obtaining in BC a general bound for the performance of the imitator policy. Finally, we analyze noise injection, a common practice in which the expert's action is executed in the environment after the application of a noise kernel. We show that this practice allows deriving stronger performance guarantees, at the price of a bias due to the noise addition",
    "checked": true,
    "id": "50b7baed2365f292b67cf95470b5cec170a69715",
    "semantic_title": "tight performance guarantees of imitator policies with continuous actions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26090": {
    "title": "Weight Predictor Network with Feature Selection for Small Sample Tabular Biomedical Data",
    "volume": "main",
    "abstract": "Tabular biomedical data is often high-dimensional but with a very small number of samples. Although recent work showed that well-regularised simple neural networks could outperform more sophisticated architectures on tabular data, they are still prone to overfitting on tiny datasets with many potentially irrelevant features. To combat these issues, we propose Weight Predictor Network with Feature Selection (WPFS) for learning neural networks from high-dimensional and small sample data by reducing the number of learnable parameters and simultaneously performing feature selection. In addition to the classification network, WPFS uses two small auxiliary networks that together output the weights of the first layer of the classification model. We evaluate on nine real-world biomedical datasets and demonstrate that WPFS outperforms other standard as well as more recent methods typically applied to tabular data. Furthermore, we investigate the proposed feature selection mechanism and show that it improves performance while providing useful insights into the learning task",
    "checked": true,
    "id": "830f264dd6d04542abdbcc54488ef491d9bd358a",
    "semantic_title": "weight predictor network with feature selection for small sample tabular biomedical data",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26091": {
    "title": "Learning Revenue Maximization Using Posted Prices for Stochastic Strategic Patient Buyers",
    "volume": "main",
    "abstract": "We consider a seller faced with buyers which have the ability to delay their decision, which we call patience. Each buyer's type is composed of value and patience, and it is sampled i.i.d. from a distribution. The seller, using posted prices, would like to maximize her revenue from selling to the buyer. In this paper, we formalize this setting and characterize the resulting Stackelberg equilibrium, where the seller first commits to her strategy, and then the buyers best respond. Following this, we show how to compute both the optimal pure and mixed strategies. We then consider a learning setting, where the seller does not have access to the distribution over buyer's types. Our main results are the following. We derive a sample complexity bound for the learning of an approximate optimal pure strategy, by computing the fat-shattering dimension of this setting. Moreover, we provide a general sample complexity bound for the approximate optimal mixed strategy. We also consider an online setting and derive a vanishing regret bound with respect to both the optimal pure strategy and the optimal mixed strategy",
    "checked": true,
    "id": "a3a22947eed7be4244ede293d0526f6201b33dc8",
    "semantic_title": "learning revenue maximization using posted prices for stochastic strategic patient buyers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26092": {
    "title": "Boundary Graph Neural Networks for 3D Simulations",
    "volume": "main",
    "abstract": "The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as prediction accuracy of particle flows and mixing entropies. BGNNs are able to accurately reproduce 3D granular flows within simulation uncertainties over hundreds of thousands of simulation timesteps. Most notably, in our experiments, particles stay within the geometric objects without using handcrafted conditions or restrictions",
    "checked": true,
    "id": "28ec0f9b24b2a909a34308f895fa7deecad31be3",
    "semantic_title": "boundary graph neural networks for 3d simulations",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26093": {
    "title": "Diffusion Models Beat GANs on Topology Optimization",
    "volume": "main",
    "abstract": "Structural topology optimization, which aims to find the optimal physical structure that maximizes mechanical performance, is vital in engineering design applications in aerospace, mechanical, and civil engineering. Recently, generative adversarial networks (GANs) have emerged as a popular alternative to traditional iterative topology optimization methods. However, GANs can be challenging to train, have limited generalizability, and often neglect important performance objectives such as mechanical compliance and manufacturability. To address these issues, we propose a new architecture called TopoDiff that uses conditional diffusion models to perform performance-aware and manufacturability-aware topology optimization. Our method introduces a surrogate model-based guidance strategy that actively favors structures with low compliance and good manufacturability. Compared to a state-of-the-art conditional GAN, our approach reduces the average error on physical performance by a factor of eight and produces eleven times fewer infeasible samples. Our work demonstrates the potential of using diffusion models in topology optimization and suggests a general framework for solving engineering optimization problems using external performance with constraint-aware guidance. We provide access to our data, code, and trained models at the following link: https://decode.mit.edu/projects/topodiff/",
    "checked": true,
    "id": "1a1a91f78ec32619a5b2a3b43e0b4d0f7ab97737",
    "semantic_title": "diffusion models beat gans on topology optimization",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26094": {
    "title": "VIDM: Video Implicit Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a powerful generative method for synthesizing high-quality and diverse set of images. In this paper, we propose a video generation method based on diffusion models, where the effects of motion are modeled in an implicit condition manner, i.e. one can sample plausible video motions according to the latent feature of frames. We improve the quality of the generated videos by proposing multiple strategies such as sampling space truncation, robustness penalty, and positional group normalization. Various experiments are conducted on datasets consisting of videos with different resolutions and different number of frames. Results show that the proposed method outperforms the state-of-the-art generative adversarial network-based methods by a significant margin in terms of FVD scores as well as perceptible visual quality",
    "checked": true,
    "id": "13c7b29a100f67d285eb3625c160d06882d4c092",
    "semantic_title": "vidm: video implicit diffusion models",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26095": {
    "title": "Towards Interpreting and Utilizing Symmetry Property in Adversarial Examples",
    "volume": "main",
    "abstract": "In this paper, we identify symmetry property in adversarial scenario by viewing adversarial attack in a fine-grained manner. A newly designed metric called attack proportion, is thus proposed to count the proportion of the adversarial examples misclassified between classes. We observe that the distribution of attack proportion is unbalanced as each class shows vulnerability to particular classes. Further, some class pairs correlate strongly and have the same degree of attack proportion for each other. We call this intriguing phenomenon symmetry property. We empirically prove this phenomenon is widespread and then analyze the reason behind the existence of symmetry property. This explanation, to some extent, could be utilized to understand robust models, which also inspires us to strengthen adversarial defenses",
    "checked": true,
    "id": "31383e9125e10365c6f58cf1dfa6f1e158bc8b4f",
    "semantic_title": "towards interpreting and utilizing symmetry property in adversarial examples",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26096": {
    "title": "The Unreasonable Effectiveness of Deep Evidential Regression",
    "volume": "main",
    "abstract": "There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to discuss corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracted from NNs",
    "checked": true,
    "id": "8b355a9c3ebc035f249407b6cbbc3904b90483fb",
    "semantic_title": "the unreasonable effectiveness of deep evidential regression",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26097": {
    "title": "HyperJump: Accelerating HyperBand via Risk Modelling",
    "volume": "main",
    "abstract": "In the literature on hyper-parameter tuning, a number of recent solutions rely on low-fidelity observations (e.g., training with sub-sampled datasets) to identify promising configurations to be tested via high-fidelity observations (e.g., using the full dataset). Among these, HyperBand is arguably one of the most popular solutions, due to its efficiency and theoretically provable robustness. In this work, we introduce HyperJump, a new approach that builds on HyperBand's robust search strategy and complements it with novel model-based risk analysis techniques that accelerate the search by skipping the evaluation of low risk configurations, i.e., configurations that are likely to be eventually discarded by HyperBand. We evaluate HyperJump on a suite of hyper-parameter optimization problems and show that it provides over one-order of magnitude speed-ups, both in sequential and parallel deployments, on a variety of deep-learning, kernel-based learning and neural architectural search problems when compared to HyperBand and to several state-of-the-art optimizers",
    "checked": true,
    "id": "10a17ea81145a70d0bcece01d4338f7060a9023c",
    "semantic_title": "hyperjump: accelerating hyperband via risk modelling",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26098": {
    "title": "MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series",
    "volume": "main",
    "abstract": "Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy. In addition, a novel upward masking strategy is designed in MHCCL to remove outliers of clusters at each partition to refine prototypes, which helps speed up the hierarchical clustering process and improves the clustering quality. We conduct experimental evaluations on seven widely-used multivariate time series datasets. The results demonstrate the superiority of MHCCL over the state-of-the-art approaches for unsupervised time series representation learning",
    "checked": true,
    "id": "08a103c145772c83f9544c8798dbcc292bdff762",
    "semantic_title": "mhccl: masked hierarchical cluster-wise contrastive learning for multivariate time series",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26099": {
    "title": "Off-Policy Proximal Policy Optimization",
    "volume": "main",
    "abstract": "Proximal Policy Optimization (PPO) is an important reinforcement learning method, which has achieved great success in sequential decision-making problems. However, PPO faces the issue of sample inefficiency, which is due to the PPO cannot make use of off-policy data. In this paper, we propose an Off-Policy Proximal Policy Optimization method (Off-Policy PPO) that improves the sample efficiency of PPO by utilizing off-policy data. Specifically, we first propose a clipped surrogate objective function that can utilize off-policy data and avoid excessively large policy updates. Next, we theoretically clarify the stability of the optimization process of the proposed surrogate objective by demonstrating the degree of policy update distance is consistent with that in the PPO. We then describe the implementation details of the proposed Off-Policy PPO which iteratively updates policies by optimizing the proposed clipped surrogate objective. Finally, the experimental results on representative continuous control tasks validate that our method outperforms the state-of-the-art methods on most tasks",
    "checked": true,
    "id": "24d5bf9d25d1f9f46f0a72e9df522d49d747e446",
    "semantic_title": "off-policy proximal policy optimization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26100": {
    "title": "Information-Theoretic Causal Discovery and Intervention Detection over Multiple Environments",
    "volume": "main",
    "abstract": "Given multiple datasets over a fixed set of random variables, each collected from a different environment, we are interested in discovering the shared underlying causal network and the local interventions per environment, without assuming prior knowledge on which datasets are observational or interventional, and without assuming the shape of the causal dependencies. We formalize this problem using the Algorithmic Model of Causation, instantiate a consistent score via the Minimum Description Length principle, and show under which conditions the network and interventions are identifiable. To efficiently discover causal networks and intervention targets in practice, we introduce the ORION algorithm, which through extensive experiments we show outperforms the state of the art in causal inference over multiple environments",
    "checked": true,
    "id": "0613184b7dbc1793408f89ef133855c676acbc96",
    "semantic_title": "information-theoretic causal discovery and intervention detection over multiple environments",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26101": {
    "title": "AIO-P: Expanding Neural Performance Predictors beyond Image Classification",
    "volume": "main",
    "abstract": "Evaluating neural network performance is critical to deep neural network design but a costly procedure. Neural predictors provide an efficient solution by treating architectures as samples and learning to estimate their performance on a given task. However, existing predictors are task-dependent, predominantly estimating neural network performance on image classification benchmarks. They are also search-space dependent; each predictor is designed to make predictions for a specific architecture search space with predefined topologies and set of operations. In this paper, we propose a novel All-in-One Predictor (AIO-P), which aims to pretrain neural predictors on architecture examples from multiple, separate computer vision (CV) task domains and multiple architecture spaces, and then transfer to unseen downstream CV tasks or neural architectures. We describe our proposed techniques for general graph representation, efficient predictor pretraining and knowledge infusion techniques, as well as methods to transfer to downstream tasks/spaces. Extensive experimental results show that AIO-P can achieve Mean Absolute Error (MAE) and Spearman's Rank Correlation (SRCC) below 1p% and above 0.5, respectively, on a breadth of target downstream CV tasks with or without fine-tuning, outperforming a number of baselines. Moreover, AIO-P can directly transfer to new architectures not seen during training, accurately rank them and serve as an effective performance estimator when paired with an algorithm designed to preserve performance while reducing FLOPs",
    "checked": true,
    "id": "20d00e647349b7147316fa4672451a898e80de86",
    "semantic_title": "aio-p: expanding neural performance predictors beyond image classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26102": {
    "title": "GENNAPE: Towards Generalized Neural Architecture Performance Estimators",
    "volume": "main",
    "abstract": "Predicting neural architecture performance is a challenging task and is crucial to neural architecture design and search. Existing approaches either rely on neural performance predictors which are limited to modeling architectures in a predefined design space involving specific sets of operators and connection rules, and cannot generalize to unseen architectures, or resort to Zero-Cost Proxies which are not always accurate. In this paper, we propose GENNAPE, a Generalized Neural Architecture Performance Estimator, which is pretrained on open neural architecture benchmarks, and aims to generalize to completely unseen architectures through combined innovations in network representation, contrastive pretraining, and a fuzzy clustering-based predictor ensemble. Specifically, GENNAPE represents a given neural network as a Computation Graph (CG) of atomic operations which can model an arbitrary architecture. It first learns a graph encoder via Contrastive Learning to encourage network separation by topological features, and then trains multiple predictor heads, which are soft-aggregated according to the fuzzy membership of a neural network. Experiments show that GENNAPE pretrained on NAS-Bench-101 can achieve superior transferability to 5 different public neural network benchmarks, including NAS-Bench-201, NAS-Bench-301, MobileNet and ResNet families under no or minimum fine-tuning. We further introduce 3 challenging newly labelled neural network benchmarks: HiAML, Inception and Two-Path, which can concentrate in narrow accuracy ranges. Extensive experiments show that GENNAPE can correctly discern high-performance architectures in these families. Finally, when paired with a search algorithm, GENNAPE can find architectures that improve accuracy while reducing FLOPs on three families",
    "checked": true,
    "id": "01b4709a79a52105f8afc05d533548823ca81aaa",
    "semantic_title": "gennape: towards generalized neural architecture performance estimators",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26103": {
    "title": "Adaptive Perturbation-Based Gradient Estimation for Discrete Latent Variable Models",
    "volume": "main",
    "abstract": "The integration of discrete algorithmic components in deep learning architectures has numerous applications. Recently, Implicit Maximum Likelihood Estimation, a class of gradient estimators for discrete exponential family distributions, was proposed by combining implicit differentiation through perturbation with the path-wise gradient estimator. However, due to the finite difference approximation of the gradients, it is especially sensitive to the choice of the finite difference step size, which needs to be specified by the user. In this work, we present Adaptive IMLE (AIMLE), the first adaptive gradient estimator for complex discrete distributions: it adaptively identifies the target distribution for IMLE by trading off the density of gradient information with the degree of bias in the gradient estimates. We empirically evaluate our estimator on synthetic examples, as well as on Learning to Explain, Discrete Variational Auto-Encoders, and Neural Relational Inference tasks. In our experiments, we show that our adaptive gradient estimator can produce faithful estimates while requiring orders of magnitude fewer samples than other gradient estimators",
    "checked": true,
    "id": "6282364a3a0486219e4878225b58ed850660ad4c",
    "semantic_title": "adaptive perturbation-based gradient estimation for discrete latent variable models",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26104": {
    "title": "Why Capsule Neural Networks Do Not Scale: Challenging the Dynamic Parse-Tree Assumption",
    "volume": "main",
    "abstract": "Capsule neural networks replace simple, scalar-valued neurons with vector-valued capsules. They are motivated by the pattern recognition system in the human brain, where complex objects are decomposed into a hierarchy of simpler object parts. Such a hierarchy is referred to as a parse-tree. Conceptually, capsule neural networks have been defined to mimic this behavior. The capsule neural network (CapsNet), by Sabour, Frosst, and Hinton, is the first actual implementation of the conceptual idea of capsule neural networks. CapsNets achieved state-of-the-art performance on simple image recognition tasks with fewer parameters and greater robustness to affine transformations than comparable approaches. This sparked extensive follow-up research. However, despite major efforts, no work was able to scale the CapsNet architecture to more reasonable-sized datasets. Here, we provide a reason for this failure and argue that it is most likely not possible to scale CapsNets beyond toy examples. In particular, we show that the concept of a parse-tree, the main idea behind capsule neuronal networks, is not present in CapsNets. We also show theoretically and experimentally that CapsNets suffer from a vanishing gradient problem that results in the starvation of many capsules during training",
    "checked": true,
    "id": "40e95fe0b847e9fac8253900a7fafccf38c560ba",
    "semantic_title": "why capsule neural networks do not scale: challenging the dynamic parse-tree assumption",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26105": {
    "title": "Multiplex Graph Representation Learning via Common and Private Information Mining",
    "volume": "main",
    "abstract": "Self-supervised multiplex graph representation learning (SMGRL) has attracted increasing interest, but previous SMGRL methods still suffer from the following issues: (i) they focus on the common information only (but ignore the private information in graph structures) to lose some essential characteristics related to downstream tasks, and (ii) they ignore the redundant information in node representations of each graph. To solve these issues, this paper proposes a new SMGRL method by jointly mining the common information and the private information in the multiplex graph while minimizing the redundant information within node representations. Specifically, the proposed method investigates the decorrelation losses to extract the common information and minimize the redundant information, while investigating the reconstruction losses to maintain the private information. Comprehensive experimental results verify the superiority of the proposed method, on four public benchmark datasets",
    "checked": true,
    "id": "c55d0c529b758c3a7112f3f9464ce4c281761548",
    "semantic_title": "multiplex graph representation learning via common and private information mining",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26106": {
    "title": "Fundamentals of Task-Agnostic Data Valuation",
    "volume": "main",
    "abstract": "We study valuing the data of a data owner/seller for a data seeker/buyer. Data valuation is often carried out for a specific task assuming a particular utility metric, such as test accuracy on a validation set, that may not exist in practice. In this work, we focus on task-agnostic data valuation without any validation requirements. The data buyer has access to a limited amount of data (which could be publicly available) and seeks more data samples from a data seller. We formulate the problem as estimating the differences in the statistical properties of the data at the seller with respect to the baseline data available at the buyer. We capture these statistical differences through second moment by measuring diversity and relevance of the seller's data for the buyer; we estimate these measures through queries to the seller without requesting the raw data. We design the queries with the proposed approach so that the seller is blind to the buyer's raw data and has no knowledge to fabricate responses to the queries to obtain a desired outcome of the diversity and relevance trade-off. We will show through extensive experiments on real tabular and image datasets that the proposed estimates capture the diversity and relevance of the seller's data for the buyer",
    "checked": true,
    "id": "b8a070507b4cdcf4cc67b1e21aa7005fe2611c33",
    "semantic_title": "fundamentals of task-agnostic data valuation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26107": {
    "title": "Exploring the Interaction between Local and Global Latent Configurations for Clustering Single-Cell RNA-Seq: A Unified Perspective",
    "volume": "main",
    "abstract": "The most recent approaches for clustering single-cell RNA-sequencing data rely on deep auto-encoders. However, three major challenges remain unaddressed. First, current models overlook the impact of the cumulative errors induced by the pseudo-supervised embedding clustering task (Feature Randomness). Second, existing methods neglect the effect of the strong competition between embedding clustering and reconstruction (Feature Drift). Third, the previous deep clustering models regularly fail to consider the topological information of the latent data, even though the local and global latent configurations can bring complementary views to the clustering task. To address these challenges, we propose a novel approach that explores the interaction between local and global latent configurations to progressively adjust the reconstruction and embedding clustering tasks. We elaborate a topological and probabilistic filter to mitigate Feature Randomness and a cell-cell graph structure and content correction mechanism to counteract Feature Drift. The Zero-Inflated Negative Binomial model is also integrated to capture the characteristics of gene expression profiles. We conduct detailed experiments on real-world datasets from multiple representative genome sequencing platforms. Our approach outperforms the state-of-the-art clustering methods in various evaluation metrics",
    "checked": true,
    "id": "aceb297295bb86f617eaed59e9a9eb39c839ebb9",
    "semantic_title": "exploring the interaction between local and global latent configurations for clustering single-cell rna-seq: a unified perspective",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26108": {
    "title": "Corruption-Tolerant Algorithms for Generalized Linear Models",
    "volume": "main",
    "abstract": "This paper presents SVAM (Sequential Variance-Altered MLE), a unified framework for learning generalized linear models under adversarial label corruption in training data. SVAM extends to tasks such as least squares regression, logistic regression, and gamma regression, whereas many existing works on learning with label corruptions focus only on least squares regression. SVAM is based on a novel variance reduction technique that may be of independent interest and works by iteratively solving weighted MLEs over variance-altered versions of the GLM objective. SVAM offers provable model recovery guarantees superior to the state-of-the-art for robust regression even when a constant fraction of training labels are adversarially corrupted. SVAM also empirically outperforms several existing problem-specific techniques for robust regression and classification. Code for SVAM is available at https://github.com/purushottamkar/svam/",
    "checked": true,
    "id": "2eb9e99cc848db3c4715d7e0161ea34146847fe1",
    "semantic_title": "corruption-tolerant algorithms for generalized linear models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26109": {
    "title": "Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization",
    "volume": "main",
    "abstract": "In the sequential decision making setting, an agent aims to achieve systematic generalization over a large, possibly infinite, set of environments. Such environments are modeled as discrete Markov decision processes with both states and actions represented through a feature vector. The underlying structure of the environments allows the transition dynamics to be factored into two components: one that is environment-specific and another that is shared. Consider a set of environments that share the laws of motion as an example. In this setting, the agent can take a finite amount of reward-free interactions from a subset of these environments. The agent then must be able to approximately solve any planning task defined over any environment in the original set, relying on the above interactions only. Can we design a provably efficient algorithm that achieves this ambitious goal of systematic generalization? In this paper, we give a partially positive answer to this question. First, we provide a tractable formulation of systematic generalization by employing a causal viewpoint. Then, under specific structural assumptions, we provide a simple learning algorithm that guarantees any desired planning error up to an unavoidable sub-optimality term, while showcasing a polynomial sample complexity",
    "checked": true,
    "id": "aa4ca2d92b79d19bd938748d1233f7dc58195955",
    "semantic_title": "provably efficient causal model-based reinforcement learning for systematic generalization",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26110": {
    "title": "Mean Estimation of Truncated Mixtures of Two Gaussians: A Gradient Based Approach",
    "volume": "main",
    "abstract": "Even though data is abundant, it is often subjected to some form of censoring or truncation which inherently creates biases. Removing such biases and performing parameter estimation is a classical challenge in Statistics. In this paper, we focus on the problem of estimating the means of a mixture of two balanced d-dimensional Gaussians when the samples are prone to truncation. A recent theoretical study on the performance of the Expectation-Maximization (EM) algorithm for the aforementioned problem showed EM almost surely converges for d=1 and exhibits local convergence for d>1 to the true means. Nevertheless, the EM algorithm for the case of truncated mixture of two Gaussians is not easy to implement as it requires solving a set of nonlinear equations at every iteration which makes the algorithm impractical. In this work, we propose a gradient based variant of the EM algorithm that has global convergence guarantees when d=1 and local convergence for d>1 to the true means. Moreover, the update rule at every iteration is easy to compute which makes the proposed method practical. We also provide numerous experiments to obtain more insights into the effect of truncation on the convergence to the true parameters in high dimensions",
    "checked": true,
    "id": "395471bcd2fa219a58965edf342949749b94fba2",
    "semantic_title": "mean estimation of truncated mixtures of two gaussians: a gradient based approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26111": {
    "title": "An Operator Theoretic Approach for Analyzing Sequence Neural Networks",
    "volume": "main",
    "abstract": "Analyzing the inner mechanisms of deep neural networks is a fundamental task in machine learning. Existing work provides limited analysis or it depends on local theories, such as fixed-point analysis. In contrast, we propose to analyze trained neural networks using an operator theoretic approach which is rooted in Koopman theory, the Koopman Analysis of Neural Networks (KANN). Key to our method is the Koopman operator, which is a linear object that globally represents the dominant behavior of the network dynamics. The linearity of the Koopman operator facilitates analysis via its eigenvectors and eigenvalues. Our method reveals that the latter eigendecomposition holds semantic information related to the neural network inner workings. For instance, the eigenvectors highlight positive and negative n-grams in the sentiments analysis task; similarly, the eigenvectors capture the salient features of healthy heart beat signals in the ECG classification problem",
    "checked": true,
    "id": "d96c0d6c609f460f6761fe56113a461be719989b",
    "semantic_title": "an operator theoretic approach for analyzing sequence neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26112": {
    "title": "Do Invariances in Deep Neural Networks Align with Human Perception?",
    "volume": "main",
    "abstract": "An evaluation criterion for safe and trustworthy deep learning is how well the invariances captured by representations of deep neural networks (DNNs) are shared with humans. We identify challenges in measuring these invariances. Prior works used gradient-based methods to generate identically represented inputs (IRIs), ie, inputs which have identical representations (on a given layer) of a neural network, and thus capture invariances of a given network. One necessary criterion for a network's invariances to align with human perception is for its IRIs look 'similar' to humans. Prior works, however, have mixed takeaways; some argue that later layers of DNNs do not learn human-like invariances yet others seem to indicate otherwise. We argue that the loss function used to generate IRIs can heavily affect takeaways about invariances of the network and is the primary reason for these conflicting findings. We propose an adversarial regularizer on the IRI generation loss that finds IRIs that make any model appear to have very little shared invariance with humans. Based on this evidence, we argue that there is scope for improving models to have human-like invariances, and further, to have meaningful comparisons between models one should use IRIs generated using the regularizer-free loss. We then conduct an in-depth investigation of how different components (eg architectures, training losses, data augmentations) of the deep learning pipeline contribute to learning models that have good alignment with humans. We find that architectures with residual connections trained using a (self-supervised) contrastive loss with l_p ball adversarial data augmentation tend to learn invariances that are most aligned with humans. Code: github.com/nvedant07/Human-NN-Alignment",
    "checked": true,
    "id": "3d7003050ece3dec9b9104917ac97d440081e70a",
    "semantic_title": "do invariances in deep neural networks align with human perception?",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26113": {
    "title": "Counterfactual Learning with General Data-Generating Policies",
    "volume": "main",
    "abstract": "Off-policy evaluation (OPE) attempts to predict the performance of counterfactual policies using log data from a different policy. We extend its applicability by developing an OPE method for a class of both full support and deficient support logging policies in contextual-bandit settings. This class includes deterministic bandit (such as Upper Confidence Bound) as well as deterministic decision-making based on supervised and unsupervised learning. We prove that our method's prediction converges in probability to the true performance of a counterfactual policy as the sample size increases. We validate our method with experiments on partly and entirely deterministic logging policies. Finally, we apply it to evaluate coupon targeting policies by a major online platform and show how to improve the existing policy",
    "checked": true,
    "id": "83ac9832f81f2e2caa9a3620d8f0f8822e54eb8d",
    "semantic_title": "counterfactual learning with general data-generating policies",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26114": {
    "title": "Efficient and Accurate Learning of Mixtures of Plackett-Luce Models",
    "volume": "main",
    "abstract": "Mixture models of Plackett-Luce (PL), one of the most fundamental ranking models, are an active research area of both theoretical and practical significance. Most previously proposed parameter estimation algorithms instantiate the EM algorithm, often with random initialization. However, such an initialization scheme may not yield a good initial estimate and the algorithms require multiple restarts, incurring a large time complexity. As for the EM procedure, while the E-step can be performed efficiently, maximizing the log-likelihood in the M-step is difficult due to the combinatorial nature of the PL likelihood function. Therefore, previous authors favor algorithms that maximize surrogate likelihood functions. However, the final estimate may deviate from the true maximum likelihood estimate as a consequence. In this paper, we address these known limitations. We propose an initialization algorithm that can provide a provably accurate initial estimate and an EM algorithm that maximizes the true log-likelihood function efficiently. Experiments on both synthetic and real datasets show that our algorithm is competitive in terms of accuracy and speed to baseline algorithms, especially on datasets with a large number of items",
    "checked": true,
    "id": "b10dbb26277dd57374b5274929a9f10871d1130c",
    "semantic_title": "efficient and accurate learning of mixtures of plackett-luce models",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26115": {
    "title": "Behavioral Learning in Security Games: Threat of Multi-Step Manipulative Attacks",
    "volume": "main",
    "abstract": "This paper studies the problem of multi-step manipulative attacks in Stackelberg security games, in which a clever attacker attempts to orchestrate its attacks over multiple time steps to mislead the defender's learning of the attacker's behavior. This attack manipulation eventually influences the defender's patrol strategy towards the attacker's benefit. Previous work along this line of research only focuses on one-shot games in which the defender learns the attacker's behavior and then designs a corresponding strategy only once. Our work, on the other hand, investigates the long-term impact of the attacker's manipulation in which current attack and defense choices of players determine the future learning and patrol planning of the defender. This paper has three key contributions. First, we introduce a new multi-step manipulative attack game model that captures the impact of sequential manipulative attacks carried out by the attacker over the entire time horizon. Second, we propose a new algorithm to compute an optimal manipulative attack plan for the attacker, which tackles the challenge of multiple connected optimization components involved in the computation across multiple time steps. Finally, we present extensive experimental results on the impact of such misleading attacks, showing a significant benefit for the attacker and loss for the defender",
    "checked": true,
    "id": "86984f5d5b99cfe975c251098749c8b247184c27",
    "semantic_title": "behavioral learning in security games: threat of multi-step manipulative attacks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26116": {
    "title": "On Instance-Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation",
    "volume": "main",
    "abstract": "Sample-efficient offline reinforcement learning (RL) with linear function approximation has been studied extensively recently. Much of the prior work has yielded instance-independent rates that hold even for the worst-case realization of problem instances. This work seeks to understand instance-dependent bounds for offline RL with linear function approximation. We present an algorithm called Bootstrapped and Constrained Pessimistic Value Iteration (BCP-VI), which leverages data bootstrapping and constrained optimization on top of pessimism. We show that under a partial data coverage assumption, that of concentrability with respect to an optimal policy, the proposed algorithm yields a fast rate for offline RL when there is a positive gap in the optimal Q-value functions, even if the offline data were collected adaptively. Moreover, when the linear features of the optimal actions in the states reachable by an optimal policy span those reachable by the behavior policy and the optimal actions are unique, offline RL achieves absolute zero sub-optimality error when the number of episodes exceeds a (finite) instance-dependent threshold. To the best of our knowledge, these are the first results that give a fast rate bound on the sub-optimality and an absolute zero sub-optimality bound for offline RL with linear function approximation from adaptive data with partial coverage. We also provide instance-agnostic and instance-dependent information-theoretical lower bounds to complement our upper bounds",
    "checked": true,
    "id": "b61a3d718a192e39a437d32a6ed4037b8c29cc41",
    "semantic_title": "on instance-dependent bounds for offline reinforcement learning with linear function approximation",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26117": {
    "title": "Fast Saturating Gate for Learning Long Time Scales with Recurrent Neural Networks",
    "volume": "main",
    "abstract": "Gate functions in recurrent models, such as an LSTM and GRU, play a central role in learning various time scales in modeling time series data by using a bounded activation function. However, it is difficult to train gates to capture extremely long time scales due to gradient vanishing of the bounded function for large inputs, which is known as the saturation problem. We closely analyze the relation between saturation of the gate function and efficiency of the training. We prove that the gradient vanishing of the gate function can be mitigated by accelerating the convergence of the saturating function, i.e., making the output of the function converge to 0 or 1 faster. Based on the analysis results, we propose a gate function called fast gate that has a doubly exponential convergence rate with respect to inputs by simple function composition. We empirically show that our method outperforms previous methods in accuracy and computational efficiency on benchmark tasks involving extremely long time scales",
    "checked": true,
    "id": "bf979c9598ea185a2b436c52dce0da6382e14f24",
    "semantic_title": "fast saturating gate for learning long time scales with recurrent neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26118": {
    "title": "Backpropagation-Free Deep Learning with Recursive Local Representation Alignment",
    "volume": "main",
    "abstract": "Training deep neural networks on large-scale datasets requires significant hardware resources whose costs (even on cloud platforms) put them out of reach of smaller organizations, groups, and individuals. Backpropagation (backprop), the workhorse for training these networks, is an inherently sequential process that is difficult to parallelize. Furthermore, researchers must continually develop various specialized techniques, such as particular weight initializations and enhanced activation functions, to ensure stable parameter optimization. Our goal is to seek an effective, neuro-biologically plausible alternative to backprop that can be used to train deep networks. In this paper, we propose a backprop-free procedure, recursive local representation alignment, for training large-scale architectures. Experiments with residual networks on CIFAR-10 and the large benchmark, ImageNet, show that our algorithm generalizes as well as backprop while converging sooner due to weight updates that are parallelizable and computationally less demanding. This is empirical evidence that a backprop-free algorithm can scale up to larger datasets",
    "checked": true,
    "id": "5ea7d9d34cbe1b6c33c2f1677d577319ed658f16",
    "semantic_title": "backpropagation-free deep learning with recursive local representation alignment",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26119": {
    "title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration & Planning",
    "volume": "main",
    "abstract": "We study the problem of episodic reinforcement learning in continuous state-action spaces with unknown rewards and transitions. Specifically, we consider the setting where the rewards and transitions are modeled using parametric bilinear exponential families. We propose an algorithm, that a) uses penalized maximum likelihood estimators to learn the unknown parameters, b) injects a calibrated Gaussian noise in the parameter of rewards to ensure exploration, and c) leverages linearity of the bilinear exponential family transitions with respect to an underlying RKHS to perform tractable planning. We provide a frequentist regret upper-bound for our algorithm which, in the case of tabular MDPs, is order-optimal with respect to H and K, where H is the episode length and K is the number of episodes. Our analysis improves the existing bounds for the bilinear exponential family of MDPs by square root of H and removes the handcrafted clipping deployed in existing RLSVI-type algorithms",
    "checked": false,
    "id": "b61c60d75f25c8b8cf23131f82d6efaced2ad3cc",
    "semantic_title": "bilinear exponential family of mdps: frequentist regret bound with tractable exploration and planning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26120": {
    "title": "H-TSP: Hierarchically Solving the Large-Scale Traveling Salesman Problem",
    "volume": "main",
    "abstract": "We propose an end-to-end learning framework based on hierarchical reinforcement learning, called H-TSP, for addressing the large-scale Traveling Salesman Problem (TSP). The proposed H-TSP constructs a solution of a TSP instance starting from the scratch relying on two components: the upper-level policy chooses a small subset of nodes (up to 200 in our experiment) from all nodes that are to be traversed, while the lower-level policy takes the chosen nodes as input and outputs a tour connecting them to the existing partial route (initially only containing the depot). After jointly training the upper-level and lower-level policies, our approach can directly generate solutions for the given TSP instances without relying on any time-consuming search procedures. To demonstrate effectiveness of the proposed approach, we have conducted extensive experiments on randomly generated TSP instances with different numbers of nodes. We show that H-TSP can achieve comparable results (gap 3.42% vs. 7.32%) as SOTA search-based approaches, and more importantly, we reduce the time consumption up to two orders of magnitude (3.32s vs. 395.85s). To the best of our knowledge, H-TSP is the first end-to-end deep reinforcement learning approach that can scale to TSP instances of up to 10000 nodes. Although there are still gaps to SOTA results with respect to solution quality, we believe that H-TSP will be useful for practical applications, particularly those that are time-sensitive e.g., on-call routing and ride hailing service",
    "checked": false,
    "id": "0973b3c792ee4ee8fc4a22e3b6bc6cd9da9bc98c",
    "semantic_title": "h-tsp: hierarchically solving the large-scale travelling salesman problem",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26121": {
    "title": "Ising-Traffic: Using Ising Machine Learning to Predict Traffic Congestion under Uncertainty",
    "volume": "main",
    "abstract": "This paper addresses the challenges in accurate and real-time traffic congestion prediction under uncertainty by proposing Ising-Traffic, a dual-model Ising-based traffic prediction framework that delivers higher accuracy and lower latency than SOTA solutions. While traditional solutions face the dilemma from the trade-off between algorithm complexity and computational efficiency, our Ising-based method breaks away from the trade-off leveraging the Ising model's strong expressivity and the Ising machine's strong computation power. In particular, Ising-Traffic formulates traffic prediction under uncertainty into two Ising models: Reconstruct-Ising and Predict-Ising. Reconstruct-Ising is mapped onto modern Ising machines and handles uncertainty in traffic accurately with negligible latency and energy consumption, while Predict-Ising is mapped onto traditional processors and predicts future congestion precisely with only at most 1.8% computational demands of existing solutions. Our evaluation shows Ising-Traffic delivers on average 98X speedups and 5% accuracy improvement over SOTA",
    "checked": true,
    "id": "3f73e7838bddfd67a265dca2dcc0492091ec5449",
    "semantic_title": "ising-traffic: using ising machine learning to predict traffic congestion under uncertainty",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26122": {
    "title": "FedMDFG: Federated Learning with Multi-Gradient Descent and Fair Guidance",
    "volume": "main",
    "abstract": "Fairness has been considered as a critical problem in federated learning (FL). In this work, we analyze two direct causes of unfairness in FL - an unfair direction and an improper step size when updating the model. To solve these issues, we introduce an effective way to measure fairness of the model through the cosine similarity, and then propose a federated multiple gradient descent algorithm with fair guidance (FedMDFG) to drive the model fairer. We first convert FL into a multi-objective optimization problem (MOP) and design an advanced multiple gradient descent algorithm to calculate a fair descent direction by adding a fair-driven objective to MOP. A low-communication-cost line search strategy is then designed to find a better step size for the model update. We further show the theoretical analysis on how it can enhance fairness and guarantee the convergence. Finally, extensive experiments in several FL scenarios verify that FedMDFG is robust and outperforms the SOTA FL algorithms in convergence and fairness. The source code is available at https://github.com/zibinpan/FedMDFG",
    "checked": true,
    "id": "e92224b72b14ea4cf8db9801ad328f0bc65f0625",
    "semantic_title": "fedmdfg: federated learning with multi-gradient descent and fair guidance",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26123": {
    "title": "Geometric Inductive Biases for Identifiable Unsupervised Learning of Disentangled Representations",
    "volume": "main",
    "abstract": "The model identifiability is a considerable issue in the unsupervised learning of disentangled representations. The PCA inductive biases revealed recently for unsupervised disentangling in VAE-based models are shown to improve local alignment of latent dimensions with principal components of the data. In this paper, in additional to the PCA inductive biases, we propose novel geometric inductive biases from the manifold perspective for unsupervised disentangling, which induce the model to capture the global geometric properties of the data manifold with guaranteed model identifiability. We also propose a Geometric Disentangling Regularized AutoEncoder (GDRAE) that combines the PCA and the proposed geometric inductive biases in one unified framework. The experimental results show the usefulness of the geometric inductive biases in unsupervised disentangling and the effectiveness of our GDRAE in capturing the geometric inductive biases",
    "checked": true,
    "id": "fb2db75f2ee31bdd138543762383cdadbc524213",
    "semantic_title": "geometric inductive biases for identifiable unsupervised learning of disentangled representations",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26124": {
    "title": "Isometric Manifold Learning Using Hierarchical Flow",
    "volume": "main",
    "abstract": "We propose the Hierarchical Flow (HF) model constrained by isometric regularizations for manifold learning that combines manifold learning goals such as dimensionality reduction, inference, sampling, projection and density estimation into one unified framework. Our proposed HF model is regularized to not only produce embeddings preserving the geometric structure of the manifold, but also project samples onto the manifold in a manner conforming to the rigorous definition of projection. Theoretical guarantees are provided for our HF model to satisfy the two desired properties. In order to detect the real dimensionality of the manifold, we also propose a two-stage dimensionality reduction algorithm, which is a time-efficient algorithm thanks to the hierarchical architecture design of our HF model. Experimental results justify our theoretical analysis, demonstrate the superiority of our dimensionality reduction algorithm in cost of training time, and verify the effect of the aforementioned properties in improving performances on downstream tasks such as anomaly detection",
    "checked": true,
    "id": "558e966b86efc2ae667115c1f769bab5955c1a1a",
    "semantic_title": "isometric manifold learning using hierarchical flow",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26125": {
    "title": "Evidential Conditional Neural Processes",
    "volume": "main",
    "abstract": "The Conditional Neural Process (CNP) family of models offer a promising direction to tackle few-shot problems by achieving better scalability and competitive predictive performance. However, the current CNP models only capture the overall uncertainty for the prediction made on a target data point. They lack a systematic fine-grained quantification on the distinct sources of uncertainty that are essential for model training and decision-making under the few-shot setting. We propose Evidential Conditional Neural Processes (ECNP), which replace the standard Gaussian distribution used by CNP with a much richer hierarchical Bayesian structure through evidential learning to achieve epistemic-aleatoric uncertainty decomposition. The evidential hierarchical structure also leads to a theoretically justified robustness over noisy training tasks. Theoretical analysis on the proposed ECNP establishes the relationship with CNP while offering deeper insights on the roles of the evidential parameters. Extensive experiments conducted on both synthetic and real-world data demonstrate the effectiveness of our proposed model in various few-shot settings",
    "checked": true,
    "id": "a116fe984ed47fe320fc78a94ed72fa6e5e9e915",
    "semantic_title": "evidential conditional neural processes",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26126": {
    "title": "Balanced Column-Wise Block Pruning for Maximizing GPU Parallelism",
    "volume": "main",
    "abstract": "Pruning has been an effective solution to reduce the number of computations and the memory requirement in deep learning. The pruning unit plays an important role in exploiting the GPU resources efficiently. The filter is proposed as a simple pruning unit of structured pruning. However, since the filter is quite large as pruning unit, the accuracy drop is considerable with a high pruning ratio. GPU rearranges the weight and input tensors into tiles (blocks) for efficient computation. To fully utilize GPU resources, this tile structure should be considered, which is the goal of block pruning. However, previous block pruning prunes both row vectors and column vectors. Pruning of row vectors in a tile corresponds to filter pruning, and it also interferes with column-wise block pruning of the following layer. In contrast, column vectors are much smaller than row vectors and can achieve lower accuracy drop. Additionally, if the pruning ratio for each tile is different, GPU utilization can be limited by imbalanced workloads by irregular-sized blocks. The same pruning ratio for the weight tiles processed in parallel enables the actual inference process to fully utilize the resources without idle time. This paper proposes balanced column-wise block pruning, named BCBP, to satisfy two conditions: the column-wise minimal size of the pruning unit and balanced workloads. We demonstrate that BCBP is superior to previous pruning methods through comprehensive experiments",
    "checked": true,
    "id": "6872c4cf5268a64f897cf3bcd2af0a5e1e0210e0",
    "semantic_title": "balanced column-wise block pruning for maximizing gpu parallelism",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26127": {
    "title": "Dynamic Structure Pruning for Compressing CNNs",
    "volume": "main",
    "abstract": "Structure pruning is an effective method to compress and accelerate neural networks. While filter and channel pruning are preferable to other structure pruning methods in terms of realistic acceleration and hardware compatibility, pruning methods with a finer granularity, such as intra-channel pruning, are expected to be capable of yielding more compact and computationally efficient networks. Typical intra-channel pruning methods utilize a static and hand-crafted pruning granularity due to a large search space, which leaves room for improvement in their pruning performance. In this work, we introduce a novel structure pruning method, termed as dynamic structure pruning, to identify optimal pruning granularities for intra-channel pruning. In contrast to existing intra-channel pruning methods, the proposed method automatically optimizes dynamic pruning granularities in each layer while training deep neural networks. To achieve this, we propose a differentiable group learning method designed to efficiently learn a pruning granularity based on gradient-based learning of filter groups. The experimental results show that dynamic structure pruning achieves state-of-the-art pruning performance and better realistic acceleration on a GPU compared with channel pruning. In particular, it reduces the FLOPs of ResNet50 by 71.85% without accuracy degradation on the ImageNet dataset. Our code is available at https://github.com/irishev/DSP",
    "checked": true,
    "id": "9a58c40f637c8a7441553fe896abc22dedeceed8",
    "semantic_title": "dynamic structure pruning for compressing cnns",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26128": {
    "title": "Scaling Marginalized Importance Sampling to High-Dimensional State-Spaces via State Abstraction",
    "volume": "main",
    "abstract": "We consider the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of an evaluation policy, pie, using a fixed dataset, D, collected by one or more policies that may be different from pie. Current OPE algorithms may produce poor OPE estimates under policy distribution shift i.e., when the probability of a particular state-action pair occurring under pie is very different from the probability of that same pair occurring in D. In this work, we propose to improve the accuracy of OPE estimators by projecting the high-dimensional state-space into a low-dimensional state-space using concepts from the state abstraction literature. Specifically, we consider marginalized importance sampling (MIS) OPE algorithms which compute state-action distribution correction ratios to produce their OPE estimate. In the original ground state-space, these ratios may have high variance which may lead to high variance OPE. However, we prove that in the lower-dimensional abstract state-space the ratios can have lower variance resulting in lower variance OPE. We then highlight the challenges that arise when estimating the abstract ratios from data, identify sufficient conditions to overcome these issues, and present a minimax optimization problem whose solution yields these abstract ratios. Finally, our empirical evaluation on difficult, high-dimensional state-space OPE tasks shows that the abstract ratios can make MIS OPE estimators achieve lower mean-squared error and more robust to hyperparameter tuning than the ground ratios",
    "checked": true,
    "id": "c6f39c3b73e5966730362bc757f44fe127b4390d",
    "semantic_title": "scaling marginalized importance sampling to high-dimensional state-spaces via state abstraction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26129": {
    "title": "Conceptual Reinforcement Learning for Language-Conditioned Tasks",
    "volume": "main",
    "abstract": "Despite the broad application of deep reinforcement learning (RL), transferring and adapting the policy to unseen but similar environments is still a significant challenge. Recently, the language-conditioned policy is proposed to facilitate policy transfer through learning the joint representation of observation and text that catches the compact and invariant information across various environments. Existing studies of language-conditioned RL methods often learn the joint representation as a simple latent layer for the given instances (episode-specific observation and text), which inevitably includes noisy or irrelevant information and cause spurious correlations that are dependent on instances, thus hurting generalization performance and training efficiency. To address the above issue, we propose a conceptual reinforcement learning (CRL) framework to learn the concept-like joint representation for language-conditioned policy. The key insight is that concepts are compact and invariant representations in human cognition through extracting similarities from numerous instances in real-world. In CRL, we propose a multi-level attention encoder and two mutual information constraints for learning compact and invariant concepts. Verified in two challenging environments, RTFM and Messenger, CRL significantly improves the training efficiency (up to 70%) and generalization ability (up to 30%) to the new environment dynamics",
    "checked": true,
    "id": "bc8fc93514b753adda5214528486361d681892a6",
    "semantic_title": "conceptual reinforcement learning for language-conditioned tasks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26130": {
    "title": "Weighted Policy Constraints for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) aims to learn policy from the passively collected offline dataset. Applying existing RL methods on the static dataset straightforwardly will raise distribution shift, causing these unconstrained RL methods to fail. To cope with the distribution shift problem, a common practice in offline RL is to constrain the policy explicitly or implicitly close to behavioral policy. However, the available dataset usually contains sub-optimal or inferior actions, constraining the policy near all these actions will make the policy inevitably learn inferior behaviors, limiting the performance of the algorithm. Based on this observation, we propose a weighted policy constraints (wPC) method that only constrains the learned policy to desirable behaviors, making room for policy improvement on other parts. Our algorithm outperforms existing state-of-the-art offline RL algorithms on the D4RL offline gym datasets. Moreover, the proposed algorithm is simple to implement with few hyper-parameters, making the proposed wPC algorithm a robust offline RL method with low computational complexity",
    "checked": true,
    "id": "cf07c1a9ec10aa546384fbd0cd62b793c3c6b978",
    "semantic_title": "weighted policy constraints for offline reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26131": {
    "title": "Latent Autoregressive Source Separation",
    "volume": "main",
    "abstract": "Autoregressive models have achieved impressive results over a wide range of domains in terms of generation quality and downstream task performance. In the continuous domain, a key factor behind this success is the usage of quantized latent spaces (e.g., obtained via VQ-VAE autoencoders), which allow for dimensionality reduction and faster inference times. However, using existing pre-trained models to perform new non-trivial tasks is difficult since it requires additional fine-tuning or extensive training to elicit prompting. This paper introduces LASS as a way to perform vector-quantized Latent Autoregressive Source Separation (i.e., de-mixing an input signal into its constituent sources) without requiring additional gradient-based optimization or modifications of existing models. Our separation method relies on the Bayesian formulation in which the autoregressive models are the priors, and a discrete (non-parametric) likelihood function is constructed by performing frequency counts over latent sums of addend tokens. We test our method on images and audio with several sampling strategies (e.g., ancestral, beam search) showing competitive results with existing approaches in terms of separation quality while offering at the same time significant speedups in terms of inference time and scalability to higher dimensional data",
    "checked": true,
    "id": "03d3b8e2a746b4d375c0d0de9ea43d4a37f4b0bd",
    "semantic_title": "latent autoregressive source separation",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26132": {
    "title": "Explaining Random Forests Using Bipolar Argumentation and Markov Networks",
    "volume": "main",
    "abstract": "Random forests are decision tree ensembles that can be used to solve a variety of machine learning problems. However, as the number of trees and their individual size can be large, their decision making process is often incomprehensible. We show that their decision process can be naturally represented as an argumentation problem, which allows creating global explanations via argumentative reasoning. We generalize sufficient and necessary argumentative explanations using a Markov network encoding, discuss the relevance of these explanations and establish relationships to families of abductive explanations from the literature. As the complexity of the explanation problems is high, we present an efficient approximation algorithm with probabilistic approximation guarantees",
    "checked": false,
    "id": "df6e8c416a0e9ba1f12421b3af34e277e6579728",
    "semantic_title": "explaining random forests using bipolar argumentation and markov networks (technical report)",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26133": {
    "title": "A Model-Agnostic Heuristics for Selective Classification",
    "volume": "main",
    "abstract": "Selective classification (also known as classification with reject option) conservatively extends a classifier with a selection function to determine whether or not a prediction should be accepted (i.e., trusted, used, deployed). This is a highly relevant issue in socially sensitive tasks, such as credit scoring. State-of-the-art approaches rely on Deep Neural Networks (DNNs) that train at the same time both the classifier and the selection function. These approaches are model-specific and computationally expensive. We propose a model-agnostic approach, as it can work with any base probabilistic binary classification algorithm, and it can be scalable to large tabular datasets if the base classifier is so. The proposed algorithm, called SCROSS, exploits a cross-fitting strategy and theoretical results for quantile estimation to build the selection function. Experiments on real-world data show that SCROSS improves over existing methods",
    "checked": true,
    "id": "4ddaaf045afc618f3a1c849d6a088b7982e8477a",
    "semantic_title": "a model-agnostic heuristics for selective classification",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26134": {
    "title": "Experimental Observations of the Topology of Convolutional Neural Network Activations",
    "volume": "main",
    "abstract": "Topological data analysis (TDA) is a branch of computational mathematics, bridging algebraic topology and data science, that provides compact, noise-robust representations of complex structures. Deep neural networks (DNNs) learn millions of parameters associated with a series of transformations defined by the model architecture resulting in high-dimensional, difficult to interpret internal representations of input data. As DNNs become more ubiquitous across multiple sectors of our society, there is increasing recognition that mathematical methods are needed to aid analysts, researchers, and practitioners in understanding and interpreting how these models' internal representations relate to the final classification. In this paper we apply cutting edge techniques from TDA with the goal of gaining insight towards interpretability of convolutional neural networks used for image classification. We use two common TDA approaches to explore several methods for modeling hidden layer activations as high-dimensional point clouds, and provide experimental evidence that these point clouds capture valuable structural information about the model's process. First, we demonstrate that a distance metric based on persistent homology can be used to quantify meaningful differences between layers and discuss these distances in the broader context of existing representational similarity metrics for neural network interpretability. Second, we show that a mapper graph can provide semantic insight as to how these models organize hierarchical class knowledge at each layer. These observations demonstrate that TDA is a useful tool to help deep learning practitioners unlock the hidden structures of their models",
    "checked": true,
    "id": "da55906dec5bd68bd45b2a0806e1d92a35fa0344",
    "semantic_title": "experimental observations of the topology of convolutional neural network activations",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26135": {
    "title": "CMVAE: Causal Meta VAE for Unsupervised Meta-Learning",
    "volume": "main",
    "abstract": "Unsupervised meta-learning aims to learn the meta knowledge from unlabeled data and rapidly adapt to novel tasks. However, existing approaches may be misled by the context-bias (e.g. background) from the training data. In this paper, we abstract the unsupervised meta-learning problem into a Structural Causal Model (SCM) and point out that such bias arises due to hidden confounders. To eliminate the confounders, we define the priors are conditionally independent, learn the relationships between priors and intervene on them with casual factorization. Furthermore, we propose Causal Meta VAE (CMVAE) that encodes the priors into latent codes in the causal space and learns their relationships simultaneously to achieve the downstream few-shot image classification task. Results on toy datasets and three benchmark datasets demonstrate that our method can remove the context-bias and it outperforms other state-of-the-art unsupervised meta-learning algorithms because of bias-removal. Code is available at https://github.com/GuodongQi/CMVAE",
    "checked": true,
    "id": "f3c276c23f48d5f373346d5668045736f8bb4a06",
    "semantic_title": "cmvae: causal meta vae for unsupervised meta-learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26136": {
    "title": "Rethinking Data-Free Quantization as a Zero-Sum Game",
    "volume": "main",
    "abstract": "Data-free quantization (DFQ) recovers the performance of quantized network (Q) without accessing the real data, but generates the fake sample via a generator (G) by learning from full-precision network (P) instead. However, such sample generation process is totally independence of Q, specialized as failing to consider the adaptability of the generated samples, i.e., beneficial or adversarial, over the learning process of Q, resulting into non-ignorable performance loss. Building on this, several crucial questions --- how to measure and exploit the sample adaptability to Q under varied bit-width scenarios? how to generate the samples with desirable adaptability to benefit the quantized network? --- impel us to revisit DFQ. In this paper, we answer the above questions from a game-theory perspective to specialize DFQ as a zero-sum game between two players --- a generator and a quantized network, and further propose an Adaptability-aware Sample Generation (AdaSG) method. Technically, AdaSG reformulates DFQ as a dynamic maximization-vs-minimization game process anchored on the sample adaptability. The maximization process aims to generate the sample with desirable adaptability, such sample adaptability is further reduced by the minimization process after calibrating Q for performance recovery. The Balance Gap is defined to guide the stationarity of the game process to maximally benefit Q. The theoretical analysis and empirical studies verify the superiority of AdaSG over the state-of-the-arts. Our code is available at https://github.com/hfutqian/AdaSG",
    "checked": true,
    "id": "afe7d5e4a309dea2ac2a8e53c3db61b135b85092",
    "semantic_title": "rethinking data-free quantization as a zero-sum game",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26137": {
    "title": "Mixture Uniform Distribution Modeling and Asymmetric Mix Distillation for Class Incremental Learning",
    "volume": "main",
    "abstract": "Exemplar rehearsal-based methods with knowledge distillation (KD) have been widely used in class incremental learning (CIL) scenarios. However, they still suffer from performance degradation because of severely distribution discrepancy between training and test set caused by the limited storage memory on previous classes. In this paper, we mathematically model the data distribution and the discrepancy at the incremental stages with mixture uniform distribution (MUD). Then, we propose the asymmetric mix distillation method to uniformly minimize the error of each class from distribution discrepancy perspective. Specifically, we firstly promote mixup in CIL scenarios with the incremental mix samplers and incremental mix factor to calibrate the raw training data distribution. Next, mix distillation label augmentation is incorporated into the data distribution to inherit the knowledge information from the previous models. Based on the above augmented data distribution, our trained model effectively alleviates the performance degradation and extensive experimental results validate that our method exhibits superior performance on CIL benchmarks",
    "checked": true,
    "id": "aa1c7dbacc44b797698744a40c322f55258ddafc",
    "semantic_title": "mixture uniform distribution modeling and asymmetric mix distillation for class incremental learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26138": {
    "title": "Mutual-Enhanced Incongruity Learning Network for Multi-Modal Sarcasm Detection",
    "volume": "main",
    "abstract": "Sarcasm is a sophisticated linguistic phenomenon that is prevalent on today's social media platforms. Multi-modal sarcasm detection aims to identify whether a given sample with multi-modal information (i.e., text and image) is sarcastic. This task's key lies in capturing both inter- and intra-modal incongruities within the same context. Although existing methods have achieved compelling success, they are disturbed by irrelevant information extracted from the whole image and text, or overlooking some important information due to the incomplete input. To address these limitations, we propose a Mutual-enhanced Incongruity Learning Network for multi-modal sarcasm detection, named MILNet. In particular, we design a local semantic-guided incongruity learning module and a global incongruity learning module. Moreover, we introduce a mutual enhancement module to take advantage of the underlying consistency between the two modules to boost the performance. Extensive experiments on a widely-used dataset demonstrate the superiority of our model over cutting-edge methods",
    "checked": true,
    "id": "1656d9cfe2e7474ce11a862db970f34368db36d9",
    "semantic_title": "mutual-enhanced incongruity learning network for multi-modal sarcasm detection",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26139": {
    "title": "Training Meta-Surrogate Model for Transferable Adversarial Attack",
    "volume": "main",
    "abstract": "The problem of adversarial attacks to a black-box model when no queries are allowed has posed a great challenge to the community and has been extensively investigated. In this setting, one simple yet effective method is to transfer the obtained adversarial examples from attacking surrogate models to fool the target model. Previous works have studied what kind of attacks to the surrogate model can generate more transferable adversarial examples, but their performances are still limited due to the mismatches between surrogate models and the target model. In this paper, we tackle this problem from a novel angle---instead of using the original surrogate models, can we obtain a Meta-Surrogate Model (MSM) such that attacks to this model can be easily transferred to other models? We show that this goal can be mathematically formulated as a bi-level optimization problem and design a differentiable attacker to make training feasible. Given one or a set of surrogate models, our method can thus obtain an MSM such that adversarial examples generated on MSM enjoy eximious transferability. Comprehensive experiments on Cifar-10 and ImageNet demonstrate that by attacking the MSM, we can obtain stronger transferable adversarial examples to deceive black-box models including adversarially trained ones, with much higher success rates than existing methods",
    "checked": true,
    "id": "a80a626f87b4a30cfff3eea8ee39625075c2f451",
    "semantic_title": "training meta-surrogate model for transferable adversarial attack",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26140": {
    "title": "Stochastic Contextual Bandits with Long Horizon Rewards",
    "volume": "main",
    "abstract": "The growing interest in complex decision-making and language modeling problems highlights the importance of sample-efficient learning over very long horizons. This work takes a step in this direction by investigating contextual linear bandits where the current reward depends on at most s prior actions and contexts (not necessarily consecutive), up to a time horizon of h. In order to avoid polynomial dependence on h, we propose new algorithms that leverage sparsity to discover the dependence pattern and arm parameters jointly. We consider both the data-poor (T= h) regimes and derive respective regret upper bounds O(d square-root(sT) +min(q, T) and O( square-root(sdT) ), with sparsity s, feature dimension d, total time horizon T, and q that is adaptive to the reward dependence pattern. Complementing upper bounds, we also show that learning over a single trajectory brings inherent challenges: While the dependence pattern and arm parameters form a rank-1 matrix, circulant matrices are not isometric over rank-1 manifolds and sample complexity indeed benefits from the sparse reward dependence structure. Our results necessitate a new analysis to address long-range temporal dependencies across data and avoid polynomial dependence on the reward horizon h. Specifically, we utilize connections to the restricted isometry property of circulant matrices formed by dependent sub-Gaussian vectors and establish new guarantees that are also of independent interest",
    "checked": true,
    "id": "45fbf50820d9c6756225a089dc995db368624f3d",
    "semantic_title": "stochastic contextual bandits with long horizon rewards",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26141": {
    "title": "Gradient-Variation Bound for Online Convex Optimization with Constraints",
    "volume": "main",
    "abstract": "We study online convex optimization with constraints consisting of multiple functional constraints and a relatively simple constraint set, such as a Euclidean ball. As enforcing the constraints at each time step through projections is computationally challenging in general, we allow decisions to violate the functional constraints but aim to achieve a low regret and cumulative violation of the constraints over a horizon of T time steps. First-order methods achieve an O(sqrt{T}) regret and an O(1) constraint violation, which is the best-known bound under the Slater's condition, but do not take into account the structural information of the problem. Furthermore, the existing algorithms and analysis are limited to Euclidean space. In this paper, we provide an instance-dependent bound for online convex optimization with complex constraints obtained by a novel online primal-dual mirror-prox algorithm. Our instance-dependent regret is quantified by the total gradient variation V_*(T) in the sequence of loss functions. The proposed algorithm works in general normed spaces and simultaneously achieves an O(sqrt{V_*(T)}) regret and an O(1) constraint violation, which is never worse than the best-known (O(sqrt{T}), O(1)) result and improves over previous works that applied mirror-prox-type algorithms for this problem achieving O(T^{2/3}) regret and constraint violation. Finally, our algorithm is computationally efficient, as it only performs mirror descent steps in each iteration instead of solving a general Lagrangian minimization problem",
    "checked": false,
    "id": "d163217444371c45e97438e7de7b997a6da9ae75",
    "semantic_title": "dynamic regret of distributed online frank-wolfe convex optimization",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26142": {
    "title": "Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes",
    "volume": "main",
    "abstract": "We consider a sequential decision making problem where the agent faces the environment characterized by the stochastic discrete events and seeks an optimal intervention policy such that its long-term reward is maximized. This problem exists ubiquitously in social media, finance and health informatics but is rarely investigated by the conventional research in reinforcement learning. To this end, we present a novel framework of the model-based reinforcement learning where the agent's actions and observations are asynchronous stochastic discrete events occurring in continuous-time. We model the dynamics of the environment by Hawkes process with external intervention control term and develop an algorithm to embed such process in the Bellman equation which guides the direction of the value gradient. We demonstrate the superiority of our method in both synthetic simulator and real-data experiments",
    "checked": true,
    "id": "7cf779d889dbf155e089289bab1495be2b186b11",
    "semantic_title": "bellman meets hawkes: model-based reinforcement learning via temporal point processes",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26143": {
    "title": "GLUECons: A Generic Benchmark for Learning under Constraints",
    "volume": "main",
    "abstract": "Recent research has shown that integrating domain knowledge into deep learning architectures is effective; It helps reduce the amount of required data, improves the accuracy of the models' decisions, and improves the interpretability of models. However, the research community lacks a convened benchmark for systematically evaluating knowledge integration methods. In this work, we create a benchmark that is a collection of nine tasks in the domains of natural language processing and computer vision. In all cases, we model external knowledge as constraints, specify the sources of the constraints for each task, and implement various models that use these constraints. We report the results of these models using a new set of extended evaluation criteria in addition to the task performances for a more in-depth analysis. This effort provides a framework for a more comprehensive and systematic comparison of constraint integration techniques and for identifying related research challenges. It will facilitate further research for alleviating some problems of state-of-the-art neural models",
    "checked": true,
    "id": "757ce0529971d496e3c17155b405747f73bc18c3",
    "semantic_title": "gluecons: a generic benchmark for learning under constraints",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26144": {
    "title": "Provable Detection of Propagating Sampling Bias in Prediction Models",
    "volume": "main",
    "abstract": "With an increased focus on incorporating fairness in machine learning models, it becomes imperative not only to assess and mitigate bias at each stage of the machine learning pipeline but also to understand the downstream impacts of bias across stages. Here we consider a general, but realistic, scenario in which a predictive model is learned from (potentially biased) training data, and model predictions are assessed post-hoc for fairness by some auditing method. We provide a theoretical analysis of how a specific form of data bias, differential sampling bias, propagates from the data stage to the prediction stage. Unlike prior work, we evaluate the downstream impacts of data biases quantitatively rather than qualitatively and prove theoretical guarantees for detection. Under reasonable assumptions, we quantify how the amount of bias in the model predictions varies as a function of the amount of differential sampling bias in the data, and at what point this bias becomes provably detectable by the auditor. Through experiments on two criminal justice datasets-- the well-known COMPAS dataset and historical data from NYPD's stop and frisk policy-- we demonstrate that the theoretical results hold in practice even when our assumptions are relaxed",
    "checked": true,
    "id": "eab871a8f01a58cd589f5b8853d99aac80419a79",
    "semantic_title": "provable detection of propagating sampling bias in prediction models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26145": {
    "title": "Diffusing Gaussian Mixtures for Generating Categorical Data",
    "volume": "main",
    "abstract": "Learning a categorical distribution comes with its own set of challenges. A successful approach taken by state-of-the-art works is to cast the problem in a continuous domain to take advantage of the impressive performance of the generative models for continuous data. Amongst them are the recently emerging diffusion probabilistic models, which have the observed advantage of generating high-quality samples. Recent advances for categorical generative models have focused on log likelihood improvements. In this work, we propose a generative model for categorical data based on diffusion models with a focus on high-quality sample generation, and propose sampled-based evaluation methods. The efficacy of our method stems from performing diffusion in the continuous domain while having its parameterization informed by the structure of the categorical nature of the target distribution. Our method of evaluation highlights the capabilities and limitations of different generative models for generating categorical data, and includes experiments on synthetic and real-world protein datasets",
    "checked": true,
    "id": "8ae47f3e238304f33c8cfe5f24c054339dbc4080",
    "semantic_title": "diffusing gaussian mixtures for generating categorical data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26146": {
    "title": "Hypernetworks for Zero-Shot Transfer in Reinforcement Learning",
    "volume": "main",
    "abstract": "In this paper, hypernetworks are trained to generate behaviors across a range of unseen task conditions, via a novel TD-based training objective and data from a set of near-optimal RL solutions for training tasks. This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on zero-shot performance at test time, enabled by knowledge of the task parameters (also known as context). Our technical approach is based upon viewing each RL algorithm as a mapping from the MDP specifics to the near-optimal value function and policy and seek to approximate it with a hypernetwork that can generate near-optimal value functions and policies, given the parameters of the MDP. We show that, under certain conditions, this mapping can be considered as a supervised learning problem. We empirically evaluate the effectiveness of our method for zero-shot transfer to new reward and transition dynamics on a series of continuous control tasks from DeepMind Control Suite. Our method demonstrates significant improvements over baselines from multitask and meta RL approaches",
    "checked": true,
    "id": "f36f1a3b75778e6924461c04a607da3db4c0b534",
    "semantic_title": "hypernetworks for zero-shot transfer in reinforcement learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26147": {
    "title": "Automata Cascades: Expressivity and Sample Complexity",
    "volume": "main",
    "abstract": "Every automaton can be decomposed into a cascade of basic prime automata. This is the Prime Decomposition Theorem by Krohn and Rhodes. Guided by this theory, we propose automata cascades as a structured, modular, way to describe automata as complex systems made of many components, each implementing a specific functionality. Any automaton can serve as a component; using specific components allows for a fine-grained control of the expressivity of the resulting class of automata; using prime automata as components implies specific expressivity guarantees. Moreover, specifying automata as cascades allows for describing the sample complexity of automata in terms of their components. We show that the sample complexity is linear in the number of components and the maximum complexity of a single component, modulo logarithmic factors. This opens to the possibility of learning automata representing large dynamical systems consisting of many parts interacting with each other. It is in sharp contrast with the established understanding of the sample complexity of automata, described in terms of the overall number of states and input letters, which implies that it is only possible to learn automata where the number of states is linear in the amount of data available. Instead our results show that one can learn automata with a number of states that is exponential in the amount of data available",
    "checked": true,
    "id": "273b47ea69844de1d142ea61f72fba50afcf0107",
    "semantic_title": "automata cascades: expressivity and sample complexity",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26148": {
    "title": "ESPT: A Self-Supervised Episodic Spatial Pretext Task for Improving Few-Shot Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) techniques have recently been integrated into the few-shot learning (FSL) framework and have shown promising results in improving the few-shot image classification performance. However, existing SSL approaches used in FSL typically seek the supervision signals from the global embedding of every single image. Therefore, during the episodic training of FSL, these methods cannot capture and fully utilize the local visual information in image samples and the data structure information of the whole episode, which are beneficial to FSL. To this end, we propose to augment the few-shot learning objective with a novel self-supervised Episodic Spatial Pretext Task (ESPT). Specifically, for each few-shot episode, we generate its corresponding transformed episode by applying a random geometric transformation to all the images in it. Based on these, our ESPT objective is defined as maximizing the local spatial relationship consistency between the original episode and the transformed one. With this definition, the ESPT-augmented FSL objective promotes learning more transferable feature representations that capture the local spatial features of different images and their inter-relational structural information in each input episode, thus enabling the model to generalize better to new categories with only a few samples. Extensive experiments indicate that our ESPT method achieves new state-of-the-art performance for few-shot image classification on three mainstay benchmark datasets. The source code will be available at: https://github.com/Whut-YiRong/ESPT",
    "checked": true,
    "id": "0a52f77490300be671942909e09b4ee87e7f23a0",
    "semantic_title": "espt: a self-supervised episodic spatial pretext task for improving few-shot learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26149": {
    "title": "Planning and Learning with Adaptive Lookahead",
    "volume": "main",
    "abstract": "Some of the most powerful reinforcement learning frameworks use planning for action selection. Interestingly, their planning horizon is either fixed or determined arbitrarily by the state visitation history. Here, we expand beyond the naive fixed horizon and propose a theoretically justified strategy for adaptive selection of the planning horizon as a function of the state-dependent value estimate. We propose two variants for lookahead selection and analyze the trade-off between iteration count and computational complexity per iteration. We then devise a corresponding deep Q-network algorithm with an adaptive tree search horizon. We separate the value estimation per depth to compensate for the off-policy discrepancy between depths. Lastly, we demonstrate the efficacy of our adaptive lookahead method in a maze environment and Atari",
    "checked": true,
    "id": "4c96156ad9561006c1528ec8ead5d9d3103669a9",
    "semantic_title": "planning and learning with adaptive lookahead",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26150": {
    "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
    "volume": "main",
    "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query. We create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks",
    "checked": true,
    "id": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
    "semantic_title": "disguide: disagreement-guided data-free model extraction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26151": {
    "title": "Overcoming Concept Shift in Domain-Aware Settings through Consolidated Internal Distributions",
    "volume": "main",
    "abstract": "We develop an algorithm to improve the predictive performance of a pre-trained model under \\textit{concept shift} without retraining the model from scratch when only unannotated samples of initial concepts are accessible. We model this problem as a domain adaptation problem, where the source domain data is inaccessible during model adaptation. The core idea is based on consolidating the intermediate internal distribution, learned to represent the source domain data, after adapting the model. We provide theoretical analysis and conduct extensive experiments on five benchmark datasets to demonstrate that the proposed method is effective",
    "checked": false,
    "id": "ffec892fa0a8eecfc595e3ccd524f29f7c6aa789",
    "semantic_title": "hybrid cognitive capabilities in edge operations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26152": {
    "title": "Inferring Patient Zero on Temporal Networks via Graph Neural Networks",
    "volume": "main",
    "abstract": "The world is currently seeing frequent local outbreaks of epidemics, such as COVID-19 and Monkeypox. Preventing further propagation of the outbreak requires prompt implementation of control measures, and a critical step is to quickly infer patient zero. This backtracking task is challenging for two reasons. First, due to the sudden emergence of local epidemics, information recording the spreading process is limited. Second, the spreading process has strong randomness. To address these challenges, we tailor a gnn-based model to establish the inverse statistical association between the current and initial state implicitly. This model uses contact topology and the current state of the local population to determine the possibility that each individual could be patient zero. We benchmark our model on data from important epidemiological models on five real temporal networks, showing performance significantly superior to previous methods. We also demonstrate that our method is robust to missing information about contact structure or current state. Further, we find the individuals assigned higher inferred possibility by model are closer to patient zero in terms of core number and the activity sequence recording the times at which the individual had contact with other nodes",
    "checked": true,
    "id": "f45702cf78ff21b9bc41c77d4ca1daa5ff58d8c3",
    "semantic_title": "inferring patient zero on temporal networks via graph neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26153": {
    "title": "Accommodating Audio Modality in CLIP for Multimodal Processing",
    "volume": "main",
    "abstract": "Multimodal processing has attracted much attention lately especially with the success of pre-training. However, the exploration has mainly focused on vision-language pre-training, as introducing more modalities can greatly complicate model design and optimization. In this paper, we extend the state-of-the-art Vision-Language model CLIP to accommodate the audio modality for Vision-Language-Audio multimodal processing. Specifically, we apply inter-modal and intra-modal contrastive learning to explore the correlation between audio and other modalities in addition to the inner characteristics of the audio modality. Moreover, we further design an audio type token to dynamically learn different audio information type for different scenarios, as both verbal and nonverbal heterogeneous information is conveyed in general audios. Our proposed CLIP4VLA model is validated in different downstream tasks including video retrieval and video captioning, and achieves the state-of-the-art performance on the benchmark datasets of MSR-VTT, VATEX, and Audiocaps.The corresponding code and checkpoints will be released at https://github.com/ludanruan/CLIP4VLA",
    "checked": true,
    "id": "98ab8260c98030109ba8467f52b13d075276cf0f",
    "semantic_title": "accommodating audio modality in clip for multimodal processing",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26154": {
    "title": "Forecasting with Sparse but Informative Variables: A Case Study in Predicting Blood Glucose",
    "volume": "main",
    "abstract": "In time-series forecasting, future target values may be affected by both intrinsic and extrinsic effects. When forecasting blood glucose, for example, intrinsic effects can be inferred from the history of the target signal alone (i.e. blood glucose), but accurately modeling the impact of extrinsic effects requires auxiliary signals, like the amount of carbohydrates ingested. Standard forecasting techniques often assume that extrinsic and intrinsic effects vary at similar rates. However, when auxiliary signals are generated at a much lower frequency than the target variable (e.g., blood glucose measurements are made every 5 minutes, while meals occur once every few hours), even well-known extrinsic effects (e.g., carbohydrates increase blood glucose) may prove difficult to learn. To better utilize these sparse but informative variables (SIVs), we introduce a novel encoder/decoder forecasting approach that accurately learns the per-timepoint effect of the SIV, by (i) isolating it from intrinsic effects and (ii) restricting its learned effect based on domain knowledge. On a simulated dataset pertaining to the task of blood glucose forecasting, when the SIV is accurately recorded our approach outperforms baseline approaches in terms of rMSE (13.07 [95% CI: 11.77,14.16] vs. 14.14 [12.69,15.27]). In the presence of a corrupted SIV, the proposed approach can still result in lower error compared to the baseline but the advantage is reduced as noise increases. By isolating their effects and incorporating domain knowledge, our approach makes it possible to better utilize SIVs in forecasting",
    "checked": true,
    "id": "6e59b0e36f7801a19925a4f82a47ff309fc172c9",
    "semantic_title": "forecasting with sparse but informative variables: a case study in predicting blood glucose",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26155": {
    "title": "On the Sample Complexity of Representation Learning in Multi-Task Bandits with Global and Local Structure",
    "volume": "main",
    "abstract": "We investigate the sample complexity of learning the optimal arm for multi-task bandit problems. Arms consist of two components: one that is shared across tasks (that we call representation) and one that is task-specific (that we call predictor). The objective is to learn the optimal (representation, predictor)-pair for each task, under the assumption that the optimal representation is common to all tasks. Within this framework, efficient learning algorithms should transfer knowledge across tasks. We consider the best-arm identification problem with fixed confidence, where, in each round, the learner actively selects both a task, and an arm, and observes the corresponding reward. We derive instance-specific sample complexity lower bounds, which apply to any algorithm that identifies the best representation, and the best predictor for a task, with prescribed confidence levels. We devise an algorithm, OSRL-SC, that can learn the optimal representation, and the optimal predictors, separately, and whose sample complexity approaches the lower bound. Theoretical and numerical results demonstrate that OSRL-SC achieves a better scaling with respect to the number of tasks compared to the classical best-arm identification algorithm. The code can be found here https://github.com/rssalessio/OSRL-SC",
    "checked": true,
    "id": "138a1de6ec6730fc75c3931018155803ebcb2ca1",
    "semantic_title": "on the sample complexity of representation learning in multi-task bandits with global and local structure",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26156": {
    "title": "Simultaneously Updating All Persistence Values in Reinforcement Learning",
    "volume": "main",
    "abstract": "In Reinforcement Learning, the performance of learning agents is highly sensitive to the choice of time discretization. Agents acting at high frequencies have the best control opportunities, along with some drawbacks, such as possible inefficient exploration and vanishing of the action advantages. The repetition of the actions, i.e., action persistence, comes into help, as it allows the agent to visit wider regions of the state space and improve the estimation of the action effects. In this work, we derive a novel operator, the All-Persistence Bellman Operator, which allows an effective use of both the low-persistence experience, by decomposition into sub-transition, and the high-persistence experience, thanks to the introduction of a suitable bootstrap procedure. In this way, we employ transitions collected at any time scale to update simultaneously the action values of the considered persistence set. We prove the contraction property of the All-Persistence Bellman Operator and, based on it, we extend classic Q-learning and DQN. After providing a study on the effects of persistence, we experimentally evaluate our approach in both tabular contexts and more challenging frameworks, including some Atari games",
    "checked": true,
    "id": "1711bda8fbf86301c9297b2925419fd8680cd43f",
    "semantic_title": "simultaneously updating all persistence values in reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26157": {
    "title": "Continual Learning with Scaled Gradient Projection",
    "volume": "main",
    "abstract": "In neural networks, continual learning results in gradient interference among sequential tasks, leading to catastrophic forgetting of old tasks while learning new ones. This issue is addressed in recent methods by storing the important gradient spaces for old tasks and updating the model orthogonally during new tasks. However, such restrictive orthogonal gradient updates hamper the learning capability of the new tasks resulting in sub-optimal performance. To improve new learning while minimizing forgetting, in this paper we propose a Scaled Gradient Projection (SGP) method, where we combine the orthogonal gradient projections with scaled gradient steps along the important gradient spaces for the past tasks. The degree of gradient scaling along these spaces depends on the importance of the bases spanning them. We propose an efficient method for computing and accumulating importance of these bases using the singular value decomposition of the input representations for each task. We conduct extensive experiments ranging from continual image classification to reinforcement learning tasks and report better performance with less training overhead than the state-of-the-art approaches",
    "checked": true,
    "id": "8023869b534ae182e3e7d5df25690a062106912d",
    "semantic_title": "continual learning with scaled gradient projection",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26158": {
    "title": "Fast Offline Policy Optimization for Large Scale Recommendation",
    "volume": "main",
    "abstract": "Personalised interactive systems such as recommender systems require selecting relevant items from massive catalogs dependent on context. Reward-driven offline optimisation of these systems can be achieved by a relaxation of the discrete problem resulting in policy learning or REINFORCE style learning algorithms. Unfortunately, this relaxation step requires computing a sum over the entire catalogue making the complexity of the evaluation of the gradient (and hence each stochastic gradient descent iterations) linear in the catalogue size. This calculation is untenable in many real world examples such as large catalogue recommender systems, severely limiting the usefulness of this method in practice. In this paper, we derive an approximation of these policy learning algorithms that scale logarithmically with the catalogue size. Our contribution is based upon combining three novel ideas: a new Monte Carlo estimate of the gradient of a policy, the self normalised importance sampling estimator and the use of fast maximum inner product search at training time. Extensive experiments show that our algorithm is an order of magnitude faster than naive approaches yet produces equally good policies",
    "checked": true,
    "id": "01c27e295956a30f7208c2e32e73a2cc1a27caee",
    "semantic_title": "fast offline policy optimization for large scale recommendation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26159": {
    "title": "Losses over Labels: Weakly Supervised Learning via Direct Loss Construction",
    "volume": "main",
    "abstract": "Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all \"label\" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during training. We call our method Losses over Labels (LoL) as it creates losses directly from heuristics without going through the intermediate step of a label. We show that LoL improves upon existing weak supervision methods on several benchmark text and image classification tasks and further demonstrate that incorporating gradient information leads to better performance on almost every task",
    "checked": true,
    "id": "bc49647a37e649c3602c16fb06faeeb48d6ab66f",
    "semantic_title": "losses over labels: weakly supervised learning via direct loss construction",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26160": {
    "title": "Representation Learning by Detecting Incorrect Location Embeddings",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our method on ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks. Moreover, we show that when downstream tasks are strongly reliant on shape (such as in the YOGA-82 pose dataset), our pre-trained features yield a significant gain over prior work",
    "checked": true,
    "id": "8c1601b65e354ba042e6952e233fff17b5475a94",
    "semantic_title": "representation learning by detecting incorrect location embeddings",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26161": {
    "title": "Sparse Coding in a Dual Memory System for Lifelong Learning",
    "volume": "main",
    "abstract": "Efficient continual learning in humans is enabled by a rich set of neurophysiological mechanisms and interactions between multiple memory systems. The brain efficiently encodes information in non-overlapping sparse codes, which facilitates the learning of new associations faster with controlled interference with previous associations. To mimic sparse coding in DNNs, we enforce activation sparsity along with a dropout mechanism which encourages the model to activate similar units for semantically similar inputs and have less overlap with activation patterns of semantically dissimilar inputs. This provides us with an efficient mechanism for balancing the reusability and interference of features, depending on the similarity of classes across tasks. Furthermore, we employ sparse coding in a multiple-memory replay mechanism. Our method maintains an additional long-term semantic memory that aggregates and consolidates information encoded in the synaptic weights of the working model. Our extensive evaluation and characteristics analysis show that equipped with these biologically inspired mechanisms, the model can further mitigate forgetting. Code available at \\url{https://github.com/NeurAI-Lab/SCoMMER}",
    "checked": true,
    "id": "362211dccace94cafc72c819d381b836c443593e",
    "semantic_title": "sparse coding in a dual memory system for lifelong learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26162": {
    "title": "Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity",
    "volume": "main",
    "abstract": "We present CrissCross, a self-supervised framework for learning audio-visual representations. A novel notion is introduced in our framework whereby in addition to learning the intra-modal and standard 'synchronous' cross-modal relations, CrissCross also learns 'asynchronous' cross-modal relationships. We perform in-depth studies showing that by relaxing the temporal synchronicity between the audio and visual modalities, the network learns strong generalized representations useful for a variety of downstream tasks. To pretrain our proposed solution, we use 3 different datasets with varying sizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations are evaluated on a number of downstream tasks namely action recognition, sound classification, and action retrieval. Our experiments show that CrissCross either outperforms or achieves performances on par with the current state-of-the-art self-supervised methods on action recognition and action retrieval with UCF101 and HMDB51, as well as sound classification with ESC50 and DCASE. Moreover, CrissCross outperforms fully-supervised pretraining while pretrained on Kinetics-Sound",
    "checked": true,
    "id": "00768994cec0d2e74545180a59ab5407469da9ff",
    "semantic_title": "self-supervised audio-visual representation learning with relaxed cross-modal synchronicity",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26163": {
    "title": "Dropout Is NOT All You Need to Prevent Gradient Leakage",
    "volume": "main",
    "abstract": "Gradient inversion attacks on federated learning systems reconstruct client training data from exchanged gradient information. To defend against such attacks, a variety of defense mechanisms were proposed. However, they usually lead to an unacceptable trade-off between privacy and model utility. Recent observations suggest that dropout could mitigate gradient leakage and improve model utility if added to neural networks. Unfortunately, this phenomenon has not been systematically researched yet. In this work, we thoroughly analyze the effect of dropout on iterative gradient inversion attacks. We find that state of the art attacks are not able to reconstruct the client data due to the stochasticity induced by dropout during model training. Nonetheless, we argue that dropout does not offer reliable protection if the dropout induced stochasticity is adequately modeled during attack optimization. Consequently, we propose a novel Dropout Inversion Attack (DIA) that jointly optimizes for client data and dropout masks to approximate the stochastic client model. We conduct an extensive systematic evaluation of our attack on four seminal model architectures and three image classification datasets of increasing complexity. We find that our proposed attack bypasses the protection seemingly induced by dropout and reconstructs client data with high fidelity. Our work demonstrates that privacy inducing changes to model architectures alone cannot be assumed to reliably protect from gradient leakage and therefore should be combined with complementary defense mechanisms",
    "checked": true,
    "id": "ed5671f95cefda22861c0cddd54d50bcdcdd90fa",
    "semantic_title": "dropout is not all you need to prevent gradient leakage",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26164": {
    "title": "Exploration via Epistemic Value Estimation",
    "volume": "main",
    "abstract": "How to efficiently explore in reinforcement learning is an open problem. Many exploration algorithms employ the epistemic uncertainty of their own value predictions -- for instance to compute an exploration bonus or upper confidence bound. Unfortunately the required uncertainty is difficult to estimate in general with function approximation. We propose epistemic value estimation (EVE): a recipe that is compatible with sequential decision making and with neural network function approximators. It equips agents with a tractable posterior over all their parameters from which epistemic value uncertainty can be computed efficiently. We use the recipe to derive an epistemic Q-Learning agent and observe competitive performance on a series of benchmarks. Experiments confirm that the EVE recipe facilitates efficient exploration in hard exploration tasks",
    "checked": true,
    "id": "0670b6552f53d2ed8b56a712bb0db4783138123a",
    "semantic_title": "exploration via epistemic value estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26165": {
    "title": "Multi-Source Survival Domain Adaptation",
    "volume": "main",
    "abstract": "Survival analysis is the branch of statistics that studies the relation between the characteristics of living entities and their respective survival times, taking into account the partial information held by censored cases. A good analysis can, for example, determine whether one medical treatment for a group of patients is better than another. With the rise of machine learning, survival analysis can be modeled as learning a function that maps studied patients to their survival times. To succeed with that, there are three crucial issues to be tackled. First, some patient data is censored: we do not know the true survival times for all patients. Second, data is scarce, which led past research to treat different illness types as domains in a multi-task setup. Third, there is the need for adaptation to new or extremely rare illness types, where little or no labels are available. In contrast to previous multi-task setups, we want to investigate how to efficiently adapt to a new survival target domain from multiple survival source domains. For this, we introduce a new survival metric and the corresponding discrepancy measure between survival distributions. These allow us to define domain adaptation for survival analysis while incorporating censored data, which would otherwise have to be dropped. Our experiments on two cancer data sets reveal a superb performance on target domains, a better treatment recommendation, and a weight matrix with a plausible explanation",
    "checked": true,
    "id": "00d3b4aacc5cc95277f7e3e0f560d79781a4e65a",
    "semantic_title": "multi-source survival domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26166": {
    "title": "What Do You MEME? Generating Explanations for Visual Semantic Role Labelling in Memes",
    "volume": "main",
    "abstract": "Memes are powerful means for effective communication on social media. Their effortless amalgamation of viral visuals and compelling messages can have far-reaching implications with proper marketing. Previous research on memes has primarily focused on characterizing their affective spectrum and detecting whether the meme's message insinuates any intended harm, such as hate, offense, racism, etc. However, memes often use abstraction, which can be elusive. Here, we introduce a novel task - EXCLAIM, generating explanations for visual semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset that offers natural language explanations of connotative roles for three types of entities - heroes, villains, and victims, encompassing 4,680 entities present in 3K memes. We also benchmark ExHVV with several strong unimodal and multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task learning framework that endeavors to address EXCLAIM optimally by jointly learning to predict the correct semantic roles and correspondingly to generate suitable natural language explanations. LUMEN distinctly outperforms the best baseline across 18 standard natural language generation evaluation metrics. Our systematic evaluation and analyses demonstrate that characteristic multimodal cues required for adjudicating semantic roles are also helpful for generating suitable explanations",
    "checked": true,
    "id": "9b659d02ffe695707077b6a3b3475fc67d168e96",
    "semantic_title": "what do you meme? generating explanations for visual semantic role labelling in memes",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26167": {
    "title": "Post-hoc Uncertainty Learning Using a Dirichlet Meta-Model",
    "volume": "main",
    "abstract": "It is known that neural networks have the problem of being over-confident when directly using the output label distribution to generate uncertainty measures. Existing methods mainly resolve this issue by retraining the entire model to impose the uncertainty quantification capability so that the learned model can achieve desired performance in accuracy and uncertainty prediction simultaneously. However, training the model from scratch is computationally expensive, and a trade-off might exist between prediction accuracy and uncertainty quantification. To this end, we consider a more practical post-hoc uncertainty learning setting, where a well-trained base model is given, and we focus on the uncertainty quantification task at the second stage of training. We propose a novel Bayesian uncertainty learning approach using the Dirichlet meta-model, which is effective and computationally efficient. Our proposed method requires no additional training data and is flexible enough to quantify different uncertainties and easily adapt to different application settings, including out-of-domain data detection, misclassification detection, and trustworthy transfer learning. Finally, we demonstrate our proposed meta-model approach's flexibility and superior empirical performance on these applications over multiple representative image classification benchmarks",
    "checked": true,
    "id": "0647c14721528fc1950f883036e0806d7b3be6f9",
    "semantic_title": "post-hoc uncertainty learning using a dirichlet meta-model",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26168": {
    "title": "Neighbor Contrastive Learning on Learnable Graph Augmentation",
    "volume": "main",
    "abstract": "Recent years, graph contrastive learning (GCL), which aims to learn representations from unlabeled graphs, has made great progress. However, the existing GCL methods mostly adopt human-designed graph augmentations, which are sensitive to various graph datasets. In addition, the contrastive losses originally developed in computer vision have been directly applied to graph data, where the neighboring nodes are regarded as negatives and consequently pushed far apart from the anchor. However, this is contradictory with the homophily assumption of net-works that connected nodes often belong to the same class and should be close to each other. In this work, we propose an end-to-end automatic GCL method, named NCLA to apply neighbor contrastive learning on learnable graph augmentation. Several graph augmented views with adaptive topology are automatically learned by the multi-head graph attention mechanism, which can be compatible with various graph datasets without prior domain knowledge. In addition, a neighbor contrastive loss is devised to allow multiple positives per anchor by taking network topology as the supervised signals. Both augmentations and embeddings are learned end-to-end in the proposed NCLA. Extensive experiments on the benchmark datasets demonstrate that NCLA yields the state-of-the-art node classification performance on self-supervised GCL and even exceeds the supervised ones, when the labels are extremely limited. Our code is released at https://github.com/shenxiaocam/NCLA",
    "checked": true,
    "id": "be6161dd5eaa1b1ecc4e62348b43e52f134ed3f5",
    "semantic_title": "neighbor contrastive learning on learnable graph augmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26169": {
    "title": "ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-Cost Proxies",
    "volume": "main",
    "abstract": "Designing neural architectures requires immense manual efforts. This has promoted the development of neural architecture search (NAS) to automate the design. While previous NAS methods achieve promising results but run slowly, zero-cost proxies run extremely fast but are less promising. Therefore, it's of great potential to accelerate NAS via those zero-cost proxies. The existing method has two limitations, which are unforeseeable reliability and one-shot usage. To address the limitations, we present ProxyBO, an efficient Bayesian optimization (BO) framework that utilizes the zero-cost proxies to accelerate neural architecture search. We apply the generalization ability measurement to estimate the fitness of proxies on the task during each iteration and design a novel acquisition function to combine BO with zero-cost proxies based on their dynamic influence. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public benchmarks. Concretely, ProxyBO achieves up to 5.41× and 3.86× speedups over the state-of-the-art approaches REA and BRP-NAS",
    "checked": true,
    "id": "9891b45b1aaac6a602e848381d145c825645e040",
    "semantic_title": "proxybo: accelerating neural architecture search via bayesian optimization with zero-cost proxies",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26170": {
    "title": "Contrastive Predictive Autoencoders for Dynamic Point Cloud Self-Supervised Learning",
    "volume": "main",
    "abstract": "We present a new self-supervised paradigm on point cloud sequence understanding. Inspired by the discriminative and generative self-supervised methods, we design two tasks, namely point cloud sequence based Contrastive Prediction and Reconstruction (CPR), to collaboratively learn more comprehensive spatiotemporal representations. Specifically, dense point cloud segments are first input into an encoder to extract embeddings. All but the last ones are then aggregated by a context-aware autoregressor to make predictions for the last target segment. Towards the goal of modeling multi-granularity structures, local and global contrastive learning are performed between predictions and targets. To further improve the generalization of representations, the predictions are also utilized to reconstruct raw point cloud sequences by a decoder, where point cloud colorization is employed to discriminate against different frames. By combining classic contrast and reconstruction paradigms, it makes the learned representations with both global discrimination and local perception. We conduct experiments on four point cloud sequence benchmarks, and report the results on action recognition and gesture recognition under multiple experimental settings. The performances are comparable with supervised methods and show powerful transferability",
    "checked": true,
    "id": "1297adc46af434f188ca8ea1c95638849eed36b3",
    "semantic_title": "contrastive predictive autoencoders for dynamic point cloud self-supervised learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26171": {
    "title": "Fixed-Weight Difference Target Propagation",
    "volume": "main",
    "abstract": "Target Propagation (TP) is a biologically more plausible algorithm than the error backpropagation (BP) to train deep networks, and improving practicality of TP is an open issue. TP methods require the feedforward and feedback networks to form layer-wise autoencoders for propagating the target values generated at the output layer. However, this causes certain drawbacks; e.g., careful hyperparameter tuning is required to synchronize the feedforward and feedback training, and frequent updates of the feedback path are usually required than that of the feedforward path. Learning of the feedforward and feedback networks is sufficient to make TP methods capable of training, but is having these layer-wise autoencoders a necessary condition for TP to work? We answer this question by presenting Fixed-Weight Difference Target Propagation (FW-DTP) that keeps the feedback weights constant during training. We confirmed that this simple method, which naturally resolves the abovementioned problems of TP, can still deliver informative target values to hidden layers for a given task; indeed, FW-DTP consistently achieves higher test performance than a baseline, the Difference Target Propagation (DTP), on four classification datasets. We also present a novel propagation architecture that explains the exact form of the feedback function of DTP to analyze FW-DTP. Our code is available at https://github.com/TatsukichiShibuya/Fixed-Weight-Difference-Target-Propagation",
    "checked": true,
    "id": "a1b07aad0583274d2c87e0baacd0b18183d2d582",
    "semantic_title": "fixed-weight difference target propagation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26172": {
    "title": "Concurrent Multi-Label Prediction in Event Streams",
    "volume": "main",
    "abstract": "Streams of irregularly occurring events are commonly modeled as a marked temporal point process. Many real-world datasets such as e-commerce transactions and electronic health records often involve events where multiple event types co-occur, e.g. multiple items purchased or multiple diseases diagnosed simultaneously. In this paper, we tackle multi-label prediction in such a problem setting, and propose a novel Transformer-based Conditional Mixture of Bernoulli Network (TCMBN) that leverages neural density estimation to capture complex temporal dependence as well as probabilistic dependence between concurrent event types. We also propose potentially incorporating domain knowledge in the objective by regularizing the predicted probability. To represent probabilistic dependence of concurrent event types graphically, we design a two-step approach that first learns the mixture of Bernoulli network and then solves a least-squares semi-definite constrained program to numerically approximate the sparse precision matrix from a learned covariance matrix. This approach proves to be effective for event prediction while also providing an interpretable and possibly non-stationary structure for insights into event co-occurrence. We demonstrate the superior performance of our approach compared to existing baselines on multiple synthetic and real benchmarks",
    "checked": true,
    "id": "f2706bbbf0a69729410f62a826b18aa5b1fbe052",
    "semantic_title": "concurrent multi-label prediction in event streams",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26173": {
    "title": "A Generalized Unbiased Risk Estimator for Learning with Augmented Classes",
    "volume": "main",
    "abstract": "In contrast to the standard learning paradigm where all classes can be observed in training data, learning with augmented classes (LAC) tackles the problem where augmented classes unobserved in the training data may emerge in the test phase. Previous research showed that given unlabeled data, an unbiased risk estimator (URE) can be derived, which can be minimized for LAC with theoretical guarantees. However, this URE is only restricted to the specific type of one-versus-rest loss functions for multi-class classification, making it not flexible enough when the loss needs to be changed with the dataset in practice. In this paper, we propose a generalized URE that can be equipped with arbitrary loss functions while maintaining the theoretical guarantees, given unlabeled data for LAC. To alleviate the issue of negative empirical risk commonly encountered by previous studies, we further propose a novel risk-penalty regularization term. Experiments demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "5dd1c14cc13e80ded379a8ed4cb04cbaed9aa01d",
    "semantic_title": "a generalized unbiased risk estimator for learning with augmented classes",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26174": {
    "title": "Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI",
    "volume": "main",
    "abstract": "Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \\textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations",
    "checked": true,
    "id": "8f8b06780acbe9e2a0356c8acfcfa209bed40bd6",
    "semantic_title": "logical satisfiability of counterfactuals for faithful explanations in nli",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26175": {
    "title": "SLIQ: Quantum Image Similarity Networks on Noisy Quantum Computers",
    "volume": "main",
    "abstract": "Exploration into quantum machine learning has grown tremendously in recent years due to the ability of quantum computers to speed up classical programs. However, these ef- forts have yet to solve unsupervised similarity detection tasks due to the challenge of porting them to run on quantum com- puters. To overcome this challenge, we propose SLIQ, the first open-sourced work for resource-efficient quantum sim- ilarity detection networks, built with practical and effective quantum learning and variance-reducing algorithms",
    "checked": true,
    "id": "e00e31758a224b638cc24577d3813e01a1eb0427",
    "semantic_title": "sliq: quantum image similarity networks on noisy quantum computers",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26176": {
    "title": "Adaptive Mixing of Auxiliary Losses in Supervised Learning",
    "volume": "main",
    "abstract": "In many supervised learning scenarios, auxiliary losses are used in order to introduce additional information or constraints into the supervised learning objective. For instance, knowledge distillation aims to mimic outputs of a powerful teacher model; similarly, in rule-based approaches, weak labeling information is provided by labeling functions which may be noisy rule-based approximations to true labels. We tackle the problem of learning to combine these losses in a principled manner. Our proposal, AMAL, uses a bi-level optimization criterion on validation data to learn optimal mixing weights, at an instance-level, over the training data. We describe a meta-learning approach towards solving this bi-level objective, and show how it can be applied to different scenarios in supervised learning. Experiments in a number of knowledge distillation and rule denoising domains show that AMAL provides noticeable gains over competitive baselines in those domains. We empirically analyze our method and share insights into the mechanisms through which it provides performance gains. The code for AMAL is at: https://github.com/durgas16/AMAL.git",
    "checked": true,
    "id": "063acc1ea6ebeb4e4310e2990583f8fed253bf05",
    "semantic_title": "adaptive mixing of auxiliary losses in supervised learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26177": {
    "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning",
    "volume": "main",
    "abstract": "Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over any number of training rounds). Our framework also carefully accounts for the fairness and the average number of participating users at each round. Our experiments on MNIST, CIFAR-10 and CIFAR-100 datasets in the IID and the non-IID settings demonstrate the performance improvement over the baselines, both in terms of privacy protection and test accuracy",
    "checked": true,
    "id": "2cb10b11951246878ad3fbd26313014e7d55b863",
    "semantic_title": "securing secure aggregation: mitigating multi-round privacy leakage in federated learning",
    "citation_count": 32
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26178": {
    "title": "Mixture Manifold Networks: A Computationally Efficient Baseline for Inverse Modeling",
    "volume": "main",
    "abstract": "We propose and show the efficacy of a new method to address generic inverse problems. Inverse modeling is the task whereby one seeks to determine the hidden parameters of a natural system that produce a given set of observed measurements. Recent work has shown impressive results using deep learning, but we note that there is a trade-off between model performance and computational time. For some applications, the computational time at inference for the best performing inverse modeling method may be overly prohibitive to its use. In seeking a faster, high-performing model, we present a new method that leverages multiple manifolds as a mixture of backward (e.g., inverse) models in a forward-backward model architecture. These multiple backwards models all share a common forward model, and their training is mitigated by generating training examples from the forward model. The proposed method thus has two innovations: 1) the multiple Manifold Mixture Network (MMN) architecture, and 2) the training procedure involving augmenting backward model training data using the forward model. We demonstrate the advantages of our method by comparing to several baselines on four benchmark inverse problems, and we furthermore provide analysis to motivate its design",
    "checked": true,
    "id": "3fd8d46297d671e3400e4e4aba2f991b1bb14726",
    "semantic_title": "mixture manifold networks: a computationally efficient baseline for inverse modeling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26179": {
    "title": "Sharing Pattern Submodels for Prediction with Missing Values",
    "volume": "main",
    "abstract": "Missing values are unavoidable in many applications of machine learning and present challenges both during training and at test time. When variables are missing in recurring patterns, fitting separate pattern submodels have been proposed as a solution. However, fitting models independently does not make efficient use of all available data. Conversely, fitting a single shared model to the full data set relies on imputation which often leads to biased results when missingness depends on unobserved factors. We propose an alternative approach, called sharing pattern submodels (SPSM), which i) makes predictions that are robust to missing values at test time, ii) maintains or improves the predictive power of pattern submodels, and iii) has a short description, enabling improved interpretability. Parameter sharing is enforced through sparsity-inducing regularization which we prove leads to consistent estimation. Finally, we give conditions for when a sharing model is optimal, even when both missingness and the target outcome depend on unobserved variables. Classification and regression experiments on synthetic and real-world data sets demonstrate that our models achieve a favorable tradeoff between pattern specialization and information sharing",
    "checked": true,
    "id": "41b729a8ff8e0d02f72774649ea5566db413ad18",
    "semantic_title": "sharing pattern submodels for prediction with missing values",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26180": {
    "title": "Scalable Optimal Multiway-Split Decision Trees with Constraints",
    "volume": "main",
    "abstract": "There has been a surge of interest in learning optimal decision trees using mixed-integer programs (MIP) in recent years, as heuristic-based methods do not guarantee optimality and find it challenging to incorporate constraints that are critical for many practical applications. However, existing MIP methods that build on an arc-based formulation do not scale well as the number of binary variables is in the order of 2 to the power of the depth of the tree and the size of the dataset. Moreover, they can only handle sample-level constraints and linear metrics. In this paper, we propose a novel path-based MIP formulation where the number of decision variables is independent of dataset size. We present a scalable column generation framework to solve the MIP. Our framework produces a multiway-split tree which is more interpretable than the typical binary-split trees due to its shorter rules. Our framework is more general as it can handle nonlinear metrics such as F1 score, and incorporate a broader class of constraints. We demonstrate its efficacy with extensive experiments. We present results on datasets containing up to 1,008,372 samples while existing MIP-based decision tree models do not scale well on data beyond a few thousand points. We report superior or competitive results compared to the state-of-art MIP-based methods with up to a 24X reduction in runtime",
    "checked": true,
    "id": "4cba7dfa7dc169223b1eff0c7d49955fb1cec695",
    "semantic_title": "scalable optimal multiway-split decision trees with constraints",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26181": {
    "title": "REMIT: Reinforced Multi-Interest Transfer for Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "Cold-start problem is one of the most challenging problems for recommender systems. One promising solution to this problem is cross-domain recommendation (CDR) which leverages rich information from an auxiliary source domain to improve the performance of recommender system in the target domain. In particular, the family of embedding and mapping methods for CDR is very effective, which explicitly learn a mapping function from source embeddings to target embeddings to transfer user's preferences. Recent works usually transfer an overall source embedding by modeling a common or personalized preference bridge for all users. However, a unified user embedding cannot reflect the user's multiple interests in auxiliary source domain. In this paper, we propose a novel framework called reinforced multi-interest transfer for CDR (REMIT). Specifically, we first construct a heterogeneous information network and employ different meta-path based aggregations to get user's multiple interests in source domain, then transform different interest embeddings with different meta-generated personalized bridge functions for each user. To better coordinate the transformed user interest embeddings and the item embedding in target domain, we systematically develop a reinforced method to dynamically assign weights to transformed interests for different training instances and optimize the performance of target model. In addition, the REMIT is a general framework that can be applied upon various base models in target domain. Our extensive experimental results on large real-world datasets demonstrate the superior performance and compatibility of REMIT",
    "checked": true,
    "id": "435514442f10030d15c6f895eb7b10912a766680",
    "semantic_title": "remit: reinforced multi-interest transfer for cross-domain recommendation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26182": {
    "title": "Cooperative and Adversarial Learning: Co-enhancing Discriminability and Transferability in Domain Adaptation",
    "volume": "main",
    "abstract": "Discriminability and transferability are two goals of feature learning for domain adaptation (DA), as we aim to find the transferable features from the source domain that are helpful for discriminating the class label in the target domain. Modern DA approaches optimize discriminability and transferability by adopting two separate modules for the two goals upon a feature extractor, but lack fully exploiting their relationship. This paper argues that by letting the discriminative module and transfer module help each other, better DA can be achieved. We propose Cooperative and Adversarial LEarning (CALE) to combine the optimization of discriminability and transferability into a whole, provide one solution for making the discriminative module and transfer module guide each other. Specifically, CALE generates cooperative (easy) examples and adversarial (hard) examples with both discriminative module and transfer module. While the easy examples that contain the module knowledge can be used to enhance each other, the hard ones are used to enhance the robustness of the corresponding goal. Experimental results show the effectiveness of CALE for unifying the learning of discriminability and transferability, as well as its superior performance",
    "checked": true,
    "id": "c7418ac06f023a210f293a66489b4a8fd4a7e4ea",
    "semantic_title": "cooperative and adversarial learning: co-enhancing discriminability and transferability in domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26183": {
    "title": "Fair-CDA: Continuous and Directional Augmentation for Group Fairness",
    "volume": "main",
    "abstract": "In this work, we propose Fair-CDA, a fine-grained data augmentation strategy for imposing fairness constraints. We use a feature disentanglement method to extract the features highly related to the sensitive attributes. Then we show that group fairness can be achieved by regularizing the models on transition paths of sensitive features between groups. By adjusting the perturbation strength in the direction of the paths, our proposed augmentation is controllable and auditable. To alleviate the accuracy degradation caused by fairness constraints, we further introduce a calibrated model to impute labels for the augmented data. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. Experimental results show that Fair-CDA consistently outperforms state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and MovieLens. Especially, Fair-CDA obtains an 86.3% relative improvement for fairness while maintaining the accuracy on the Adult dataset. Moreover, we evaluate Fair-CDA in an online recommendation system to demonstrate the effectiveness of our method in terms of accuracy and fairness",
    "checked": true,
    "id": "e9030eaaecd19228637d4ddc6ee86745152bd579",
    "semantic_title": "fair-cda: continuous and directional augmentation for group fairness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26184": {
    "title": "Neural Spline Search for Quantile Probabilistic Modeling",
    "volume": "main",
    "abstract": "Accurate estimation of output quantiles is crucial in many use cases, where it is desired to model the range of possibility. Modeling target distribution at arbitrary quantile levels and at arbitrary input attribute levels are important to offer a comprehensive picture of the data, and requires the quantile function to be expressive enough. The quantile function describing the target distribution using quantile levels is critical for quantile regression. Although various parametric forms for the distributions (that the quantile function specifies) can be adopted, an everlasting problem is selecting the most appropriate one that can properly approximate the data distributions. In this paper, we propose a non-parametric and data-driven approach, Neural Spline Search (NSS), to represent the observed data distribution without parametric assumptions. NSS is flexible and expressive for modeling data distributions by transforming the inputs with a series of monotonic spline regressions guided by symbolic operators. We demonstrate that NSS outperforms previous methods on synthetic, real-world regression and time-series forecasting tasks",
    "checked": true,
    "id": "9990802f546a575e2dd7226742e2a94f1ea50302",
    "semantic_title": "neural spline search for quantile probabilistic modeling",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26185": {
    "title": "Domain Adaptation with Adversarial Training on Penultimate Activations",
    "volume": "main",
    "abstract": "Enhancing model prediction confidence on target data is an important objective in Unsupervised Domain Adaptation (UDA). In this paper, we explore adversarial training on penultimate activations, i.e., input features of the final linear classification layer. We show that this strategy is more efficient and better correlated with the objective of boosting prediction confidence than adversarial training on input images or intermediate features, as used in previous works. Furthermore, with activation normalization commonly used in domain adaptation to reduce domain gap, we derive two variants and systematically analyze the effects of normalization on our adversarial training. This is illustrated both in theory and through empirical analysis on real adaptation tasks. Extensive experiments are conducted on popular UDA benchmarks under both standard setting and source-data free setting. The results validate that our method achieves the best scores against previous arts. Code is available at https://github.com/tsun/APA",
    "checked": true,
    "id": "784d2998bf94b0863e7134be4e6d87dcf2884b59",
    "semantic_title": "domain adaptation with adversarial training on penultimate activations",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26186": {
    "title": "Fast Convergence in Learning Two-Layer Neural Networks with Separable Data",
    "volume": "main",
    "abstract": "Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds",
    "checked": true,
    "id": "4ac4e716d5e1aaacb15d6fc4c89decce59d45e37",
    "semantic_title": "fast convergence in learning two-layer neural networks with separable data",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26187": {
    "title": "Federated Learning on Non-IID Graphs via Structural Knowledge Sharing",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have shown their superiority in modeling graph data. Owing to the advantages of federated learning, federated graph learning (FGL) enables clients to train strong GNN models in a distributed manner without sharing their private data. A core challenge in federated systems is the non-IID problem, which also widely exists in real-world graph data. For example, local data of clients may come from diverse datasets or even domains, e.g., social networks and molecules, increasing the difficulty for FGL methods to capture commonly shared knowledge and learn a generalized encoder. From real-world graph datasets, we observe that some structural properties are shared by various domains, presenting great potential for sharing structural knowledge in FGL. Inspired by this, we propose FedStar, an FGL framework that extracts and shares the common underlying structure information for inter-graph federated learning tasks. To explicitly extract the structure information rather than encoding them along with the node features, we define structure embeddings and encode them with an independent structure encoder. Then, the structure encoder is shared across clients while the feature-based knowledge is learned in a personalized way, making FedStar capable of capturing more structure-based domain-invariant information and avoiding feature misalignment issues. We perform extensive experiments over both cross-dataset and cross-domain non-IID FGL settings, demonstrating the superiority of FedStar",
    "checked": true,
    "id": "2f325e874aa12ebbf8f5028076e7c4c75831ce54",
    "semantic_title": "federated learning on non-iid graphs via structural knowledge sharing",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26188": {
    "title": "Metric Multi-View Graph Clustering",
    "volume": "main",
    "abstract": "Graph-based methods have hitherto been used to pursue the coherent patterns of data due to its ease of implementation and efficiency. These methods have been increasingly applied in multi-view learning and achieved promising performance in various clustering tasks. However, despite their noticeable empirical success, existing graph-based multi-view clustering methods may still suffer the suboptimal solution considering that multi-view data can be very complicated in raw feature space. Moreover, existing methods usually adopt the similarity metric by an ad hoc approach, which largely simplifies the relationship among real-world data and results in an inaccurate output. To address these issues, we propose to seamlessly integrates metric learning and graph learning for multi-view clustering. Specifically, we employ a useful metric to depict the inherent structure with linearity-aware of affinity graph representation learned based on the self-expressiveness property. Furthermore, instead of directly utilizing the raw features, we prefer to recover a smooth representation such that the geometric structure of the original data can be retained. We model the above concerns into a unified learning framework, and hence complements each learning subtask in a mutual reinforcement manner. The empirical studies corroborate our theoretical findings, and demonstrate that the proposed method is able to boost the multi-view clustering performance",
    "checked": true,
    "id": "1a01f8994899713953d4a6f4a1cc2a638054a4a5",
    "semantic_title": "metric multi-view graph clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26189": {
    "title": "DE-net: Dynamic Text-Guided Image Editing Adversarial Networks",
    "volume": "main",
    "abstract": "Text-guided image editing models have shown remarkable results. However, there remain two problems. First, they employ fixed manipulation modules for various editing requirements (e.g., color changing, texture changing, content adding and removing), which results in over-editing or insufficient editing. Second, they do not clearly distinguish between text-required and text-irrelevant parts, which leads to inaccurate editing. To solve these limitations, we propose: (i) a Dynamic Editing Block (DEBlock) that composes different editing modules dynamically for various editing requirements. (ii) a Composition Predictor (Comp-Pred), which predicts the composition weights for DEBlock according to the inference on target texts and source images. (iii) a Dynamic text-adaptive Convolution Block (DCBlock) that queries source image features to distinguish text-required parts and text-irrelevant parts. Extensive experiments demonstrate that our DE-Net achieves excellent performance and manipulates source images more correctly and accurately",
    "checked": true,
    "id": "471c1fb7477a94504b7ba3e796bcf3de13db3b57",
    "semantic_title": "de-net: dynamic text-guided image editing adversarial networks",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26190": {
    "title": "Knowledge Amalgamation for Multi-Label Classification via Label Dependency Transfer",
    "volume": "main",
    "abstract": "Multi-label classification (MLC), which assigns multiple labels to each instance, is crucial to domains from computer vision to text mining. Conventional methods for MLC require huge amounts of labeled data to capture complex dependencies between labels. However, such labeled datasets are expensive, or even impossible, to acquire. Worse yet, these pre-trained MLC models can only be used for the particular label set covered in the training data. Despite this severe limitation, few methods exist for expanding the set of labels predicted by pre-trained models. Instead, we acquire vast amounts of new labeled data and retrain a new model from scratch. Here, we propose combining the knowledge from multiple pre-trained models (teachers) to train a new student model that covers the union of the labels predicted by this set of teachers. This student supports a broader label set than any one of its teachers without using labeled data. We call this new problem knowledge amalgamation for multi-label classification. Our new method, Adaptive KNowledge Transfer (ANT), trains a student by learning from each teacher's partial knowledge of label dependencies to infer the global dependencies between all labels across the teachers. We show that ANT succeeds in unifying label dependencies among teachers, outperforming five state-of-the-art methods on eight real-world datasets",
    "checked": true,
    "id": "681b1f0320a724f9586ceba8f1f90d80b0f7d2e4",
    "semantic_title": "knowledge amalgamation for multi-label classification via label dependency transfer",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26191": {
    "title": "Leveraging Contaminated Datasets to Learn Clean-Data Distribution with Purified Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) are known for their strong abilities on capturing the underlying distribution of training instances. Since the seminal work of GAN, many variants of GAN have been proposed. However, existing GANs are almost established on the assumption that the training dataset is clean. But in many real-world applications, this may not hold, that is, the training dataset may be contaminated by a proportion of undesired instances. When training on such datasets, existing GANs will learn a mixture distribution of desired and contaminated instances, rather than the desired distribution of desired data only (target distribution). To learn the target distribution from contaminated datasets, two purified generative adversarial networks (PuriGAN) are developed, in which the discriminators are augmented with the capability to distinguish between target and contaminated instances by leveraging an extra dataset solely composed of contamination instances. We prove that under some mild conditions, the proposed PuriGANs are guaranteed to converge to the distribution of desired instances. Experimental results on several datasets demonstrate that the proposed PuriGANs are able to generate much better images from the desired distribution than comparable baselines when trained on contaminated datasets. In addition, we also demonstrate the usefulness of PuriGAN on downstream applications by applying it to the tasks of semi-supervised anomaly detection on contaminated datasets and PU-learning. Experimental results show that PuriGAN is able to deliver the best performance over comparable baselines on both tasks",
    "checked": true,
    "id": "9d910b5b48859c66df9db3f8c75f653ece102676",
    "semantic_title": "leveraging contaminated datasets to learn clean-data distribution with purified generative adversarial networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26192": {
    "title": "Heterogeneous Graph Masked Autoencoders",
    "volume": "main",
    "abstract": "Generative self-supervised learning (SSL), especially masked autoencoders, has become one of the most exciting learning paradigms and has shown great potential in handling graph data. However, real-world graphs are always heterogeneous, which poses three critical challenges that existing methods ignore: 1) how to capture complex graph structure? 2) how to incorporate various node attributes? and 3) how to encode different node positions? In light of this, we study the problem of generative SSL on heterogeneous graphs and propose HGMAE, a novel heterogeneous graph masked autoencoder model to address these challenges. HGMAE captures comprehensive graph information via two innovative masking techniques and three unique training strategies. In particular, we first develop metapath masking and adaptive attribute masking with dynamic mask rate to enable effective and stable learning on heterogeneous graphs. We then design several training strategies including metapath-based edge reconstruction to adopt complex structural information, target attribute restoration to incorporate various node attributes, and positional feature prediction to encode node positional information. Extensive experiments demonstrate that HGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets. Codes are available at https://github.com/meettyj/HGMAE",
    "checked": true,
    "id": "6109164f49ae02a92579eb0c374fbd5f7948e740",
    "semantic_title": "heterogeneous graph masked autoencoders",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26193": {
    "title": "Unbalanced CO-optimal Transport",
    "volume": "main",
    "abstract": "Optimal transport (OT) compares probability distributions by computing a meaningful alignment between their samples. CO-optimal transport (COOT) takes this comparison further by inferring an alignment between features as well. While this approach leads to better alignments and generalizes both OT and Gromov-Wasserstein distances, we provide a theoretical result showing that it is sensitive to outliers that are omnipresent in real-world data. This prompts us to propose unbalanced COOT for which we provably show its robustness to noise in the compared datasets. To the best of our knowledge, this is the first such result for OT methods in incomparable spaces. With this result in hand, we provide empirical evidence of this robustness for the challenging tasks of heterogeneous domain adaptation with and without varying proportions of classes and simultaneous alignment of samples and features across two single-cell measurements",
    "checked": true,
    "id": "2973a48b471f647a13fbb4d832d0ff5c742a998c",
    "semantic_title": "unbalanced co-optimal transport",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26194": {
    "title": "Linear Regularizers Enforce the Strict Saddle Property",
    "volume": "main",
    "abstract": "Satisfaction of the strict saddle property has become a standard assumption in non-convex optimization, and it ensures that many first-order optimization algorithms will almost always escape saddle points. However, functions exist in machine learning that do not satisfy this property, such as the loss function of a neural network with at least two hidden layers. First-order methods such as gradient descent may converge to non-strict saddle points of such functions, and there do not currently exist any first-order methods that reliably escape non-strict saddle points. To address this need, we demonstrate that regularizing a function with a linear term enforces the strict saddle property, and we provide justification for only regularizing locally, i.e., when the norm of the gradient falls below a certain threshold. We analyze bifurcations that may result from this form of regularization, and then we provide a selection rule for regularizers that depends only on the gradient of an objective function. This rule is shown to guarantee that gradient descent will escape the neighborhoods around a broad class of non-strict saddle points, and this behavior is demonstrated on numerical examples of non-strict saddle points common in the optimization literature",
    "checked": true,
    "id": "1703faed266b7a9dc3e67f5b6254044b86b1a959",
    "semantic_title": "linear regularizers enforce the strict saddle property",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26195": {
    "title": "Policy-Adaptive Estimator Selection for Off-Policy Evaluation",
    "volume": "main",
    "abstract": "Off-policy evaluation (OPE) aims to accurately evaluate the performance of counterfactual policies using only offline logged data. Although many estimators have been developed, there is no single estimator that dominates the others, because the estimators' accuracy can vary greatly depending on a given OPE task such as the evaluation policy, number of actions, and noise level. Thus, the data-driven estimator selection problem is becoming increasingly important and can have a significant impact on the accuracy of OPE. However, identifying the most accurate estimator using only the logged data is quite challenging because the ground-truth estimation accuracy of estimators is generally unavailable. This paper thus studies this challenging problem of estimator selection for OPE for the first time. In particular, we enable an estimator selection that is adaptive to a given OPE task, by appropriately subsampling available logged data and constructing pseudo policies useful for the underlying estimator selection task. Comprehensive experiments on both synthetic and real-world company data demonstrate that the proposed procedure substantially improves the estimator selection compared to a non-adaptive heuristic. Note that complete version with technical appendix is available on arXiv: http://arxiv.org/abs/2211.13904",
    "checked": true,
    "id": "a9d1e2b1ccfdae5eb4ea96e3d1803fe55aaf13c1",
    "semantic_title": "policy-adaptive estimator selection for off-policy evaluation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26196": {
    "title": "A Fair Generative Model Using LeCam Divergence",
    "volume": "main",
    "abstract": "We explore a fairness-related challenge that arises in generative models. The challenge is that biased training data with imbalanced demographics may yield a high asymmetry in size of generated samples across distinct groups. We focus on practically-relevant scenarios wherein demographic labels are not available and therefore the design of a fair generative model is non-straightforward. In this paper, we propose an optimization framework that regulates the unfairness under such practical settings via one statistical measure, LeCam (LC)-divergence. Specifically to quantify the degree of unfairness, we employ a balanced-yet-small reference dataset and then measure its distance with generated samples using the LC-divergence, which is shown to be particularly instrumental to a small size of the reference dataset. We take a variational optimization approach to implement the LC-based measure. Experiments on benchmark real datasets demonstrate that the proposed framework can significantly improve the fairness performance while maintaining realistic sample quality for a wide range of the reference set size all the way down to 1% relative to training set",
    "checked": true,
    "id": "14f5e4afdd089120aa171edb7bfd424f243a618d",
    "semantic_title": "a fair generative model using lecam divergence",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26197": {
    "title": "Efficient Distribution Similarity Identification in Clustered Federated Learning via Principal Angles between Client Data Subspaces",
    "volume": "main",
    "abstract": "Clustered federated learning (FL) has been shown to produce promising results by grouping clients into clusters. This is especially effective in scenarios where separate groups of clients have significant differences in the distributions of their local data. Existing clustered FL algorithms are essentially trying to group together clients with similar distributions so that clients in the same cluster can leverage each other's data to better perform federated learning. However, prior clustered FL algorithms attempt to learn these distribution similarities indirectly during training, which can be quite time consuming as many rounds of federated learning may be required until the formation of clusters is stabilized. In this paper, we propose a new approach to federated learning that directly aims to efficiently identify distribution similarities among clients by analyzing the principal angles between the client data subspaces. Each client applies a truncated singular value decomposition (SVD) step on its local data in a single-shot manner to derive a small set of principal vectors, which provides a signature that succinctly captures the main characteristics of the underlying distribution. This small set of principal vectors is provided to the server so that the server can directly identify distribution similarities among the clients to form clusters. This is achieved by comparing the similarities of the principal angles between the client data subspaces spanned by those principal vectors. The approach provides a simple, yet effective clustered FL framework that addresses a broad range of data heterogeneity issues beyond simpler forms of Non-IIDness like label skews. Our clustered FL approach also enables convergence guarantees for non-convex objectives",
    "checked": true,
    "id": "2f45018de9e420ab4126becceee857331b488637",
    "semantic_title": "efficient distribution similarity identification in clustered federated learning via principal angles between client data subspaces",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26198": {
    "title": "Training-Time Attacks against K-nearest Neighbors",
    "volume": "main",
    "abstract": "Nearest neighbor-based methods are commonly used for classification tasks and as subroutines of other data-analysis methods. An attacker with the capability of inserting their own data points into the training set can manipulate the inferred nearest neighbor structure. We distill this goal to the task of performing a training-set data insertion attack against k-Nearest Neighbor classification (kNN). We prove that computing an optimal training-time (a.k.a. poisoning) attack against kNN classification is NP-Hard, even when k = 1 and the attacker can insert only a single data point. We provide an anytime algorithm to perform such an attack, and a greedy algorithm for general k and attacker budget. We provide theoretical bounds and empirically demonstrate the effectiveness and practicality of our methods on synthetic and real-world datasets. Empirically, we find that kNN is vulnerable in practice and that dimensionality reduction is an effective defense. We conclude with a discussion of open problems illuminated by our analysis",
    "checked": true,
    "id": "76dc6a8643daa0134a5372ca524e275cf756e9a3",
    "semantic_title": "training-time attacks against k-nearest neighbors",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26199": {
    "title": "Machines of Finite Depth: Towards a Formalization of Neural Networks",
    "volume": "main",
    "abstract": "We provide a unifying framework where artificial neural networks and their architectures can be formally described as particular cases of a general mathematical construction---machines of finite depth. Unlike neural networks, machines have a precise definition, from which several properties follow naturally. Machines of finite depth are modular (they can be combined), efficiently computable, and differentiable. The backward pass of a machine is again a machine and can be computed without overhead using the same procedure as the forward pass. We prove this statement theoretically and practically via a unified implementation that generalizes several classical architectures---dense, convolutional, and recurrent neural networks with a rich shortcut structure---and their respective backpropagation rules",
    "checked": true,
    "id": "5b932ddfe7053b49edf1e7d32a4a3cbfa70288e4",
    "semantic_title": "machines of finite depth: towards a formalization of neural networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26200": {
    "title": "Kalman Bayesian Neural Networks for Closed-Form Online Learning",
    "volume": "main",
    "abstract": "Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art",
    "checked": true,
    "id": "89f0e11f76c4d91b0838a940c31203e5635a45ce",
    "semantic_title": "kalman bayesian neural networks for closed-form online learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26201": {
    "title": "Auto-Weighted Multi-View Clustering for Large-Scale Data",
    "volume": "main",
    "abstract": "Multi-view clustering has gained broad attention owing to its capacity to exploit complementary information across multiple data views. Although existing methods demonstrate delightful clustering performance, most of them are of high time complexity and cannot handle large-scale data. Matrix factorization-based models are a representative of solving this problem. However, they assume that the views share a dimension-fixed consensus coefficient matrix and view-specific base matrices, limiting their representability. Moreover, a series of large-scale algorithms that bear one or more hyperparameters are impractical in real-world applications. To address the two issues, we propose an auto-weighted multi-view clustering (AWMVC) algorithm. Specifically, AWMVC first learns coefficient matrices from corresponding base matrices of different dimensions, then fuses them to obtain an optimal consensus matrix. By mapping original features into distinctive low-dimensional spaces, we can attain more comprehensive knowledge, thus obtaining better clustering results. Moreover, we design a six-step alternative optimization algorithm proven to be convergent theoretically. Also, AWMVC shows excellent performance on various benchmark datasets compared with existing ones. The code of AWMVC is publicly available at https://github.com/wanxinhang/AAAI-2023-AWMVC",
    "checked": true,
    "id": "4140a8b76168c5817dab043b3dc3ef9bc6e9cf72",
    "semantic_title": "auto-weighted multi-view clustering for large-scale data",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26202": {
    "title": "Quantum Multi-Armed Bandits and Stochastic Linear Bandits Enjoy Logarithmic Regrets",
    "volume": "main",
    "abstract": "Multi-arm bandit (MAB) and stochastic linear bandit (SLB) are important models in reinforcement learning, and it is well-known that classical algorithms for bandits with time horizon T suffer from the regret of at least the square root of T. In this paper, we study MAB and SLB with quantum reward oracles and propose quantum algorithms for both models with the order of the polylog T regrets, exponentially improving the dependence in terms of T. To the best of our knowledge, this is the first provable quantum speedup for regrets of bandit problems and in general exploitation in reinforcement learning. Compared to previous literature on quantum exploration algorithms for MAB and reinforcement learning, our quantum input model is simpler and only assumes quantum oracles for each individual arm",
    "checked": true,
    "id": "218859bdb4492fcfa4b2f5fb2c945c25ae37691d",
    "semantic_title": "quantum multi-armed bandits and stochastic linear bandits enjoy logarithmic regrets",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26203": {
    "title": "FedABC: Targeting Fair Competition in Personalized Federated Learning",
    "volume": "main",
    "abstract": "Federated learning aims to collaboratively train models without accessing their client's local private data. The data may be Non-IID for different clients and thus resulting in poor performance. Recently, personalized federated learning (PFL) has achieved great success in handling Non-IID data by enforcing regularization in local optimization or improving the model aggregation scheme on the server. However, most of the PFL approaches do not take into account the unfair competition issue caused by the imbalanced data distribution and lack of positive samples for some classes in each client. To address this issue, we propose a novel and generic PFL framework termed Federated Averaging via Binary Classification, dubbed FedABC. In particular, we adopt the ``one-vs-all'' training strategy in each client to alleviate the unfair competition between classes by constructing a personalized binary classification problem for each class. This may aggravate the class imbalance challenge and thus a novel personalized binary classification loss that incorporates both the under-sampling and hard sample mining strategies is designed. Extensive experiments are conducted on two popular datasets under different settings, and the results demonstrate that our FedABC can significantly outperform the existing counterparts",
    "checked": true,
    "id": "7ea77fcebe88acbb1396549b297965ad40d38875",
    "semantic_title": "fedabc: targeting fair competition in personalized federated learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26204": {
    "title": "Spearman Rank Correlation Screening for Ultrahigh-Dimensional Censored Data",
    "volume": "main",
    "abstract": "Herein, we propose a Spearman rank correlation-based screening procedure for ultrahigh-dimensional data with censored response cases. The proposed method is model-free without specifying any regression forms of predictors or response variables and is robust under the unknown monotone transformations of these response variable and predictors. The sure-screening and rank-consistency properties are established under some mild regularity conditions. Simulation studies demonstrate that the new screening method performs well in the presence of a heavy-tailed distribution, strongly dependent predictors or outliers, and offers superior performance over the existing nonparametric screening procedures. In particular, the new screening method still works well when a response variable is observed under a high censoring rate. An illustrative example is provided",
    "checked": false,
    "id": "da2815a08ac8a2885f9c6bec3b2de513bebf683d",
    "semantic_title": "cluster with informative cluster a fast and scalable approach",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26205": {
    "title": "Stability-Based Generalization Analysis for Mixtures of Pointwise and Pairwise Learning",
    "volume": "main",
    "abstract": "Recently, some mixture algorithms of pointwise and pairwise learning (PPL) have been formulated by employing the hybrid error metric of \"pointwise loss + pairwise loss\" and have shown empirical effectiveness on feature selection, ranking and recommendation tasks. However, to the best of our knowledge, the learning theory foundation of PPL has not been touched in the existing works. In this paper, we try to fill this theoretical gap by investigating the generalization properties of PPL. After extending the definitions of algorithmic stability to the PPL setting, we establish the high-probability generalization bounds for uniformly stable PPL algorithms. Moreover, explicit convergence rates of stochastic gradient descent (SGD) and regularized risk minimization (RRM) for PPL are stated by developing the stability analysis technique of pairwise learning. In addition, the refined generalization bounds of PPL are obtained by replacing uniform stability with on-average stability",
    "checked": true,
    "id": "5f79eb1c0d0a044ed0dbb36883ff10a0b09867c5",
    "semantic_title": "stability-based generalization analysis for mixtures of pointwise and pairwise learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26206": {
    "title": "Effective Continual Learning for Text Classification with Lightweight Snapshots",
    "volume": "main",
    "abstract": "Continual learning is known for suffering from catastrophic forgetting, a phenomenon where previously learned concepts are forgotten upon learning new tasks. A natural remedy is to use trained models for old tasks as ‘teachers' to regularize the update of the current model to prevent such forgetting. However, this requires storing all past models, which is very space-consuming for large models, e.g. BERT, thus impractical in real-world applications. To tackle this issue, we propose to construct snapshots of seen tasks whose key knowledge is captured in lightweight adapters. During continual learning, we transfer knowledge from past snapshots to the current model through knowledge distillation, allowing the current model to review previously learned knowledge while learning new tasks. We also design representation recalibration to better handle the class-incremental setting. Experiments over various task sequences show that our approach effectively mitigates catastrophic forgetting and outperforms all baselines",
    "checked": true,
    "id": "a21037a45a1a7b886fd3e09e953cb79de19ccc3d",
    "semantic_title": "effective continual learning for text classification with lightweight snapshots",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26207": {
    "title": "Optimistic Whittle Index Policy: Online Learning for Restless Bandits",
    "volume": "main",
    "abstract": "Restless multi-armed bandits (RMABs) extend multi-armed bandits to allow for stateful arms, where the state of each arm evolves restlessly with different transitions depending on whether that arm is pulled. Solving RMABs requires information on transition dynamics, which are often unknown upfront. To plan in RMAB settings with unknown transitions, we propose the first online learning algorithm based on the Whittle index policy, using an upper confidence bound (UCB) approach to learn transition dynamics. Specifically, we estimate confidence bounds of the transition probabilities and formulate a bilinear program to compute optimistic Whittle indices using these estimates. Our algorithm, UCWhittle, achieves sublinear O(H \\sqrt{T log T}) frequentist regret to solve RMABs with unknown transitions in T episodes with a constant horizon H. Empirically, we demonstrate that UCWhittle leverages the structure of RMABs and the Whittle index policy solution to achieve better performance than existing online learning baselines across three domains, including one constructed from a real-world maternal and childcare dataset",
    "checked": true,
    "id": "44a32991258416e1fb0bbc2d91a960edce960f2e",
    "semantic_title": "optimistic whittle index policy: online learning for restless bandits",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26208": {
    "title": "AEC-GAN: Adversarial Error Correction GANs for Auto-Regressive Long Time-Series Generation",
    "volume": "main",
    "abstract": "Large-scale high-quality data is critical for training modern deep neural networks. However, data acquisition can be costly or time-consuming for many time-series applications, thus researchers turn to generative models for generating synthetic time-series data. In particular, recent generative adversarial networks (GANs) have achieved remarkable success in time-series generation. Despite their success, existing GAN models typically generate the sequences in an auto-regressive manner, and we empirically observe that they suffer from severe distribution shifts and bias amplification, especially when generating long sequences. To resolve this problem, we propose Adversarial Error Correction GAN (AEC-GAN), which is capable of dynamically correcting the bias in the past generated data to alleviate the risk of distribution shifts and thus can generate high-quality long sequences. AEC-GAN contains two main innovations: (1) We develop an error correction module to mitigate the bias. In the training phase, we adversarially perturb the realistic time-series data and then optimize this module to reconstruct the original data. In the generation phase, this module can act as an efficient regulator to detect and mitigate the bias. (2) We propose an augmentation method to facilitate GAN's training by introducing adversarial examples. Thus, AEC-GAN can generate high-quality sequences of arbitrary lengths, and the synthetic data can be readily applied to downstream tasks to boost their performance. We conduct extensive experiments on six widely used datasets and three state-of-the-art time-series forecasting models to evaluate the quality of our synthetic time-series data in different lengths and downstream tasks. Both the qualitative and quantitative experimental results demonstrate the superior performance of AEC-GAN over other deep generative models for time-series generation",
    "checked": true,
    "id": "7a084f14b45e83fd2d870d0cdbe38560cd2c07c9",
    "semantic_title": "aec-gan: adversarial error correction gans for auto-regressive long time-series generation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26209": {
    "title": "The Implicit Regularization of Momentum Gradient Descent in Overparametrized Models",
    "volume": "main",
    "abstract": "The study of the implicit regularization induced by gradient-based optimization in deep learning is a long-standing pursuit. In the present paper, we characterize the implicit regularization of momentum gradient descent (MGD) in the continuous-time view, so-called momentum gradient flow (MGF). We show that the components of weight vector are learned for a deep linear neural networks at different evolution rates, and this evolution gap increases with the depth. Firstly, we show that if the depth equals one, the evolution gap between the weight vector components is linear, which is consistent with the performance of ridge. In particular, we establish a tight coupling between MGF and ridge for the least squares regression. In detail, we show that when the regularization parameter of ridge is inversely proportional to the square of the time parameter of MGF, the risk of MGF is no more than 1.54 times that of ridge, and their relative Bayesian risks are almost indistinguishable. Secondly, if the model becomes deeper, i.e. the depth is greater than or equal to 2, the evolution gap becomes more significant, which implies an implicit bias towards sparse solutions. The numerical experiments strongly support our theoretical results",
    "checked": true,
    "id": "ca887ea35e2eeeb17801cbed3515ea4965c65789",
    "semantic_title": "the implicit regularization of momentum gradient descent in overparametrized models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26210": {
    "title": "Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning",
    "volume": "main",
    "abstract": "Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to react at the first sight of a task change, thus being applicable in non-stationary tasks. MoSS also exhibits strong generalization robustness in out-of-distributions tasks which benefits from the reliable and robust task representation. The policy is built on top of an off-policy RL algorithm and the entire network is trained completely off-policy to ensure high sample efficiency. On MuJoCo and Meta-World benchmarks, MoSS outperforms prior works in terms of asymptotic performance, sample efficiency (3-50x faster), adaptation efficiency, and generalization robustness on broad and diverse task distributions",
    "checked": true,
    "id": "eabfc1846af8f09d6c65e45cb04f83fa397f6799",
    "semantic_title": "meta-reinforcement learning based on self-supervised task representation learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26211": {
    "title": "Hierarchical Contrastive Learning for Temporal Point Processes",
    "volume": "main",
    "abstract": "As an important sequential model, the temporal point process (TPP) plays a central role in real-world sequence modeling and analysis, whose learning is often based on the maximum likelihood estimation (MLE). However, due to imperfect observations, such as incomplete and sparse sequences that are common in practice, the MLE of TPP models often suffers from overfitting and leads to unsatisfactory generalization power. In this work, we develop a novel hierarchical contrastive (HCL) learning method for temporal point processes, which provides a new regularizer of MLE. In principle, our HCL considers the noise contrastive estimation (NCE) problem at the event-level and at the sequence-level jointly. Given a sequence, the event-level NCE maximizes the probability of each observed event given its history while penalizing the conditional probabilities of the unobserved events. At the same time, we generate positive and negative event sequences from the observed sequence and maximize the discrepancy between their likelihoods through the sequence-level NCE. Instead of using time-consuming simulation methods, we generate the positive and negative sequences via a simple but efficient model-guided thinning process. Experimental results show that the MLE method assisted by the HCL regularizer outperforms classic MLE and other contrastive learning methods in learning various TPP models consistently. The code is available at https://github.com/qingmeiwangdaily/HCL_TPP",
    "checked": true,
    "id": "a074e060cde0ddbb535e22863df43322a5627efb",
    "semantic_title": "hierarchical contrastive learning for temporal point processes",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26212": {
    "title": "Beyond ADMM: A Unified Client-Variance-Reduced Adaptive Federated Learning Framework",
    "volume": "main",
    "abstract": "As a novel distributed learning paradigm, federated learning (FL) faces serious challenges in dealing with massive clients with heterogeneous data distribution and computation and communication resources. Various client-variance-reduction schemes and client sampling strategies have been respectively introduced to improve the robustness of FL. Among others, primal-dual algorithms such as the alternating direction of method multipliers (ADMM) have been found being resilient to data distribution and outperform most of the primal-only FL algorithms. However, the reason behind remains a mystery still. In this paper, we firstly reveal the fact that the federated ADMM is essentially a client-variance-reduced algorithm. While this explains the inherent robustness of federated ADMM, the vanilla version of it lacks the ability to be adaptive to the degree of client heterogeneity. Besides, the global model at the server under client sampling is biased which slows down the practical convergence. To go beyond ADMM, we propose a novel primal-dual FL algorithm, termed FedVRA, that allows one to adaptively control the variance-reduction level and biasness of the global model. In addition, FedVRA unifies several representative FL algorithms in the sense that they are either special instances of FedVRA or are close to it. Extensions of FedVRA to semi/un-supervised learning are also presented. Experiments based on (semi-)supervised image classification tasks demonstrate superiority of FedVRA over the existing schemes in learning scenarios with massive heterogeneous clients and client sampling",
    "checked": true,
    "id": "e10a8abd5c7f2d6ecbe76412a56762d61b0efcbf",
    "semantic_title": "beyond admm: a unified client-variance-reduced adaptive federated learning framework",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26213": {
    "title": "State-Conditioned Adversarial Subgoal Generation",
    "volume": "main",
    "abstract": "Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks by performing decision-making and control at successively higher levels of temporal abstraction. However, off-policy HRL often suffers from the problem of a non-stationary high-level policy since the low-level policy is constantly changing. In this paper, we propose a novel HRL approach for mitigating the non-stationarity by adversarially enforcing the high-level policy to generate subgoals compatible with the current instantiation of the low-level policy. In practice, the adversarial learning is implemented by training a simple state conditioned discriminator network concurrently with the high-level policy which determines the compatibility level of subgoals. Comparison to state-of-the-art algorithms shows that our approach improves both learning efficiency and performance in challenging continuous control tasks",
    "checked": true,
    "id": "c397a67aad5bd5d08364135ac01b028af930c604",
    "semantic_title": "state-conditioned adversarial subgoal generation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26214": {
    "title": "Deep Attentive Model for Knowledge Tracing",
    "volume": "main",
    "abstract": "Knowledge Tracing (KT) is a crucial task in the field of online education, since it aims to predict students' performance on exercises based on their learning history. One typical solution for knowledge tracing is to combine the classic models in educational psychology, such as Item Response Theory (IRT) and Cognitive Diagnosis (CD), with Deep Neural Networks (DNN) technologies. In this solution, a student and related exercises are mapped into feature vectors based on the student's performance at the current time step, however, it does not consider the impact of historical behavior sequences, and the relationships between historical sequences and students. In this paper, we develop DAKTN, a novel model which assimilates the historical sequences to tackle this challenge for better knowledge tracing. To be specific, we apply a pooling layer to incorporate the student behavior sequence in the embedding layer. After that, we further design a local activation unit, which can adaptively calculate the representation vectors by taking the relevance of historical sequences into consideration with respect to candidate student and exercises. Through experimental results on three real-world datasets, DAKTN significantly outperforms state-of-the-art baseline models. We also present the reasonableness of DAKTN by ablation testing",
    "checked": true,
    "id": "e0fc4b0410f3f7a31e3cc1c2b96d856d51fbd82a",
    "semantic_title": "deep attentive model for knowledge tracing",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26215": {
    "title": "Correspondence-Free Domain Alignment for Unsupervised Cross-Domain Image Retrieval",
    "volume": "main",
    "abstract": "Cross-domain image retrieval aims at retrieving images across different domains to excavate cross-domain classificatory or correspondence relationships. This paper studies a less-touched problem of cross-domain image retrieval, i.e., unsupervised cross-domain image retrieval, considering the following practical assumptions: (i) no correspondence relationship, and (ii) no category annotations. It is challenging to align and bridge distinct domains without cross-domain correspondence. To tackle the challenge, we present a novel Correspondence-free Domain Alignment (CoDA) method to effectively eliminate the cross-domain gap through In-domain Self-matching Supervision (ISS) and Cross-domain Classifier Alignment (CCA). To be specific, ISS is presented to encapsulate discriminative information into the latent common space by elaborating a novel self-matching supervision mechanism. To alleviate the cross-domain discrepancy, CCA is proposed to align distinct domain-specific classifiers. Thanks to the ISS and CCA, our method could encode the discrimination into the domain-invariant embedding space for unsupervised cross-domain image retrieval. To verify the effectiveness of the proposed method, extensive experiments are conducted on four benchmark datasets compared with six state-of-the-art methods",
    "checked": true,
    "id": "5a572fb106197d8d4f0a81f26abb2b20e2e0aa3f",
    "semantic_title": "correspondence-free domain alignment for unsupervised cross-domain image retrieval",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26216": {
    "title": "Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference",
    "volume": "main",
    "abstract": "This paper focuses on the prevalent stage interference and stage performance imbalance of incremental learning. To avoid obvious stage learning bottlenecks, we propose a new incremental learning framework, which leverages a series of stage-isolated classifiers to perform the learning task at each stage, without interference from others. To be concrete, to aggregate multiple stage classifiers as a uniform one impartially, we first introduce a temperature-controlled energy metric for indicating the confidence score levels of the stage classifiers. We then propose an anchor-based energy self-normalization strategy to ensure the stage classifiers work at the same energy level. Finally, we design a voting-based inference augmentation strategy for robust inference. The proposed method is rehearsal-free and can work for almost all incremental learning scenarios. We evaluate the proposed method on four large datasets. Extensive results demonstrate the superiority of the proposed method in setting up new state-of-the-art overall performance. Code is available at https://github.com/iamwangyabin/ESN",
    "checked": true,
    "id": "30fd486635c6174ea340c0ee003b241d32f011ce",
    "semantic_title": "isolation and impartial aggregation: a paradigm of incremental learning without interference",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26217": {
    "title": "Robust Self-Supervised Multi-Instance Learning with Structure Awareness",
    "volume": "main",
    "abstract": "Multi-instance learning (MIL) is a supervised learning where each example is a labeled bag with many instances. The typical MIL strategies are to train an instance-level feature extractor followed by aggregating instances features as bag-level representation with labeled information. However, learning such a bag-level representation highly depends on a large number of labeled datasets, which are difficult to get in real-world scenarios. In this paper, we make the first attempt to propose a robust Self-supervised Multi-Instance LEarning architecture with Structure awareness (SMILEs) that learns unsupervised bag representation. Our proposed approach is: 1) permutation invariant to the order of instances in bag; 2) structure-aware to encode the topological structures among the instances; and 3) robust against instances noise or permutation. Specifically, to yield robust MIL model without label information, we augment the multi-instance bag and train the representation encoder to maximize the correspondence between the representations of the same bag in its different augmented forms. Moreover, to capture topological structures from nearby instances in bags, our framework learns optimal graph structures for the bags and these graphs are optimized together with message passing layers and the ordered weighted averaging operator towards contrastive loss. Our main theorem characterizes the permutation invariance of the bag representation. Compared with state-of-the-art supervised MIL baselines, SMILEs achieves average improvement of 4.9%, 4.4% in classification accuracy on 5 benchmark datasets and 20 newsgroups datasets, respectively. In addition, we show that the model is robust to the input corruption",
    "checked": true,
    "id": "4494cded00ff0ef0d686cc2261b891686d7ad3d6",
    "semantic_title": "robust self-supervised multi-instance learning with structure awareness",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26218": {
    "title": "Distributed Projection-Free Online Learning for Smooth and Convex Losses",
    "volume": "main",
    "abstract": "We investigate the problem of distributed online convex optimization with complicated constraints, in which the projection operation could be the computational bottleneck. To avoid projections, distributed online projection-free methods have been proposed and attain an O(T^{3/4}) regret bound for general convex losses. However, they cannot utilize the smoothness condition, which has been exploited in the centralized setting to improve the regret. In this paper, we propose a new distributed online projection-free method with a tighter regret bound of O(T^{2/3}) for smooth and convex losses. Specifically, we first provide a distributed extension of Follow-the-Perturbed-Leader so that the smoothness can be utilized in the distributed setting. Then, we reduce the computational cost via sampling and blocking techniques. In this way, our method only needs to solve one linear optimization per round on average. Finally, we conduct experiments on benchmark datasets to verify the effectiveness of our proposed method",
    "checked": true,
    "id": "3d2f11656462ed2dfc3c58da48ba12609460541b",
    "semantic_title": "distributed projection-free online learning for smooth and convex losses",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26219": {
    "title": "USER: Unsupervised Structural Entropy-Based Robust Graph Neural Network",
    "volume": "main",
    "abstract": "Unsupervised/self-supervised graph neural networks (GNN) are susceptible to the inherent randomness in the input graph data, which adversely affects the model's performance in downstream tasks. In this paper, we propose USER, an unsupervised and robust version of GNN based on structural entropy, to alleviate the interference of graph perturbations and learn appropriate representations of nodes without label information. To mitigate the effects of undesirable perturbations, we analyze the property of intrinsic connectivity and define the intrinsic connectivity graph. We also identify the rank of the adjacency matrix as a crucial factor in revealing a graph that provides the same embeddings as the intrinsic connectivity graph. To capture such a graph, we introduce structural entropy in the objective function. Extensive experiments conducted on clustering and link prediction tasks under random-perturbation and meta-attack over three datasets show that USER outperforms benchmarks and is robust to heavier perturbations",
    "checked": true,
    "id": "bd7dbcb7e51edc56427b5eb1c7d0a4275dfe44a6",
    "semantic_title": "user: unsupervised structural entropy-based robust graph neural network",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26220": {
    "title": "AutoNF: Automated Architecture Optimization of Normalizing Flows with Unconstrained Continuous Relaxation Admitting Optimal Discrete Solution",
    "volume": "main",
    "abstract": "Normalizing flows (NF) build upon invertible neural networks and have wide applications in probabilistic modeling. Currently, building a powerful yet computationally efficient flow model relies on empirical fine-tuning over a large design space. While introducing neural architecture search (NAS) to NF is desirable, the invertibility constraint of NF brings new challenges to existing NAS methods whose application is limited to unstructured neural networks. Developing efficient NAS methods specifically for NF remains an open problem. We present AutoNF, the first automated NF architectural optimization framework. First, we present a new mixture distribution formulation that allows efficient differentiable architecture search of flow models without violating the invertibility constraint. Second, under the new formulation, we convert the original NP-hard combinatorial NF architectural optimization problem to an unconstrained continuous relaxation admitting the discrete optimal architectural solution, circumventing the loss of optimality due to binarization in architectural optimization. We evaluate AutoNF with various density estimation datasets and show its superior performance-cost trade-offs over a set of existing hand-crafted baselines",
    "checked": true,
    "id": "79a5783a3dc2c19c632bc5d39a57b17c87ce9633",
    "semantic_title": "autonf: automated architecture optimization of normalizing flows with unconstrained continuous relaxation admitting optimal discrete solution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26221": {
    "title": "SEnsor Alignment for Multivariate Time-Series Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Unsupervised Domain Adaptation (UDA) methods can reduce label dependency by mitigating the feature discrepancy between labeled samples in a source domain and unlabeled samples in a similar yet shifted target domain. Though achieving good performance, these methods are inapplicable for Multivariate Time-Series (MTS) data. MTS data are collected from multiple sensors, each of which follows various distributions. However, most UDA methods solely focus on aligning global features but cannot consider the distinct distributions of each sensor. To cope with such concerns, a practical domain adaptation scenario is formulated as Multivariate Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose SEnsor Alignment (SEA) for MTS-UDA to reduce the domain discrepancy at both the local and global sensor levels. At the local sensor level, we design the endo-feature alignment to align sensor features and their correlations across domains, whose information represents the features of each sensor and the interactions between sensors. Further, to reduce domain discrepancy at the global sensor level, we design the exo-feature alignment to enforce restrictions on the global sensor features. Meanwhile, MTS also incorporates the essential spatial-temporal dependencies information between sensors, which cannot be transferred by existing UDA methods. Therefore, we model the spatial-temporal information of MTS with a multi-branch self-attention mechanism for simple and effective transfer across domains. Empirical results demonstrate the state-of-the-art performance of our proposed SEA on two public MTS datasets for MTS-UDA. The code is available at https://github.com/Frank-Wang-oss/SEA",
    "checked": true,
    "id": "33c948c4a5631806830df3dda01528cabd44bede",
    "semantic_title": "sensor alignment for multivariate time-series unsupervised domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26222": {
    "title": "Unlabeled Imperfect Demonstrations in Adversarial Imitation Learning",
    "volume": "main",
    "abstract": "Adversarial imitation learning has become a widely used imitation learning framework. The discriminator is often trained by taking expert demonstrations and policy trajectories as examples respectively from two categories (positive vs. negative) and the policy is then expected to produce trajectories that are indistinguishable from the expert demonstrations. But in the real world, the collected expert demonstrations are more likely to be imperfect, where only an unknown fraction of the demonstrations are optimal. Instead of treating imperfect expert demonstrations as absolutely positive or negative, we investigate unlabeled imperfect expert demonstrations as they are. A positive-unlabeled adversarial imitation learning algorithm is developed to dynamically sample expert demonstrations that can well match the trajectories from the constantly optimized agent policy. The trajectories of an initial agent policy could be closer to those non-optimal expert demonstrations, but within the framework of adversarial imitation learning, agent policy will be optimized to cheat the discriminator and produce trajectories that are similar to those optimal expert demonstrations. Theoretical analysis shows that our method learns from the imperfect demonstrations via a self-paced way. Experimental results on MuJoCo and RoboSuite platforms demonstrate the effectiveness of our method from different aspects",
    "checked": true,
    "id": "fbf48e013c963aa90fa3bfd04604935d952d674a",
    "semantic_title": "unlabeled imperfect demonstrations in adversarial imitation learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26223": {
    "title": "FedGS: Federated Graph-Based Sampling with Arbitrary Client Availability",
    "volume": "main",
    "abstract": "While federated learning has shown strong results in opti- mizing a machine learning model without direct access to the original data, its performance may be hindered by in- termittent client availability which slows down the conver- gence and biases the final learned model. There are significant challenges to achieve both stable and bias-free training un- der arbitrary client availability. To address these challenges, we propose a framework named Federated Graph-based Sam- pling (FEDGS), to stabilize the global model update and mitigate the long-term bias given arbitrary client availabil- ity simultaneously. First, we model the data correlations of clients with a Data-Distribution-Dependency Graph (3DG) that helps keep the sampled clients data apart from each other, which is theoretically shown to improve the approximation to the optimal model update. Second, constrained by the far- distance in data distribution of the sampled clients, we fur- ther minimize the variance of the numbers of times that the clients are sampled, to mitigate long-term bias. To validate the effectiveness of FEDGS, we conduct experiments on three datasets under a comprehensive set of seven client availability modes. Our experimental results confirm FEDGS's advantage in both enabling a fair client-sampling scheme and improving the model performance under arbitrary client availability. Our code is available at https://github.com/WwZzz/FedGS",
    "checked": true,
    "id": "98a325486370aae953268046e9e1d6fb2ed86fed",
    "semantic_title": "fedgs: federated graph-based sampling with arbitrary client availability",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26224": {
    "title": "Efficient Exploration in Resource-Restricted Reinforcement Learning",
    "volume": "main",
    "abstract": "In many real-world applications of reinforcement learning (RL), performing actions requires consuming certain types of resources that are non-replenishable in each episode. Typical applications include robotic control with limited energy and video games with consumable items. In tasks with non-replenishable resources, we observe that popular RL methods such as soft actor critic suffer from poor sample efficiency. The major reason is that, they tend to exhaust resources fast and thus the subsequent exploration is severely restricted due to the absence of resources. To address this challenge, we first formalize the aforementioned problem as a resource-restricted reinforcement learning, and then propose a novel resource-aware exploration bonus (RAEB) to make reasonable usage of resources. An appealing feature of RAEB is that, it can significantly reduce unnecessary resource-consuming trials while effectively encouraging the agent to explore unvisited states. Experiments demonstrate that the proposed RAEB significantly outperforms state-of-the-art exploration strategies in resource-restricted reinforcement learning environments, improving the sample efficiency by up to an order of magnitude",
    "checked": true,
    "id": "3d3b0704c61d47c7bafb70ae2670b2786b8e4d81",
    "semantic_title": "efficient exploration in resource-restricted reinforcement learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26225": {
    "title": "Efficient Explorative Key-Term Selection Strategies for Conversational Contextual Bandits",
    "volume": "main",
    "abstract": "Conversational contextual bandits elicit user preferences by occasionally querying for explicit feedback on key-terms to accelerate learning. However, there are aspects of existing approaches which limit their performance. First, information gained from key-term-level conversations and arm-level recommendations is not appropriately incorporated to speed up learning. Second, it is important to ask explorative key-terms to quickly elicit the user's potential interests in various domains to accelerate the convergence of user preference estimation, which has never been considered in existing works. To tackle these issues, we first propose ``ConLinUCB\", a general framework for conversational bandits with better information incorporation, combining arm-level and key-term-level feedback to estimate user preference in one step at each time. Based on this framework, we further design two bandit algorithms with explorative key-term selection strategies, ConLinUCB-BS and ConLinUCB-MCR. We prove tighter regret upper bounds of our proposed algorithms. Particularly, ConLinUCB-BS achieves a better regret bound than the previous result. Extensive experiments on synthetic and real-world data show significant advantages of our algorithms in learning accuracy (up to 54% improvement) and computational efficiency (up to 72% improvement), compared to the classic ConUCB algorithm, showing the potential benefit to recommender systems",
    "checked": true,
    "id": "79aaa0a554a1f415c9cc59eeeab8ebda4fc15379",
    "semantic_title": "efficient explorative key-term selection strategies for conversational contextual bandits",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26226": {
    "title": "Code-Aware Cross-Program Transfer Hyperparameter Optimization",
    "volume": "main",
    "abstract": "Hyperparameter tuning is an essential task in automatic machine learning and big data management. To accelerate tuning, many recent studies focus on augmenting BO, the primary hyperparameter tuning strategy, by transferring information from other tuning tasks. However, existing studies ignore program similarities in their transfer mechanism, thus they are sub-optimal in cross-program transfer when tuning tasks involve different programs. This paper proposes CaTHPO, a code-aware cross-program transfer hyperparameter optimization framework, which makes three improvements. (1) It learns code-aware program representation in a self-supervised manner to give an off-the-shelf estimate of program similarities. (2) It adjusts the surrogate and AF in BO based on program similarities, thus the hyperparameter search is guided by accumulated information across similar programs. (3) It presents a safe controller to dynamically prune undesirable sample points based on tuning experiences of similar programs. Extensive experiments on tuning various recommendation models and Spark applications have demonstrated that CatHPO can steadily obtain better and more robust hyperparameter performances within fewer samples than state-of-the-art competitors",
    "checked": true,
    "id": "27c5a594fb37d33640fadeaa07d0466211a60578",
    "semantic_title": "code-aware cross-program transfer hyperparameter optimization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26227": {
    "title": "Predictive Multiplicity in Probabilistic Classification",
    "volume": "main",
    "abstract": "Machine learning models are often used to inform real world risk assessment tasks: predicting consumer default risk, predicting whether a person suffers from a serious illness, or predicting a person's risk to appear in court. Given multiple models that perform almost equally well for a prediction task, to what extent do predictions vary across these models? If predictions are relatively consistent for similar models, then the standard approach of choosing the model that optimizes a penalized loss suffices. But what if predictions vary significantly for similar models? In machine learning, this is referred to as predictive multiplicity i.e. the prevalence of conflicting predictions assigned by near-optimal competing models. In this paper, we present a framework for measuring predictive multiplicity in probabilistic classification (predicting the probability of a positive outcome). We introduce measures that capture the variation in risk estimates over the set of competing models, and develop optimization-based methods to compute these measures efficiently and reliably for convex empirical risk minimization problems. We demonstrate the incidence and prevalence of predictive multiplicity in real-world tasks. Further, we provide insight into how predictive multiplicity arises by analyzing the relationship between predictive multiplicity and data set characteristics (outliers, separability, and majority-minority structure). Our results emphasize the need to report predictive multiplicity more widely",
    "checked": true,
    "id": "4217a85c206e0dafd8fc39da2adc55c311ba373a",
    "semantic_title": "predictive multiplicity in probabilistic classification",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26228": {
    "title": "Feature Distribution Fitting with Direction-Driven Weighting for Few-Shot Images Classification",
    "volume": "main",
    "abstract": "Few-shot learning has received increasing attention and witnessed significant advances in recent years. However, most of the few-shot learning methods focus on the optimization of training process, and the learning of metric and sample generating networks. They ignore the importance of learning the ground-truth feature distributions of few-shot classes. This paper proposes a direction-driven weighting method to make the feature distributions of few-shot classes precisely fit the ground-truth distributions. The learned feature distributions can generate an unlimited number of training samples for the few-shot classes to avoid overfitting. Specifically, the proposed method consists of two optimization strategies. The direction-driven strategy is for capturing more complete direction information that can describe the feature distributions. The similarity-weighting strategy is proposed to estimate the impact of different classes in the fitting procedure and assign corresponding weights. Our method outperforms the current state-of-the-art performance by an average of 3% for 1-shot on standard few-shot learning benchmarks like miniImageNet, CIFAR-FS, and CUB. The excellent performance and compelling visualization show that our method can more accurately estimate the ground-truth distributions",
    "checked": true,
    "id": "f8680befeea08e92981739a9cf68e3332f9fa7f5",
    "semantic_title": "feature distribution fitting with direction-driven weighting for few-shot images classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26229": {
    "title": "Learning Instrumental Variable from Data Fusion for Treatment Effect Estimation",
    "volume": "main",
    "abstract": "The advent of the big data era brought new opportunities and challenges to draw treatment effect in data fusion, that is, a mixed dataset collected from multiple sources (each source with an independent treatment assignment mechanism). Due to possibly omitted source labels and unmeasured confounders, traditional methods cannot estimate individual treatment assignment probability and infer treatment effect effectively. Therefore, we propose to reconstruct the source label and model it as a Group Instrumental Variable (GIV) to implement IV-based Regression for treatment effect estimation. In this paper, we conceptualize this line of thought and develop a unified framework (Meta-EM) to (1) map the raw data into a representation space to construct Linear Mixed Models for the assigned treatment variable; (2) estimate the distribution differences and model the GIV for the different treatment assignment mechanisms; and (3) adopt an alternating training strategy to iteratively optimize the representations and the joint distribution to model GIV for IV regression. Empirical results demonstrate the advantages of our Meta-EM compared with state-of-the-art methods. The project page with the code and the Supplementary materials is available at https://github.com/causal-machine-learning-lab/meta-em",
    "checked": true,
    "id": "7aae7745d8ee4dab3e7db7037e501474332d18fd",
    "semantic_title": "learning instrumental variable from data fusion for treatment effect estimation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26230": {
    "title": "Towards In-Distribution Compatible Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Deep neural network, despite its remarkable capability of discriminating targeted in-distribution samples, shows poor performance on detecting anomalous out-of-distribution data. To address this defect, state-of-the-art solutions choose to train deep networks on an auxiliary dataset of outliers. Various training criteria for these auxiliary outliers are proposed based on heuristic intuitions. However, we find that these intuitively designed outlier training criteria can hurt in-distribution learning and eventually lead to inferior performance. To this end, we identify three causes of the in-distribution incompatibility: contradictory gradient, false likelihood, and distribution shift. Based on our new understandings, we propose a new out-of-distribution detection method by adapting both the top-design of deep models and the loss function. Our method achieves in-distribution compatibility by pursuing less interference with the probabilistic characteristic of in-distribution features. On several benchmarks, our method not only achieves the state-of-the-art out-of-distribution detection performance but also improves the in-distribution accuracy",
    "checked": true,
    "id": "5aa0ccbd38dca97b6d20b37bf9f3c8a414fa26a0",
    "semantic_title": "towards in-distribution compatible out-of-distribution detection",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26231": {
    "title": "Non-IID Transfer Learning on Graphs",
    "volume": "main",
    "abstract": "Transfer learning refers to the transfer of knowledge or information from a relevant source domain to a target domain. However, most existing transfer learning theories and algorithms focus on IID tasks, where the source/target samples are assumed to be independent and identically distributed. Very little effort is devoted to theoretically studying the knowledge transferability on non-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we propose rigorous generalization bounds and algorithms for cross-network transfer learning from a source graph to a target graph. The crucial idea is to characterize the cross-network knowledge transferability from the perspective of the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a novel Graph Subtree Discrepancy to measure the graph distribution shift between source and target graphs. Then the generalization error bounds on cross-network transfer learning, including both cross-network node classification and link prediction tasks, can be derived in terms of the source knowledge and the Graph Subtree Discrepancy across domains. This thereby motivates us to propose a generic graph adaptive network (GRADE) to minimize the distribution shift between source and target graphs for cross-network transfer learning. Experimental results verify the effectiveness and efficiency of our GRADE framework on both cross-network node classification and cross-domain recommendation tasks",
    "checked": true,
    "id": "aaca28e64dd9dfd3cbcd8082d0169ebdd7326a88",
    "semantic_title": "non-iid transfer learning on graphs",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26232": {
    "title": "Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting It into MLPs: An Effective GNN-to-MLP Distillation Framework",
    "volume": "main",
    "abstract": "Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?\" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency knowledge during distillation; we have described in detail what it represents, how it arises, what impact it has, and how to deal with it. In this paper, we propose an efficient Full-Frequency GNN-to-MLP (FF-G2M) distillation framework, which extracts both low-frequency and high-frequency knowledge from GNNs and injects it into MLPs. Extensive experiments show that FF-G2M improves over the vanilla MLPs by 12.6% and outperforms its corresponding teacher GNNs by 2.6% averaged over six graph datasets and three common GNN architectures",
    "checked": true,
    "id": "0648a22c8f65fbe5767650c5e779d9b2888515ea",
    "semantic_title": "extracting low-/high- frequency knowledge from graph neural networks and injecting it into mlps: an effective gnn-to-mlp distillation framework",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26233": {
    "title": "Symphony in the Latent Space: Provably Integrating High-Dimensional Techniques with Non-linear Machine Learning Models",
    "volume": "main",
    "abstract": "This paper revisits building machine learning algorithms that involve interactions between entities, such as those between financial assets in an actively managed portfolio, or interactions between users in a social network. Our goal is to forecast the future evolution of ensembles of multivariate time series in such applications (e.g., the future return of a financial asset or the future popularity of a Twitter account). Designing ML algorithms for such systems requires addressing the challenges of high-dimensional interactions and non-linearity. Existing approaches usually adopt an ad-hoc approach to integrating high-dimensional techniques into non-linear models and recent studies have shown these approaches have questionable efficacy in time-evolving interacting systems. To this end, we propose a novel framework, which we dub as the additive influence model. Under our modeling assumption, we show that it is possible to decouple the learning of high-dimensional interactions from the learning of non-linear feature interactions. To learn the high-dimensional interactions, we leverage kernel-based techniques, with provable guarantees, to embed the entities in a low-dimensional latent space. To learn the non-linear feature-response interactions, we generalize prominent machine learning techniques, including designing a new statistically sound non-parametric method and an ensemble learning algorithm optimized for vector regressions. Extensive experiments on two common applications demonstrate that our new algorithms deliver significantly stronger forecasting power compared to standard and recently proposed methods",
    "checked": true,
    "id": "099c63db93000de4d6bcefe196f46afeb22d2fd0",
    "semantic_title": "symphony in the latent space: provably integrating high-dimensional techniques with non-linear machine learning models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26234": {
    "title": "Decentralized Riemannian Algorithm for Nonconvex Minimax Problems",
    "volume": "main",
    "abstract": "The minimax optimization over Riemannian manifolds (possibly nonconvex constraints) has been actively applied to solve many problems, such as robust dimensionality reduction and deep neural networks with orthogonal weights (Stiefel manifold). Although many optimization algorithms for minimax problems have been developed in the Euclidean setting, it is difficult to convert them into Riemannian cases, and algorithms for nonconvex minimax problems with nonconvex constraints are even rare. On the other hand, to address the big data challenges, decentralized (serverless) training techniques have recently been emerging since they can reduce communications overhead and avoid the bottleneck problem on the server node. Nonetheless, the algorithm for decentralized Riemannian minimax problems has not been studied. In this paper, we study the distributed nonconvex-strongly-concave minimax optimization problem over the Stiefel manifold and propose both deterministic and stochastic minimax methods. The Steifel manifold is a non-convex set. The global function is represented as the finite sum of local functions. For the deterministic setting, we propose DRGDA and prove that our deterministic method achieves a gradient complexity of O( epsilon(-2)) under mild conditions. For the stochastic setting, we propose DRSGDA and prove that our stochastic method achieves a gradient complexity of O( epsilon(-4)). The DRGDA and DRSGDA are the first algorithms for distributed minimax optimization with nonconvex constraints with exact convergence. Extensive experimental results on the Deep Neural Networks (DNNs) training over the Stiefel manifold demonstrate the efficiency of our algorithms",
    "checked": true,
    "id": "594d1a5df8db8de6dfaf2f703d55202a2bb7a58d",
    "semantic_title": "decentralized riemannian algorithm for nonconvex minimax problems",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26235": {
    "title": "Faster Adaptive Federated Learning",
    "volume": "main",
    "abstract": "Federated learning has attracted increasing attention with the emergence of distributed data. While extensive federated learning algorithms have been proposed for the non-convex distributed problem, the federated learning in practice still faces numerous challenges, such as the large training iterations to converge since the sizes of models and datasets keep increasing, and the lack of adaptivity by SGD-based model updates. Meanwhile, the study of adaptive methods in federated learning is scarce and existing works either lack a complete theoretical convergence guarantee or have slow sample complexity. In this paper, we propose an efficient adaptive algorithm (i.e., FAFED) based on the momentum-based variance reduced technique in cross-silo FL. We first explore how to design the adaptive algorithm in the FL setting. By providing a counter-example, we prove that a simple combination of FL and adaptive methods could lead to divergence. More importantly, we provide a convergence analysis for our method and prove that our algorithm is the first adaptive FL algorithm to reach the best-known samples O(epsilon(-3)) and O(epsilon(-2)) communication rounds to find an epsilon-stationary point without large batches. The experimental results on the language modeling task and image classification task with heterogeneous data demonstrate the efficiency of our algorithms",
    "checked": true,
    "id": "1fd1109ec922dcd683190a42dd5712b4eb6113b2",
    "semantic_title": "faster adaptive federated learning",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26236": {
    "title": "Practical Markov Boundary Learning without Strong Assumptions",
    "volume": "main",
    "abstract": "Theoretically, the Markov boundary (MB) is the optimal solution for feature selection. However, existing MB learning algorithms often fail to identify some critical features in real-world feature selection tasks, mainly because the strict assumptions of existing algorithms, on either data distribution, variable types, or correctness of criteria, cannot be satisfied in application scenarios. This paper takes further steps toward opening the door to real-world applications for MB. We contribute in particular to a practical MB learning strategy, which can maintain feasibility and effectiveness in real-world data where variables can be numerical or categorical with linear or nonlinear, pairwise or multivariate relationships. Specifically, the equivalence between MB and the minimal conditional covariance operator (CCO) is investigated, which inspires us to design the objective function based on the predictability evaluation of the mapping variables in a reproducing kernel Hilbert space. Based on this, a kernel MB learning algorithm is proposed, where nonlinear multivariate dependence could be considered without extra requirements on data distribution and variable types. Extensive experiments demonstrate the efficacy of these contributions",
    "checked": true,
    "id": "5e3a726719af0b96b4ab4d117ac5e3e54bef5dc1",
    "semantic_title": "practical markov boundary learning without strong assumptions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26237": {
    "title": "FedNP: Towards Non-IID Federated Learning via Federated Neural Propagation",
    "volume": "main",
    "abstract": "Traditional federated learning (FL) algorithms, such as FedAvg, fail to handle non-i.i.d data because they learn a global model by simply averaging biased local models that are trained on non-i.i.d local data, therefore failing to model the global data distribution. In this paper, we present a novel Bayesian FL algorithm that successfully handles such a non-i.i.d FL setting by enhancing the local training task with an auxiliary task that explicitly estimates the global data distribution. One key challenge in estimating the global data distribution is that the data are partitioned in FL, and therefore the ground-truth global data distribution is inaccessible. To address this challenge, we propose an expectation-propagation-inspired probabilistic neural network, dubbed federated neural propagation (FedNP), which efficiently estimates the global data distribution given non-i.i.d data partitions. Our algorithm is sampling-free and end-to-end differentiable, can be applied with any conventional FL frameworks and learns richer global data representation. Experiments on both image classification tasks with synthetic non-i.i.d image data partitions and real-world non-i.i.d speech recognition tasks demonstrate that our framework effectively alleviates the performance deterioration caused by non-i.i.d data",
    "checked": true,
    "id": "4648ca42ed12b940fbe4f1b2b3d2a14b1d29dc44",
    "semantic_title": "fednp: towards non-iid federated learning via federated neural propagation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26238": {
    "title": "MetaZSCIL: A Meta-Learning Approach for Generalized Zero-Shot Class Incremental Learning",
    "volume": "main",
    "abstract": "Generalized zero-shot learning (GZSL) aims to recognize samples whose categories may not have been seen at training. Standard GZSL cannot handle dynamic addition of new seen and unseen classes. In order to address this limitation, some recent attempts have been made to develop continual GZSL methods. However, these methods require end-users to continuously collect and annotate numerous seen class samples, which is unrealistic and hampers the applicability in the real-world. Accordingly, in this paper, we propose a more practical and challenging setting named Generalized Zero-Shot Class Incremental Learning (CI-GZSL). Our setting aims to incrementally learn unseen classes without any training samples, while recognizing all classes previously encountered. We further propose a bi-level meta-learning based method called MetaZSCIL to directly optimize the network to learn how to incrementally learn. Specifically, we sample sequential tasks from seen classes during the offline training to simulate the incremental learning process. For each task, the model is learned using a meta-objective such that it is capable to perform fast adaptation without forgetting. Note that our optimization can be flexibly equipped with most existing generative methods to tackle CI-GZSL. This work introduces a feature generative framework that leverages visual feature distribution alignment to produce replayed samples of previously seen classes to reduce catastrophic forgetting. Extensive experiments conducted on five widely used benchmarks demonstrate the superiority of our proposed method",
    "checked": true,
    "id": "07ea2ebadf13d1f84c72d44ae5a9ed36af168a69",
    "semantic_title": "metazscil: a meta-learning approach for generalized zero-shot class incremental learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26239": {
    "title": "Adversarial Weight Perturbation Improves Generalization in Graph Neural Networks",
    "volume": "main",
    "abstract": "A lot of theoretical and empirical evidence shows that the flatter local minima tend to improve generalization. Adversarial Weight Perturbation (AWP) is an emerging technique to efficiently and effectively find such minima. In AMP we minimize the loss w.r.t. a bounded worst-case perturbation of the model parameters thereby favoring local minima with a small loss in a neighborhood around them. The benefits of AWP, and more generally the connections between flatness and generalization, have been extensively studied for i.i.d. data such as images. In this paper, we extensively study this phenomenon for graph data. Along the way, we first derive a generalization bound for non-i.i.d. node classification tasks. Then we identify a vanishing-gradient issue with all existing formulations of AWP and we propose a new Weighted Truncated AWP (WT-AWP) to alleviate this issue. We show that regularizing graph neural networks with WT-AWP consistently improves both natural and robust generalization across many different graph learning tasks and models",
    "checked": false,
    "id": "1e0930a6c4ad4c62c725cfa35e76081eb971a83a",
    "semantic_title": "adversarial weight perturbation improves generalization in graph neural network",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26240": {
    "title": "Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "In offline multi-agent reinforcement learning (MARL), agents estimate policies from a given dataset. We study reward-poisoning attacks in this setting where an exogenous attacker modifies the rewards in the dataset before the agents see the dataset. The attacker wants to guide each agent into a nefarious target policy while minimizing the Lp norm of the reward modification. Unlike attacks on single-agent RL, we show that the attacker can install the target policy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow. This attack can be significantly cheaper than separate single-agent attacks. We show that the attack works on various MARL agents including uncertainty-aware learners, and we exhibit linear programs to efficiently solve the attack problem. We also study the relationship between the structure of the datasets and the minimal attack cost. Our work paves the way for studying defense in offline MARL",
    "checked": true,
    "id": "1f18bd6c3d216d8f7c6186267cfe6dce5a66d7a7",
    "semantic_title": "reward poisoning attacks on offline multi-agent reinforcement learning",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26241": {
    "title": "Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Research in model-based reinforcement learning has made significant progress in recent years. Compared to single-agent settings, the exponential dimension growth of the joint state-action space in multi-agent systems dramatically increases the complexity of the environment dynamics, which makes it infeasible to learn an accurate global model and thus necessitates the use of agent-wise local models. However, during multi-step model rollouts, the prediction of one local model can affect the predictions of other local models in the next step. As a result, local prediction errors can be propagated to other localities and eventually give rise to considerably large global errors. Furthermore, since the models are generally used to predict for multiple steps, simply minimizing one-step prediction errors regardless of their long-term effect on other models may further aggravate the propagation of local errors. To this end, we propose Models as AGents (MAG), a multi-agent model optimization framework that reversely treats the local models as multi-step decision making agents and the current policies as the dynamics during the model rollout process. In this way, the local models are able to consider the multi-step mutual affect between each other before making predictions. Theoretically, we show that the objective of MAG is approximately equivalent to maximizing a lower bound of the true environment return. Experiments on the challenging StarCraft II benchmark demonstrate the effectiveness of MAG",
    "checked": true,
    "id": "a8e839087b28dc469a47a184cbd197b21f32d9e6",
    "semantic_title": "models as agents: optimizing multi-step predictions of interactive local models in model-based multi-agent reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26242": {
    "title": "Differentially Private Learning with Per-Sample Adaptive Clipping",
    "volume": "main",
    "abstract": "Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy without the typical hyperparameter tuning process of using a constant clipping while significantly reducing the deviation between the update and true batch-averaged gradient. We provide a rigorous theoretical convergence analysis and show that with convergence rate at the same order, the proposed algorithm achieves a lower non-vanishing bound, which is maintained over training iterations, compared with NSGD/Auto-S. In addition, through extensive experimental evaluation, we show that DP-PSAC outperforms or matches the state-of-the-art methods on multiple main-stream vision and language tasks",
    "checked": true,
    "id": "2d3c5468de7641262e9c6cb8366049098e88371e",
    "semantic_title": "differentially private learning with per-sample adaptive clipping",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26243": {
    "title": "Zero-Cost Operation Scoring in Differentiable Architecture Search",
    "volume": "main",
    "abstract": "We formalize and analyze a fundamental component of dif- ferentiable neural architecture search (NAS): local \"opera- tion scoring\" at each operation choice. We view existing operation scoring functions as inexact proxies for accuracy, and we find that they perform poorly when analyzed empir- ically on NAS benchmarks. From this perspective, we intro- duce a novel perturbation-based zero-cost operation scor- ing (Zero-Cost-PT) approach, which utilizes zero-cost prox- ies that were recently studied in multi-trial NAS but de- grade significantly on larger search spaces, typical for dif- ferentiable NAS. We conduct a thorough empirical evalu- ation on a number of NAS benchmarks and large search spaces, from NAS-Bench-201, NAS-Bench-1Shot1, NAS- Bench-Macro, to DARTS-like and MobileNet-like spaces, showing significant improvements in both search time and accuracy. On the ImageNet classification task on the DARTS search space, our approach improved accuracy compared to the best current training-free methods (TE-NAS) while be- ing over 10× faster (total searching time 25 minutes on a single GPU), and observed significantly better transferabil- ity on architectures searched on the CIFAR-10 dataset with an accuracy increase of 1.8 pp. Our code is available at: https://github.com/zerocostptnas/zerocost operation score",
    "checked": true,
    "id": "e8a8c7dbf7b499c17c8e74a80b7f99918e467b69",
    "semantic_title": "zero-cost operation scoring in differentiable architecture search",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26244": {
    "title": "HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks",
    "volume": "main",
    "abstract": "Low-rank compression is an important model compression strategy for obtaining compact neural network models. In general, because the rank values directly determine the model complexity and model accuracy, proper selection of layer-wise rank is very critical and desired. To date, though many low-rank compression approaches, either selecting the ranks in a manual or automatic way, have been proposed, they suffer from costly manual trials or unsatisfied compression performance. In addition, all of the existing works are not designed in a hardware-aware way, limiting the practical performance of the compressed models on real-world hardware platforms. To address these challenges, in this paper we propose HALOC, a hardware-aware automatic low-rank compression framework. By interpreting automatic rank selection from an architecture search perspective, we develop an end-to-end solution to determine the suitable layer-wise ranks in a differentiable and hardware-aware way. We further propose design principles and mitigation strategy to efficiently explore the rank space and reduce the potential interference problem. Experimental results on different datasets and hardware platforms demonstrate the effectiveness of our proposed approach. On CIFAR-10 dataset, HALOC enables 0.07% and 0.38% accuracy increase over the uncompressed ResNet-20 and VGG-16 models with 72.20% and 86.44% fewer FLOPs, respectively. On ImageNet dataset, HALOC achieves 0.9% higher top-1 accuracy than the original ResNet-18 model with 66.16% fewer FLOPs. HALOC also shows 0.66% higher top-1 accuracy increase than the state-of-the-art automatic low-rank compression solution with fewer computational and memory costs. In addition, HALOC demonstrates the practical speedups on different hardware platforms, verified by the measurement results on desktop GPU, embedded GPU and ASIC accelerator",
    "checked": true,
    "id": "60004428c219a22443a33fe27051f246fa99e263",
    "semantic_title": "haloc: hardware-aware automatic low-rank compression for compact neural networks",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26245": {
    "title": "Bayesian Federated Neural Matching That Completes Full Information",
    "volume": "main",
    "abstract": "Federated learning is a contemporary machine learning paradigm where locally trained models are distilled into a global model. Due to the intrinsic permutation invariance of neural networks, Probabilistic Federated Neural Matching (PFNM) employs a Bayesian nonparametric framework in the generation process of local neurons, and then creates a linear sum assignment formulation in each alternative optimization iteration. But according to our theoretical analysis, the optimization iteration in PFNM omits global information from existing. In this study, we propose a novel approach that overcomes this flaw by introducing a Kullback-Leibler divergence penalty at each iteration. The effectiveness of our approach is demonstrated by experiments on both image classification and semantic segmentation tasks",
    "checked": true,
    "id": "ff13b88c34855d1342f58e4c36509b285c1ddeb2",
    "semantic_title": "bayesian federated neural matching that completes full information",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26246": {
    "title": "CDMA: A Practical Cross-Device Federated Learning Algorithm for General Minimax Problems",
    "volume": "main",
    "abstract": "Minimax problems arise in a wide range of important applications including robust adversarial learning and Generative Adversarial Network (GAN) training. Recently, algorithms for minimax problems in the Federated Learning (FL) paradigm have received considerable interest. Existing federated algorithms for general minimax problems require the full aggregation (i.e., aggregation of local model information from all clients) in each training round. Thus, they are inapplicable to an important setting of FL known as the cross-device setting, which involves numerous unreliable mobile/IoT devices. In this paper, we develop the first practical algorithm named CDMA for general minimax problems in the cross-device FL setting. CDMA is based on a Start-Immediately-With-Enough-Responses mechanism, in which the server first signals a subset of clients to perform local computation and then starts to aggregate the local results reported by clients once it receives responses from enough clients in each round. With this mechanism, CDMA is resilient to the low client availability. In addition, CDMA is incorporated with a lightweight global correction in the local update steps of clients, which mitigates the impact of slow network connections. We establish theoretical guarantees of CDMA under different choices of hyperparameters and conduct experiments on AUC maximization, robust adversarial network training, and GAN training tasks. Theoretical and experimental results demonstrate the efficiency of CDMA",
    "checked": true,
    "id": "91ec9f720172dada9ef45f9d11e53df084ec4d3d",
    "semantic_title": "cdma: a practical cross-device federated learning algorithm for general minimax problems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26247": {
    "title": "Towards Optimal Randomized Strategies in Adversarial Example Game",
    "volume": "main",
    "abstract": "The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications. A recent line of work shows that the use of randomization in adversarial training is the key to find optimal strategies against adversarial example attacks. However, in a fully randomized setting where both the defender and the attacker can use randomized strategies, there are no efficient algorithm for finding such an optimal strategy. To fill the gap, we propose the first algorithm of its kind, called FRAT, which models the problem with a new infinite-dimensional continuous-time flow on probability distribution spaces. FRAT maintains a lightweight mixture of models for the defender, with flexibility to efficiently update mixing weights and model parameters at each iteration. Furthermore, FRAT utilizes lightweight sampling subroutines to construct a random strategy for the attacker. We prove that the continuous-time limit of FRAT converges to a mixed Nash equilibria in a zero-sum game formed by a defender and an attacker. Experimental results also demonstrate the efficiency of FRAT on CIFAR-10 and CIFAR-100 datasets",
    "checked": true,
    "id": "e6e7544a50373e59271019304e91f2a6b450ed46",
    "semantic_title": "towards optimal randomized strategies in adversarial example game",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26248": {
    "title": "A Tale of Two Latent Flows: Learning Latent Space Normalizing Flow with Short-Run Langevin Flow for Approximate Inference",
    "volume": "main",
    "abstract": "We study a normalizing flow in the latent space of a top-down generator model, in which the normalizing flow model plays the role of the informative prior model of the generator. We propose to jointly learn the latent space normalizing flow prior model and the top-down generator model by a Markov chain Monte Carlo (MCMC)-based maximum likelihood algorithm, where a short-run Langevin sampling from the intractable posterior distribution is performed to infer the latent variables for each observed example, so that the parameters of the normalizing flow prior and the generator can be updated with the inferred latent variables. We show that, under the scenario of non-convergent short-run MCMC, the finite step Langevin dynamics is a flow-like approximate inference model and the learning objective actually follows the perturbation of the maximum likelihood estimation (MLE). We further point out that the learning framework seeks to (i) match the latent space normalizing flow and the aggregated posterior produced by the short-run Langevin flow, and (ii) bias the model from MLE such that the short-run Langevin flow inference is close to the true posterior. Empirical results of extensive experiments validate the effectiveness of the proposed latent space normalizing flow model in the tasks of image generation, image reconstruction, anomaly detection, supervised image inpainting and unsupervised image recovery",
    "checked": true,
    "id": "2314afc804b9c425cefda5c7854ac4110e7b4812",
    "semantic_title": "a tale of two latent flows: learning latent space normalizing flow with short-run langevin flow for approximate inference",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26249": {
    "title": "Semi-supervised Learning with Support Isolation by Small-Paced Self-Training",
    "volume": "main",
    "abstract": "In this paper, we address a special scenario of semi-supervised learning, where the label missing is caused by a preceding filtering mechanism, i.e., an instance can enter a subsequent process in which its label is revealed if and only if it passes the filtering mechanism. The rejected instances are prohibited to enter the subsequent labeling process due to economical or ethical reasons, making the support of the labeled and unlabeled distributions isolated from each other. In this case, semi-supervised learning approaches which rely on certain coherence of the labeled and unlabeled distribution would suffer from the consequent distribution mismatch, and hence result in poor prediction performance. In this paper, we propose a Small-Paced Self-Training framework, which iteratively discovers labeled and unlabeled instance subspaces with bounded Wasserstein distance. We theoretically prove that such a framework may achieve provably low error on the pseudo labels during learning. Experiments on both benchmark and pneumonia diagnosis tasks show that our method is effective",
    "checked": true,
    "id": "076372bb5d005e1d4b554b02fc14044d541dd78b",
    "semantic_title": "semi-supervised learning with support isolation by small-paced self-training",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26250": {
    "title": "On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Despite impressive success in many tasks, deep learning models are shown to rely on spurious features, which will catastrophically fail when generalized to out-of-distribution (OOD) data. Invariant Risk Minimization (IRM) is proposed to alleviate this issue by extracting domain-invariant features for OOD generalization. Nevertheless, recent work shows that IRM is only effective for a certain type of distribution shift (e.g., correlation shift) while it fails for other cases (e.g., diversity shift). Meanwhile, another thread of method, Adversarial Training (AT), has shown better domain transfer performance, suggesting that it has the potential to be an effective candidate for extracting domain-invariant features. This paper investigates this possibility by exploring the similarity between the IRM and AT objectives. Inspired by this connection, we propose Domain-wise Adversarial Training (DAT), an AT-inspired method for alleviating distribution shift by domain-specific perturbations. Extensive experiments show that our proposed DAT can effectively remove domain-varying features and improve OOD generalization under both correlation shift and diversity shift",
    "checked": true,
    "id": "48a433a400c9abb6714c351b5423b396212bf0c5",
    "semantic_title": "on the connection between invariant learning and adversarial training for out-of-distribution generalization",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26251": {
    "title": "Decentralized Stochastic Multi-Player Multi-Armed Walking Bandits",
    "volume": "main",
    "abstract": "Multi-player multi-armed bandit is an increasingly relevant decision-making problem, motivated by applications to cognitive radio systems. Most research for this problem focuses exclusively on the settings that players have full access to all arms and receive no reward when pulling the same arm. Hence all players solve the same bandit problem with the goal of maximizing their cumulative reward. However, these settings neglect several important factors in many real-world applications, where players have limited access to a dynamic local subset of arms (i.e., an arm could sometimes be ``walking'' and not accessible to the player). To this end, this paper proposes a multi-player multi-armed walking bandits model, aiming to address aforementioned modeling issues. The goal now is to maximize the reward, however, players can only pull arms from the local subset and only collect a full reward if no other players pull the same arm. We adopt Upper Confidence Bound (UCB) to deal with the exploration-exploitation tradeoff and employ distributed optimization techniques to properly handle collisions. By carefully integrating these two techniques, we propose a decentralized algorithm with near-optimal guarantee on the regret, and can be easily implemented to obtain competitive empirical performance",
    "checked": true,
    "id": "3fd11026cf0ac132decd37379f9bc9952799c301",
    "semantic_title": "decentralized stochastic multi-player multi-armed walking bandits",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26252": {
    "title": "Federated Generative Model on Multi-Source Heterogeneous Data in IoT",
    "volume": "main",
    "abstract": "The study of generative models is a promising branch of deep learning techniques, which has been successfully applied to different scenarios, such as Artificial Intelligence and the Internet of Things. While in most of the existing works, the generative models are realized as a centralized structure, raising the threats of security and privacy and the overburden of communication costs. Rare efforts have been committed to investigating distributed generative models, especially when the training data comes from multiple heterogeneous sources under realistic IoT settings. In this paper, to handle this challenging problem, we design a federated generative model framework that can learn a powerful generator for the hierarchical IoT systems. Particularly, our generative model framework can solve the problem of distributed data generation on multi-source heterogeneous data in two scenarios, i.e., feature related scenario and label related scenario. In addition, in our federated generative models, we develop a synchronous and an asynchronous updating methods to satisfy different application requirements. Extensive experiments on a simulated dataset and multiple real datasets are conducted to evaluate the data generation performance of our proposed generative models through comparison with the state-of-the-arts",
    "checked": true,
    "id": "8bf3da565ba41d6f250783985a6dc3fe8b4530a5",
    "semantic_title": "federated generative model on multi-source heterogeneous data in iot",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26253": {
    "title": "Contrastive Open Set Recognition",
    "volume": "main",
    "abstract": "In conventional recognition tasks, models are only trained to recognize learned targets, but it is usually difficult to collect training examples of all potential categories. In the testing phase, when models receive test samples from unknown classes, they mistakenly classify the samples into known classes. Open set recognition (OSR) is a more realistic recognition task, which requires the classifier to detect unknown test samples while keeping a high classification accuracy of known classes. In this paper, we study how to improve the OSR performance of deep neural networks from the perspective of representation learning. We employ supervised contrastive learning to improve the quality of feature representations, propose a new supervised contrastive learning method that enables the model to learn from soft training targets, and design an OSR framework on its basis. With the proposed method, we are able to make use of label smoothing and mixup when training deep neural networks contrastively, so as to improve both the robustness of outlier detection in OSR tasks and the accuracy in conventional classification tasks. We validate our method on multiple benchmark datasets and testing scenarios, achieving experimental results that verify the effectiveness of the proposed method",
    "checked": true,
    "id": "f67894d00bb31297f3d6c8a8a8d8de9276d3b6fc",
    "semantic_title": "contrastive open set recognition",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26254": {
    "title": "Progressive Deep Multi-View Comprehensive Representation Learning",
    "volume": "main",
    "abstract": "Multi-view Comprehensive Representation Learning (MCRL) aims to synthesize information from multiple views to learn comprehensive representations of data items. Prevalent deep MCRL methods typically concatenate synergistic view-specific representations or average aligned view-specific representations in the fusion stage. However, the performance of synergistic fusion methods inevitably degenerate or even fail when partial views are missing in real-world applications; the aligned based fusion methods usually cannot fully exploit the complementarity of multi-view data. To eliminate all these drawbacks, in this work we present a Progressive Deep Multi-view Fusion (PDMF) method. Considering the multi-view comprehensive representation should contain complete information and the view-specific data contain partial information, we deem that it is unstable to directly learn the mapping from partial information to complete information. Hence, PDMF employs a progressive learning strategy, which contains the pre-training and fine-tuning stages. In the pre-training stage, PDMF decodes the auxiliary comprehensive representation to the view-specific data. It also captures the consistency and complementarity by learning the relations between the dimensions of the auxiliary comprehensive representation and all views. In the fine-tuning stage, PDMF learns the mapping from the original data to the comprehensive representation with the help of the auxiliary comprehensive representation and relations. Experiments conducted on a synthetic toy dataset and 4 real-world datasets show that PDMF outperforms state-of-the-art baseline methods. The code is released at https://github.com/winterant/PDMF",
    "checked": true,
    "id": "3f78556059320f881c4ff5f7ce7fe8a35f393941",
    "semantic_title": "progressive deep multi-view comprehensive representation learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26255": {
    "title": "A Survey on Model Compression and Acceleration for Pretrained Language Models",
    "volume": "main",
    "abstract": "Despite achieving state-of-the-art performance on many NLP tasks, the high energy cost and long inference delay prevent Transformer-based pretrained language models (PLMs) from seeing broader adoption including for edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression and acceleration for pretrained language models, including benchmarks, metrics and methodology",
    "checked": true,
    "id": "c546e5447f412bf4f274e490996718641b211aa6",
    "semantic_title": "a survey on model compression and acceleration for pretrained language models",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26256": {
    "title": "GraphPrompt: Graph-Based Prompt Templates for Biomedical Synonym Prediction",
    "volume": "main",
    "abstract": "In the expansion of biomedical dataset, the same category may be labeled with different terms, thus being tedious and onerous to curate these terms. Therefore, automatically mapping synonymous terms onto the ontologies is desirable, which we name as biomedical synonym prediction task. Unlike biomedical concept normalization (BCN), no clues from context can be used to enhance synonym prediction, making it essential to extract graph features from ontology. We introduce an expert-curated dataset OBO-syn encompassing 70 different types of concepts and 2 million curated concept-term pairs for evaluating synonym prediction methods. We find BCN methods perform weakly on this task for not making full use of graph information. Therefore, we propose GraphPrompt, a prompt-based learning approach that creates prompt templates according to the graphs. GraphPrompt obtained 37.2% and 28.5% improvement on zero-shot and few-shot settings respectively, indicating the effectiveness of these graph-based prompt templates. We envision that our method GraphPrompt and OBO-syn dataset can be broadly applied to graph-based NLP tasks, and serve as the basis for analyzing diverse and accumulating biomedical data. All the data and codes are avalible at: https://github.com/HanwenXuTHU/GraphPrompt",
    "checked": true,
    "id": "a6628b4ac0b432659a0092add3eb371608d3e065",
    "semantic_title": "graphprompt: graph-based prompt templates for biomedical synonym prediction",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26257": {
    "title": "Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation",
    "volume": "main",
    "abstract": "While Reinforcement Learning can achieve impressive results for complex tasks, the learned policies are generally prone to fail in downstream tasks with even minor model mismatch or unexpected perturbations. Recent works have demonstrated that a policy population with diverse behavior characteristics can generalize to downstream environments with various discrepancies. However, such policies might result in catastrophic damage during the deployment in practical scenarios like real-world systems due to the unrestricted behaviors of trained policies. Furthermore, training diverse policies without regulation of the behavior can result in inadequate feasible policies for extrapolating to a wide range of test conditions with dynamics shifts. In this work, we aim to train diverse policies under the regularization of the behavior patterns. We motivate our paradigm by observing the inverse dynamics in the environment with partial state information and propose Diversity in Regulation (DiR) training diverse policies with regulated behaviors to discover desired patterns that benefit the generalization. Considerable empirical results on various variations of different environments indicate that our method attains improvements over other diversity-driven counterparts",
    "checked": true,
    "id": "03446d8730c4bc5cb3abc9d8c13e8e6d3898e2ae",
    "semantic_title": "open-ended diverse solution discovery with regulated behavior patterns for cross-domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26258": {
    "title": "Efficient Top-K Feature Selection Using Coordinate Descent Method",
    "volume": "main",
    "abstract": "Sparse learning based feature selection has been widely investigated in recent years. In this study, we focus on the l2,0-norm based feature selection, which is effective for exact top-k feature selection but challenging to optimize. To solve the general l2,0-norm constrained problems, we novelly develop a parameter-free optimization framework based on the coordinate descend (CD) method, termed CD-LSR. Specifically, we devise a skillful conversion from the original problem to solving one continuous matrix and one discrete selection matrix. Then the nontrivial l2,0-norm constraint can be solved efficiently by solving the selection matrix with CD method. We impose the l2,0-norm on a vanilla least square regression (LSR) model for feature selection and optimize it with CD-LSR. Extensive experiments exhibit the efficiency of CD-LSR, as well as the discrimination ability of l2,0-norm to identify informative features. More importantly, the versatility of CD-LSR facilitates the applications of the l2,0-norm in more sophisticated models. Based on the competitive performance of l2,0-norm on the baseline LSR model, the satisfactory performance of its applications is reasonably expected. The source MATLAB code are available at: https://github.com/solerxl/Code_For_AAAI_2023",
    "checked": true,
    "id": "2e7fdbb628a22000fcdc3138d83a4f6c1bb14633",
    "semantic_title": "efficient top-k feature selection using coordinate descent method",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26259": {
    "title": "Label-Specific Feature Augmentation for Long-Tailed Multi-Label Text Classification",
    "volume": "main",
    "abstract": "Multi-label text classification (MLTC) involves tagging a document with its most relevant subset of labels from a label set. In real applications, labels usually follow a long-tailed distribution, where most labels (called as tail-label) only contain a small number of documents and limit the performance of MLTC. To facilitate this low-resource problem, researchers introduced a simple but effective strategy, data augmentation (DA). However, most existing DA approaches struggle in multi-label settings. The main reason is that the augmented documents for one label may inevitably influence the other co-occurring labels and further exaggerate the long-tailed problem. To mitigate this issue, we propose a new pair-level augmentation framework for MLTC, called Label-Specific Feature Augmentation (LSFA), which merely augments positive feature-label pairs for the tail-labels. LSFA contains two main parts. The first is for label-specific document representation learning in the high-level latent space, the second is for augmenting tail-label features in latent space by transferring the documents second-order statistics (intra-class semantic variations) from head labels to tail labels. At last, we design a new loss function for adjusting classifiers based on augmented datasets. The whole learning procedure can be effectively trained. Comprehensive experiments on benchmark datasets have shown that the proposed LSFA outperforms the state-of-the-art counterparts",
    "checked": true,
    "id": "3e525d0a7f6886b82d3b5e82d5a40d7d514eb2f0",
    "semantic_title": "label-specific feature augmentation for long-tailed multi-label text classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26260": {
    "title": "Neighborhood-Regularized Self-Training for Learning with Few Labels",
    "volume": "main",
    "abstract": "Training deep neural networks (DNNs) with limited supervision has been a popular research topic as it can significantly alleviate the annotation burden. Self-training has been successfully applied in semi-supervised learning tasks, but one drawback of self-training is that it is vulnerable to the label noise from incorrect pseudo labels. Inspired by the fact that samples with similar labels tend to share similar representations, we develop a neighborhood-based sample selection approach to tackle the issue of noisy pseudo labels. We further stabilize self-training via aggregating the predictions from different rounds during sample selection. Experiments on eight tasks show that our proposed method outperforms the strongest self-training baseline with 1.83% and 2.51% performance gain for text and graph datasets on average. Our further analysis demonstrates that our proposed data selection strategy reduces the noise of pseudo labels by 36.8% and saves 57.3% of the time when compared with the best baseline. Our code and appendices will be uploaded to: https://github.com/ritaranx/NeST",
    "checked": true,
    "id": "728a34d0cbd7fef95ce8c6016e4db803d565b860",
    "semantic_title": "neighborhood-regularized self-training for learning with few labels",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26261": {
    "title": "Resilient Binary Neural Network",
    "volume": "main",
    "abstract": "Binary neural networks (BNNs) have received ever-increasing popularity for their great capability of reducing storage burden as well as quickening inference time. However, there is a severe performance drop compared with {real-valued} networks, due to its intrinsic frequent weight oscillation during training. In this paper, we introduce a Resilient Binary Neural Network (ReBNN) to mitigate the frequent oscillation for better BNNs' training. We identify that the weight oscillation mainly stems from the non-parametric scaling factor. To address this issue, we propose to parameterize the scaling factor and introduce a weighted reconstruction loss to build an adaptive training objective. For the first time, we show that the weight oscillation is controlled by the balanced parameter attached to the reconstruction loss, which provides a theoretical foundation to parameterize it in back propagation. Based on this, we learn our ReBNN by calculating the balanced parameter based on its maximum magnitude, which can effectively mitigate the weight oscillation with a resilient training process. Extensive experiments are conducted upon various network models, such as ResNet and Faster-RCNN for computer vision, as well as BERT for natural language processing. The results demonstrate the overwhelming performance of our ReBNN over prior arts. For example, our ReBNN achieves 66.9% Top-1 accuracy with ResNet-18 backbone on the ImageNet dataset, surpassing existing state-of-the-arts by a significant margin. Our code is open-sourced at https://github.com/SteveTsui/ReBNN",
    "checked": true,
    "id": "a13eb514c83580a1678efa562c7a02c60b6983fd",
    "semantic_title": "resilient binary neural network",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26262": {
    "title": "Transfer Learning Enhanced DeepONet for Long-Time Prediction of Evolution Equations",
    "volume": "main",
    "abstract": "Deep operator network (DeepONet) has demonstrated great success in various learning tasks, including learning solution operators of partial differential equations. In particular, it provides an efficient approach to predicting the evolution equations in a finite time horizon. Nevertheless, the vanilla DeepONet suffers from the issue of stability degradation in the long- time prediction. This paper proposes a transfer-learning aided DeepONet to enhance the stability. Our idea is to use transfer learning to sequentially update the DeepONets as the surro- gates for propagators learned in different time frames. The evolving DeepONets can better track the varying complexities of the evolution equations, while only need to be updated by efficient training of a tiny fraction of the operator networks. Through systematic experiments, we show that the proposed method not only improves the long-time accuracy of Deep- ONet while maintaining similar computational cost but also substantially reduces the sample size of the training set",
    "checked": true,
    "id": "a2ab63a272fa0762a75eb3d1b0a86506a9db3623",
    "semantic_title": "transfer learning enhanced deeponet for long-time prediction of evolution equations",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26263": {
    "title": "BridgeTower: Building Bridges between Encoders in Vision-Language Representation Learning",
    "volume": "main",
    "abstract": "Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets. Code and checkpoints are available at https://github.com/microsoft/BridgeTower",
    "checked": false,
    "id": "4c2668b3ae22fa592716480ec56012775b139f52",
    "semantic_title": "bridge-tower: building bridges between encoders in vision-language representation learning",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26264": {
    "title": "USDNL: Uncertainty-Based Single Dropout in Noisy Label Learning",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) possess powerful prediction capability thanks to their over-parameterization design, although the large model complexity makes it suffer from noisy supervision. Recent approaches seek to eliminate impacts from noisy labels by excluding data points with large loss values and showing promising performance. However, these approaches usually associate with significant computation overhead and lack of theoretical analysis. In this paper, we adopt a perspective to connect label noise with epistemic uncertainty. We design a simple, efficient, and theoretically provable robust algorithm named USDNL for DNNs with uncertainty-based Dropout. Specifically, we estimate the epistemic uncertainty of the network prediction after early training through single Dropout. The epistemic uncertainty is then combined with cross-entropy loss to select the clean samples during training. Finally, we theoretically show the equivalence of replacing selection loss with single cross-entropy loss. Compared to existing small-loss selection methods, USDNL features its simplicity for practical scenarios by only applying Dropout to a standard network, while still achieving high model accuracy. Extensive empirical results on both synthetic and real-world datasets show that USDNL outperforms other methods. Our code is available at https://github.com/kovelxyz/USDNL",
    "checked": true,
    "id": "bc81135198f11b1b8015ccb02a8da0905259d68c",
    "semantic_title": "usdnl: uncertainty-based single dropout in noisy label learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26265": {
    "title": "Trusted Fine-Grained Image Classification through Hierarchical Evidence Fusion",
    "volume": "main",
    "abstract": "Fine-Grained Image Classification (FGIC) aims to classify images into specific subordinate classes of a superclass. Due to insufficient training data and confusing data samples, FGIC may produce uncertain classification results that are untrusted for data applications. In fact, FGIC can be viewed as a hierarchical classification process and the multilayer information facilitates to reduce uncertainty and improve the reliability of FGIC. In this paper, we adopt the evidence theory to measure uncertainty and confidence in hierarchical classification process and propose a trusted FGIC method through fusing multilayer classification evidence. Comparing with the traditional approaches, the trusted FGIC method not only generates accurate classification results but also reduces the uncertainty of fine-grained classification. Specifically, we construct an evidence extractor at each classification layer to extract multilayer (multi-grained) evidence for image classification. To fuse the extracted multi-grained evidence from coarse to fine, we formulate evidence fusion with the Dirichlet hyper probability distribution and thereby hierarchically decompose the evidence of coarse-grained classes into fine-grained classes to enhance the classification performances. The ablation experiments validate that the hierarchical evidence fusion can improve the precision and also reduce the uncertainty of fine-grained classification. The comparison with state-of-the-art FGIC methods shows that our proposed method achieves competitive performances",
    "checked": true,
    "id": "a5f444de5f3048e9ae55a92cd4b80ccba05b90c7",
    "semantic_title": "trusted fine-grained image classification through hierarchical evidence fusion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26266": {
    "title": "Disentangled Representation for Causal Mediation Analysis",
    "volume": "main",
    "abstract": "Estimating direct and indirect causal effects from observational data is crucial to understanding the causal mechanisms and predicting the behaviour under different interventions. Causal mediation analysis is a method that is often used to reveal direct and indirect effects. Deep learning shows promise in mediation analysis, but the current methods only assume latent confounders that affect treatment, mediator and outcome simultaneously, and fail to identify different types of latent confounders (e.g., confounders that only affect the mediator or outcome). Furthermore, current methods are based on the sequential ignorability assumption, which is not feasible for dealing with multiple types of latent confounders. This work aims to circumvent the sequential ignorability assumption and applies the piecemeal deconfounding assumption as an alternative. We propose the Disentangled Mediation Analysis Variational AutoEncoder (DMAVAE), which disentangles the representations of latent confounders into three types to accurately estimate the natural direct effect, natural indirect effect and total effect. Experimental results show that the proposed method outperforms existing methods and has strong generalisation ability. We further apply the method to a real-world dataset to show its potential application",
    "checked": true,
    "id": "b674763fa4dadab6d65665d7a85a363bf6f914bc",
    "semantic_title": "disentangled representation for causal mediation analysis",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26267": {
    "title": "Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness",
    "checked": true,
    "id": "1566fedfe843ac88fb36803368fa84bed6db2af3",
    "semantic_title": "global concept-based interpretability for graph neural networks via neuron analysis",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26268": {
    "title": "Fast and Accurate Binary Neural Networks Based on Depth-Width Reshaping",
    "volume": "main",
    "abstract": "Network binarization (i.e., binary neural networks, BNNs) can efficiently compress deep neural networks and accelerate model inference but cause severe accuracy degradation. Existing BNNs are mainly implemented based on the commonly used full-precision network backbones, and then the accuracy is improved with various techniques. However, there is a question of whether the full-precision network backbone is well adapted to BNNs. We start from the factors of the performance degradation of BNNs and analyze the problems of directly using full-precision network backbones for BNNs: for a given computational budget, the backbone of a BNN may need to be shallower and wider compared to the backbone of a full-precision network. With this in mind, Depth-Width Reshaping (DWR) is proposed to reshape the depth and width of existing full-precision network backbones and further optimize them by incorporating pruning techniques to better fit the BNNs. Extensive experiments demonstrate the analytical result and the effectiveness of the proposed method. Compared with the original backbones, the DWR backbones constructed by the proposed method result in close to O(√s) decrease in activations, while achieving an absolute accuracy increase by up to 1.7% with comparable computational cost. Besides, by using the DWR backbones, existing methods can achieve new state-of-the-art (SOTA) accuracy (e.g., 67.2% on ImageNet with ResNet-18 as the original backbone). We hope this work provides a novel insight into the backbone design of BNNs. The code is available at https://github.com/pingxue-hfut/DWR",
    "checked": true,
    "id": "18890b992894944bec789d1b4d9b2356382d8c28",
    "semantic_title": "fast and accurate binary neural networks based on depth-width reshaping",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26269": {
    "title": "Learning the Finer Things: Bayesian Structure Learning at the Instantiation Level",
    "volume": "main",
    "abstract": "Successful machine learning methods require a trade-off between memorization and generalization. Too much memorization and the model cannot generalize to unobserved examples. Too much over-generalization and we risk under-fitting the data. While we commonly measure their performance through cross validation and accuracy metrics, how should these algorithms cope in domains that are extremely under-determined where accuracy is always unsatisfactory? We present a novel probabilistic graphical model structure learning approach that can learn, generalize and explain in these elusive domains by operating at the random variable instantiation level. Using Minimum Description Length (MDL) analysis, we propose a new decomposition of the learning problem over all training exemplars, fusing together minimal entropy inferences to construct a final knowledge base. By leveraging Bayesian Knowledge Bases (BKBs), a framework that operates at the instantiation level and inherently subsumes Bayesian Networks (BNs), we develop both a theoretical MDL score and associated structure learning algorithm that demonstrates significant improvements over learned BNs on 40 benchmark datasets. Further, our algorithm incorporates recent off-the-shelf DAG learning techniques enabling tractable results even on large problems. We then demonstrate the utility of our approach in a significantly under-determined domain by learning gene regulatory networks on breast cancer gene mutational data available from The Cancer Genome Atlas (TCGA)",
    "checked": true,
    "id": "f00c90ad2f54475c2f43512a36d1fd8d673b366f",
    "semantic_title": "learning the finer things: bayesian structure learning at the instantiation level",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26270": {
    "title": "Semidefinite Programming versus Burer-Monteiro Factorization for Matrix Sensing",
    "volume": "main",
    "abstract": "Many fundamental low-rank optimization problems, such as matrix completion, phase retrieval, and robust PCA, can be formulated as the matrix sensing problem. Two main approaches for solving matrix sensing are based on semidefinite programming (SDP) and Burer-Monteiro (B-M) factorization. The former suffers from high computational and space complexities, whereas the latter may return a spurious solution due to the non-convexity of the problem. The existing theoretical guarantees for the success of these methods have led to similar conservative conditions, which may wrongly imply that these methods have comparable performances. In this paper, we shed light on some major differences between these two methods. First, we present a class of structured matrix completion problems for which the B-M methods fail with an overwhelming probability, while the SDP method works correctly. Second, we identify a class of highly sparse matrix completion problems for which the B-M method works and the SDP method fails. Third, we prove that although the B-M method exhibits the same performance independent of the rank of the unknown solution, the success of the SDP method is correlated to the rank of the solution and improves as the rank increases. Unlike the existing literature that has mainly focused on those instances of matrix sensing for which both SDP and B-M work, this paper offers the first result on the unique merit of each method over the alternative approach",
    "checked": true,
    "id": "3a113d264a02bd84fc6b3ea663fee2cdfb5b5bcf",
    "semantic_title": "semidefinite programming versus burer-monteiro factorization for matrix sensing",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26271": {
    "title": "DeFL: Defending against Model Poisoning Attacks in Federated Learning via Critical Learning Periods Awareness",
    "volume": "main",
    "abstract": "Federated learning (FL) is known to be susceptible to model poisoning attacks in which malicious clients hamper the accuracy of the global model by sending manipulated model updates to the central server during the FL training process. Existing defenses mainly focus on Byzantine-robust FL aggregations, and largely ignore the impact of the underlying deep neural network (DNN) that is used to FL training. Inspired by recent findings on critical learning periods (CLP) in DNNs, where small gradient errors have irrecoverable impact on the final model accuracy, we propose a new defense, called a CLP-aware defense against poisoning of FL (DeFL). The key idea of DeFL is to measure fine-grained differences between DNN model updates via an easy-to-compute federated gradient norm vector (FGNV) metric. Using FGNV, DeFL simultaneously detects malicious clients and identifies CLP, which in turn is leveraged to guide the adaptive removal of detected malicious clients from aggregation. As a result, DeFL not only mitigates model poisoning attacks on the global model but also is robust to detection errors. Our extensive experiments on three benchmark datasets demonstrate that DeFL produces significant performance gain over conventional defenses against state-of-the-art model poisoning attacks",
    "checked": true,
    "id": "85e7ca020c740d9dfadfa520b9d6d2c8f383ec7f",
    "semantic_title": "defl: defending against model poisoning attacks in federated learning via critical learning periods awareness",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26272": {
    "title": "T2G-FORMER: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction",
    "volume": "main",
    "abstract": "Recent development of deep neural networks (DNNs) for tabular learning has largely benefited from the capability of DNNs for automatic feature interaction. However, the heterogeneity nature of tabular features makes such features relatively independent, and developing effective methods to promote tabular feature interaction still remains an open problem. In this paper, we propose a novel Graph Estimator, which automatically estimates the relations among tabular features and builds graphs by assigning edges between related features. Such relation graphs organize independent tabular features into a kind of graph data such that interaction of nodes (tabular features) can be conducted in an orderly fashion. Based on our proposed Graph Estimator, we present a bespoke Transformer network tailored for tabular learning, called T2G-Former, which processes tabular data by performing tabular feature interaction guided by the relation graphs. A specific Cross-level Readout collects salient features predicted by the layers in T2G-Former across different levels, and attains global semantics for final prediction. Comprehensive experiments show that our T2G-Former achieves superior performance among DNNs and is competitive with non-deep Gradient Boosted Decision Tree models. The code and detailed results are available at https://github.com/jyansir/t2g-former",
    "checked": true,
    "id": "611fb68240299adf0b19450b9972c55fc63df483",
    "semantic_title": "t2g-former: organizing tabular features into relation graphs promotes heterogeneous feature interaction",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26273": {
    "title": "Computably Continuous Reinforcement-Learning Objectives Are PAC-Learnable",
    "volume": "main",
    "abstract": "In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open. This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is computable, then it is PAC-learnable. In other words, if a procedure computes successive approximations of the objective's value, then the objective is PAC-learnable. We give three applications of our condition on objectives from the literature with previously unknown PAC-learnability and prove that these objectives are PAC-learnable. Overall, our result helps verify existing objectives' PAC-learnability. Also, as some studied objectives that are not uniformly continuous have been shown to be not PAC-learnable, our results could guide the design of new PAC-learnable objectives",
    "checked": false,
    "id": "94d84d1403a23a8a8486a151f52a126beb16875c",
    "semantic_title": "partitioning distributed compute jobs with reinforcement learning and graph neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26274": {
    "title": "Reinforcement Causal Structure Learning on Order Graph",
    "volume": "main",
    "abstract": "Learning directed acyclic graph (DAG) that describes the causality of observed data is a very challenging but important task. Due to the limited quantity and quality of observed data, and non-identifiability of causal graph, it is almost impossible to infer a single precise DAG. Some methods approximate the posterior distribution of DAGs to explore the DAG space via Markov chain Monte Carlo (MCMC), but the DAG space is over the nature of super-exponential growth, accurately characterizing the whole distribution over DAGs is very intractable. In this paper, we propose Reinforcement Causal Structure Learning on Order Graph (RCL-OG) that uses order graph instead of MCMC to model different DAG topological orderings and to reduce the problem size. RCL-OG first defines reinforcement learning with a new reward mechanism to approximate the posterior distribution of orderings in an efficacy way, and uses deep Q-learning to update and transfer rewards between nodes. Next, it obtains the probability transition model of nodes on order graph, and computes the posterior probability of different orderings. In this way, we can sample on this model to obtain the ordering with high probability. Experiments on synthetic and benchmark datasets show that RCL-OG provides accurate posterior probability approximation and achieves better results than competitive causal discovery algorithms",
    "checked": true,
    "id": "e283032acaed9db53c3fc447837d2793cffa2ddc",
    "semantic_title": "reinforcement causal structure learning on order graph",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26275": {
    "title": "AdaTask: A Task-Aware Adaptive Learning Rate Approach to Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task. Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the accumulative gradients and hence the learning rate of each task for each parameter in adaptive learning rate approaches (e.g., AdaGrad, RMSProp, and Adam). Comprehensive experiments on computer vision and recommender system MTL datasets demonstrate that AdaTask significantly improves the performance of dominated tasks, resulting SOTA average task-wise performance. Analysis on both synthetic and real-world datasets shows AdaTask balance parameters in every shared layer well",
    "checked": true,
    "id": "8522f9a7bc7c958b5f66373b47ff68e1833089d6",
    "semantic_title": "adatask: a task-aware adaptive learning rate approach to multi-task learning",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26276": {
    "title": "WaveForM: Graph Enhanced Wavelet Learning for Long Sequence Forecasting of Multivariate Time Series",
    "volume": "main",
    "abstract": "Multivariate time series (MTS) analysis and forecasting are crucial in many real-world applications, such as smart traffic management and weather forecasting. However, most existing work either focuses on short sequence forecasting or makes predictions predominantly with time domain features, which is not effective at removing noises with irregular frequencies in MTS. Therefore, we propose WaveForM, an end-to-end graph enhanced Wavelet learning framework for long sequence FORecasting of MTS. WaveForM first utilizes Discrete Wavelet Transform (DWT) to represent MTS in the wavelet domain, which captures both frequency and time domain features with a sound theoretical basis. To enable the effective learning in the wavelet domain, we further propose a graph constructor, which learns a global graph to represent the relationships between MTS variables, and graph-enhanced prediction modules, which utilize dilated convolution and graph convolution to capture the correlations between time series and predict the wavelet coefficients at different levels. Extensive experiments on five real-world forecasting datasets show that our model can achieve considerable performance improvement over different prediction lengths against the most competitive baseline of each dataset",
    "checked": true,
    "id": "17332418d299ef1cbd41cfcaf0a3a3141a7a8483",
    "semantic_title": "waveform: graph enhanced wavelet learning for long sequence forecasting of multivariate time series",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26277": {
    "title": "Layout Generation as Intermediate Action Sequence Prediction",
    "volume": "main",
    "abstract": "Layout generation plays a crucial role in graphic design intelligence. One important characteristic of the graphic layouts is that they usually follow certain design principles. For example, the principle of repetition emphasizes the reuse of similar visual elements throughout the design. To generate a layout, previous works mainly attempt at predicting the absolute value of bounding box for each element, where such target representation has hidden the information of higher-order design operations like repetition (e.g. copy the size of the previously generated element). In this paper, we introduce a novel action schema to encode these operations for better modeling the generation process. Instead of predicting the bounding box values, our approach autoregressively outputs the intermediate action sequence, which can then be deterministically converted to the final layout. We achieve state-of-the-art performances on three datasets. Both automatic and human evaluations show that our approach generates high-quality and diverse layouts. Furthermore, we revisit the commonly used evaluation metric FID adapted in this task, and observe that previous works use different settings to train the feature extractor for obtaining real/generated data distribution, which leads to inconsistent conclusions. We conduct an in-depth analysis on this metric and settle for a more robust and reliable evaluation setting. Code is available at this website",
    "checked": true,
    "id": "2096dbecf841988c15d97df2cdb033cfd8cd5151",
    "semantic_title": "layout generation as intermediate action sequence prediction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26278": {
    "title": "Learning-Assisted Algorithm Unrolling for Online Optimization with Budget Constraints",
    "volume": "main",
    "abstract": "Online optimization with multiple budget constraints is challenging since the online decisions over a short time horizon are coupled together by strict inventory constraints. The existing manually-designed algorithms cannot achieve satisfactory average performance for this setting because they often need a large number of time steps for convergence and/or may violate the inventory constraints. In this paper, we propose a new machine learning (ML) assisted unrolling approach, called LAAU (Learning-Assisted Algorithm Unrolling), which unrolls the agent's online decision pipeline and leverages an ML model for updating the Lagrangian multiplier online. For efficient training via backpropagation, we derive gradients of the decision pipeline over time. We also provide the average cost bounds for two cases when training data is available offline and collected online, respectively. Finally, we present numerical results to highlight that LAAU can outperform the existing baselines",
    "checked": true,
    "id": "f94c9daee84e5213703800863f6c0111b32bfbef",
    "semantic_title": "learning-assisted algorithm unrolling for online optimization with budget constraints",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26279": {
    "title": "ADEPT: A DEbiasing PrompT Framework",
    "volume": "main",
    "abstract": "Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects",
    "checked": true,
    "id": "1abd4fa45ce20175452aa238870db2aebe9c0fe0",
    "semantic_title": "adept: a debiasing prompt framework",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26280": {
    "title": "Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning",
    "volume": "main",
    "abstract": "Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model's generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-Level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets",
    "checked": true,
    "id": "2d9cc109363eae1aeb873245833b440ed917409e",
    "semantic_title": "generalized semantic segmentation by self-supervised source domain projection and multi-level contrastive learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26281": {
    "title": "CEM: Constrained Entropy Maximization for Task-Agnostic Safe Exploration",
    "volume": "main",
    "abstract": "In the absence of assigned tasks, a learning agent typically seeks to explore its environment efficiently. However, the pursuit of exploration will bring more safety risks. An under-explored aspect of reinforcement learning is how to achieve safe efficient exploration when the task is unknown. In this paper, we propose a practical Constrained Entropy Maximization (CEM) algorithm to solve task-agnostic safe exploration problems, which naturally require a finite horizon and undiscounted constraints on safety costs. The CEM algorithm aims to learn a policy that maximizes state entropy under the premise of safety. To avoid approximating the state density in complex domains, CEM leverages a k-nearest neighbor entropy estimator to evaluate the efficiency of exploration. In terms of safety, CEM minimizes the safety costs, and adaptively trades off safety and exploration based on the current constraint satisfaction. The empirical analysis shows that CEM enables the acquisition of a safe exploration policy in complex environments, resulting in improved performance in both safety and sample efficiency for target tasks",
    "checked": true,
    "id": "0a806876e419de58b650aacbf218e23f589327ad",
    "semantic_title": "cem: constrained entropy maximization for task-agnostic safe exploration",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26282": {
    "title": "Understanding Representation Learnability of Nonlinear Self-Supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box\". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses the exact version of Inverse Function Theorem to accurately describe the features learned by the local minimum. With this local minimum, we prove that the nonlinear SSL model can capture the label-related feature and hidden feature at the same time. In contrast, the nonlinear supervised learning (SL) model can only learn the label-related feature. We also present the learning processes and results of the nonlinear SSL and SL model via simulation experiments",
    "checked": true,
    "id": "65909b7b029f8214f574f81f4dfbef1fe7289037",
    "semantic_title": "understanding representation learnability of nonlinear self-supervised learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26283": {
    "title": "Simple and Efficient Heterogeneous Graph Neural Network",
    "volume": "main",
    "abstract": "Heterogeneous graph neural networks (HGNNs) have the powerful capability to embed rich structural and semantic information of a heterogeneous graph into node representations. Existing HGNNs inherit many mechanisms from graph neural networks (GNNs) designed for homogeneous graphs, especially the attention mechanism and the multi-layer structure. These mechanisms bring excessive complexity, but seldom work studies whether they are really effective on heterogeneous graphs. In this paper, we conduct an in-depth and detailed study of these mechanisms and propose the Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To easily capture structural information, SeHGNN pre-computes the neighbor aggregation using a light-weight mean aggregator, which reduces complexity by removing overused neighbor attention and avoiding repeated neighbor aggregation in every training epoch. To better utilize semantic information, SeHGNN adopts the single-layer structure with long metapaths to extend the receptive field, as well as a transformer-based semantic fusion module to fuse features from different metapaths. As a result, SeHGNN exhibits the characteristics of a simple network structure, high prediction accuracy, and fast training speed. Extensive experiments on five real-world heterogeneous graphs demonstrate the superiority of SeHGNN over the state-of-the-arts on both accuracy and training speed",
    "checked": true,
    "id": "db5f713db2730eb07c0bd7c5103e0c7e2b3e9ceb",
    "semantic_title": "simple and efficient heterogeneous graph neural network",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26284": {
    "title": "T-distributed Spherical Feature Representation for Imbalanced Classification",
    "volume": "main",
    "abstract": "Real-world classification tasks often show an extremely imbalanced problem. The extreme imbalance will cause a strong bias that the decision boundary of the classifier is completely dominated by the categories with abundant samples, which are also called the head categories. Current methods have alleviated the imbalanced impact from mainly three aspects: class re-balance, decoupling and domain adaptation. However, the existing criterion with the winner-take-all strategy still leads to the crowding problem in the eigenspace. The head categories with many samples can extract features more accurately, but occupy most of the eigenspace. The tail categories sharing the rest of the narrow eigenspace are too crowded together to accurately extract features. Above these issues, we propose a novel T-distributed spherical metric for equalized eigenspace in the imbalanced classification, which has the following innovations: 1) We design the T-distributed spherical metric, which has the characteristics of high kurtosis. Instead of the winner-take-all strategy, the T-distributed spherical metric produces a high logit only when the extracted feature is close enough to the category center, without a strong bias against other categories. 2) The T-distributed spherical metric is integrated into the classifier, which is able to equalize the eigenspace for alleviating the crowding issue in the imbalanced problem. The equalized eigenspace by the T-distributed spherical classifier is capable of improving the accuracy of the tail categories while maintaining the accuracy of the head, which significantly promotes the intraclass compactness and interclass separability of features. Extensive experiments on large-scale imbalanced datasets verify our method, which shows superior results in the long-tailed CIFAR-100/-10 with the imbalanced ratio IR = 100/50. Our method also achieves excellent results on the large-scale ImageNet-LT dataset and the iNaturalist dataset with various backbones. In addition, we provide a case study of the real clinical classification of pancreatic tumor subtypes with 6 categories. Among them, the largest number of PDAC accounts for 315 cases, and the least CP has only 8 cases. After 4-fold cross-validation, we achieved a top-1 accuracy of 69.04%",
    "checked": true,
    "id": "012b5b6e90821784c0221f75d627469425e4b178",
    "semantic_title": "t-distributed spherical feature representation for imbalanced classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26285": {
    "title": "Cluster-Guided Contrastive Graph Clustering Network",
    "volume": "main",
    "abstract": "Benefiting from the intrinsic supervision information exploitation capability, contrastive learning has achieved promising performance in the field of deep graph clustering recently. However, we observe that two drawbacks of the positive and negative sample construction mechanisms limit the performance of existing algorithms from further improvement. 1) The quality of positive samples heavily depends on the carefully designed data augmentations, while inappropriate data augmentations would easily lead to the semantic drift and indiscriminative positive samples. 2) The constructed negative samples are not reliable for ignoring important clustering information. To solve these problems, we propose a Cluster-guided Contrastive deep Graph Clustering network (CCGC) by mining the intrinsic supervision information in the high-confidence clustering results. Specifically, instead of conducting complex node or edge perturbation, we construct two views of the graph by designing special Siamese encoders whose weights are not shared between the sibling sub-networks. Then, guided by the high-confidence clustering information, we carefully select and construct the positive samples from the same high-confidence cluster in two views. Moreover, to construct semantic meaningful negative sample pairs, we regard the centers of different high-confidence clusters as negative samples, thus improving the discriminative capability and reliability of the constructed sample pairs. Lastly, we design an objective function to pull close the samples from the same cluster while pushing away those from other clusters by maximizing and minimizing the cross-view cosine similarity between positive and negative samples. Extensive experimental results on six datasets demonstrate the effectiveness of CCGC compared with the existing state-of-the-art algorithms. The code of CCGC is available at https://github.com/xihongyang1999/CCGC on Github",
    "checked": true,
    "id": "9706835a2bf6d727993e7bc4610067dc35cc5336",
    "semantic_title": "cluster-guided contrastive graph clustering network",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26286": {
    "title": "Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) enables the agent to effectively learn from logged data, which significantly extends the applicability of RL algorithms in real-world scenarios where exploration can be expensive or unsafe. Previous works have shown that extracting primitive skills from the recurring and temporally extended structures in the logged data yields better learning. However, these methods suffer greatly when the primitives have limited representation ability to recover the original policy space, especially in offline settings. In this paper, we give a quantitative characterization of the performance of offline hierarchical learning and highlight the importance of learning lossless primitives. To this end, we propose to use a flow-based structure as the representation for low-level policies. This allows us to represent the behaviors in the dataset faithfully while keeping the expression ability to recover the whole policy space. We show that such lossless primitives can drastically improve the performance of hierarchical policies. The experimental results and extensive ablation studies on the standard D4RL benchmark show that our method has a good representation ability for policies and achieves superior performance in most tasks",
    "checked": true,
    "id": "9d1445f1845a2880ff9c752845660e9c294aa7b5",
    "semantic_title": "flow to control: offline reinforcement learning with lossless primitive discovery",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26287": {
    "title": "Prototypical Partial Optimal Transport for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain without requiring the same label sets of both domains. The existence of domain and category shift makes the task challenging and requires us to distinguish \"known\" samples (i.e., samples whose labels exist in both domains) and \"unknown\" samples (i.e., samples whose labels exist in only one domain) in both domains before reducing the domain gap. In this paper, we consider the problem from the point of view of distribution matching which we only need to align two distributions partially. A novel approach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is proposed to conduct partial distribution alignment for UniDA. In training phase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT to reweight source prototypes and target samples, and design reweighted entropy loss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\" samples. Experiments on four benchmarks show that our method outperforms the previous state-of-the-art UniDA methods",
    "checked": true,
    "id": "85d1cd4db2246b32c4b68bfdb1532c7d89bd64ea",
    "semantic_title": "prototypical partial optimal transport for universal domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26288": {
    "title": "DeCOM: Decomposed Policy for Constrained Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "In recent years, multi-agent reinforcement learning (MARL) has presented impressive performance in various applications. However, physical limitations, budget restrictions, and many other factors usually impose constraints on a multi-agent system (MAS), which cannot be handled by traditional MARL frameworks. Specifically, this paper focuses on constrained MASes where agents work cooperatively to maximize the expected team-average return under various constraints on expected team-average costs, and develops a constrained cooperative MARL framework, named DeCOM, for such MASes. In particular, DeCOM decomposes the policy of each agent into two modules, which empowers information sharing among agents to achieve better cooperation. In addition, with such modularization, the training algorithm of DeCOM separates the original constrained optimization into an unconstrained optimization on reward and a constraints satisfaction problem on costs. DeCOM then iteratively solves these problems in a computationally efficient manner, which makes DeCOM highly scalable. We also provide theoretical guarantees on the convergence of DeCOM's policy update algorithm. Finally, we conduct extensive experiments to show the effectiveness of DeCOM with various types of costs in both moderate-scale and large-scale (with 500 agents) environments that originate from real-world applications",
    "checked": true,
    "id": "708b04a43ceb2a914652d2b3a8b5afe28b1875ba",
    "semantic_title": "decom: decomposed policy for constrained cooperative multi-agent reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26289": {
    "title": "Purifier: Defending Data Inference Attacks via Transforming Confidence Scores",
    "volume": "main",
    "abstract": "Neural networks are susceptible to data inference attacks such as the membership inference attack, the adversarial model inversion attack and the attribute inference attack, where the attacker could infer useful information such as the membership, the reconstruction or the sensitive attributes of a data sample from the confidence scores predicted by the target classifier. In this paper, we propose a method, namely PURIFIER, to defend against membership inference attacks. It transforms the confidence score vectors predicted by the target classifier and makes purified confidence scores indistinguishable in individual shape, statistical distribution and prediction label between members and non-members. The experimental results show that PURIFIER helps defend membership inference attacks with high effectiveness and efficiency, outperforming previous defense methods, and also incurs negligible utility loss. Besides, our further experiments show that PURIFIER is also effective in defending adversarial model inversion attacks and attribute inference attacks. For example, the inversion error is raised about 4+ times on the Facescrub530 classifier, and the attribute inference accuracy drops significantly when PURIFIER is deployed in our experiment",
    "checked": true,
    "id": "f605cb07365acd44f7ddc64711606c834f62afef",
    "semantic_title": "purifier: defending data inference attacks via transforming confidence scores",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26290": {
    "title": "i-Code: An Integrative and Composable Multimodal Learning Framework",
    "volume": "main",
    "abstract": "Human intelligence is multimodal; we integrate visual, linguistic, and acoustic signals to maintain a holistic worldview. Most current pretraining methods, however, are limited to one or two modalities. We present i-Code, a self-supervised pretraining framework where users may flexibly combine the modalities of vision, speech, and language into unified and general-purpose vector representations. In this framework, data from each modality are first given to pretrained single-modality encoders. The encoder outputs are then integrated with a multimodal fusion network, which uses novel merge- and co-attention mechanisms to effectively combine information from the different modalities. The entire system is pretrained end-to-end with new objectives including masked modality unit modeling and cross-modality contrastive learning. Unlike previous research using only video for pretraining, the i-Code framework can dynamically process single, dual, and triple-modality data during training and inference, flexibly projecting different combinations of modalities into a single representation space. Experimental results demonstrate how i-Code can outperform state-of-the-art techniques on five multimodal understanding tasks and single-modality benchmarks, improving by as much as 11% and demonstrating the power of integrative multimodal pretraining",
    "checked": true,
    "id": "b60879dda0183160c9d0a611cb7e381e6942cf75",
    "semantic_title": "i-code: an integrative and composable multimodal learning framework",
    "citation_count": 20
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26291": {
    "title": "Learning Dynamic Latent Spaces for Lifelong Generative Modelling",
    "volume": "main",
    "abstract": "Task Free Continual Learning (TFCL) aims to capture novel concepts from non-stationary data streams without forgetting previously learned knowledge. Mixture models, which add new components when certain conditions are met, have shown promising results in TFCL tasks. However, such approaches do not make use of the knowledge already accumulated for positive knowledge transfer. In this paper, we develop a new model, namely the Online Recursive Variational Autoencoder (ORVAE). ORVAE utilizes the prior knowledge by selectively incorporating the newly learnt information, by adding new components, according to the knowledge already known from the past learnt data. We introduce a new attention mechanism to regularize the structural latent space in which the most important information is reused while the information that interferes with novel samples is inactivated. The proposed attention mechanism can maximize the benefit from the forward transfer for learning novel information without forgetting previously learnt knowledge. We perform several experiments which show that ORVAE achieves state-of-the-art results under TFCL",
    "checked": true,
    "id": "94d1500cb6b280ff201047bf831593c4659581b7",
    "semantic_title": "learning dynamic latent spaces for lifelong generative modelling",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26292": {
    "title": "Lifelong Compression Mixture Model via Knowledge Relationship Graph",
    "volume": "main",
    "abstract": "Task-Free Continual Learning (TFCL) represents a challenging scenario for lifelong learning because the model, under this paradigm, does not access any task information. The Dynamic Expansion Model (DEM) has shown promising results in this scenario due to its scalability and generalisation power. However, DEM focuses only on addressing forgetting and ignores minimizing the model size, which limits its deployment in practical systems. In this work, we aim to simultaneously address network forgetting and model size optimization by developing the Lifelong Compression Mixture Model (LGMM) equipped with the Maximum Mean Discrepancy (MMD) based expansion criterion for model expansion. A diversity-aware sample selection approach is proposed to selectively store a variety of samples to promote information diversity among the components of the LGMM, which allows more knowledge to be captured with an appropriate model size. In order to avoid having multiple components with similar knowledge in the LGMM, we propose a data-free component discarding mechanism that evaluates a knowledge relation graph matrix describing the relevance between each pair of components. A greedy selection procedure is proposed to identify and remove the redundant components from the LGMM. The proposed discarding mechanism can be performed during or after the training. Experiments on different datasets show that LGMM achieves the best performance for TFCL",
    "checked": true,
    "id": "e8a560ebfb5ccf6caa8f838080c42f3686dcbdc5",
    "semantic_title": "lifelong compression mixture model via knowledge relationship graph",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26293": {
    "title": "Lifelong Variational Autoencoder via Online Adversarial Expansion Strategy",
    "volume": "main",
    "abstract": "The Variational Autoencoder (VAE) suffers from a significant loss of information when trained on a non-stationary data distribution. This loss in VAE models, called catastrophic forgetting, has not been studied theoretically before. We analyse the forgetting behaviour of a VAE in continual generative modelling by developing a new lower bound on the data likelihood, which interprets the forgetting process as an increase in the probability distance between the generator's distribution and the evolved data distribution. The proposed bound shows that a VAE-based dynamic expansion model can achieve better performance if its capacity increases appropriately considering the shift in the data distribution. Based on this analysis, we propose a novel expansion criterion that aims to preserve the information diversity among the VAE components, while ensuring that it acquires more knowledge with fewer parameters. Specifically, we implement this expansion criterion from the perspective of a multi-player game and propose the Online Adversarial Expansion Strategy (OAES), which considers all previously learned components as well as the currently updated component as multiple players in a game, while an adversary model evaluates their performance. The proposed OAES can dynamically estimate the discrepancy between each player and the adversary without accessing task information. This leads to the gradual addition of new components while ensuring the knowledge diversity among all of them. We show theoretically and empirically that the proposed extension strategy can enable a VAE model to achieve the best performance given an appropriate model size",
    "checked": true,
    "id": "a360413dc4fea6fe6d7d85e0396e2e55fc6a856c",
    "semantic_title": "lifelong variational autoencoder via online adversarial expansion strategy",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26294": {
    "title": "Continual Variational Autoencoder via Continual Generative Knowledge Distillation",
    "volume": "main",
    "abstract": "Humans and other living beings have the ability of short and long-term memorization during their entire lifespan. However, most existing Continual Learning (CL) methods can only account for short-term information when training on infinite streams of data. In this paper, we develop a new unsupervised continual learning framework consisting of two memory systems using Variational Autoencoders (VAEs). We develop a Short-Term Memory (STM), and a parameterised scalable memory implemented by a Teacher model aiming to preserve the long-term information. To incrementally enrich the Teacher's knowledge during training, we propose the Knowledge Incremental Assimilation Mechanism (KIAM), which evaluates the knowledge similarity between the STM and the already accumulated information as signals to expand the Teacher's capacity. Then we train a VAE as a Student module and propose a new Knowledge Distillation (KD) approach that gradually transfers generative knowledge from the Teacher to the Student module. To ensure the quality and diversity of knowledge in KD, we propose a new expert pruning approach that selectively removes the Teacher's redundant parameters, associated with unnecessary experts which have learnt overlapping information with other experts. This mechanism further reduces the complexity of the Teacher's module while ensuring the diversity of knowledge for the KD procedure. We show theoretically and empirically that the proposed framework can train a statistically diversified Teacher module for continual VAE learning which is applicable to learning infinite data streams",
    "checked": true,
    "id": "8be84129d04f13cfba51fc8ab453044721d16ecf",
    "semantic_title": "continual variational autoencoder via continual generative knowledge distillation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26295": {
    "title": "Certifiable Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Machine learning methods suffer from test-time performance degeneration when faced with out-of-distribution (OoD) data whose distribution is not necessarily the same as training data distribution. Although a plethora of algorithms have been proposed to mitigate this issue, it has been demonstrated that achieving better performance than ERM simultaneously on different types of distributional shift datasets is challenging for existing approaches. Besides, it is unknown how and to what extent these methods work on any OoD datum without theoretical guarantees. In this paper, we propose a certifiable out-of-distribution generalization method that provides provable OoD generalization performance guarantees via a functional optimization framework leveraging random distributions and max-margin learning for each input datum. With this approach, the proposed algorithmic scheme can provide certified accuracy for each input datum's prediction on the semantic space and achieves better performance simultaneously on OoD datasets dominated by correlation shifts or diversity shifts. Our code is available at https://github.com/ZlatanWilliams/StochasticDisturbanceLearning",
    "checked": true,
    "id": "6a99abc1a04244c4950c5ad999b5ebb20bdc1bd0",
    "semantic_title": "certifiable out-of-distribution generalization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26296": {
    "title": "Random Walk Conformer: Learning Graph Representation from Long and Short Range",
    "volume": "main",
    "abstract": "While graph neural networks (GNNs) have achieved notable success in various graph mining tasks, conventional GNNs only model the pairwise correlation in 1-hop neighbors without considering the long-term relations and the high-order patterns, thus limiting their performances. Recently, several works have addressed these issues by exploring the motif, i.e., frequent subgraphs. However, these methods usually require an unacceptable computational time to enumerate all possible combinations of motifs. In this paper, we introduce a new GNN framework, namely Random Walk Conformer (RWC), to exploit global correlations and local patterns based on the random walk, which is a promising method to discover the graph structure. Besides, we propose random walk encoding to help RWC capture topological information, which is proven more expressive than conventional spatial encoding. Extensive experiment results manifest that RWC achieves state-of-the-art performance on graph classification and regression tasks. The source code of RWC is available at https://github.com/b05901024/RandomWalkConformer",
    "checked": true,
    "id": "d6c2ce832157f54e1fe717bd3b1a9c4d4619c26e",
    "semantic_title": "random walk conformer: learning graph representation from long and short range",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26297": {
    "title": "Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost",
    "volume": "main",
    "abstract": "Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an \"ensemble'' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yields much stronger sparse subnetworks than the original LTs without requiring any extra training or inference cost. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that our method achieves significant performance gains in both, in-distribution and out-of-distribution scenarios. Impressively, evaluated with VGG-16 and ResNet-18, the produced sparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and 2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-trained dense-model up to 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C. Our source code can be found at https://github.com/luuyin/Lottery-pools",
    "checked": true,
    "id": "173a41e7f0eebca7deadaa623d36f83ad93c9bf5",
    "semantic_title": "lottery pools: winning more by interpolating tickets without increasing training or inference cost",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26298": {
    "title": "GOHSP: A Unified Framework of Graph and Optimization-Based Heterogeneous Structured Pruning for Vision Transformer",
    "volume": "main",
    "abstract": "The recently proposed Vision transformers (ViTs) have shown very impressive empirical performance in various computer vision tasks, and they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then severely hinder their potential deployment in many practical resources constrained applications. To mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable practical efficiency. However, unlike its current popularity for CNNs and RNNs, structured pruning for ViT models is little explored. In this paper, we propose GOHSP, a unified framework of Graph and Optimization-based Structured Pruning for ViT models. We first develop a graph-based ranking for measuring the importance of attention heads, and the extracted importance information is further integrated to an optimization-based procedure to impose the heterogeneous structured sparsity patterns on the ViT models. Experimental results show that our proposed GOHSP demonstrates excellent compression performance. On CIFAR-10 dataset, our approach can bring 40% parameters reduction with no accuracy loss for ViT-Small model. On ImageNet dataset, with 30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our approach achieves 1.65% and 0.76% accuracy increase over the existing structured pruning methods, respectively",
    "checked": true,
    "id": "3ea79430455304c782572dfb6ca3e5230b0351de",
    "semantic_title": "gohsp: a unified framework of graph and optimization-based heterogeneous structured pruning for vision transformer",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26299": {
    "title": "Policy-Based Primal-Dual Methods for Convex Constrained Markov Decision Processes",
    "volume": "main",
    "abstract": "We study convex Constrained Markov Decision Processes (CMDPs) in which the objective is concave and the constraints are convex in the state-action occupancy measure. We propose a policy-based primal-dual algorithm that updates the primal variable via policy gradient ascent and updates the dual variable via projected sub-gradient descent. Despite the loss of additivity structure and the nonconvex nature, we establish the global convergence of the proposed algorithm by leveraging a hidden convexity in the problem, and prove the O(T^-1/3) convergence rate in terms of both optimality gap and constraint violation. When the objective is strongly concave in the occupancy measure, we prove an improved convergence rate of O(T^-1/2). By introducing a pessimistic term to the constraint, we further show that a zero constraint violation can be achieved while preserving the same convergence rate for the optimality gap. This work is the first one in the literature that establishes non-asymptotic convergence guarantees for policy-based primal-dual methods for solving infinite-horizon discounted convex CMDPs",
    "checked": true,
    "id": "7a9c4cc5bd91fe5a0a2a0e0693f1b2bc89cd70e4",
    "semantic_title": "policy-based primal-dual methods for convex constrained markov decision processes",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26300": {
    "title": "Priori Anchor Labels Supervised Scalable Multi-View Bipartite Graph Clustering",
    "volume": "main",
    "abstract": "Although multi-view clustering (MVC) has achieved remarkable performance by integrating the complementary information of views, it is inefficient when facing scalable data. Proverbially, anchor strategy can mitigate such a challenge a certain extent. However, the unsupervised dynamic strategy usually cannot obtain the optimal anchors for MVC. The main reasons are that it does not consider the fairness of different views and lacks the priori supervised guidance. To completely solve these problems, we first propose the priori anchor graph regularization (PAGG) for scalable multi-view bipartite graph clustering, dubbed as SMGC method. Specifically, SMGC learns a few representative consensus anchors to simulate the numerous view data well, and constructs a bipartite graph to bridge the affinities between the anchors and original data points. In order to largely improve the quality of anchors, PAGG predefines prior anchor labels to constrain the anchors with discriminative cluster structure and fair view allocation, such that a better bipartite graph can be obtained for fast clustering. Experimentally, abundant of experiments are accomplished on six scalable benchmark datasets, and the experimental results fully demonstrate the effectiveness and efficiency of our SMGC",
    "checked": true,
    "id": "b3753e2b30662c7d0135fd28cd56c0c193cf7d65",
    "semantic_title": "priori anchor labels supervised scalable multi-view bipartite graph clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26301": {
    "title": "STARS: Spatial-Temporal Active Re-sampling for Label-Efficient Learning from Noisy Annotations",
    "volume": "main",
    "abstract": "Active learning (AL) aims to sample the most informative data instances for labeling, which makes the model fitting data efficient while significantly reducing the annotation cost. However, most existing AL models make a strong assumption that the annotated data instances are always assigned correct labels, which may not hold true in many practical settings. In this paper, we develop a theoretical framework to formally analyze the impact of noisy annotations and show that systematically re-sampling guarantees to reduce the noise rate, which can lead to improved generalization capability. More importantly, the theoretical framework demonstrates the key benefit of conducting active re-sampling on label-efficient learning, which is critical for AL. The theoretical results also suggest essential properties of an active re-sampling function with a fast convergence speed and guaranteed error reduction. This inspires us to design a novel spatial-temporal active re-sampling function by leveraging the important spatial and temporal properties of maximum-margin classifiers. Extensive experiments conducted on both synthetic and real-world data clearly demonstrate the effectiveness of the proposed active re-sampling function",
    "checked": true,
    "id": "830a4bc446c7c9d87975a95b7cea54e2d11d5727",
    "semantic_title": "stars: spatial-temporal active re-sampling for label-efficient learning from noisy annotations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26302": {
    "title": "Boosted Dynamic Neural Networks",
    "volume": "main",
    "abstract": "Early-exiting dynamic neural networks (EDNN), as one type of dynamic neural networks, has been widely studied recently. A typical EDNN has multiple prediction heads at different layers of the network backbone. During inference, the model will exit at either the last prediction head or an intermediate prediction head where the prediction confidence is higher than a predefined threshold. To optimize the model, these prediction heads together with the network backbone are trained on every batch of training data. This brings a train-test mismatch problem that all the prediction heads are optimized on all types of data in training phase while the deeper heads will only see difficult inputs in testing phase. Treating training and testing inputs differently at the two phases will cause the mismatch between training and testing data distributions. To mitigate this problem, we formulate an EDNN as an additive model inspired by gradient boosting, and propose multiple training techniques to optimize the model effectively. We name our method BoostNet. Our experiments show it achieves the state-of-the-art performance on CIFAR100 and ImageNet datasets in both anytime and budgeted-batch prediction modes. Our code is released at https://github.com/SHI-Labs/Boosted-Dynamic-Networks",
    "checked": true,
    "id": "eda07e0ab8396f45db07c39a1c7fa643d08b07c3",
    "semantic_title": "boosted dynamic neural networks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26303": {
    "title": "Stable Learning via Sparse Variable Independence",
    "volume": "main",
    "abstract": "The problem of covariate-shift generalization has attracted intensive research attention. Previous stable learning algorithms employ sample reweighting schemes to decorrelate the covariates when there is no explicit domain information about training data. However, with finite samples, it is difficult to achieve the desirable weights that ensure perfect independence to get rid of the unstable variables. Besides, decorrelating within stable variables may bring about high variance of learned models because of the over-reduced effective sample size. A tremendous sample size is required for these algorithms to work. In this paper, with theoretical justification, we propose SVI (Sparse Variable Independence) for the covariate-shift generalization problem. We introduce sparsity constraint to compensate for the imperfectness of sample reweighting under the finite-sample setting in previous methods. Furthermore, we organically combine independence-based sample reweighting and sparsity-based variable selection in an iterative way to avoid decorrelating within stable variables, increasing the effective sample size to alleviate variance inflation. Experiments on both synthetic and real-world datasets demonstrate the improvement of covariate-shift generalization performance brought by SVI",
    "checked": true,
    "id": "e1b5ca7d17c5b46a0c93c1a4e828ddf2a94f389a",
    "semantic_title": "stable learning via sparse variable independence",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26304": {
    "title": "Compressing Transformers: Features Are Low-Rank, but Weights Are Not!",
    "volume": "main",
    "abstract": "Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks, but high computational costs and reliance on large training datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been effective in compressing CNN models, but its application to transformers has been less explored and is less effective. Existing methods require the complete dataset to fine-tune compressed models, which are both time-consuming and data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layer's output features rather than the model weights. A second stage, GFM, optimizes the entire compressed network holistically. Both AAFM and GFM only use few training samples without labels, that is, they are few-shot, unsupervised, fast and effective. For example, with only 2K images without labels, 33% of the parameters are removed in DeiT-B with 18.8% relative throughput increase, but only a 0.23% accuracy loss for ImageNet recognition. The proposed methods are successfully applied to the language modeling task in NLP, too. Besides, the few-shot compressed models generalize well in downstream tasks",
    "checked": true,
    "id": "568160c414aa26284b04cdf3e13d25b9d7a5a290",
    "semantic_title": "compressing transformers: features are low-rank, but weights are not!",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26305": {
    "title": "Offline Imitation Learning with Suboptimal Demonstrations via Relaxed Distribution Matching",
    "volume": "main",
    "abstract": "Offline imitation learning (IL) promises the ability to learn performant policies from pre-collected demonstrations without interactions with the environment. However, imitating behaviors fully offline typically requires numerous expert data. To tackle this issue, we study the setting where we have limited expert data and supplementary suboptimal data. In this case, a well-known issue is the distribution shift between the learned policy and the behavior policy that collects the offline data. Prior works mitigate this issue by regularizing the KL divergence between the stationary state-action distributions of the learned policy and the behavior policy. We argue that such constraints based on exact distribution matching can be overly conservative and hamper policy learning, especially when the imperfect offline data is highly suboptimal. To resolve this issue, we present RelaxDICE, which employs an asymmetrically-relaxed f-divergence for explicit support regularization. Specifically, instead of driving the learned policy to exactly match the behavior policy, we impose little penalty whenever the density ratio between their stationary state-action distributions is upper bounded by a constant. Note that such formulation leads to a nested min-max optimization problem, which causes instability in practice. RelaxDICE addresses this challenge by supporting a closed-form solution for the inner maximization problem. Extensive empirical study shows that our method significantly outperforms the best prior offline IL method in six standard continuous control environments with over 30% performance gain on average, across 22 settings where the imperfect dataset is highly suboptimal",
    "checked": true,
    "id": "d05a0088e6ab6ba367650527c7e1cc46524da3dc",
    "semantic_title": "offline imitation learning with suboptimal demonstrations via relaxed distribution matching",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26306": {
    "title": "High-Level Semantic Feature Matters Few-Shot Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "In few-shot unsupervised domain adaptation (FS-UDA), most existing methods followed the few-shot learning (FSL) methods to leverage the low-level local features (learned from conventional convolutional models, e.g., ResNet) for classification. However, the goal of FS-UDA and FSL are relevant yet distinct, since FS-UDA aims to classify the samples in target domain rather than source domain. We found that the local features are insufficient to FS-UDA, which could introduce noise or bias against classification, and not be used to effectively align the domains. To address the above issues, we aim to refine the local features to be more discriminative and relevant to classification. Thus, we propose a novel task-specific semantic feature learning method (TSECS) for FS-UDA. TSECS learns high-level semantic features for image-to-class similarity measurement. Based on the high-level features, we design a cross-domain self-training strategy to leverage the few labeled samples in source domain to build the classifier in target domain. In addition, we minimize the KL divergence of the high-level feature distributions between source and target domains to shorten the distance of the samples between the two domains. Extensive experiments on DomainNet show that the proposed method significantly outperforms SOTA methods in FS-UDA by a large margin (i.e., ~10%)",
    "checked": true,
    "id": "01b14c1c3e2b5b4042d57f1a2be47d423c7fd68b",
    "semantic_title": "high-level semantic feature matters few-shot unsupervised domain adaptation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26307": {
    "title": "Coordinate Descent Methods for DC Minimization: Optimality Conditions and Global Convergence",
    "volume": "main",
    "abstract": "Difference-of-Convex (DC) minimization, referring to the problem of minimizing the difference of two convex functions, has been found rich applications in statistical learning and studied extensively for decades. However, existing methods are primarily based on multi-stage convex relaxation, only leading to weak optimality of critical points. This paper proposes a coordinate descent method for minimizing a class of DC functions based on sequential nonconvex approximation. Our approach iteratively solves a nonconvex one-dimensional subproblem globally, and it is guaranteed to converge to a coordinate-wise stationary point. We prove that this new optimality condition is always stronger than the standard critical point condition and directional point condition under a mildlocally bounded nonconvexity assumption. For comparisons, we also include a naive variant of coordinate descent methods based on sequential convex approximation in our study. When the objective function satisfies a globally bounded nonconvexity assumption and Luo-Tseng error bound assumption, coordinate descent methods achieve Q-linear convergence rate. Also, for many applications of interest, we show that the nonconvex one-dimensional subproblem can be computed exactly and efficiently using a breakpoint searching method. Finally, we have conducted extensive experiments on several statistical learning tasks to show the superiority of our approach",
    "checked": true,
    "id": "a8dfe3f91bed3c7536b99607e342ee9badc3dc9c",
    "semantic_title": "coordinate descent methods for dc minimization: optimality conditions and global convergence",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26308": {
    "title": "CEMA – Cost-Efficient Machine-Assisted Document Annotations",
    "volume": "main",
    "abstract": "We study the problem of semantically annotating textual documents that are complex in the sense that the documents are long, feature rich, and domain specific. Due to their complexity, such annotation tasks require trained human workers, which are very expensive in both time and money. We propose CEMA, a method for deploying machine learning to assist humans in complex document annotation. CEMA estimates the human cost of annotating each document and selects the set of documents to be annotated that strike the best balance between model accuracy and human cost. We conduct experiments on complex annotation tasks in which we compare CEMA against other document selection and annotation strategies. Our results show that CEMA is the most cost-efficient solution for those tasks",
    "checked": false,
    "id": "1dd63eff40175f2ad76b395313fa13c23393ebef",
    "semantic_title": "cema - cost-efficient machine-assisted document annotations",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26309": {
    "title": "Joint Multimodal Entity-Relation Extraction Based on Edge-Enhanced Graph Alignment Network and Word-Pair Relation Tagging",
    "volume": "main",
    "abstract": "Multimodal named entity recognition (MNER) and multimodal relation extraction (MRE) are two fundamental subtasks in the multimodal knowledge graph construction task. However, the existing methods usually handle two tasks independently, which ignores the bidirectional interaction between them. This paper is the first to propose jointly performing MNER and MRE as a joint multimodal entity-relation extraction (JMERE) task . Besides, the current MNER and MRE models only consider aligning the visual objects with textual entities in visual and textual graphs but ignore the entity-entity relationships and object-object relationships. To address the above challenges, we propose an edge-enhanced graph alignment network and a word-pair relation tagging (EEGA) for the JMERE task. Specifically, we first design a word-pair relation tagging to exploit the bidirectional interaction between MNER and MRE and avoid error propagation. Then, we propose an edge-enhanced graph alignment network to enhance the JMERE task by aligning nodes and edges in the cross-graph. Compared with previous methods, the proposed method can leverage the edge information to auxiliary alignment between objects and entities and find the correlations between entity-entity relationships and object-object relationships. Experiments are conducted to show the effectiveness of our model",
    "checked": true,
    "id": "74e14a0f55bdc0b8fa7548e0762f58cf493a12a3",
    "semantic_title": "joint multimodal entity-relation extraction based on edge-enhanced graph alignment network and word-pair relation tagging",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26310": {
    "title": "ODE-RSSM: Learning Stochastic Recurrent State Space Model from Irregularly Sampled Data",
    "volume": "main",
    "abstract": "For the complicated input-output systems with nonlinearity and stochasticity, Deep State Space Models (SSMs) are effective for identifying systems in the latent state space, which are of great significance for representation, forecasting, and planning in online scenarios. However, most SSMs are designed for discrete-time sequences and inapplicable when the observations are irregular in time. To solve the problem, we propose a novel continuous-time SSM named Ordinary Differential Equation Recurrent State Space Model (ODE-RSSM). ODE-RSSM incorporates an ordinary differential equation (ODE) network (ODE-Net) to model the continuous-time evolution of latent states between adjacent time points. Inspired from the equivalent linear transformation on integration limits, we propose an efficient reparameterization method for solving batched ODEs with non-uniform time spans in parallel for efficiently training the ODE-RSSM with irregularly sampled sequences. We also conduct extensive experiments to evaluate the proposed ODE-RSSM and the baselines on three input-output datasets, one of which is a rollout of a private industrial dataset with strong long-term delay and stochasticity. The results demonstrate that the ODE-RSSM achieves better performance than other baselines in open loop prediction even if the time spans of predicted points are uneven and the distribution of length is changeable. Code is availiable at https://github.com/yuanzhaolin/ODE-RSSM",
    "checked": true,
    "id": "3468d4cbfd51bd2013946e7a7df1fb95253f707f",
    "semantic_title": "ode-rssm: learning stochastic recurrent state space model from irregularly sampled data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26311": {
    "title": "Value-Consistent Representation Learning for Data-Efficient Reinforcement Learning",
    "volume": "main",
    "abstract": "Deep reinforcement learning (RL) algorithms suffer severe performance degradation when the interaction data is scarce, which limits their real-world application. Recently, visual representation learning has been shown to be effective and promising for boosting sample efficiency in RL. These methods usually rely on contrastive learning and data augmentation to train a transition model, which is different from how the model is used in RL---performing value-based planning. Accordingly, the learned representation by these visual methods may be good for recognition but not optimal for estimating state value and solving the decision problem. To address this issue, we propose a novel method, called value-consistent representation learning (VCR), to learn representations that are directly related to decision-making. More specifically, VCR trains a model to predict the future state (also referred to as the \"imagined state'') based on the current one and a sequence of actions. Instead of aligning this imagined state with a real state returned by the environment, VCR applies a Q value head on both of the states and obtains two distributions of action values. Then a distance is computed and minimized to force the imagined state to produce a similar action value prediction as that by the real state. We develop two implementations of the above idea for the discrete and continuous action spaces respectively. We conduct experiments on Atari 100k and DeepMind Control Suite benchmarks to validate their effectiveness for improving sample efficiency. It has been demonstrated that our methods achieve new state-of-the-art performance for search-free RL algorithms",
    "checked": true,
    "id": "e3c598f5233d91f887edb626bb02ba86010a69c0",
    "semantic_title": "value-consistent representation learning for data-efficient reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26312": {
    "title": "Learning Conflict-Noticed Architecture for Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-task learning has been widely used in many applications to enable more efficient learning by sharing part of the architecture across multiple tasks. However, a major challenge is the gradient conflict when optimizing the shared parameters, where the gradients of different tasks could have opposite directions. Directly averaging those gradients will impair the performance of some tasks and cause negative transfer. Different from most existing works that manipulate gradients to mitigate the gradient conflict, in this paper, we address this problem from the perspective of architecture learning and propose a Conflict-Noticed Architecture Learning (CoNAL) method to alleviate the gradient conflict by learning architectures. By introducing purely-specific modules specific to each task in the search space, the CoNAL method can automatically learn when to switch to purely-specific modules in the tree-structured network architectures when the gradient conflict occurs. To handle multi-task problems with a large number of tasks, we propose a progressive extension of the CoNAL method. Extensive experiments on computer vision, natural language processing, and reinforcement learning benchmarks demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "8f9e13380a4ccf99e8d4a0f1dd4c36cd311f8b60",
    "semantic_title": "learning conflict-noticed architecture for multi-task learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26313": {
    "title": "Quantum Multi-Agent Meta Reinforcement Learning",
    "volume": "main",
    "abstract": "Although quantum supremacy is yet to come, there has recently been an increasing interest in identifying the potential of quantum machine learning (QML) in the looming era of practical quantum computing. Motivated by this, in this article we re-design multi-agent reinforcement learning (MARL) based on the unique characteristics of quantum neural networks (QNNs) having two separate dimensions of trainable parameters: angle parameters affecting the output qubit states, and pole parameters associated with the output measurement basis. Exploiting this dyadic trainability as meta-learning capability, we propose quantum meta MARL (QM2ARL) that first applies angle training for meta-QNN learning, followed by pole training for few-shot or local-QNN training. To avoid overfitting, we develop an angle-to-pole regularization technique injecting noise into the pole domain during angle training. Furthermore, by exploiting the pole as the memory address of each trained QNN, we introduce the concept of pole memory allowing one to save and load trained QNNs using only two-parameter pole values. We theoretically prove the convergence of angle training under the angle-to-pole regularization, and by simulation corroborate the effectiveness of QM2ARL in achieving high reward and fast convergence, as well as of the pole memory in fast adaptation to a time-varying environment",
    "checked": true,
    "id": "6e85ed4c60ccc8eef4cc5ad69e677d0f9f5bdbfc",
    "semantic_title": "quantum multi-agent meta reinforcement learning",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26314": {
    "title": "Linking Sketch Patches by Learning Synonymous Proximity for Graphic Sketch Representation",
    "volume": "main",
    "abstract": "Graphic sketch representations are effective for representing sketches. Existing methods take the patches cropped from sketches as the graph nodes, and construct the edges based on sketch's drawing order or Euclidean distances on the canvas. However, the drawing order of a sketch may not be unique, while the patches from semantically related parts of a sketch may be far away from each other on the canvas. In this paper, we propose an order-invariant, semantics-aware method for graphic sketch representations. The cropped sketch patches are linked according to their global semantics or local geometric shapes, namely the synonymous proximity, by computing the cosine similarity between the captured patch embeddings. Such constructed edges are learnable to adapt to the variation of sketch drawings, which enable the message passing among synonymous patches. Aggregating the messages from synonymous patches by graph convolutional networks plays a role of denoising, which is beneficial to produce robust patch embeddings and accurate sketch representations. Furthermore, we enforce a clustering constraint over the embeddings jointly with the network learning. The synonymous patches are self-organized as compact clusters, and their embeddings are guided to move towards their assigned cluster centroids. It raises the accuracy of the computed synonymous proximity. Experimental results show that our method significantly improves the performance on both controllable sketch synthesis and sketch healing",
    "checked": true,
    "id": "b5458d11c6b64ab51d96581a708bb0ad40001a81",
    "semantic_title": "linking sketch patches by learning synonymous proximity for graphic sketch representation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26315": {
    "title": "Neural Integro-Differential Equations",
    "volume": "main",
    "abstract": "Modeling continuous dynamical systems from discretely sampled observations is a fundamental problem in data science. Often, such dynamics are the result of non-local processes that present an integral over time. As such, these systems are modeled with Integro-Differential Equations (IDEs); generalizations of differential equations that comprise both an integral and a differential component. For example, brain dynamics are not accurately modeled by differential equations since their behavior is non-Markovian, i.e. dynamics are in part dictated by history. Here, we introduce the Neural IDE (NIDE), a novel deep learning framework based on the theory of IDEs where integral operators are learned using neural networks. We test NIDE on several toy and brain activity datasets and demonstrate that NIDE outperforms other models. These tasks include time extrapolation as well as predicting dynamics from unseen initial conditions, which we test on whole-cortex activity recordings in freely behaving mice. Further, we show that NIDE can decompose dynamics into their Markovian and non-Markovian constituents, via the learned integral operator, which we test on fMRI brain activity recordings of people on ketamine. Finally, the integrand of the integral operator provides a latent space that gives insight into the underlying dynamics, which we demonstrate on wide-field brain imaging recordings. Altogether, NIDE is a novel approach that enables modeling of complex non-local dynamics with neural networks",
    "checked": true,
    "id": "9e740a91d943873000968afb0b137c83d65959a2",
    "semantic_title": "neural integro-differential equations",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26316": {
    "title": "Leveraging Structure for Improved Classification of Grouped Biased Data",
    "volume": "main",
    "abstract": "We consider semi-supervised binary classification for applications in which data points are naturally grouped (e.g., survey responses grouped by state) and the labeled data is biased (e.g., survey respondents are not representative of the population). The groups overlap in the feature space and consequently the input-output patterns are related across the groups. To model the inherent structure in such data, we assume the partition-projected class-conditional invariance across groups, defined in terms of the group-agnostic feature space. We demonstrate that under this assumption, the group carries additional information about the class, over the group-agnostic features, with provably improved area under the ROC curve. Further assuming invariance of partition-projected class-conditional distributions across both labeled and unlabeled data, we derive a semi-supervised algorithm that explicitly leverages the structure to learn an optimal, group-aware, probability-calibrated classifier, despite the bias in the labeled data. Experiments on synthetic and real data demonstrate the efficacy of our algorithm over suitable baselines and ablative models, spanning standard supervised and semi-supervised learning approaches, with and without incorporating the group directly as a feature",
    "checked": true,
    "id": "7331eb901d313073d7bfe8368af826d923b6a1b2",
    "semantic_title": "leveraging structure for improved classification of grouped biased data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26317": {
    "title": "Are Transformers Effective for Time Series Forecasting?",
    "volume": "main",
    "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future",
    "checked": true,
    "id": "5be02c8db2078bb72224438df8003552e49b23a8",
    "semantic_title": "are transformers effective for time series forecasting?",
    "citation_count": 116
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26318": {
    "title": "Substructure Aware Graph Neural Networks",
    "volume": "main",
    "abstract": "Despite the great achievements of Graph Neural Networks (GNNs) in graph learning, conventional GNNs struggle to break through the upper limit of the expressiveness of first-order Weisfeiler-Leman graph isomorphism test algorithm (1-WL) due to the consistency of the propagation paradigm of GNNs with the 1-WL.Based on the fact that it is easier to distinguish the original graph through subgraphs, we propose a novel framework neural network framework called Substructure Aware Graph Neural Networks (SAGNN) to address these issues. We first propose a Cut subgraph which can be obtained from the original graph by continuously and selectively removing edges. Then we extend the random walk encoding paradigm to the return probability of the rooted node on the subgraph to capture the structural information and use it as a node feature to improve the expressiveness of GNNs. We theoretically prove that our framework is more powerful than 1-WL, and is superior in structure perception. Our extensive experiments demonstrate the effectiveness of our framework, achieving state-of-the-art performance on a variety of well-proven graph tasks, and GNNs equipped with our framework perform flawlessly even in 3-WL failed graphs. Specifically, our framework achieves a maximum performance improvement of 83% compared to the base models and 32% compared to the previous state-of-the-art methods",
    "checked": true,
    "id": "f70fbf51b5ff4ba4c6a0766bc77831aff9176d16",
    "semantic_title": "substructure aware graph neural networks",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26319": {
    "title": "ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL) has attracted a surge of attention due to its superior performance for learning node/graph representations without labels. However, in practice, the underlying class distribution of unlabeled nodes for the given graph is usually imbalanced. This highly imbalanced class distribution inevitably deteriorates the quality of learned node representations in GCL. Indeed, we empirically find that most state-of-the-art GCL methods cannot obtain discriminative representations and exhibit poor performance on imbalanced node classification. Motivated by this observation, we propose a principled GCL framework on Imbalanced node classification (ImGCL), which automatically and adaptively balances the representations learned from GCL without labels. Specifically, we first introduce the online clustering based progressively balanced sampling (PBS) method with theoretical rationale, which balances the training sets based on pseudo-labels obtained from learned representations in GCL. We then develop the node centrality based PBS method to better preserve the intrinsic structure of graphs, by upweighting the important nodes of the given graph. Extensive experiments on multiple imbalanced graph datasets and imbalanced settings demonstrate the effectiveness of our proposed framework, which significantly improves the performance of the recent state-of-the-art GCL methods. Further experimental ablations and analyses show that the ImGCL framework consistently improves the representation quality of nodes in under-represented (tail) classes",
    "checked": true,
    "id": "09d31d7f124a30f6a7ccbf430feee62633decc05",
    "semantic_title": "imgcl: revisiting graph contrastive learning on imbalanced node classification",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26320": {
    "title": "Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment",
    "volume": "main",
    "abstract": "Existing domain generalization aims to learn a generalizable model to perform well even on unseen domains. For many real-world machine learning applications, the data distribution often shifts gradually along domain indices. For example, a self-driving car with a vision system drives from dawn to dusk, with the sky gradually darkening. Therefore, the system must be able to adapt to changes in ambient illuminations and continue to drive safely on the road. In this paper, we formulate such problems as Evolving Domain Generalization, where a model aims to generalize well on a target domain by discovering and leveraging the evolving pattern of the environment. We then propose Directional Domain Augmentation (DDA), which simulates the unseen target features by mapping source data as augmentations through a domain transformer. Specifically, we formulate DDA as a bi-level optimization problem and solve it through a novel meta-learning approach in the representation space. We evaluate the proposed method on both synthetic datasets and real-world datasets, and empirical results show that our approach can outperform other existing methods",
    "checked": false,
    "id": "0a199db260704aedc67836ae61728a954e8aa24c",
    "semantic_title": "foresee what you will learn: data augmentation for domain generalization in non-stationary environments",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26321": {
    "title": "Acceleration of Large Transformer Model Training by Sensitivity-Based Layer Dropping",
    "volume": "main",
    "abstract": "Transformer models are widely used in AI applications such as Natural Language Processing (NLP), Computer Vision (CV), etc. However, enormous computation workload be-comes an obstacle to train large transformer models efficiently. Recently, some methods focus on reducing the computation workload during the training by skipping some layers. How-ever, these methods use simple probability distribution and coarse-grained probability calculation, which significantly affect the model accuracy. To address the issue, in this paper we propose a novel method to accelerate training—Sensitivity-Based Layer Dropping (SBLD). SBLD uses lay-er-wise sensitivity data to switch on/off transformer layers in proper order to keep high accuracy. Besides, we adjust the probability of skipping transformer layers with a scheduler to accelerate training speed and get faster convergence. Our results show that SBLD solves the accuracy drop issue com-pared with prior layer dropping methods. Our SBLD method can decrease end-to-end training time by 19.67% during training of GPT-3 Medium model, the same time increasing the accuracy by 1.65% w.r.t. baseline. Furthermore, for SwinV2-L model the obtained Top-1 and Top-5 accuracies are also higher vs. the baseline. Thus, the proposed method is efficient and practical to improve the large transformer model training",
    "checked": true,
    "id": "05edd3c2ede087fa0fdfe8d2ecbf1aa95ce600f3",
    "semantic_title": "acceleration of large transformer model training by sensitivity-based layer dropping",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26322": {
    "title": "Interventional SHAP Values and Interaction Values for Piecewise Linear Regression Trees",
    "volume": "main",
    "abstract": "In recent years, game-theoretic Shapley values have gained increasing attention with respect to local model explanation by feature attributions. While the approach using Shapley values is model-independent, their (exact) computation is usually intractable, so efficient model-specific algorithms have been devised including approaches for decision trees or their ensembles in general. Our work goes further in this direction by extending the interventional TreeSHAP algorithm to piecewise linear regression trees, which gained more attention in the past few years. To this end, we introduce a decomposition of the contribution function based on decision paths, which allows a more comprehensible formulation of SHAP algorithms for tree-based models. Our algorithm can also be readily applied to computing SHAP interaction values of these models. In particular, as the main contribution of this paper, we provide a more efficient approach of interventional SHAP for tree-based models by precomputing statistics of the background data based on the tree structure",
    "checked": true,
    "id": "1ea60f94e6bea09a60b92351b72dec40bf2948f9",
    "semantic_title": "interventional shap values and interaction values for piecewise linear regression trees",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26323": {
    "title": "Enhanced Tensor Low-Rank and Sparse Representation Recovery for Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "Incomplete multi-view clustering (IMVC) has attracted remarkable attention due to the emergence of multi-view data with missing views in real applications. Recent methods attempt to recover the missing information to address the IMVC problem. However, they generally cannot fully explore the underlying properties and correlations of data similarities across views. This paper proposes a novel Enhanced Tensor Low-rank and Sparse Representation Recovery (ETLSRR) method, which reformulates the IMVC problem as a joint incomplete similarity graphs learning and complete tensor representation recovery problem. Specifically, ETLSRR learns the intra-view similarity graphs and constructs a 3-way tensor by stacking the graphs to explore the inter-view correlations. To alleviate the negative influence of missing views and data noise, ETLSRR decomposes the tensor into two parts: a sparse tensor and an intrinsic tensor, which models the noise and underlying true data similarities, respectively. Both global low-rank and local structured sparse characteristics of the intrinsic tensor are considered, which enhances the discrimination of similarity matrix. Moreover, instead of using the convex tensor nuclear norm, ETLSRR introduces a generalized non-convex tensor low-rank regularization to alleviate the biased approximation. Experiments on several datasets demonstrate the effectiveness of our method compared with the state-of-the-art methods",
    "checked": true,
    "id": "88258d707e898923359fcea017f4f0281a65be57",
    "semantic_title": "enhanced tensor low-rank and sparse representation recovery for incomplete multi-view clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26324": {
    "title": "Denoising Multi-Similarity Formulation: A Self-Paced Curriculum-Driven Approach for Robust Metric Learning",
    "volume": "main",
    "abstract": "Deep Metric Learning (DML) is a group of techniques that aim to measure the similarity between objects through the neural network. Although the number of DML methods has rapidly increased in recent years, most previous studies cannot effectively handle noisy data, which commonly exists in practical applications and often leads to serious performance deterioration. To overcome this limitation, in this paper, we build a connection between noisy samples and hard samples in the framework of self-paced learning, and propose a Balanced Self-Paced Metric Learning (BSPML) algorithm with a denoising multi-similarity formulation, where noisy samples are treated as extremely hard samples and adaptively excluded from the model training by sample weighting. Especially, due to the pairwise relationship and a new balance regularization term, the sub-problem w.r.t. sample weights is a nonconvex quadratic function. To efficiently solve this nonconvex quadratic problem, we propose a doubly stochastic projection coordinate gradient algorithm. Importantly, we theoretically prove the convergence not only for the doubly stochastic projection coordinate gradient algorithm, but also for our BSPML algorithm. Experimental results on several standard data sets demonstrate that our BSPML algorithm has better generalization ability and robustness than the state-of-the-art robust DML approaches",
    "checked": true,
    "id": "586e092999ebd347237ffe372af67bef36107369",
    "semantic_title": "denoising multi-similarity formulation: a self-paced curriculum-driven approach for robust metric learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26325": {
    "title": "Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation",
    "volume": "main",
    "abstract": "Unsupervised image segmentation aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods",
    "checked": true,
    "id": "f6fb6f464efead0c4543f97de793a439c3ca07c6",
    "semantic_title": "rethinking alignment and uniformity in unsupervised image semantic segmentation",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26326": {
    "title": "Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that aims at estimating the data-generating policy. In particular, this work considers a scenario where data are collected from multiple sources. Neglecting data heterogeneity, existing approaches cannot provide good estimates and impede policy learning. To overcome this drawback, the present study proposes a latent variable model and a model-learning algorithm to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. To illustrate the benefit of such a fine-grained characterization for multi-source data, this work showcases how the proposed model can be incorporated into an existing offline RL algorithm. Lastly, with extensive empirical evaluation this work confirms the risks of neglecting data heterogeneity and the efficacy of the proposed model",
    "checked": true,
    "id": "6616965ea639e90c2413cff00aedf0133ce80fd9",
    "semantic_title": "behavior estimation from multi-source data for offline reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26327": {
    "title": "DARL: Distance-Aware Uncertainty Estimation for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "To facilitate offline reinforcement learning, uncertainty estimation is commonly used to detect out-of-distribution data. By inspecting, we show that current explicit uncertainty estimators such as Monte Carlo Dropout and model ensemble are not competent to provide trustworthy uncertainty estimation in offline reinforcement learning. Accordingly, we propose a non-parametric distance-aware uncertainty estimator which is sensitive to the change in the input space for offline reinforcement learning. Based on our new estimator, adaptive truncated quantile critics are proposed to underestimate the out-of-distribution samples. We show that the proposed distance-aware uncertainty estimator is able to offer better uncertainty estimation compared to previous methods. Experimental results demonstrate that our proposed DARL method is competitive to the state-of-the-art methods in offline evaluation tasks",
    "checked": true,
    "id": "32a31c899c145d9502ba3f5ef9fc70671a45da76",
    "semantic_title": "darl: distance-aware uncertainty estimation for offline reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26328": {
    "title": "When Neural Networks Fail to Generalize? A Model Sensitivity Perspective",
    "volume": "main",
    "abstract": "Domain generalization (DG) aims to train a model to perform well in unseen domains under different distributions. This paper considers a more realistic yet more challenging scenario, namely Single Domain Generalization (Single-DG), where only a single source domain is available for training. To tackle this challenge, we first try to understand when neural networks fail to generalize? We empirically ascertain a property of a model that correlates strongly with its generalization that we coin as \"model sensitivity\". Based on our analysis, we propose a novel strategy of Spectral Adversarial Data Augmentation (SADA) to generate augmented images targeted at the highly sensitive frequencies. Models trained with these hard-to-learn samples can effectively suppress the sensitivity in the frequency space, which leads to improved generalization performance. Extensive experiments on multiple public datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 2.55%. The source code is available at https://github.com/DIAL-RPI/Spectral-Adversarial-Data-Augmentation",
    "checked": true,
    "id": "22cf52a10dcfec08b1c7d8b0a6eb7bbbfa1c79d5",
    "semantic_title": "when neural networks fail to generalize? a model sensitivity perspective",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26329": {
    "title": "Memorization Weights for Instance Reweighting in Adversarial Training",
    "volume": "main",
    "abstract": "Adversarial training is an effective way to defend deep neural networks (DNN) against adversarial examples. However, there are atypical samples that are rare and hard to learn, or even hurt DNNs' generalization performance on test data. In this paper, we propose a novel algorithm to reweight the training samples based on self-supervised techniques to mitigate the negative effects of the atypical samples. Specifically, a memory bank is built to record the popular samples as prototypes and calculate the memorization weight for each sample, evaluating the \"typicalness\" of a sample. All the training samples are reweigthed based on the proposed memorization weights to reduce the negative effects of atypical samples. Experimental results show the proposed method is flexible to boost state-of-the-art adversarial training methods, improving both robustness and standard accuracy of DNNs",
    "checked": true,
    "id": "00bbfc9b99d74203a64dc9f40639fe48a0deff36",
    "semantic_title": "memorization weights for instance reweighting in adversarial training",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26330": {
    "title": "FedALA: Adaptive Local Aggregation for Personalized Federated Learning",
    "volume": "main",
    "abstract": "A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy. Code is available at https://github.com/TsingZ0/FedALA",
    "checked": true,
    "id": "939a858b08e3920783c7f949687a9758a509cee9",
    "semantic_title": "fedala: adaptive local aggregation for personalized federated learning",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26331": {
    "title": "Delving into the Adversarial Robustness of Federated Learning",
    "volume": "main",
    "abstract": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, we conduct comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, we reveal the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. In this work, we propose a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings",
    "checked": true,
    "id": "29b64e41bbea997019b64696d2374c3c96dbdf5c",
    "semantic_title": "delving into the adversarial robustness of federated learning",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26332": {
    "title": "DRGCN: Dynamic Evolving Initial Residual for Deep Graph Convolutional Networks",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCNs) have been proved to be very practical to handle various graph-related tasks. It has attracted considerable research interest to study deep GCNs, due to their potential superior performance compared with shallow ones. However, simply increasing network depth will, on the contrary, hurt the performance due to the over-smoothing problem. Adding residual connection is proved to be effective for learning deep convolutional neural networks (deep CNNs), it is not trivial when applied to deep GCNs. Recent works proposed an initial residual mechanism that did alleviate the over-smoothing problem in deep GCNs. However, according to our study, their algorithms are quite sensitive to different datasets. In their setting, the personalization (dynamic) and correlation (evolving) of how residual applies are ignored. To this end, we propose a novel model called Dynamic evolving initial Residual Graph Convolutional Network (DRGCN). Firstly, we use a dynamic block for each node to adaptively fetch information from the initial representation. Secondly, we use an evolving block to model the residual evolving pattern between layers. Our experimental results show that our model effectively relieves the problem of over-smoothing in deep GCNs and outperforms the state-of-the-art (SOTA) methods on various benchmark datasets. Moreover, we develop a mini-batch version of DRGCN which can be applied to large-scale data. Coupling with several fair training techniques, our model reaches new SOTA results on the large-scale ogbn-arxiv dataset of Open Graph Benchmark (OGB). Our reproducible code is available on GitHub",
    "checked": true,
    "id": "e36c914b0437afe55b51b1dfe502bba1f40c7bdd",
    "semantic_title": "drgcn: dynamic evolving initial residual for deep graph convolutional networks",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26333": {
    "title": "Let the Data Choose: Flexible and Diverse Anchor Graph Fusion for Scalable Multi-View Clustering",
    "volume": "main",
    "abstract": "In the past few years, numerous multi-view graph clustering algorithms have been proposed to enhance the clustering performance by exploring information from multiple views. Despite the superior performance, the high time and space expenditures limit their scalability. Accordingly, anchor graph learning has been introduced to alleviate the computational complexity. However, existing approaches can be further improved by the following considerations: (i) Existing anchor-based methods share the same number of anchors across views. This strategy violates the diversity and flexibility of multi-view data distribution. (ii) Searching for the optimal anchor number within hyper-parameters takes much extra tuning time, which makes existing methods impractical. (iii) How to flexibly fuse multi-view anchor graphs of diverse sizes has not been well explored in existing literature. To address the above issues, we propose a novel anchor-based method termed Flexible and Diverse Anchor Graph Fusion for Scalable Multi-view Clustering (FDAGF) in this paper. Instead of manually tuning optimal anchor with massive hyper-parameters, we propose to optimize the contribution weights of a group of pre-defined anchor numbers to avoid extra time expenditure among views. Most importantly, we propose a novel hybrid fusion strategy for multi-size anchor graphs with theoretical proof, which allows flexible and diverse anchor graph fusion. Then, an efficient linear optimization algorithm is proposed to solve the resultant problem. Comprehensive experimental results demonstrate the effectiveness and efficiency of our proposed framework. The source code is available at https://github.com/Jeaninezpp/FDAGF",
    "checked": true,
    "id": "38e94c56f008e91db2203c16644687b15b05bfc1",
    "semantic_title": "let the data choose: flexible and diverse anchor graph fusion for scalable multi-view clustering",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26334": {
    "title": "Optimal Sparse Regression Trees",
    "volume": "main",
    "abstract": "Regression trees are one of the oldest forms of AI models, and their predictions can be made without a calculator, which makes them broadly useful, particularly for high-stakes applications. Within the large literature on regression trees, there has been little effort towards full provable optimization, mainly due to the computational hardness of the problem. This work proposes a dynamic programming-with-bounds approach to the construction of provably-optimal sparse regression trees. We leverage a novel lower bound based on an optimal solution to the k-Means clustering algorithm on one dimensional data. We are often able to find optimal sparse trees in seconds, even for challenging datasets that involve large numbers of samples and highly-correlated features",
    "checked": true,
    "id": "3d5fbe321d89febc15a450fa171b6261f9c1daf0",
    "semantic_title": "optimal sparse regression trees",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26335": {
    "title": "High-Dimensional Dueling Optimization with Preference Embedding",
    "volume": "main",
    "abstract": "In many scenarios of black-box optimization, evaluating the objective function values of solutions is expensive, while comparing a pair of solutions is relatively cheap, which yields the dueling black-box optimization. The side effect of dueling optimization is that it doubles the dimension of solution space and exacerbates the dimensionality scalability issue of black-box optimization, e.g., Bayesian optimization. To address this issue, the existing dueling optimization methods fix one solution when dueling throughout the optimization process, but it may reduce their efficacy. Fortunately, it has been observed that, in recommendation systems, the dueling results are mainly determined by the latent human preferences. In this paper, we abstract this phenomenon as the preferential intrinsic dimension and inject it into the dueling Bayesian optimization, resulting in the preferential embedding dueling Bayesian optimization (PE-DBO). PE-DBO decouples optimization and pairwise comparison via the preferential embedding matrix. Optimization is performed in the preferential intrinsic subspace with much lower dimensionality, while pairwise comparison is completed in the original dueling solution space. Theoretically, we disclose that the preference function can be approximately preserved in the lower-dimensional preferential intrinsic subspace. Experiment results verify that, on molecule discovery and web page recommendation dueling optimization tasks, the preferential intrinsic dimension exists and PE-DBO is superior in scalability compared with that of the state-of-the-art (SOTA) methods",
    "checked": true,
    "id": "561f9714db2e3eafcb28c731e99d4b122aca6525",
    "semantic_title": "high-dimensional dueling optimization with preference embedding",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26336": {
    "title": "Spectral Feature Augmentation for Graph Contrastive Learning and Beyond",
    "volume": "main",
    "abstract": "Although augmentations (e.g., perturbation of graph edges, image crops) boost the efficiency of Contrastive Learning (CL), feature level augmentation is another plausible, complementary yet not well researched strategy. Thus, we present a novel spectral feature argumentation for contrastive learning on graphs (and images). To this end, for each data view, we estimate a low-rank approximation per feature map and subtract that approximation from the map to obtain its complement. This is achieved by the proposed herein incomplete power iteration, a non-standard power iteration regime which enjoys two valuable byproducts (under mere one or two iterations): (i) it partially balances spectrum of the feature map, and (ii) it injects the noise into rebalanced singular values of the feature map (spectral augmentation). For two views, we align these rebalanced feature maps as such an improved alignment step can focus more on less dominant singular values of matrices of both views, whereas the spectral augmentation does not affect the spectral angle alignment (singular vectors are not perturbed). We derive the analytical form for: (i) the incomplete power iteration to capture its spectrum-balancing effect, and (ii) the variance of singular values augmented implicitly by the noise. We also show that the spectral augmentation improves the generalization bound. Experiments on graph/image datasets show that our spectral feature augmentation outperforms baselines, and is complementary with other augmentation strategies and compatible with various contrastive losses",
    "checked": true,
    "id": "1d3fc84e5efeb599123605268cf6c6616ac06820",
    "semantic_title": "spectral feature augmentation for graph contrastive learning and beyond",
    "citation_count": 13
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26337": {
    "title": "Scalable Bayesian Meta-Learning through Generalized Implicit Gradients",
    "volume": "main",
    "abstract": "Meta-learning owns unique effectiveness and swiftness in tackling emerging tasks with limited data. Its broad applicability is revealed by viewing it as a bi-level optimization problem. The resultant algorithmic viewpoint however, faces scalability issues when the inner-level optimization relies on gradient-based iterations. Implicit differentiation has been considered to alleviate this challenge, but it is restricted to an isotropic Gaussian prior, and only favors deterministic meta-learning approaches. This work markedly mitigates the scalability bottleneck by cross-fertilizing the benefits of implicit differentiation to probabilistic Bayesian meta-learning. The novel implicit Bayesian meta-learning (iBaML) method not only broadens the scope of learnable priors, but also quantifies the associated uncertainty. Furthermore, the ultimate complexity is well controlled regardless of the inner-level optimization trajectory. Analytical error bounds are established to demonstrate the precision and efficiency of the generalized implicit gradient over the explicit one. Extensive numerical tests are also carried out to empirically validate the performance of the proposed method",
    "checked": true,
    "id": "e960e12a260556cea93ca4e02450a5edc7e30d7c",
    "semantic_title": "scalable bayesian meta-learning through generalized implicit gradients",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26338": {
    "title": "Dynamic Heterogeneous Graph Attention Neural Architecture Search",
    "volume": "main",
    "abstract": "Dynamic heterogeneous graph neural networks (DHGNNs) have been shown to be effective in handling the ubiquitous dynamic heterogeneous graphs. However, the existing DHGNNs are hand-designed, requiring extensive human efforts and failing to adapt to diverse dynamic heterogeneous graph scenarios. In this paper, we propose to automate the design of DHGNN, which faces two major challenges: 1) how to design the search space to jointly consider the spatial-temporal dependencies and heterogeneous interactions in graphs; 2) how to design an efficient search algorithm in the potentially large and complex search space. To tackle these challenges, we propose a novel Dynamic Heterogeneous Graph Attention Search (DHGAS) method. Our proposed method can automatically discover the optimal DHGNN architecture and adapt to various dynamic heterogeneous graph scenarios without human guidance. In particular, we first propose a unified dynamic heterogeneous graph attention (DHGA) framework, which enables each node to jointly attend its heterogeneous and dynamic neighbors. Based on the framework, we design a localization space to determine where the attention should be applied and a parameterization space to determine how the attention should be parameterized. Lastly, we design a multi-stage differentiable search algorithm to efficiently explore the search space. Extensive experiments on real-world dynamic heterogeneous graph datasets demonstrate that our proposed method significantly outperforms state-of-the-art baselines for tasks including link prediction, node classification and node regression. To the best of our knowledge, DHGAS is the first dynamic heterogeneous graph neural architecture search method",
    "checked": true,
    "id": "8dd0869fd671682fa2d8e5f5f0ef14ecef04df0d",
    "semantic_title": "dynamic heterogeneous graph attention neural architecture search",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26339": {
    "title": "Dynamic Ensemble of Low-Fidelity Experts: Mitigating NAS \"Cold-Start",
    "volume": "main",
    "abstract": "Predictor-based Neural Architecture Search (NAS) employs an architecture performance predictor to improve the sample efficiency. However, predictor-based NAS suffers from the severe ``cold-start'' problem, since a large amount of architecture-performance data is required to get a working predictor. In this paper, we focus on exploiting information in cheaper-to-obtain performance estimations (i.e., low-fidelity information) to mitigate the large data requirements of predictor training. Despite the intuitiveness of this idea, we observe that using inappropriate low-fidelity information even damages the prediction ability and different search spaces have different preferences for low-fidelity information types. To solve the problem and better fuse beneficial information provided by different types of low-fidelity information, we propose a novel dynamic ensemble predictor framework that comprises two steps. In the first step, we train different sub-predictors on different types of available low-fidelity information to extract beneficial knowledge as low-fidelity experts. In the second step, we learn a gating network to dynamically output a set of weighting coefficients conditioned on each input neural architecture, which will be used to combine the predictions of different low-fidelity experts in a weighted sum. The overall predictor is optimized on a small set of actual architecture-performance data to fuse the knowledge from different low-fidelity experts to make the final prediction. We conduct extensive experiments across five search spaces with different architecture encoders under various experimental settings. For example, our methods can improve the Kendall's Tau correlation coefficient between actual performance and predicted scores from 0.2549 to 0.7064 with only 25 actual architecture-performance data on NDS-ResNet. Our method can easily be incorporated into existing predictor-based NAS frameworks to discover better architectures. Our method will be implemented in Mindspore (Huawei 2020), and the example code is published at https://github.com/A-LinCui/DELE",
    "checked": true,
    "id": "29a2acee401e86c7a6708a370a2c58bead6034d4",
    "semantic_title": "dynamic ensemble of low-fidelity experts: mitigating nas \"cold-start",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26340": {
    "title": "Tensorized Incomplete Multi-View Clustering with Intrinsic Graph Completion",
    "volume": "main",
    "abstract": "Most of the existing incomplete multi-view clustering (IMVC) methods focus on attaining a consensus representation from different views but ignore the important information hidden in the missing views and the latent intrinsic structures in each view. To tackle these issues, in this paper, a unified and novel framework, named tensorized incomplete multi-view clustering with intrinsic graph completion (TIMVC_IGC) is proposed. Firstly, owing to the effectiveness of the low-rank representation in revealing the inherent structure of the data, we exploit it to infer the missing instances and construct the complete graph for each view. Afterwards, inspired by the structural consistency, a between-view consistency constraint is imposed to guarantee the similarity of the graphs from different views. More importantly, the TIMVC_IGC simultaneously learns the low-rank structures of the different views and explores the correlations of the different graphs in a latent manifold sub-space using a low-rank tensor constraint, such that the intrinsic graphs of the different views can be obtained. Finally, a consensus representation for each sample is gained with a co-regularization term for final clustering. Experimental results on several real-world databases illustrates that the proposed method can outperform the other state-of-the-art related methods for incomplete multi-view clustering",
    "checked": true,
    "id": "cbc2b78e5b981c58410605b27b7ff5fc8e216c65",
    "semantic_title": "tensorized incomplete multi-view clustering with intrinsic graph completion",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26341": {
    "title": "Imbalanced Label Distribution Learning",
    "volume": "main",
    "abstract": "Label distribution covers a certain number of labels, representing the degree to which each label describes an instance. The learning process on the instances labeled by label distributions is called Label Distribution Learning (LDL). Although LDL has been applied successfully to many practical applications, one problem with existing LDL methods is that they are limited to data with balanced label information. However, annotation information in real-world data often exhibits imbalanced distributions, which significantly degrades the performance of existing methods. In this paper, we investigate the Imbalanced Label Distribution Learning (ILDL) problem. To handle this challenging problem, we delve into the characteristics of ILDL and empirically find that the representation distribution shift is the underlying reason for the performance degradation of existing methods. Inspired by this finding, we present a novel method named Representation Distribution Alignment (RDA). RDA aligns the distributions of feature representations and label representations to alleviate the impact of the distribution gap between the training set and the test set caused by the imbalance issue. Extensive experiments verify the superior performance of RDA. Our work fills the gap in benchmarks and techniques for practical ILDL problems",
    "checked": true,
    "id": "383bf49b0444f27a94c4acc8d11d7a0225142c37",
    "semantic_title": "imbalanced label distribution learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26342": {
    "title": "CoopInit: Initializing Generative Adversarial Networks via Cooperative Learning",
    "volume": "main",
    "abstract": "Numerous research efforts have been made to stabilize the training of the Generative Adversarial Networks (GANs), such as through regularization and architecture design. However, we identify the instability can also arise from the fragile balance at the early stage of adversarial learning. This paper proposes the CoopInit, a simple yet effective cooperative learning-based initialization strategy that can quickly learn a good starting point for GANs, with a very small computation overhead during training. The proposed algorithm consists of two learning stages: (i) Cooperative initialization stage: The discriminator of GAN is treated as an energy-based model (EBM) and is optimized via maximum likelihood estimation (MLE), with the help of the GAN's generator to provide synthetic data to approximate the learning gradients. The EBM also guides the MLE learning of the generator via MCMC teaching; (ii) Adversarial finalization stage: After a few iterations of initialization, the algorithm seamlessly transits to the regular mini-max adversarial training until convergence. The motivation is that the MLE-based initialization stage drives the model towards mode coverage, which is helpful in alleviating the issue of mode dropping during the adversarial learning stage. We demonstrate the effectiveness of the proposed approach on image generation and one-sided unpaired image-to-image translation tasks through extensive experiments",
    "checked": true,
    "id": "9a5cbcd5701f8a4db18bbfce8182a0d22b840b67",
    "semantic_title": "coopinit: initializing generative adversarial networks via cooperative learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26343": {
    "title": "AutoGraph: Optimizing DNN Computation Graph for Parallel GPU Kernel Execution",
    "volume": "main",
    "abstract": "Deep learning frameworks optimize the computation graphs and intra-operator computations to boost the inference performance on GPUs, while inter-operator parallelism is usually ignored. In this paper, a unified framework, AutoGraph, is proposed to obtain highly optimized computation graphs in favor of parallel executions of GPU kernels. A novel dynamic programming algorithm, combined with backtracking search, is adopted to explore the optimal graph optimization solution, with the fast performance estimation from the mixed critical path cost. Accurate runtime information based on GPU Multi-Stream launched with CUDA Graph is utilized to determine the convergence of the optimization. Experimental results demonstrate that our method achieves up to 3.47x speedup over existing graph optimization methods. Moreover, AutoGraph outperforms state-of-the-art parallel kernel launch frameworks by up to 1.26x",
    "checked": true,
    "id": "795a7d22873640b0cffcf9632bfa3ae00dd8566e",
    "semantic_title": "autograph: optimizing dnn computation graph for parallel gpu kernel execution",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26344": {
    "title": "Fairness and Explainability: Bridging the Gap towards Fair Model Explanations",
    "volume": "main",
    "abstract": "While machine learning models have achieved unprecedented success in real-world applications, they might make biased/unfair decisions for specific demographic groups and hence result in discriminative outcomes. Although research efforts have been devoted to measuring and mitigating bias, they mainly study bias from the result-oriented perspective while neglecting the bias encoded in the decision-making procedure. This results in their inability to capture procedure-oriented bias, which therefore limits the ability to have a fully debiasing method. Fortunately, with the rapid development of explainable machine learning, explanations for predictions are now available to gain insights into the procedure. In this work, we bridge the gap between fairness and explainability by presenting a novel perspective of procedure-oriented fairness based on explanations. We identify the procedure-based bias by measuring the gap of explanation quality between different groups with Ratio-based and Value-based Explanation Fairness. The new metrics further motivate us to design an optimization objective to mitigate the procedure-based bias where we observe that it will also mitigate bias from the prediction. Based on our designed optimization objective, we propose a Comprehensive Fairness Algorithm (CFA), which simultaneously fulfills multiple objectives - improving traditional fairness, satisfying explanation fairness, and maintaining the utility performance. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed CFA and highlight the importance of considering fairness from the explainability perspective. Our code: https://github.com/YuyingZhao/FairExplanations-CFA",
    "checked": true,
    "id": "96fb5cf9a971a956a8ebf52a4c9e816cb458bdb6",
    "semantic_title": "fairness and explainability: bridging the gap towards fair model explanations",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26345": {
    "title": "Adaptive Policy Learning for Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provide two detailed algorithms for implementing the framework through embedding value or policy-based RL algorithms into it. Finally, we conduct extensive experiments on popular continuous control tasks, and results show that our algorithm can learn the expert policy with high sample efficiency even when the quality of offline dataset is poor, e.g., random dataset",
    "checked": true,
    "id": "f6274e6ba614b46d6283c775cfe4565e8ce50bc8",
    "semantic_title": "adaptive policy learning for offline-to-online reinforcement learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26346": {
    "title": "Multi-Level Confidence Learning for Trustworthy Multimodal Classification",
    "volume": "main",
    "abstract": "With the rapid development of various data acquisition technologies, more and more multimodal data come into being. It is important to integrate different modalities which are with high-dimensional features for boosting final multimodal data classification task. However, existing multimodal classification methods mainly focus on exploiting the complementary information of different modalities, while ignoring the learning confidence during information fusion. In this paper, we propose a trustworthy multimodal classification network via multi-level confidence learning, referred to as MLCLNet. Considering that a large number of feature dimensions could not contribute to final classification performance but disturb the discriminability of different samples, we propose a feature confidence learning mechanism to suppress some redundant features, as well as enhancing the expression of discriminative feature dimensions in each modality. In order to capture the inherent sample structure information implied in each modality, we design a graph convolutional network branch to learn the corresponding structure preserved feature representation and generate modal-specific initial classification labels. Since samples from different modalities should share consistent labels, a cross-modal label fusion module is deployed to capture the label correlations of different modalities. In addition, motivated the ideally orthogonality of final fused label matrix, we design a label confidence loss to supervise the network for learning more separable data representations. To the best of our knowledge, MLCLNet is the first work which integrates both feature and label-level confidence learning for multimodal classification. Extensive experiments on four multimodal medical datasets are conducted to validate superior performance of MLCLNet when compared to other state-of-the-art methods",
    "checked": true,
    "id": "70d5e6d63ab786d8d50fa4de096613943d18feb6",
    "semantic_title": "multi-level confidence learning for trustworthy multimodal classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26347": {
    "title": "CowClip: Reducing CTR Prediction Model Training Time from 12 Hours to 10 Minutes on 1 GPU",
    "volume": "main",
    "abstract": "The click-through rate (CTR) prediction task is to predict whether a user will click on the recommended item. As mind-boggling amounts of data are produced online daily, accelerating CTR prediction model training is critical to ensuring an up-to-date model and reducing the training cost. One approach to increase the training speed is to apply large batch training. However, as shown in computer vision and natural language processing tasks, training with a large batch easily suffers from the loss of accuracy. Our experiments show that previous scaling rules fail in the training of CTR prediction neural networks. To tackle this problem, we first theoretically show that different frequencies of ids make it challenging to scale hyperparameters when scaling the batch size. To stabilize the training process in a large batch size setting, we develop the adaptive Column-wise Clipping (CowClip). It enables an easy and effective scaling rule for the embeddings, which keeps the learning rate unchanged and scales the L2 loss. We conduct extensive experiments with four CTR prediction networks on two real-world datasets and successfully scaled 128 times the original batch size without accuracy loss. In particular, for CTR prediction model DeepFM training on the Criteo dataset, our optimization framework enlarges the batch size from 1K to 128K with over 0.1% AUC improvement and reduces training time from 12 hours to 10 minutes on a single V100 GPU. Our code locates at github.com/bytedance/LargeBatchCTR",
    "checked": true,
    "id": "3ca2ba62e130c104b2c01ea496af6f690474a1db",
    "semantic_title": "cowclip: reducing ctr prediction model training time from 12 hours to 10 minutes on 1 gpu",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26348": {
    "title": "Data Imputation with Iterative Graph Reconstruction",
    "volume": "main",
    "abstract": "Effective data imputation demands rich latent ``structure\" discovery capabilities from ``plain\" tabular data. Recent advances in graph neural networks-based data imputation solutions show their structure learning potentials by translating tabular data as bipartite graphs. However, due to a lack of relations between samples, they treat all samples equally which is against one important observation: ``similar sample should give more information about missing values.\" This paper presents a novel Iterative graph Generation and Reconstruction framework for Missing data imputation(IGRM). Instead of treating all samples equally, we introduce the concept: ``friend networks\" to represent different relations among samples. To generate an accurate friend network with missing data, an end-to-end friend network reconstruction solution is designed to allow for continuous friend network optimization during imputation learning. The representation of the optimized friend network, in turn, is used to further optimize the data imputation process with differentiated message passing. Experiment results on eight benchmark datasets show that IGRM yields 39.13% lower mean absolute error compared with nine baselines and 9.04% lower than the second-best. Our code is available at https://github.com/G-AILab/IGRM",
    "checked": true,
    "id": "fcd957f1437d03bc419522d67a236696574cd81a",
    "semantic_title": "data imputation with iterative graph reconstruction",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26349": {
    "title": "Does It Pay to Optimize AUC?",
    "volume": "main",
    "abstract": "The Area Under the ROC Curve (AUC) is an important model metric for evaluating binary classifiers, and many algorithms have been proposed to optimize AUC approximately. It raises the question of whether the generally insignificant gains observed by previous studies are due to inherent limitations of the metric or the inadequate quality of optimization. To better understand the value of optimizing for AUC, we present an efficient algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier in R2, which runs in O(n+n- log n+n-) where n+ and n- are the number of positive and negative samples respectively. Furthermore, it can be naturally extended to Rd in O(n+n-d-1 log (n+n-)) by recursively calling AUC-opt in lower-dimensional spaces. We prove the problem is NP-complete when d is not fixed, reducing from the open hemisphere problem. Compared with other methods, experiments show that AUC-opt achieves statistically significant improvements between 17 to 40 in R2 and 4 to 42 in R3 of 50 t-SNE training datasets. However, generally, the gain proves insignificant on most testing datasets compared to the best standard classifiers. Similar observations are found for nonlinear AUC methods under real-world datasets",
    "checked": true,
    "id": "e903c91c38bd3bdd5e5b3455575dfc9a4b8c8cc5",
    "semantic_title": "does it pay to optimize auc?",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26350": {
    "title": "SLOTH: Structured Learning and Task-Based Optimization for Time Series Forecasting on Hierarchies",
    "volume": "main",
    "abstract": "Multivariate time series forecasting with hierarchical structure is widely used in real-world applications, e.g., sales predictions for the geographical hierarchy formed by cities, states, and countries. The hierarchical time series (HTS) forecasting includes two sub-tasks, i.e., forecasting and reconciliation. In the previous works, hierarchical information is only integrated in the reconciliation step to maintain coherency, but not in forecasting step for accuracy improvement. In this paper, we propose two novel tree-based feature integration mechanisms, i.e., top-down convolution and bottom-up attention to leverage the information of the hierarchical structure to improve the forecasting performance. Moreover, unlike most previous reconciliation methods which either rely on strong assumptions or focus on coherent constraints only, we utilize deep neural optimization networks, which not only achieve coherency without any assumptions, but also allow more flexible and realistic constraints to achieve task-based targets, e.g., lower under-estimation penalty and meaningful decision-making loss to facilitate the subsequent downstream tasks. Experiments on real-world datasets demonstrate that our tree-based feature integration mechanism achieves superior performances on hierarchical forecasting tasks compared to the state-of-the-art methods, and our neural optimization networks can be applied to real-world tasks effectively without any additional effort under coherence and task-based constraints",
    "checked": true,
    "id": "6b180f97412efe688d335f409a6589c1de6675a5",
    "semantic_title": "sloth: structured learning and task-based optimization for time series forecasting on hierarchies",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26351": {
    "title": "Robust Temporal Smoothness in Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-task learning models based on temporal smoothness assumption, in which each time point of a sequence of time points concerns a task of prediction, assume the adjacent tasks are similar to each other. However, the effect of outliers is not taken into account. In this paper, we show that even only one outlier task will destroy the performance of the entire model. To solve this problem, we propose two Robust Temporal Smoothness (RoTS) frameworks. Compared with the existing models based on temporal relation, our methods not only chase the temporal smoothness information but identify outlier tasks, however, without increasing the computational complexity. Detailed theoretical analyses are presented to evaluate the performance of our methods. Experimental results on synthetic and real-life datasets demonstrate the effectiveness of our frameworks. We also discuss several potential specific applications and extensions of our RoTS frameworks",
    "checked": true,
    "id": "7090f46777a0f35938825cb28298006e8647b308",
    "semantic_title": "robust temporal smoothness in multi-task learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26352": {
    "title": "Combining Adversaries with Anti-adversaries in Training",
    "volume": "main",
    "abstract": "Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied bounds on each training sample is presented. Meta learning is utilized to optimize the combination weights. Experiments on benchmark datasets under different learning scenarios verify our theoretical findings and the effectiveness of the proposed methodology",
    "checked": true,
    "id": "2810e9602d87f132e2b2f3eda6539d88d5330ec6",
    "semantic_title": "combining adversaries with anti-adversaries in training",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26353": {
    "title": "Gradient-Adaptive Pareto Optimization for Constrained Reinforcement Learning",
    "volume": "main",
    "abstract": "Constrained Reinforcement Learning (CRL) burgeons broad interest in recent years, which pursues maximizing long-term returns while constraining costs. Although CRL can be cast as a multi-objective optimization problem, it is still facing the key challenge that gradient-based Pareto optimization methods tend to stick to known Pareto-optimal solutions even when they yield poor returns (e.g., the safest self-driving car that never moves) or violate the constraints (e.g., the record-breaking racer that crashes the car). In this paper, we propose Gradient-adaptive Constrained Policy Optimization (GCPO for short), a novel Pareto optimization method for CRL with two adaptive gradient recalibration techniques. First, to find Pareto-optimal solutions with balanced performance over all targets, we propose gradient rebalancing which forces the agent to improve more on under-optimized objectives at every policy iteration. Second, to guarantee that the cost constraints are satisfied, we propose gradient perturbation that can temporarily sacrifice the returns for costs. Experiments on the SafetyGym benchmarks show that our method consistently outperforms previous CRL methods in reward while satisfying the constraints",
    "checked": true,
    "id": "1eb5f9b321e66f0c01184e675676e329c5d06349",
    "semantic_title": "gradient-adaptive pareto optimization for constrained reinforcement learning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26354": {
    "title": "Quantized Feature Distillation for Network Quantization",
    "volume": "main",
    "abstract": "Neural network quantization aims to accelerate and trim full-precision neural network models by using low bit approximations. Methods adopting the quantization aware training (QAT) paradigm have recently seen a rapid growth, but are often conceptually complicated. This paper proposes a novel and highly effective QAT method, quantized feature distillation (QFD). QFD first trains a quantized (or binarized) representation as the teacher, then quantize the network using knowledge distillation (KD). Quantitative results show that QFD is more flexible and effective (i.e., quantization friendly) than previous quantization methods. QFD surpasses existing methods by a noticeable margin on not only image classification but also object detection, albeit being much simpler. Furthermore, QFD quantizes ViT and Swin-Transformer on MS-COCO detection and segmentation, which verifies its potential in real world deployment. To the best of our knowledge, this is the first time that vision transformers have been quantized in object detection and image segmentation tasks",
    "checked": true,
    "id": "6dcdac10bbc7364740178dcd10c5b05906fe13a3",
    "semantic_title": "quantized feature distillation for network quantization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26355": {
    "title": "Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Recent advances in large pre-trained models showed promising results in few-shot learning. However, their generalization ability on two-dimensional Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift, has not been thoroughly investigated. Researches have shown that even with a significant amount of training data, few methods can achieve better performance than the standard empirical risk minimization method (ERM) in OoD generalization. This few-shot OoD generalization dilemma emerges as a challenging direction in deep neural network generalization research, where the performance suffers from overfitting on few-shot examples and OoD generalization errors. In this paper, leveraging a broader supervision source, we explore a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue. Specifically, the model is designed as only text representations are fine-tuned via a Bayesian modelling approach with gradient orthogonalization loss and invariant risk minimization (IRM) loss. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed during training and improve generalization to broader unseen classes. The dedicated loss is introduced to achieve better image-text alignment by disentangling the causal and non-casual parts of image features. Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD generalization performances on two-dimensional distribution shifts. Moreover, compared with CLIP-like models, Bayes-CAL yields more stable generalization performances on unseen classes. Our code is available at https://github.com/LinLLLL/BayesCAL",
    "checked": true,
    "id": "4d8c20a0513e51b1a5064c61e72611583352d4a2",
    "semantic_title": "bayesian cross-modal alignment learning for few-shot out-of-distribution generalization",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26356": {
    "title": "ContraFeat: Contrasting Deep Features for Semantic Discovery",
    "volume": "main",
    "abstract": "StyleGAN has shown strong potential for disentangled semantic control, thanks to its special design of multi-layer intermediate latent variables. However, existing semantic discovery methods on StyleGAN rely on manual selection of modified latent layers to obtain satisfactory manipulation results, which is tedious and demanding. In this paper, we propose a model that automates this process and achieves state-of-the-art semantic discovery performance. The model consists of an attention-equipped navigator module and losses contrasting deep-feature changes. We propose two model variants, with one contrasting samples in a binary manner, and another one contrasting samples with learned prototype variation patterns. The proposed losses are computed with pretrained deep features, based on our assumption that the features implicitly possess the desired semantic variation structure including consistency and orthogonality. Additionally, we design two metrics to quantitatively evaluate the performance of semantic discovery methods on FFHQ dataset, and also show that disentangled representations can be derived via a simple training process. Experimentally, we show that our models achieve state-of-the-art semantic discovery results without relying on layer-wise manual selection, and these discovered semantics can be used to manipulate real-world images",
    "checked": true,
    "id": "7224248d35e0a877a1e5bae1d9664fd9e6cc0045",
    "semantic_title": "contrafeat: contrasting deep features for semantic discovery",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26357": {
    "title": "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA",
    "volume": "main",
    "abstract": "In this paper, we propose a novel multi-modal framework for Scene Text Visual Question Answering (STVQA), which requires models to read scene text in images for question answering. Apart from text or visual objects, which could exist independently, scene text naturally links text and visual modalities together by conveying linguistic semantics while being a visual object in an image simultaneously. Different to conventional STVQA models which take the linguistic semantics and visual semantics in scene text as two separate features, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG), which explicitly unifies this two semantics with the spatial bounding box as a bridge connecting them. Specifically, at first, LTG locates the region in an image that may contain the answer words with an answer location module (ALM) consisting of a region proposal network and a language refinement network, both of which can transform to each other with one-to-one mapping via the scene text bounding box. Next, given the answer words selected by ALM, LTG generates a readable answer sequence with an answer generation module (AGM) based on a pre-trained language model. As a benefit of the explicit alignment of the visual and linguistic semantics, even without any scene text based pre-training tasks, LTG can boost the absolute accuracy by +6.06% and +6.92% on the TextVQA dataset and the ST-VQA dataset respectively, compared with a non-pre-training baseline. We further demonstrate that LTG effectively unifies visual and text modalities through the spatial bounding box connection, which is underappreciated in previous methods",
    "checked": true,
    "id": "dd67c3185aa94f089f048f654c3067ad4bb4a56d",
    "semantic_title": "locate then generate: bridging vision and language with bounding box for scene-text vqa",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26358": {
    "title": "ILSGAN: Independent Layer Synthesis for Unsupervised Foreground-Background Segmentation",
    "volume": "main",
    "abstract": "Unsupervised foreground-background segmentation aims at extracting salient objects from cluttered backgrounds, where Generative Adversarial Network (GAN) approaches, especially layered GANs, show great promise. However, without human annotations, they are typically prone to produce foreground and background layers with non-negligible semantic and visual confusion, dubbed \"information leakage\", resulting in notable degeneration of the generated segmentation mask. To alleviate this issue, we propose a simple-yet-effective explicit layer independence modeling approach, termed Independent Layer Synthesis GAN (ILSGAN), pursuing independent foreground-background layer generation by encouraging their discrepancy. Specifically, it targets minimizing the mutual information between visible and invisible regions of the foreground and background to spur interlayer independence. Through in-depth theoretical and experimental analyses, we justify that explicit layer independence modeling is critical to suppressing information leakage and contributes to impressive segmentation performance gains. Also, our ILSGAN achieves strong state-of-the-art generation quality and segmentation performance on complex real-world data",
    "checked": true,
    "id": "85b392d00feff27ed3249c6653ce2e94aa34aaa2",
    "semantic_title": "ilsgan: independent layer synthesis for unsupervised foreground-background segmentation",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26359": {
    "title": "SVP-T: A Shape-Level Variable-Position Transformer for Multivariate Time Series Classification",
    "volume": "main",
    "abstract": "Multivariate time series classiﬁcation (MTSC), one of the most fundamental time series applications, has not only gained substantial research attentions but has also emerged in many real-life applications. Recently, using transformers to solve MTSC has been reported. However, current transformer-based methods take data points of individual timestamps as inputs (timestamp-level), which only capture the temporal dependencies, not the dependencies among variables. In this paper, we propose a novel method, called SVP-T. Specifically, we ﬁrst propose to take time series subsequences, which can be from different variables and positions (time interval), as the inputs (shape-level). The temporal and variable dependencies are both handled by capturing the long- and short-term dependencies among shapes. Second, we propose a variable-position encoding layer (VP-layer) to utilize both the variable and position information of each shape. Third, we introduce a novel VP-based (Variable-Position) self-attention mechanism to allow the enhancing the attention weights of overlapping shapes. We evaluate our method on all UEA MTS datasets. SVP-T achieves the best accuracy rank when compared with several competitive state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the VP-layer and the VP-based self-attention mechanism. Finally, we present one case study to interpret the result of SVP-T",
    "checked": true,
    "id": "86f260abb52cea53b4dbf3f5c2a5669450983374",
    "semantic_title": "svp-t: a shape-level variable-position transformer for multivariate time series classification",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26360": {
    "title": "Mixed-Variable Black-Box Optimisation Using Value Proposal Trees",
    "volume": "main",
    "abstract": "Many real-world optimisation problems are defined over both categorical and continuous variables, yet efficient optimisation methods such as Bayesian Optimisation (BO) are ill-equipped to handle such mixed-variable search spaces. The optimisation breadth introduced by categorical variables in the mixed-input setting has seen recent approaches operating on local trust regions, but these methods can be greedy in suboptimal regions of the search space. In this paper, we adopt a holistic view and aim to consolidate optimisation of the categorical and continuous sub-spaces under a single acquisition metric. We develop a tree-based method which retains a global view of the optimisation spaces by identifying regions in the search space with high potential candidates which we call value proposals. Our method uses these proposals to make selections on both the categorical and continuous components of the input. We show that this approach significantly outperforms existing mixed-variable optimisation approaches across several mixed-variable black-box optimisation tasks",
    "checked": true,
    "id": "58b128b995b6740b8f5aece574bc1f4024bb7016",
    "semantic_title": "mixed-variable black-box optimisation using value proposal trees",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26361": {
    "title": "Synchronization and Diversity of Solutions",
    "volume": "main",
    "abstract": "A central computational problem in the realm of automata theory is the problem of determining whether a finite automaton A has a synchronizing word. This problem has found applications in a variety of subfields of artificial intelligence, including planning, robotics, and multi-agent systems. In this work, we study this problem within the framework of diversity of solutions, an up-and-coming trend in the field of artificial intelligence where the goal is to compute a set of solutions that are sufficiently distinct from one another. We define a notion of diversity of solutions that is suitable for contexts were solutions are strings that may have distinct lengths. Using our notion of diversity, we show that for each fixed r ∈ N, each fixed finite automaton A, and each finite automaton B given at the input, the problem of determining the existence of a diverse set {w1,w2, . . . ,wr} ⊆ L(B) of words that are synchronizing for A can be solved in polynomial time. Finally, we generalize this result to the realm of conformant planning, where the goal is to devise plans that achieve a goal irrespectively of initial conditions and of nondeterminism that may occur during their execution",
    "checked": true,
    "id": "5cdadbbb6ebcab86003228ca2be7f3386c2d8a66",
    "semantic_title": "synchronization and diversity of solutions",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26362": {
    "title": "The Multi-Agent Transportation Problem",
    "volume": "main",
    "abstract": "We introduce the multi-agent transportation (MAT) problem, where agents have to transport containers from their starting positions to their designated goal positions. Movement takes place in a common environment where collisions between agents and between containers must be avoided. In contrast to other frameworks such as multi-agent pathfinding (MAPF) or multi-agent pickup and delivery (MAPD), the agents are allowed to separate from the containers at any time, which can reduce the makespan and also allows for plans in scenarios that are unsolvable otherwise. We present a complexity analysis establishing the problem's NP-completeness and show how the problem can be reduced to a sequence of SAT problems when optimizing for makespan. A MAT solver is empirically evaluated with regard to varying input characteristics and movement constraints and compared to a MAPD solver that utilizes conflict-based search (CBS)",
    "checked": true,
    "id": "098e2881eb611e46bbf7b579ff6b34ce183f380f",
    "semantic_title": "the multi-agent transportation problem",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26363": {
    "title": "Emergent Quantized Communication",
    "volume": "main",
    "abstract": "The field of emergent communication aims to understand the characteristics of communication as it emerges from artificial agents solving tasks that require information exchange. Communication with discrete messages is considered a desired characteristic, for scientific and applied reasons. However, training a multi-agent system with discrete communication is not straightforward, requiring either reinforcement learning algorithms or relaxing the discreteness requirement via a continuous approximation such as the Gumbel-softmax. Both these solutions result in poor performance compared to fully continuous communication. In this work, we propose an alternative approach to achieve discrete communication -- quantization of communicated message. Using message quantization allows us to train the model end-to-end, achieving superior performance in multiple setups. Moreover, quantization is a natural framework that runs the gamut from continuous to discrete communication. Thus, it sets the ground for a broader view of multi-agent communication in the deep learning era",
    "checked": true,
    "id": "69e364cb69fc8c2dcd1157ecdb8758f3fd04c431",
    "semantic_title": "emergent quantized communication",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26364": {
    "title": "Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning via Polarization Policy Gradient",
    "volume": "main",
    "abstract": "Cooperative multi-agent policy gradient (MAPG) algorithms have recently attracted wide attention and are regarded as a general scheme for the multi-agent system. Credit assignment plays an important role in MAPG and can induce cooperation among multiple agents. However, most MAPG algorithms cannot achieve good credit assignment because of the game-theoretic pathology known as centralized-decentralized mismatch. To address this issue, this paper presents a novel method, Multi-Agent Polarization Policy Gradient (MAPPG). MAPPG takes a simple but efficient polarization function to transform the optimal consistency of joint and individual actions into easily realized constraints, thus enabling efficient credit assignment in MAPPG. Theoretically, we prove that individual policies of MAPPG can converge to the global optimum. Empirically, we evaluate MAPPG on the well-known matrix game and differential game, and verify that MAPPG can converge to the global optimum for both discrete and continuous action spaces. We also evaluate MAPPG on a set of StarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the state-of-the-art MAPG algorithms",
    "checked": true,
    "id": "b8fb3b575c61020537692b4f904381e3c5fe7bfa",
    "semantic_title": "learning explicit credit assignment for cooperative multi-agent reinforcement learning via polarization policy gradient",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26365": {
    "title": "Zero-Shot Assistance in Sequential Decision Problems",
    "volume": "main",
    "abstract": "We consider the problem of creating assistants that can help agents solve new sequential decision problems, assuming the agent is not able to specify the reward function explicitly to the assistant. Instead of acting in place of the agent as in current automation-based approaches, we give the assistant an advisory role and keep the agent in the loop as the main decision maker. The difficulty is that we must account for potential biases of the agent which may cause it to seemingly irrationally reject advice. To do this we introduce a novel formalization of assistance that models these biases, allowing the assistant to infer and adapt to them. We then introduce a new method for planning the assistant's actions which can scale to large decision making problems. We show experimentally that our approach adapts to these agent biases, and results in higher cumulative reward for the agent than automation-based alternatives. Lastly, we show that an approach combining advice and automation outperforms advice alone at the cost of losing some safety guarantees",
    "checked": true,
    "id": "c6a733eb2dad71c8d022f5d9be81854d198ac11a",
    "semantic_title": "zero-shot assistance in sequential decision problems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26366": {
    "title": "Multi-Unit Auctions for Allocating Chance-Constrained Resources",
    "volume": "main",
    "abstract": "Sharing scarce resources is a key challenge in multi-agent interaction, especially when individual agents are uncertain about their future consumption. We present a new auction mechanism for preallocating multi-unit resources among agents, while limiting the chance of resource violations. By planning for a chance constraint, we strike a balance between worst-case approaches, which under-utilise resources, and expected-case approaches, which lack formal guarantees. We also present an algorithm that allows agents to generate bids via multi-objective reasoning, which are then submitted to the auction. We then discuss how the auction can be extended to non-cooperative scenarios. Finally, we demonstrate empirically that our auction outperforms state-of-the-art techniques for chance-constrained multi-agent resource allocation in complex settings with up to hundreds of agents",
    "checked": true,
    "id": "62055214297cf0ab16f2382b2b79314dc7e96316",
    "semantic_title": "multi-unit auctions for allocating chance-constrained resources",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26367": {
    "title": "Reward-Based Negotiating Agent Strategies",
    "volume": "main",
    "abstract": "This study proposed a novel reward-based negotiating agent strategy using an issue-based represented deep policy network. We compared the negotiation strategies with reinforcement learning (RL) by the tournaments toward heuristics-based champion agents in multi-issue negotiation. A bilateral multi-issue negotiation in which the two agents exchange offers in turn was considered. Existing RL architectures for a negotiation strategy incorporate rich utility function that provides concrete information even though the rewards of RL are considered as generalized signals in practice. Additionally, in existing reinforcement learning architectures for negotiation strategies, both the issue-based representations of the negotiation problems and the policy network to improve the scalability of negotiation domains are yet to be considered. This study proposed a novel reward-based negotiation strategy through deep RL by considering an issue-based represented deep policy network for multi-issue negotiation. Comparative studies analyzed the significant properties of negotiation strategies with RL. The results revealed that the policy-based learning agents with issue-based representations achieved comparable or higher utility than the state-of-the-art baselines with RL and heuristics, especially in the large-sized domains. Additionally, negotiation strategies with RL based on the policy network can achieve agreements by effectively using each step",
    "checked": true,
    "id": "175474e2b044b2637cb904698a4fe497e4a4bd3b",
    "semantic_title": "reward-based negotiating agent strategies",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26368": {
    "title": "Intersection Coordination with Priority-Based Search for Autonomous Vehicles",
    "volume": "main",
    "abstract": "The development of connected and autonomous vehicles opens an opportunity to manage intersections without signals. One promising approach is to use a central autonomous intersection manager to optimize the movement of the vehicles in the intersection. Existing work uses Mixed Integer Linear Programming (MILP) to find optimal solutions for this problem but is time-consuming and cannot be applied in real-time. On the other hand, the coordination of the vehicles is essentially a Multi-Agent Path Finding (MAPF) problem, for which dozens of efficient algorithms have been proposed in recent years. Inspired by these MAPF algorithms, we propose a three-level algorithm called PSL to solve the intersection coordination problem. Theoretically, PSL is complete and polynomial-time in the number of vehicles. Empirically, PSL runs significantly faster with only a slight compromise in the solution quality than the optimal MILP method. It also generates significantly better solutions with a slightly larger runtime than the traditional First-Come-First-Served strategy",
    "checked": true,
    "id": "384a7cb9b19a8a7bb79be8582ab4f351a8cb69f5",
    "semantic_title": "intersection coordination with priority-based search for autonomous vehicles",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26369": {
    "title": "Solving Large-Scale Pursuit-Evasion Games Using Pre-trained Strategies",
    "volume": "main",
    "abstract": "Pursuit-evasion games on graphs model the coordination of police forces chasing a fleeing felon in real-world urban settings, using the standard framework of imperfect-information extensive-form games (EFGs). In recent years, solving EFGs has been largely dominated by the Policy-Space Response Oracle (PSRO) methods due to their modularity, scalability, and favorable convergence properties. However, even these methods quickly reach their limits when facing large combinatorial strategy spaces of the pursuit-evasion games. To improve their efficiency, we integrate the pre-training and fine-tuning paradigm into the core module of PSRO -- the repeated computation of the best response. First, we pre-train the pursuer's policy base model against many different strategies of the evader. Then we proceed with the PSRO loop and fine-tune the pre-trained policy to attain the pursuer's best responses. The empirical evaluation shows that our approach significantly outperforms the baselines in terms of speed and scalability, and can solve even games on street maps of megalopolises with tens of thousands of crossroads -- a scale beyond the effective reach of previous methods",
    "checked": true,
    "id": "6b4b4a321e64dd7c4d7ae48676657f42da29b2cf",
    "semantic_title": "solving large-scale pursuit-evasion games using pre-trained strategies",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26370": {
    "title": "Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition",
    "volume": "main",
    "abstract": "Value Decomposition (VD) aims to deduce the contributions of agents for decentralized policies in the presence of only global rewards, and has recently emerged as a powerful credit assignment paradigm for tackling cooperative Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges in VD is to promote diverse behaviors among agents, while existing methods directly encourage the diversity of learned agent networks with various strategies. However, we argue that these dedicated designs for agent networks are still limited by the indistinguishable VD network, leading to homogeneous agent behaviors and thus downgrading the cooperation capability. In this paper, we propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly boosting the credit-level distinguishability of the VD network to break the bottleneck of multi-agent diversity. Specifically, our approach leverages contrastive learning to maximize the mutual information between the temporal credits and identity representations of different agents, encouraging the full expressiveness of credit assignment and further the emergence of individualities. The algorithm implementation of the proposed CIA module is simple yet effective that can be readily incorporated into various VD architectures. Experiments on the SMAC benchmarks and across different VD backbones demonstrate that the proposed method yields results superior to the state-of-the-art counterparts. Our code is available at https://github.com/liushunyu/CIA",
    "checked": true,
    "id": "e72c1c09c7992e7ffaeb652693955eadad522705",
    "semantic_title": "contrastive identity-aware learning for multi-agent value decomposition",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26371": {
    "title": "Learning to Shape Rewards Using a Game of Two Partners",
    "volume": "main",
    "abstract": "Reward shaping (RS) is a powerful method in reinforcement learning (RL) for overcoming the problem of sparse or uninformative rewards. However, RS typically relies on manually engineered shaping-reward functions whose construc- tion is time-consuming and error-prone. It also requires domain knowledge which runs contrary to the goal of autonomous learning. We introduce Reinforcement Learning Optimising Shaping Algorithm (ROSA), an automated reward shaping framework in which the shaping-reward function is constructed in a Markov game between two agents. A reward-shaping agent (Shaper) uses switching controls to determine which states to add shaping rewards for more efficient learning while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. We prove that ROSA, which adopts existing RL algorithms, learns to construct a shaping-reward function that is beneficial to the task thus ensuring efficient convergence to high performance policies. We demonstrate ROSA's properties in three didactic experiments and show its superior performance against state-of-the-art RS algorithms in challenging sparse reward environments",
    "checked": true,
    "id": "7c1ed780d2dd7d780c5ff6371d4d14c36c357e3c",
    "semantic_title": "learning to shape rewards using a game of two partners",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26372": {
    "title": "Reconstructing an Epidemic Outbreak Using Steiner Connectivity",
    "volume": "main",
    "abstract": "Only a subset of infections is actually observed in an outbreak, due to multiple reasons such as asymptomatic cases and under-reporting. Therefore, reconstructing an epidemic cascade given some observed cases is an important step in responding to such an outbreak. A maximum likelihood solution to this problem ( referred to as CascadeMLE ) can be shown to be a variation of the classical Steiner subgraph problem, which connects a subset of observed infections. In contrast to prior works on epidemic reconstruction, which consider the standard Steiner tree objective, we show that a solution to CascadeMLE, based on the actual MLE objective, has a very different structure. We design a logarithmic approximation algorithm for CascadeMLE, and evaluate it on multiple synthetic and social contact networks, including a contact network constructed for a hospital. Our algorithm has significantly better performance compared to a prior baseline",
    "checked": true,
    "id": "57872bf9a7fa8016cf88372f475627fbfef71c3c",
    "semantic_title": "reconstructing an epidemic outbreak using steiner connectivity",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26373": {
    "title": "Formal Verification of Bayesian Mechanisms",
    "volume": "main",
    "abstract": "In this paper, for the first time, we study the formal verification of Bayesian mechanisms through strategic reasoning. We rely on the framework of Probabilistic Strategy Logic (PSL), which is well-suited for representing and verifying multi-agent systems with incomplete information. We take advantage of the recent results on the decidability of PSL model checking under memoryless strategies, and reduce the problem of formally verifying Bayesian mechanisms to PSL model checking. We show how to encode Bayesian-Nash equilibrium and economical properties, and illustrate our approach with different kinds of mechanisms",
    "checked": true,
    "id": "70c3332847d04e906ca793ce19fa1cf96717c703",
    "semantic_title": "formal verification of bayesian mechanisms",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26374": {
    "title": "Memory-Augmented Theory of Mind Network",
    "volume": "main",
    "abstract": "Social reasoning necessitates the capacity of theory of mind (ToM), the ability to contextualise and attribute mental states to others without having access to their internal cognitive structure. Recent machine learning approaches to ToM have demonstrated that we can train the observer to read the past and present behaviours of other agents and infer their beliefs (including false beliefs about things that no longer exist), goals, intentions and future actions. The challenges arise when the behavioural space is complex, demanding skilful space navigation for rapidly changing contexts for an extended period. We tackle the challenges by equipping the observer with novel neural memory mechanisms to encode, and hierarchical attention to selectively retrieve information about others. The memories allow rapid, selective querying of distal related past behaviours of others to deliberatively reason about their current mental state, beliefs and future behaviours. This results in ToMMY, a theory of mind model that learns to reason while making little assumptions about the underlying mental processes. We also construct a new suite of experiments to demonstrate that memories facilitate the learning process and achieve better theory of mind performance, especially for high-demand false-belief tasks that require inferring through multiple steps of changes",
    "checked": true,
    "id": "9a40c7a12dfd42719a778b0c7a1ee4ede65aa197",
    "semantic_title": "memory-augmented theory of mind network",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26375": {
    "title": "Socially Optimal Non-discriminatory Restrictions for Continuous-Action Games",
    "volume": "main",
    "abstract": "We address the following mechanism design problem: Given a multi-player Normal-Form Game (NFG) with a continuous action space, find a non-discriminatory (i.e., identical for all players) restriction of the action space which maximizes the resulting Nash Equilibrium with respect to a fixed social utility function. First, we propose a formal model of a Restricted Game and the corresponding restriction optimization problem. We then present an algorithm to find optimal non-discriminatory restrictions under some assumptions. Our experimental results with Braess' Paradox and the Cournot Game show that this method leads to an optimized social utility of the Nash Equilibria, even when the assumptions are not guaranteed to hold. Finally, we outline a generalization of our approach to the much wider scope of Stochastic Games",
    "checked": true,
    "id": "214389422b1f67fe5e545ff9a0c0ae1004188cc6",
    "semantic_title": "socially optimal non-discriminatory restrictions for continuous-action games",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26376": {
    "title": "Fault-Tolerant Offline Multi-Agent Path Planning",
    "volume": "main",
    "abstract": "We study a novel graph path planning problem for multiple agents that may crash at runtime, and block part of the workspace. In our setting, agents can detect neighboring crashed agents, and change followed paths at runtime. The objective is then to prepare a set of paths and switching rules for each agent, ensuring that all correct agents reach their destinations without collisions or deadlocks, despite unforeseen crashes of other agents. Such planning is attractive to build reliable multi-robot systems. We present problem formalization, theoretical analysis such as computational complexities, and how to solve this offline planning problem",
    "checked": true,
    "id": "46e7c11f2503e7280a8557b3549dd7018dc61a3e",
    "semantic_title": "fault-tolerant offline multi-agent path planning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26377": {
    "title": "LaCAM: Search-Based Algorithm for Quick Multi-Agent Pathfinding",
    "volume": "main",
    "abstract": "We propose a novel complete algorithm for multi-agent pathfinding (MAPF) called lazy constraints addition search for MAPF (LaCAM). MAPF is a problem of finding collision-free paths for multiple agents on graphs and is the foundation of multi-robot coordination. LaCAM uses a two-level search to find solutions quickly, even with hundreds of agents or more. At the low-level, it searches constraints about agents' locations. At the high-level, it searches a sequence of all agents' locations, following the constraints specified by the low-level. Our exhaustive experiments reveal that LaCAM is comparable to or outperforms state-of-the-art sub-optimal MAPF algorithms in a variety of scenarios, regarding success rate, planning time, and solution quality of sum-of-costs",
    "checked": true,
    "id": "62abe88c56d04bff38e90bbc81409b2594f77efd",
    "semantic_title": "lacam: search-based algorithm for quick multi-agent pathfinding",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26378": {
    "title": "Networked Anti-coordination Games Meet Graphical Dynamical Systems: Equilibria and Convergence",
    "volume": "main",
    "abstract": "Evolutionary anti-coordination games on networks capture real-world strategic situations such as traffic routing and market competition. Two key problems concerning evolutionary games are the existence of a pure Nash equilibrium (NE) and the convergence time. In this work, we study these two problems for anti-coordination games under sequential and synchronous update schemes. For each update scheme, we examine two decision modes based on whether an agent considers its own previous action (self essential) or not (self non-essential) in choosing its next action. Using a relationship between games and dynamical systems, we show that for both update schemes, finding an NE can be done efficiently under the self non-essential mode but is computationally intractable under the self essential mode. We then identify special cases for which an NE can be obtained efficiently. For convergence time, we show that the dynamics converges in a polynomial number of steps under the synchronous scheme; for the sequential scheme, the convergence time is polynomial only under the self non-essential mode. Through experiments, we empirically examine the convergence time and the equilibria for both synthetic and real-world networks",
    "checked": true,
    "id": "154e6c775a798de03d251a5e2b4791a8d65798a4",
    "semantic_title": "networked anti-coordination games meet graphical dynamical systems: equilibria and convergence",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26379": {
    "title": "Learning from Good Trajectories in Offline Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline multi-agent reinforcement learning (MARL) aims to learn effective multi-agent policies from pre-collected datasets, which is an important step toward the deployment of multi-agent systems in real-world applications. However, in practice, each individual behavior policy that generates multi-agent joint trajectories usually has a different level of how well it performs. e.g., an agent is a random policy while other agents are medium policies. In the cooperative game with global reward, one agent learned by existing offline MARL often inherits this random policy, jeopardizing the utility of the entire team. In this paper, we investigate offline MARL with explicit consideration on the diversity of agent-wise trajectories and propose a novel framework called Shared Individual Trajectories (SIT) to address this problem. Specifically, an attention-based reward decomposition network assigns the credit to each agent through a differentiable key-value memory mechanism in an offline manner. These decomposed credits are then used to reconstruct the joint offline datasets into prioritized experience replay with individual trajectories, thereafter agents can share their good trajectories and conservatively train their policies with a graph attention network (GAT) based critic. We evaluate our method in both discrete control (i.e., StarCraft II and multi-agent particle environment) and continuous control (i.e., multi-agent mujoco). The results indicate that our method achieves significantly better results in complex and mixed offline multi-agent datasets, especially when the difference of data quality between individual trajectories is large",
    "checked": true,
    "id": "5f91fe06f87bb21350d0e568548068759cbdc563",
    "semantic_title": "learning from good trajectories in offline multi-agent reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26380": {
    "title": "Resource Sharing through Multi-Round Matchings",
    "volume": "main",
    "abstract": "Applications such as employees sharing office spaces over a workweek can be modeled as problems where agents are matched to resources over multiple rounds. Agents' requirements limit the set of compatible resources and the rounds in which they want to be matched. Viewing such an application as a multi-round matching problem on a bipartite compatibility graph between agents and resources, we show that a solution (i.e., a set of matchings, with one matching per round) can be found efficiently if one exists. To cope with situations where a solution does not exist, we consider two extensions. In the first extension, a benefit function is defined for each agent and the objective is to find a multi-round matching to maximize the total benefit. For a general class of benefit functions satisfying certain properties (including diminishing returns), we show that this multi-round matching problem is efficiently solvable. This class includes utilitarian and Rawlsian welfare functions. For another benefit function, we show that the maximization problem is NP-hard. In the second extension, the objective is to generate advice to each agent (i.e., a subset of requirements to be relaxed) subject to a budget constraint so that the agent can be matched. We show that this budget-constrained advice generation problem is NP-hard. For this problem, we develop an integer linear programming formulation as well as a heuristic based on local search. We experimentally evaluate our algorithms on synthetic networks and apply them to two real-world situations: shared office spaces and matching courses to classrooms",
    "checked": true,
    "id": "62f1d21985a64eb5a169909cd0778b88c8b1d539",
    "semantic_title": "resource sharing through multi-round matchings",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26381": {
    "title": "Effective Integration of Weighted Cost-to-Go and Conflict Heuristic within Suboptimal CBS",
    "volume": "main",
    "abstract": "Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF) solver that employs a low-level single agent planner and a high-level constraint tree to resolve conflicts. The vast majority of modern MAPF solvers focus on improving CBS by reducing the size of this tree through various strategies with few methods modifying the low level planner. Typically low level planners in existing CBS methods use an unweighted cost-to-go heuristic, with suboptimal CBS methods also using a conflict heuristic to help the high level search. In this paper, we show that, contrary to prevailing CBS beliefs, a weighted cost-to-go heuristic can be used effectively alongside the conflict heuristic in two possible variants. In particular, one of these variants can obtain large speedups, 2-100x, across several scenarios and suboptimal CBS methods. Importantly, we discover that performance is related not to the weighted cost-to-go heuristic but rather to the relative conflict heuristic weight's ability to effectively balance low-level and high-level work. Additionally, to the best of our knowledge, we show the first theoretical relation of prioritized planning and bounded suboptimal CBS and demonstrate that our methods are their natural generalization",
    "checked": true,
    "id": "0e39f92ad49eef88dfd0e3b044f3404a03f7c402",
    "semantic_title": "effective integration of weighted cost-to-go and conflict heuristic within suboptimal cbs",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26382": {
    "title": "DM²: Decentralized Multi-Agent Reinforcement Learning via Distribution Matching",
    "volume": "main",
    "abstract": "Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical algorithm (DM^2), in which each individual agent matches a target distribution derived from concurrently sampled trajectories from a joint expert policy. Experimental validation on the StarCraft domain shows that combining (1) a task reward, and (2) a distribution matching reward for expert demonstrations for the same task, allows agents to outperform a naive distributed baseline. Additional experiments probe the conditions under which expert demonstrations need to be sampled to obtain the learning benefits",
    "checked": true,
    "id": "296e8f5c9c96ec07e30bfb04203835c78bf42a01",
    "semantic_title": "dm²: decentralized multi-agent reinforcement learning via distribution matching",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26383": {
    "title": "Emergence of Punishment in Social Dilemma with Environmental Feedback",
    "volume": "main",
    "abstract": "Altruistic punishment (or punishment) has been extensively shown as an important mechanism for promoting cooperation in human societies. In AI, the emergence of punishment has received much recent interest. In this paper, we contribute with a novel evolutionary game theoretic model to study the impacts of environmental feedback. Whereas a population of agents plays public goods games, there exists a third-party population whose payoffs depend not only on whether to punish or not, but also on the state of the environment (e.g., how cooperative the agents in a social dilemma are). Focusing on one-shot public goods games, we show that environmental feedback, by itself, can lead to the emergence of punishment. We analyze the co-evolution of punishment and cooperation, and derive conditions for their co-presence, co-dominance and co-extinction. Moreover, we show that the system can exhibit bistability as well as cyclic dynamics. Our findings provide a new explanation for the emergence of punishment. On the other hand, our results also alert the need for careful design of implementing punishment in multi-agent systems, as the resulting evolutionary dynamics can be somewhat complex",
    "checked": true,
    "id": "e1a0393771940b80a06fadf6e6aa763d9382b50d",
    "semantic_title": "emergence of punishment in social dilemma with environmental feedback",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26384": {
    "title": "Subspace-Aware Exploration for Sparse-Reward Multi-Agent Tasks",
    "volume": "main",
    "abstract": "Exploration under sparse rewards is a key challenge for multi-agent reinforcement learning problems. One possible solution to this issue is to exploit inherent task structures for an acceleration of exploration. In this paper, we present a novel exploration approach, which encodes a special structural prior on the reward function into exploration, for sparse-reward multi-agent tasks. Specifically, a novel entropic exploration objective which encodes the structural prior is proposed to accelerate the discovery of rewards. By maximizing the lower bound of this objective, we then propose an algorithm with moderate computational cost, which can be applied to practical tasks. Under the sparse-reward setting, we show that the proposed algorithm significantly outperforms the state-of-the-art algorithms in the multiple-particle environment, the Google Research Football and StarCraft II micromanagement tasks. To the best of our knowledge, on some hard tasks (such as 27m_vs_30m}) which have relatively larger number of agents and need non-trivial strategies to defeat enemies, our method is the first to learn winning strategies under the sparse-reward setting",
    "checked": true,
    "id": "9c4ece695bfe0a9e99ee21323313882078b4ef4d",
    "semantic_title": "subspace-aware exploration for sparse-reward multi-agent tasks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26385": {
    "title": "Consensus Learning for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Almost all multi-agent reinforcement learning algorithms without communication follow the principle of centralized training with decentralized execution. During the centralized training, agents can be guided by the same signals, such as the global state. However, agents lack the shared signal and choose actions given local observations during execution. Inspired by viewpoint invariance and contrastive learning, we propose consensus learning for cooperative multi-agent reinforcement learning in this study. Although based on local observations, different agents can infer the same consensus in discrete spaces without communication. We feed the inferred one-hot consensus to the network of agents as an explicit input in a decentralized way, thereby fostering their cooperative spirit. With minor model modifications, our suggested framework can be extended to a variety of multi-agent reinforcement learning algorithms. Moreover, we carry out these variants on some fully cooperative tasks and get convincing results",
    "checked": true,
    "id": "74a184436ce058caf6f821f16dd7dbac9f652ca2",
    "semantic_title": "consensus learning for cooperative multi-agent reinforcement learning",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26386": {
    "title": "HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism",
    "volume": "main",
    "abstract": "Recently, some challenging tasks in multi-agent systems have been solved by some hierarchical reinforcement learning methods. Inspired by the intra-level and inter-level coordination in the human nervous system, we propose a novel value decomposition framework HAVEN based on hierarchical reinforcement learning for fully cooperative multi-agent problems. To address the instability arising from the concurrent optimization of policies between various levels and agents, we introduce the dual coordination mechanism of inter-level and inter-agent strategies by designing reward functions in a two-level hierarchy. HAVEN does not require domain knowledge and pre-training, and can be applied to any value decomposition variant. Our method achieves desirable results on different decentralized partially observable Markov decision process domains and outperforms other popular multi-agent hierarchical reinforcement learning algorithms",
    "checked": true,
    "id": "8a17c4a4e6ff86f444c7c6811e307d52f443d149",
    "semantic_title": "haven: hierarchical cooperative multi-agent reinforcement learning with dual coordination mechanism",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26387": {
    "title": "Hierarchical Mean-Field Deep Reinforcement Learning for Large-Scale Multiagent Systems",
    "volume": "main",
    "abstract": "Learning for efficient coordination in large-scale multiagent systems suffers from the problem of the curse of dimensionality due to the exponential growth of agent interactions. Mean-Field (MF)-based methods address this issue by transforming the interactions within the whole system into a single agent played with the average effect of its neighbors. However, considering the neighbors merely by their average may ignore the varying influences of each neighbor, and learning with this kind of local average effect would likely lead to inferior system performance due to lack of an efficient coordination mechanism in the whole population level. In this work, we propose a Hierarchical Mean-Field (HMF) learning framework to further improve the performance of existing MF methods. The basic idea is to approximate the average effect for a sub-group of agents by considering their different influences within the sub-group, and realize population-level coordination through the interactions among different sub-groups. Empirical studies show that HMF significantly outperforms existing baselines on both challenging cooperative and mixed cooperative-competitive tasks with different scales of agent populations",
    "checked": true,
    "id": "3d169fc1b93ce317a60e77a1b263d3f82804f85d",
    "semantic_title": "hierarchical mean-field deep reinforcement learning for large-scale multiagent systems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26388": {
    "title": "Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers",
    "volume": "main",
    "abstract": "Cooperative Multi-agent Reinforcement Learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a Limited Policy Adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving high robustness under various policy perturbations. Concretely, to avoid the ego-system overfitting to a specific attacker, we maintain a set of attackers, which is optimized to guarantee the attackers high attacking quality and behavior diversity. The goal of quality is to minimize the ego-system coordination effect, and a novel diversity regularizer based on sparse action is applied to diversify the behaviors among attackers. The ego-system is then paired with a population of attackers selected from the maintained attacker set, and alternately trained against the constantly evolving attackers. Extensive experiments on multiple scenarios from SMAC indicate our ROMANCE provides comparable or better robustness and generalization ability than other baselines",
    "checked": true,
    "id": "223066f0c1f0d834848bf41f96a2136ef40467fb",
    "semantic_title": "robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26389": {
    "title": "DACOM: Learning Delay-Aware Communication for Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Communication is supposed to improve multi-agent collaboration and overall performance in cooperative Multi-agent reinforcement learning (MARL). However, such improvements are prevalently limited in practice since most existing communication schemes ignore communication overheads (e.g., communication delays). In this paper, we demonstrate that ignoring communication delays has detrimental effects on collaborations, especially in delay-sensitive tasks such as autonomous driving. To mitigate this impact, we design a delay-aware multi-agent communication model (DACOM) to adapt communication to delays. Specifically, DACOM introduces a component, TimeNet, that is responsible for adjusting the waiting time of an agent to receive messages from other agents such that the uncertainty associated with delay can be addressed. Our experiments reveal that DACOM has a non-negligible performance improvement over other mechanisms by making a better trade-off between the benefits of communication and the costs of waiting for messages",
    "checked": true,
    "id": "738d51cb7d22c3db5bd46eda05fcf1048ea2ad0d",
    "semantic_title": "dacom: learning delay-aware communication for multi-agent reinforcement learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26390": {
    "title": "Effective and Stable Role-Based Multi-Agent Collaboration by Structural Information Principles",
    "volume": "main",
    "abstract": "Role-based learning is a promising approach to improving the performance of Multi-Agent Reinforcement Learning (MARL). Nevertheless, without manual assistance, current role-based methods cannot guarantee stably discovering a set of roles to effectively decompose a complex task, as they assume either a predefined role structure or practical experience for selecting hyperparameters. In this article, we propose a mathematical Structural Information principles-based Role Discovery method, namely SIRD, and then present a SIRD optimizing MARL framework, namely SR-MARL, for multi-agent collaboration. The SIRD transforms role discovery into a hierarchical action space clustering. Specifically, the SIRD consists of structuralization, sparsification, and optimization modules, where an optimal encoding tree is generated to perform abstracting to discover roles. The SIRD is agnostic to specific MARL algorithms and flexibly integrated with various value function factorization approaches. Empirical evaluations on the StarCraft II micromanagement benchmark demonstrate that, compared with state-of-the-art MARL algorithms, the SR-MARL framework improves the average test win rate by 0.17%, 6.08%, and 3.24%, and reduces the deviation by 16.67%, 30.80%, and 66.30%, under easy, hard, and super hard scenarios",
    "checked": true,
    "id": "832d9fda09f1c36c4eda2010295ef68cdf95ad37",
    "semantic_title": "effective and stable role-based multi-agent collaboration by structural information principles",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26391": {
    "title": "Learning to Play General-Sum Games against Multiple Boundedly Rational Agents",
    "volume": "main",
    "abstract": "We study the problem of training a principal in a multi-agent general-sum game using reinforcement learning (RL). Learning a robust principal policy requires anticipating the worst possible strategic responses of other agents, which is generally NP-hard. However, we show that no-regret dynamics can identify these worst-case responses in poly-time in smooth games. We propose a framework that uses this policy evaluation method for efficiently learning a robust principal policy using RL. This framework can be extended to provide robustness to boundedly rational agents too. Our motivating application is automated mechanism design: we empirically demonstrate our framework learns robust mechanisms in both matrix games and complex spatiotemporal games. In particular, we learn a dynamic tax policy that improves the welfare of a simulated trade-and-barter economy by 15%, even when facing previously unseen boundedly rational RL taxpayers",
    "checked": true,
    "id": "7403a725fcffee8107a078f10dfc0df957b78057",
    "semantic_title": "learning to play general-sum games against multiple boundedly rational agents",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26392": {
    "title": "Towards Robust Metrics for Concept Representation Evaluation",
    "volume": "main",
    "abstract": "Recent work on interpretability has focused on concept-based explanations, where deep learning models are explained in terms of high-level units of information, referred to as concepts. Concept learning models, however, have been shown to be prone to encoding impurities in their representations, failing to fully capture meaningful features of their inputs. While concept learning lacks metrics to measure such phenomena, the field of disentanglement learning has explored the related notion of underlying factors of variation in the data, with plenty of metrics to measure the purity of such factors. In this paper, we show that such metrics are not appropriate for concept learning and propose novel metrics for evaluating the purity of concept representations in both approaches. We show the advantage of these metrics over existing ones and demonstrate their utility in evaluating the robustness of concept representations and interventions performed on them. In addition, we show their utility for benchmarking state-of-the-art methods from both families and find that, contrary to common assumptions, supervision alone may not be sufficient for pure concept representations",
    "checked": true,
    "id": "b6ea8f01aafcaf689f190c5193880d1cba0461ef",
    "semantic_title": "towards robust metrics for concept representation evaluation",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26393": {
    "title": "On the Vulnerability of Backdoor Defenses for Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is a popular distributed machine learning paradigm which enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for possible backdoor attacks which aims to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting by proposing a new federated backdoor attack framework for possible countermeasures. Different from traditional training (on triggered data) and rescaling (the malicious client model) based backdoor injection, the proposed backdoor attack framework (1) directly modifies (a small proportion of) local model weights to inject the backdoor trigger via sign flips; (2) jointly optimize the trigger pattern with the client model, thus is more persistent and stealthy for circumventing existing defenses. In a case study, we examine the strength and weaknesses of several recent federated backdoor defenses from three major categories and provide suggestions to the practitioners when training federated models in practice",
    "checked": true,
    "id": "efaab402913dbec3a01a503803dda48554be97e8",
    "semantic_title": "on the vulnerability of backdoor defenses for federated learning",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26394": {
    "title": "Distributionally Robust Optimization with Probabilistic Group",
    "volume": "main",
    "abstract": "Modern machine learning models may be susceptible to learning spurious correlations that hold on average but not for the atypical group of samples. To address the problem, previous approaches minimize the empirical worst-group risk. Despite the promise, they often assume that each sample belongs to one and only one group, which does not allow expressing the uncertainty in group labeling. In this paper, we propose a novel framework PG-DRO, which explores the idea of probabilistic group membership for distributionally robust optimization. Key to our framework, we consider soft group membership instead of hard group annotations. The group probabilities can be flexibly generated using either supervised learning or zero-shot approaches. Our framework accommodates samples with group membership ambiguity, offering stronger flexibility and generality than the prior art. We comprehensively evaluate PG-DRO on both image classification and natural language processing benchmarks, establishing superior performance",
    "checked": true,
    "id": "14de7ffbbc1ce1afd75f32e43787b92a0165a5a7",
    "semantic_title": "distributionally robust optimization with probabilistic group",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26395": {
    "title": "Correct for Whom? Subjectivity and the Evaluation of Personalized Image Aesthetics Assessment Models",
    "volume": "main",
    "abstract": "The problem of image aesthetic quality assessment is surprisingly difficult to define precisely. Most early work attempted to estimate the average aesthetic rating of a group of observers, while some recent work has shifted to an approach based on few-shot personalization. In this paper, we connect few-shot personalization, via Immanuel Kant's concept of disinterested judgment, to an argument from feminist aesthetics about the biased tendencies of objective standards for subjective pleasures. To empirically investigate this philosophical debate, we introduce PR-AADB, a relabeling of the existing AADB dataset with labels for pairs of images, and measure how well the existing groundtruth predicts our new pairwise labels. We find, consistent with the feminist critique, that both the existing groundtruth and few-shot personalized predictions represent some users' preferences significantly better than others, but that it is difficult to predict when and for whom the existing groundtruth will be correct. We thus advise against using benchmark datasets to evaluate models for personalized IAQA, and recommend caution when attempting to account for subjective difference using machine learning more generally",
    "checked": true,
    "id": "ea821689157f662ef611e7c6e65e33653fb11d7c",
    "semantic_title": "correct for whom? subjectivity and the evaluation of personalized image aesthetics assessment models",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26396": {
    "title": "Covariate-Shift Generalization via Random Sample Weighting",
    "volume": "main",
    "abstract": "Shifts in the marginal distribution of covariates from training to the test phase, named covariate-shifts, often lead to unstable prediction performance across agnostic testing data, especially under model misspecification. Recent literature on invariant learning attempts to learn an invariant predictor from heterogeneous environments. However, the performance of the learned predictor depends heavily on the availability and quality of provided environments. In this paper, we propose a simple and effective non-parametric method for generating heterogeneous environments via Random Sample Weighting (RSW). Given the training dataset from a single source environment, we randomly generate a set of covariate-determining sample weights and use each weighted training distribution to simulate an environment. We theoretically show that under appropriate conditions, such random sample weighting can produce sufficient heterogeneity to be exploited by common invariance constraints to find the invariant variables for stable prediction under covariate shifts. Extensive experiments on both simulated and real-world datasets clearly validate the effectiveness of our method",
    "checked": true,
    "id": "79f4bf84b817f47fa74a8b183cde7641ccfecc4f",
    "semantic_title": "covariate-shift generalization via random sample weighting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26397": {
    "title": "Fairness in Contextual Resource Allocation Systems: Metrics and Incompatibility Results",
    "volume": "main",
    "abstract": "We study critical systems that allocate scarce resources to satisfy basic needs, such as homeless services that provide housing. These systems often support communities disproportionately affected by systemic racial, gender, or other injustices, so it is crucial to design these systems with fairness considerations in mind. To address this problem, we propose a framework for evaluating fairness in contextual resource allocation systems that is inspired by fairness metrics in machine learning. This framework can be applied to evaluate the fairness properties of a historical policy, as well as to impose constraints in the design of new (counterfactual) allocation policies. Our work culminates with a set of incompatibility results that investigate the interplay between the different fairness metrics we propose. Notably, we demonstrate that: 1) fairness in allocation and fairness in outcomes are usually incompatible; 2) policies that prioritize based on a vulnerability score will usually result in unequal outcomes across groups, even if the score is perfectly calibrated; 3) policies using contextual information beyond what is needed to characterize baseline risk and treatment effects can be fairer in their outcomes than those using just baseline risk and treatment effects; and 4) policies using group status in addition to baseline risk and treatment effects are as fair as possible given all available information. Our framework can help guide the discussion among stakeholders in deciding which fairness metrics to impose when allocating scarce resources",
    "checked": true,
    "id": "ffae901292c459ac22c299279381db6ffafd1d90",
    "semantic_title": "fairness in contextual resource allocation systems: metrics and incompatibility results",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26398": {
    "title": "Improvement-Focused Causal Recourse (ICR)",
    "volume": "main",
    "abstract": "Algorithmic recourse recommendations inform stakeholders of how to act to revert unfavorable decisions. However, existing methods may recommend actions that lead to acceptance (i.e., revert the model's decision) but do not lead to improvement (i.e., may not revert the underlying real-world state). To recommend such actions is to recommend fooling the predictor. We introduce a novel method, Improvement-Focused Causal Recourse (ICR), which involves a conceptual shift: Firstly, we require ICR recommendations to guide toward improvement. Secondly, we do not tailor the recommendations to be accepted by a specific predictor. Instead, we leverage causal knowledge to design decision systems that predict accurately pre- and post-recourse, such that improvement guarantees translate into acceptance guarantees. Curiously, optimal pre-recourse classifiers are robust to ICR actions and thus suitable post-recourse. In semi-synthetic experiments, we demonstrate that given correct causal knowledge ICR, in contrast to existing approaches, guides toward both acceptance and improvement",
    "checked": true,
    "id": "14b76875dc3d243b9f040db7df0373eb9256a196",
    "semantic_title": "improvement-focused causal recourse (icr)",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26399": {
    "title": "Explaining Model Confidence Using Counterfactuals",
    "volume": "main",
    "abstract": "Displaying confidence scores in human-AI interaction has been shown to help build trust between humans and AI systems. However, most existing research uses only the confidence score as a form of communication. As confidence scores are just another model output, users may want to understand why the algorithm is confident to determine whether to accept the confidence score. In this paper, we show that counterfactual explanations of confidence scores help study participants to better understand and better trust a machine learning model's prediction. We present two methods for understanding model confidence using counterfactual explanation: (1) based on counterfactual examples; and (2) based on visualisation of the counterfactual space. Both increase understanding and trust for study participants over a baseline of no explanation, but qualitative results show that they are used quite differently, leading to recommendations of when to use each one and directions of designing better explanations",
    "checked": true,
    "id": "459e64919f6451fd93313ee2d230ddef0d3fa08a",
    "semantic_title": "explaining model confidence using counterfactuals",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26400": {
    "title": "Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model",
    "volume": "main",
    "abstract": "Federated Learning, as a popular paradigm for collaborative training, is vulnerable against privacy attacks. Different privacy levels regarding users' attitudes need to be satisfied locally, while a strict privacy guarantee for the global model is also required centrally. Personalized Local Differential Privacy (PLDP) is suitable for preserving users' varying local privacy, yet only provides a central privacy guarantee equivalent to the worst-case local privacy level. Thus, achieving strong central privacy as well as personalized local privacy with a utility-promising model is a challenging problem. In this work, a general framework (APES) is built up to strengthen model privacy under personalized local privacy by leveraging the privacy amplification effect of the shuffle model. To tighten the privacy bound, we quantify the heterogeneous contributions to the central privacy user by user. The contributions are characterized by the ability of generating \"echos\" from the perturbation of each user, which is carefully measured by proposed methods Neighbor Divergence and Clip-Laplace Mechanism. Furthermore, we propose a refined framework (S-APES) with the post-sparsification technique to reduce privacy loss in high-dimension scenarios. To the best of our knowledge, the impact of shuffling on personalized local privacy is considered for the first time. We provide a strong privacy amplification effect, and the bound is tighter than the baseline result based on existing methods for uniform local privacy. Experiments demonstrate that our frameworks ensure comparable or higher accuracy for the global model",
    "checked": true,
    "id": "3102b553be4f4b09bf6b2e1982ca880d1305dc38",
    "semantic_title": "echo of neighbors: privacy amplification for personalized private federated learning with shuffle model",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26401": {
    "title": "XRand: Differentially Private Defense against Explanation-Guided Attacks",
    "volume": "main",
    "abstract": "Recent development in the field of explainable artificial intelligence (XAI) has helped improve trust in Machine-Learning-as-a-Service (MLaaS) systems, in which an explanation is provided together with the model prediction in response to each query. However, XAI also opens a door for adversaries to gain insights into the black-box models in MLaaS, thereby making the models more vulnerable to several attacks. For example, feature-based explanations (e.g., SHAP) could expose the top important features that a black-box model focuses on. Such disclosure has been exploited to craft effective backdoor triggers against malware classifiers. To address this trade-off, we introduce a new concept of achieving local differential privacy (LDP) in the explanations, and from that we establish a defense, called XRand, against such attacks. We show that our mechanism restricts the information that the adversary can learn about the top important features, while maintaining the faithfulness of the explanations",
    "checked": true,
    "id": "52ea77624f405563ce08778c2efa15dd9846bb63",
    "semantic_title": "xrand: differentially private defense against explanation-guided attacks",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26402": {
    "title": "Mitigating Adversarial Norm Training with Moral Axioms",
    "volume": "main",
    "abstract": "This paper addresses the issue of adversarial attacks on ethical AI systems. We investigate using moral axioms and rules of deontic logic in a norm learning framework to mitigate adversarial norm training. This model of moral intuition and construction provides AI systems with moral guard rails yet still allows for learning conventions. We evaluate our approach by drawing inspiration from a study commonly used in moral development research. This questionnaire aims to test an agent's ability to reason to moral conclusions despite opposed testimony. Our findings suggest that our model can still correctly evaluate moral situations and learn conventions in an adversarial training environment. We conclude that adding axiomatic moral prohibitions and deontic inference rules to a norm learning model makes it less vulnerable to adversarial attacks",
    "checked": true,
    "id": "b33afce5c6ab0cbd247a4e04018130346d0a9b2c",
    "semantic_title": "mitigating adversarial norm training with moral axioms",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26403": {
    "title": "Equity Promotion in Public Transportation",
    "volume": "main",
    "abstract": "There are many news articles reporting the obstacles confronting poverty-stricken households in access to public transits. These barriers create a great deal of inconveniences for these impoverished families and more importantly, they contribute a lot of social inequalities. A typical approach addressing the issue is to build more transport infrastructure to offer more opportunities to access the public transits especially for those deprived communities. Examples include adding more bus lines connecting needy residents to railways systems and extending existing bus lines to areas with low socioeconomic status. Recently, a new strategy is proposed, which is to harness the ubiquitous ride-hailing services to connect disadvantaged households with the nearest public transportations. Compared with the former infrastructure-based solution, the ride-hailing-based strategy enjoys a few exclusive benefits such as higher effectiveness and more flexibility. In this paper, we propose an optimization model to study how to integrate the two approaches together for equity-promotion purposes. Specifically, we aim to design a strategy of allocating a given limited budget to different candidate programs such that the overall social equity is maximized, which is defined as the minimum covering ratio among all pre-specified protected groups of households (based on race, income, etc.). We have designed a linear-programming (LP) based rounding algorithm, which proves to achieve an optimal approximation ratio of 1-1/e. Additionally, we test our algorithm against a few baselines on real data assembled by outsourcing multiple public datasets collected in the city of Chicago. Experimental results confirm our theoretical predictions and demonstrate the effectiveness of our LP-based strategy in promoting social equity, especially when the budget is insufficient",
    "checked": true,
    "id": "7f3951422106a23841e8c712db068c130a03f3ae",
    "semantic_title": "equity promotion in public transportation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26404": {
    "title": "Online Platforms and the Fair Exposure Problem under Homophily",
    "volume": "main",
    "abstract": "In the wake of increasing political extremism, online platforms have been criticized for contributing to polarization. One line of criticism has focused on echo chambers and the recommended content served to users by these platforms. In this work, we introduce the fair exposure problem: given limited intervention power of the platform, the goal is to enforce balance in the spread of content (e.g., news articles) among two groups of users through constraints similar to those imposed by the Fairness Doctrine in the United States in the past. Groups are characterized by different affiliations (e.g., political views) and have different preferences for content. We develop a stylized framework that models intra- and inter-group content propagation under homophily, and we formulate the platform's decision as an optimization problem that aims at maximizing user engagement, potentially under fairness constraints. Our main notion of fairness requires that each group see a mixture of their preferred and non-preferred content, encouraging information diversity. Promoting such information diversity is often viewed as desirable and a potential means for breaking out of harmful echo chambers. We study the solutions to both the fairness-agnostic and fairness-aware problems. We prove that a fairness-agnostic approach inevitably leads to group-homogeneous targeting by the platform. This is only partially mitigated by imposing fairness constraints: we show that there exist optimal fairness-aware solutions which target one group with different types of content and the other group with only one type that is not necessarily the group's most preferred. Finally, using simulations with real-world data, we study the system dynamics and quantify the price of fairness",
    "checked": true,
    "id": "88e40be2fe3db2aec4385a1db2f3fd9a493a55de",
    "semantic_title": "online platforms and the fair exposure problem under homophily",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26405": {
    "title": "Minimax AUC Fairness: Efficient Algorithm with Provable Convergence",
    "volume": "main",
    "abstract": "The use of machine learning models in consequential decision making often exacerbates societal inequity, in particular yielding disparate impact on members of marginalized groups defined by race and gender. The area under the ROC curve (AUC) is widely used to evaluate the performance of a scoring function in machine learning, but is studied in algorithmic fairness less than other performance metrics. Due to the pairwise nature of the AUC, defining an AUC-based group fairness metric is pairwise-dependent and may involve both intra-group and inter-group AUCs. Importantly, considering only one category of AUCs is not sufficient to mitigate unfairness in AUC optimization. In this paper, we propose a minimax learning and bias mitigation framework that incorporates both intra-group and inter-group AUCs while maintaining utility. Based on this Rawlsian framework, we design an efficient stochastic optimization algorithm and prove its convergence to the minimum group-level AUC. We conduct numerical experiments on both synthetic and real-world datasets to validate the effectiveness of the minimax framework and the proposed optimization algorithm",
    "checked": true,
    "id": "a20a2ec487a1fc7923fcec479e9f86e399151a34",
    "semantic_title": "minimax auc fairness: efficient algorithm with provable convergence",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26406": {
    "title": "Faster Fair Machine via Transferring Fairness Constraints to Virtual Samples",
    "volume": "main",
    "abstract": "Fair classification is an emerging and important research topic in machine learning community. Existing methods usually formulate the fairness metrics as additional inequality constraints, and then embed them into the original objective. This makes fair classification problems unable to be effectively tackled by some solvers specific to unconstrained optimization. Although many new tailored algorithms have been designed to attempt to overcome this limitation, they often increase additional computation burden and cannot cope with all types of fairness metrics. To address these challenging issues, in this paper, we propose a novel method for fair classification. Specifically, we theoretically demonstrate that all types of fairness with linear and non-linear covariance functions can be transferred to two virtual samples, which makes the existing state-of-the-art classification solvers be applicable to these cases. Meanwhile, we generalize the proposed method to multiple fairness constraints. We take SVM as an example to show the effectiveness of our new idea. Empirically, we test the proposed method on real-world datasets and all results confirm its excellent performance",
    "checked": true,
    "id": "518a6d6cbffef3f547863cd496eabd8ad3f817a8",
    "semantic_title": "faster fair machine via transferring fairness constraints to virtual samples",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26407": {
    "title": "Learning Control Policies for Stochastic Systems with Reach-Avoid Guarantees",
    "volume": "main",
    "abstract": "We study the problem of learning controllers for discrete-time non-linear stochastic dynamical systems with formal reach-avoid guarantees. This work presents the first method for providing formal reach-avoid guarantees, which combine and generalize stability and safety guarantees, with a tolerable probability threshold p in [0,1] over the infinite time horizon. Our method leverages advances in machine learning literature and it represents formal certificates as neural networks. In particular, we learn a certificate in the form of a reach-avoid supermartingale (RASM), a novel notion that we introduce in this work. Our RASMs provide reachability and avoidance guarantees by imposing constraints on what can be viewed as a stochastic extension of level sets of Lyapunov functions for deterministic systems. Our approach solves several important problems -- it can be used to learn a control policy from scratch, to verify a reach-avoid specification for a fixed control policy, or to fine-tune a pre-trained policy if it does not satisfy the reach-avoid specification. We validate our approach on 3 stochastic non-linear reinforcement learning tasks",
    "checked": true,
    "id": "a6c7846fb31946622302eeba2186d8c4b39e425a",
    "semantic_title": "learning control policies for stochastic systems with reach-avoid guarantees",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26408": {
    "title": "Robust Neuro-Symbolic Goal and Plan Recognition",
    "volume": "main",
    "abstract": "Goal Recognition is the task of discerning the intended goal of an agent given a sequence of observations, whereas Plan Recognition consists of identifying the plan to achieve such intended goal. Regardless of the underlying techniques, most recognition approaches are directly affected by the quality of the available observations. In this paper, we develop neuro-symbolic recognition approaches that can combine learning and planning techniques, compensating for noise and missing observations using prior data. We evaluate our approaches in standard human-designed planning domains as well as domain models automatically learned from real-world data. Empirical experimentation shows that our approaches reliably infer goals and compute correct plans in the experimental datasets. An ablation study shows that outperform approaches that rely exclusively on the domain model, or exclusively on machine learning in problems with both noisy observations and low observability",
    "checked": true,
    "id": "d9cb686c0c48d271f6806efb5a494d738b585fa1",
    "semantic_title": "robust neuro-symbolic goal and plan recognition",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26409": {
    "title": "Heuristic Search for Multi-Objective Probabilistic Planning",
    "volume": "main",
    "abstract": "Heuristic search is a powerful approach that has successfully been applied to a broad class of planning problems, including classical planning, multi-objective planning, and probabilistic planning modelled as a stochastic shortest path (SSP) problem. Here, we extend the reach of heuristic search to a more expressive class of problems, namely multi-objective stochastic shortest paths (MOSSPs), which require computing a coverage set of non-dominated policies. We design new heuristic search algorithms MOLAO* and MOLRTDP, which extend well-known SSP algorithms to the multi-objective case. We further construct a spectrum of domain-independent heuristic functions differing in their ability to take into account the stochastic and multi-objective features of the problem to guide the search. Our experiments demonstrate the benefits of these algorithms and the relative merits of the heuristics",
    "checked": true,
    "id": "091d3e219240e8ee5c496867b3f59e69abe10da4",
    "semantic_title": "heuristic search for multi-objective probabilistic planning",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26410": {
    "title": "Zero-Knowledge Proofs for Classical Planning Problems",
    "volume": "main",
    "abstract": "In classical planning, the aim is to find a sequence of deterministic actions leading from the initial to a goal state. In this work, we consider the scenario where a party who knows the solution to a planning task, called the prover, wants to convince a second party, the verifier, that it has the solution without revealing any information about the solution itself. This is relevant in domains where privacy is important, for example when plans contain sensitive information or when the solution should not be revealed upfront. We achieve this by introducing a zero-knowledge protocol for plan existence. By restricting ourselves to tasks with polynomially-bounded plan length, we are able to construct a protocol that can be run efficiently by both the prover and verifier. The resulting protocol does not rely on any reduction, has a constant number of rounds, and runs in time polynomial in the size of the task",
    "checked": true,
    "id": "f484252fea5416e03247b649b563833f8eb92cfb",
    "semantic_title": "zero-knowledge proofs for classical planning problems",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26411": {
    "title": "Planning with Hidden Parameter Polynomial MDPs",
    "volume": "main",
    "abstract": "For many applications of Markov Decision Processes (MDPs), the transition function cannot be specified exactly. Bayes-Adaptive MDPs (BAMDPs) extend MDPs to consider transition probabilities governed by latent parameters. To act optimally in BAMDPs, one must maintain a belief distribution over the latent parameters. Typically, this distribution is described by a set of sample (particle) MDPs, and associated weights which represent the likelihood of a sample MDP being the true underlying MDP. However, as the number of dimensions of the latent parameter space increases, the number of sample MDPs required to sufficiently represent the belief distribution grows exponentially. Thus, maintaining an accurate belief in the form of a set of sample MDPs over complex latent spaces is computationally intensive, which in turn affects the performance of planning for these models. In this paper, we propose an alternative approach for maintaining the belief over the latent parameters. We consider a class of BAMDPs where the transition probabilities can be expressed in closed form as a polynomial of the latent parameters, and outline a method to maintain a closed-form belief distribution for the latent parameters which results in an accurate belief representation. Furthermore, the closed-form representation does away with the need to tune the number of sample MDPs required to represent the belief. We evaluate two domains and empirically show that the polynomial, closed-form, belief representation results in better plans than a sampling-based belief representation",
    "checked": true,
    "id": "3129d24d2b60ba4ffbf0060f430e1abce32e541d",
    "semantic_title": "planning with hidden parameter polynomial mdps",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26412": {
    "title": "Privacy Attacks on Schedule-Driven Data",
    "volume": "main",
    "abstract": "Schedules define how resources process jobs in diverse domains, reaching from healthcare to transportation, and, therefore, denote a valuable starting point for analysis of the underlying system. However, publishing a schedule may disclose private information on the considered jobs. In this paper, we provide a first threat model for published schedules, thereby defining a completely new class of data privacy problems. We then propose distance-based measures to assess the privacy loss incurred by a published schedule, and show their theoretical properties for an uninformed adversary, which can be used as a benchmark for informed attacks. We show how an informed attack on a published schedule can be phrased as an inverse scheduling problem. We instantiate this idea by formulating the inverse of a well-studied single-machine scheduling problem, namely minimizing the total weighted completion times. An empirical evaluation for synthetic scheduling problems shows the effectiveness of informed privacy attacks and compares the results to theoretical bounds on uninformed attacks",
    "checked": true,
    "id": "71c9259299e0edb95c2a755db292b5c0720b8cde",
    "semantic_title": "privacy attacks on schedule-driven data",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26413": {
    "title": "Markov Decision Processes with Time-Varying Geometric Discounting",
    "volume": "main",
    "abstract": "Canonical models of Markov decision processes (MDPs) usually consider geometric discounting based on a constant discount factor. While this standard modeling approach has led to many elegant results, some recent studies indicate the necessity of modeling time-varying discounting in certain applications. This paper studies a model of infinite-horizon MDPs with time-varying discount factors. We take a game-theoretic perspective – whereby each time step is treated as an independent decision maker with their own (fixed) discount factor – and we study the subgame perfect equilibrium (SPE) of the resulting game as well as the related algorithmic problems. We present a constructive proof of the existence of an SPE and demonstrate the EXPTIME-hardness of computing an SPE. We also turn to the approximate notion of epsilon-SPE and show that an epsilon-SPE exists under milder assumptions. An algorithm is presented to compute an epsilon-SPE, of which an upper bound of the time complexity, as a function of the convergence property of the time-varying discount factor, is provided",
    "checked": true,
    "id": "47fa00c8948f945f85d3ebeeb434b0fb4ffbef24",
    "semantic_title": "markov decision processes with time-varying geometric discounting",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26414": {
    "title": "Learning-Augmented Algorithms for Online TSP on the Line",
    "volume": "main",
    "abstract": "We study the online Traveling Salesman Problem (TSP) on the line augmented with machine-learned predictions. In the classical problem, there is a stream of requests released over time along the real line. The goal is to minimize the makespan of the algorithm. We distinguish between the open variant and the closed one, in which we additionally require the algorithm to return to the origin after serving all requests. The state of the art is a 1.64-competitive algorithm and a 2.04-competitive algorithm for the closed and open variants, respectively. In both cases, a tight lower bound is known. In both variants, our primary prediction model involves predicted positions of the requests. We introduce algorithms that (i) obtain a tight 1.5 competitive ratio for the closed variant and a 1.66 competitive ratio for the open variant in the case of perfect predictions, (ii) are robust against unbounded prediction error, and (iii) are smooth, i.e., their performance degrades gracefully as the prediction error increases. Moreover, we further investigate the learning-augmented setting in the open variant by additionally considering a prediction for the last request served by the optimal offline algorithm. Our algorithm for this enhanced setting obtains a 1.33 competitive ratio with perfect predictions while also being smooth and robust, beating the lower bound of 1.44 we show for our original prediction setting for the open variant. Also, we provide a lower bound of 1.25 for this enhanced setting",
    "checked": true,
    "id": "45850640a59627263b5a9b995be5f248aefc20bc",
    "semantic_title": "learning-augmented algorithms for online tsp on the line",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26415": {
    "title": "Networked Restless Bandits with Positive Externalities",
    "volume": "main",
    "abstract": "Restless multi-armed bandits are often used to model budget-constrained resource allocation tasks where receipt of the resource is associated with an increased probability of a favorable state transition. Prior work assumes that individual arms only benefit if they receive the resource directly. However, many allocation tasks occur within communities and can be characterized by positive externalities that allow arms to derive partial benefit when their neighbor(s) receive the resource. We thus introduce networked restless bandits, a novel multi-armed bandit setting in which arms are both restless and embedded within a directed graph. We then present Greta, a graph-aware, Whittle index-based heuristic algorithm that can be used to efficiently construct a constrained reward-maximizing action vector at each timestep. Our empirical results demonstrate that Greta outperforms comparison policies across a range of hyperparameter values and graph topologies. Code and appendices are available at https://github.com/crherlihy/networked_restless_bandits",
    "checked": true,
    "id": "f3d6ad6b700d5fc50b2f552bc735083b41a926dd",
    "semantic_title": "networked restless bandits with positive externalities",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26416": {
    "title": "Planning for Learning Object Properties",
    "volume": "main",
    "abstract": "Autonomous agents embedded in a physical environment need the ability to recognize objects and their properties from sensory data. Such a perceptual ability is often implemented by supervised machine learning models, which are pre-trained using a set of labelled data. In real-world, open-ended deployments, however, it is unrealistic to assume to have a pre-trained model for all possible environments. Therefore, agents need to dynamically learn/adapt/extend their perceptual abilities online, in an autonomous way, by exploring and interacting with the environment where they operate. This paper describes a way to do so, by exploiting symbolic planning. Specifically, we formalize the problem of automatically training a neural network to recognize object properties as a symbolic planning problem (using PDDL). We use planning techniques to produce a strategy for automating the training dataset creation and the learning process. Finally, we provide an experimental evaluation in both a simulated and a real environment, which shows that the proposed approach is able to successfully learn how to recognize new object properties",
    "checked": true,
    "id": "fa6786bd22a1eb67a4e1f28719103ba0e0c2398d",
    "semantic_title": "planning for learning object properties",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26417": {
    "title": "Fully Online Matching with Stochastic Arrivals and Departures",
    "volume": "main",
    "abstract": "We study a fully online matching problem with stochastic arrivals and departures. In this model, each online arrival follows a known identical and independent distribution over a fixed set of agent types. Its sojourn time is unknown in advance and follows type-specific distributions with known expectations. The goal is to maximize the weighted reward from successful matches. To solve this problem, we first propose a linear program (LP)-based algorithm whose competitive ratio is lower bounded by 0.155 under mild conditions. We further achieve better ratios in some special cases. To demonstrate the challenges of the problem, we further establish several hardness results. In particular, we show that no online algorithm can achieve a competitive ratio better than 2/3 in this model and there is no LP-based algorithm (with respect to our proposed LP) with a competitive ratio better than 1/3. Finally, we demonstrate the effectiveness and efficiency of our algorithm numerically",
    "checked": true,
    "id": "0f618e81ea843aa638fd2445da298d52353e62bc",
    "semantic_title": "fully online matching with stochastic arrivals and departures",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26418": {
    "title": "Towards Automated Modeling Assistance: An Efficient Approach for Repairing Flawed Planning Domains",
    "volume": "main",
    "abstract": "Designing a planning domain is a difficult task in AI planning. Assisting tools are thus required if we want planning to be used more broadly. In this paper, we are interested in automatically correcting a flawed domain. In particular, we are concerned with the scenario where a domain contradicts a plan that is known to be valid. Our goal is to repair the domain so as to turn the plan into a solution. Specifically, we consider both grounded and lifted representations support for negative preconditions and show how to explore the space of repairs to find the optimal one efficiently. As an evidence of the efficiency of our approach, the experiment results show that all flawed domains except one in the benchmark set can be repaired optimally by our approach within one second",
    "checked": true,
    "id": "8b4da4a73bb88131c8a19d9c1c4dc3ccaadd2e6e",
    "semantic_title": "towards automated modeling assistance: an efficient approach for repairing flawed planning domains",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26419": {
    "title": "Was Fixing This Really That Hard? On the Complexity of Correcting HTN Domains",
    "volume": "main",
    "abstract": "Automated modeling assistance is indispensable to the AI planning being deployed in practice, notably in industry and other non-academic contexts. Yet, little progress has been made that goes beyond smart interfaces like programming environments. They focus on autocompletion, but lack intelligent support for guiding the modeler. As a theoretical foundation of a first step towards this direction, we study the computational complexity of correcting a flawed Hierarchical Task Network (HTN) planning domain. Specifically, a modeler provides a (white) list of plans that are supposed to be solutions, and likewise a (black) list of plans that shall not be solutions. We investigate the complexity of finding a set of (optimal or suboptimal) model corrections so that those plans are (resp. not) solutions to the corrected model. More specifically, we factor out each hardness source that contributes towards NP-hardness, including one that we deem important for many other complexity investigations that go beyond our specific context of application. All complexities range between NP and Sigma-2-p, rising the hope for efficient practical tools in the future",
    "checked": true,
    "id": "267246808c31153e8ff766fd663d59595d3fddcc",
    "semantic_title": "was fixing this really that hard? on the complexity of correcting htn domains",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26420": {
    "title": "On Total-Order HTN Plan Verification with Method Preconditions – An Extension of the CYK Parsing Algorithm",
    "volume": "main",
    "abstract": "In this paper, we consider the plan verification problem for totally ordered (TO) HTN planning. The problem is proved to be solvable in polynomial time by recognizing its connection to the membership decision problem for context-free grammars. Currently, most HTN plan verification approaches do not have special treatments for the TO configuration, and the only one features such an optimization still relies on an exhaustive search. Hence, we will develop a new TOHTN plan verification approach in this paper by extending the standard CYK parsing algorithm which acts as the best decision procedure in general",
    "checked": false,
    "id": "7493b69460ae3d93c07bd579335943fbf9338639",
    "semantic_title": "on total-order htn plan verification with method preconditions - an extension of the cyk parsing algorithm",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26421": {
    "title": "A Dynamics and Task Decoupled Reinforcement Learning Architecture for High-Efficiency Dynamic Target Intercept",
    "volume": "main",
    "abstract": "Due to the flexibility and ease of control, unmanned aerial vehicles (UAVs) have been increasingly used in various scenarios and applications in recent years. Training UAVs with reinforcement learning (RL) for a specific task is often expensive in terms of time and computation. However, it is known that the main effort of the learning process is made to fit the low-level physical dynamics systems instead of the high-level task itself. In this paper, we study to apply UAVs in the dynamic target intercept (DTI) task, where the dynamics systems equipped by different UAV models are correspondingly distinct. To this end, we propose a dynamics and task decoupled RL architecture to address the inefficient learning procedure, where the RL module focuses on modeling the DTI task without involving physical dynamics, and the design of states, actions, and rewards are completely task-oriented while the dynamics control module can adaptively convert actions from the RL module to dynamics signals to control different UAVs without retraining the RL module. We show the efficiency and efficacy of our results in comparison and ablation experiments against state-of-the-art methods",
    "checked": true,
    "id": "5f08ec6254022a2bfaeebb9d9c72404a1351c078",
    "semantic_title": "a dynamics and task decoupled reinforcement learning architecture for high-efficiency dynamic target intercept",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26422": {
    "title": "AlphaRoute: Large-Scale Coordinated Route Planning via Monte Carlo Tree Search",
    "volume": "main",
    "abstract": "This paper proposes AlphaRoute, an AlphaGo inspired algorithm for coordinating large-scale routes, built upon graph attention reinforcement learning and Monte Carlo Tree Search (MCTS). We first partition the road network into regions and model large-scale coordinated route planning as a Markov game, where each partitioned region is treated as a player instead of each driver. Then, AlphaRoute applies a bilevel optimization framework, consisting of several region planners and a global planner, where the region planner coordinates the route choices for vehicles located in the region and generates several strategies, and the global planner evaluates the combination of strategies. AlphaRoute is built on graph attention network for evaluating each state and MCTS algorithm for dynamically visiting and simulating the future state for narrowing down the search space. AlphaRoute is capable of 1) bridging user fairness and system efficiency, 2) achieving higher search efficiency by alleviating the curse of dimensionality problems, and 3) making an effective and informed route planning by simulating over the future to capture traffic dynamics. Comprehensive experiments are conducted on two real-world road networks as compared with several baselines to evaluate the performance, and results show that AlphaRoute achieves the lowest travel time, and is efficient and effective for coordinating large-scale routes and alleviating the traffic congestion problem. The code will be publicly available",
    "checked": true,
    "id": "b39aecbf900ebdcc4846324ec887d981247df29f",
    "semantic_title": "alpharoute: large-scale coordinated route planning via monte carlo tree search",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26423": {
    "title": "Learning Rational Subgoals from Demonstrations and Instructions",
    "volume": "main",
    "abstract": "We present a framework for learning useful subgoals that support efficient long-term planning to achieve novel goals. At the core of our framework is a collection of rational subgoals (RSGs), which are essentially binary classifiers over the environmental states. RSGs can be learned from weakly-annotated data, in the form of unsegmented demonstration trajectories, paired with abstract task descriptions, which are composed of terms initially unknown to the agent (e.g., collect-wood then craft-boat then go-across-river). Our framework also discovers dependencies between RSGs, e.g., the task collect-wood is a helpful subgoal for the task craft-boat. Given a goal description, the learned subgoals and the derived dependencies facilitate off-the-shelf planning algorithms, such as A* and RRT, by setting helpful subgoals as waypoints to the planner, which significantly improves performance-time efficiency. Project page: https://rsg.csail.mit.edu",
    "checked": true,
    "id": "2ddd59c64c0407f2386241b3d5f369bbea4baa8e",
    "semantic_title": "learning rational subgoals from demonstrations and instructions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26424": {
    "title": "Learning Safe Numeric Action Models",
    "volume": "main",
    "abstract": "Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent's actions, given in some planning domain description language. Yet obtaining such an action model is a notoriously hard task. This task is even more challenging in mission-critical domains, where a trial-and-error approach to learning how to act is not an option. In such domains, the action model used to generate plans must be safe, in the sense that plans generated with it must be applicable and achieve their goals. Learning safe action models for planning has been recently explored for domains in which states are sufficiently described with Boolean variables. In this work, we go beyond this limitation and propose the NSAM algorithm. NSAM runs in time that is polynomial in the number of observations and, under certain conditions, is guaranteed to return safe action models. We analyze its worst-case sample complexity, which may be intractable for some domains. Empirically, however, NSAM can quickly learn a safe action model that can solve most problems in the domain",
    "checked": true,
    "id": "85a6b466c642a50987953ea2562eb6956bfb5788",
    "semantic_title": "learning safe numeric action models",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26425": {
    "title": "Automated Verification of Social Laws in Numeric Settings",
    "volume": "main",
    "abstract": "It is possible for agents operating in a shared environment to interfere with one another. One mechanism of coordination is called Social Law. Enacting such a law in a multi-agent setting restricts agents' behaviors. Robustness, in this case, ensures that the agents do not harmfully interfere with each other and that each agent achieves its goals regardless of what other agents do. Previous work on social law verification examined only the case of boolean state variables. However, many real-world problems require reasoning with numeric variables. Moreover, numeric fluents allow a more compact representation of multiple planning problems. In this paper, we develop a method to verify whether a given social law is robust via compilation to numeric planning. A solution to this compilation constitutes a counterexample to the robustness of the problem, i.e., evidence of cross-agent conflict. Thus, the social law is robust if and only if the proposed compilation is unsolvable. We empirically verify robustness in multiple domains using state-of-the-art numeric planners. Additionally, this compilation raises a challenge by generating a set of non-trivial numeric domains where unsolvability should be either proved or disproved",
    "checked": true,
    "id": "f60df74cf98cfee51397b9827bc5979f6341fb7b",
    "semantic_title": "automated verification of social laws in numeric settings",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26426": {
    "title": "Expressive Optimal Temporal Planning via Optimization Modulo Theory",
    "volume": "main",
    "abstract": "Temporal Planning is the problem of synthesizing a course of actions given a predictive model of a system subject to temporal constraints. This kind of planning finds natural applications in the automation of industrial processes and in robotics when the timing and deadlines are important. Finding any plan in temporal planning is often not enough as it is sometimes needed to optimize a certain objective function: particularly interesting are the minimization of the makespan and the optimization of the costs of actions. Despite the importance of the problem, only few works in the literature tackled the problem of optimal temporal planning because of the complicated intermix of planning and scheduling. In this paper, we address the problem of optimal temporal planning for a very expressive class of problems using a reduction of the bounded planning problem to Optimization Modulo Theory (OMT) a powerful discrete/continuous optimization framework. We theoretically and empirically show the expressive power of this approach and we set a baseline for future research in this area",
    "checked": true,
    "id": "bc38b1104ce96be3d05047ce8d31728b19f8495e",
    "semantic_title": "expressive optimal temporal planning via optimization modulo theory",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26427": {
    "title": "Flexible Budgets in Restless Bandits: A Primal-Dual Algorithm for Efficient Budget Allocation",
    "volume": "main",
    "abstract": "Restless multi-armed bandits (RMABs) are an important model to optimize allocation of limited resources in sequential decision-making settings. Typical RMABs assume the budget --- the number of arms pulled --- to be fixed for each step in the planning horizon. However, for realistic real-world planning, resources are not necessarily limited at each planning step; we may be able to distribute surplus resources in one round to an earlier or later round. In real-world planning settings, this flexibility in budget is often constrained to within a subset of consecutive planning steps, e.g., weekly planning of a monthly budget. In this paper we define a general class of RMABs with flexible budget, which we term F-RMABs, and provide an algorithm to optimally solve for them. We derive a min-max formulation to find optimal policies for F-RMABs and leverage gradient primal-dual algorithms to solve for reward-maximizing policies with flexible budgets. We introduce a scheme to sample expected gradients to apply primal-dual algorithms to the F-RMAB setting and make an otherwise computationally expensive approach tractable. Additionally, we provide heuristics that trade off solution quality for efficiency and present experimental comparisons of different F-RMAB solution approaches",
    "checked": true,
    "id": "06b26284e85e767313d2b4291ee01521a850d4a6",
    "semantic_title": "flexible budgets in restless bandits: a primal-dual algorithm for efficient budget allocation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26428": {
    "title": "Structurally Restricted Fragments of Numeric Planning – a Complexity Analysis",
    "volume": "main",
    "abstract": "Numeric planning is known to be undecidable even under severe restrictions. Prior work has investigated the decidability boundaries by restricting the expressiveness of the planning formalism in terms of the numeric functions allowed in conditions and effects. We study a well-known restricted form of Hoffmann's simple numeric planning, which is undecidable. We analyze the complexity by imposing restrictions on the causal structure, exploiting a novel method for bounding variable domain sizes. First, we show that plan existence for tasks where all numeric variables are root nodes in the causal graph is in PSPACE. Second, we show that for tasks with only numeric leaf variables the problem is decidable, and that it is in PSPACE if the propositional state space has a fixed size. Our work lays a strong foundation for future investigations of structurally more complex tasks. From a practical perspective, our method allows to employ heuristics and methods that are geared towards finite variable domains (such as pattern database heuristics or decoupled search) to solve non-trivial families of numeric planning problems",
    "checked": false,
    "id": "243ace8039ae88f57d03cc62ca20af8db703c747",
    "semantic_title": "structurally restricted fragments of numeric planning - a complexity analysis",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26429": {
    "title": "Predicate Invention for Bilevel Planning",
    "volume": "main",
    "abstract": "Efficient planning in continuous state and action spaces is fundamentally hard, even when the transition model is deterministic and known. One way to alleviate this challenge is to perform bilevel planning with abstractions, where a high-level search for abstract plans is used to guide planning in the original transition space. Previous work has shown that when state abstractions in the form of symbolic predicates are hand-designed, operators and samplers for bilevel planning can be learned from demonstrations. In this work, we propose an algorithm for learning predicates from demonstrations, eliminating the need for manually specified state abstractions. Our key idea is to learn predicates by optimizing a surrogate objective that is tractable but faithful to our real efficient-planning objective. We use this surrogate objective in a hill-climbing search over predicate sets drawn from a grammar. Experimentally, we show across four robotic planning environments that our learned abstractions are able to quickly solve held-out tasks, outperforming six baselines",
    "checked": true,
    "id": "1a42134ea9d5ac1bb230e8b09dd9b0bc7fbc33b2",
    "semantic_title": "predicate invention for bilevel planning",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26430": {
    "title": "Smoothed Online Combinatorial Optimization Using Imperfect Predictions",
    "volume": "main",
    "abstract": "Smoothed online combinatorial optimization considers a learner who repeatedly chooses a combinatorial decision to minimize an unknown changing cost function with a penalty on switching decisions in consecutive rounds. We study smoothed online combinatorial optimization problems when an imperfect predictive model is available, where the model can forecast the future cost functions with uncertainty. We show that using predictions to plan for a finite time horizon leads to regret dependent on the total predictive uncertainty and an additional switching cost. This observation suggests choosing a suitable planning window to balance between uncertainty and switching cost, which leads to an online algorithm with guarantees on the upper and lower bounds of the cumulative regret. Empirically, our algorithm shows a significant improvement in cumulative regret compared to other baselines in synthetic online distributed streaming problems",
    "checked": true,
    "id": "905096ec9dfe7ebd4782628820fbad840feece06",
    "semantic_title": "smoothed online combinatorial optimization using imperfect predictions",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26431": {
    "title": "Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health",
    "volume": "main",
    "abstract": "This paper studies restless multi-armed bandit (RMAB) problems with unknown arm transition dynamics but with known correlated arm features. The goal is to learn a model to predict transition dynamics given features, where the Whittle index policy solves the RMAB problems using predicted transitions. However, prior works often learn the model by maximizing the predictive accuracy instead of final RMAB solution quality, causing a mismatch between training and evaluation objectives. To address this shortcoming, we propose a novel approach for decision-focused learning in RMAB that directly trains the predictive model to maximize the Whittle index solution quality. We present three key contributions: (i) we establish differentiability of the Whittle index policy to support decision-focused learning; (ii) we significantly improve the scalability of decision-focused learning approaches in sequential problems, specifically RMAB problems; (iii) we apply our algorithm to a previously collected dataset of maternal and child health to demonstrate its performance. Indeed, our algorithm is the first for decision-focused learning in RMAB that scales to real-world problem sizes",
    "checked": true,
    "id": "b6b6ddc7a6a13441ff87b98133dd920ba488e544",
    "semantic_title": "scalable decision-focused learning in restless multi-armed bandits with application to maternal and child health",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26432": {
    "title": "Neural TSP Solver with Progressive Distillation",
    "volume": "main",
    "abstract": "Travelling salesman problem (TSP) is NP-Hard with exponential search space. Recently, the adoption of encoder-decoder models as neural TSP solvers has emerged as an attractive topic because they can instantly obtain near-optimal results for small-scale instances. Nevertheless, their training efficiency and solution quality degrade dramatically when dealing with large-scale problems. To address the issue, we propose a novel progressive distillation framework, by adopting curriculum learning to train TSP samples in increasing order of their problem size and progressively distilling high-level knowledge from small models to large models via a distillation loss. In other words, the trained small models are used as the teacher network to guide action selection when training large models. To accelerate training speed, we also propose a Delaunary-graph based action mask and a new attention-based decoder to reduce decoding cost. Experimental results show that our approach establishes clear advantages over existing encoder-decoder models in terms of training effectiveness and solution quality. In addition, we validate its usefulness as an initial solution generator for the state-of-the-art TSP solvers, whose probability of obtaining the optimal solution can be further improved in such a hybrid manner",
    "checked": true,
    "id": "98abf3344d73e24eef4138a3238860602b15096e",
    "semantic_title": "neural tsp solver with progressive distillation",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26433": {
    "title": "The Linear Distance Traveling Tournament Problem Allows an EPTAS",
    "volume": "main",
    "abstract": "The Traveling Tournament Problem (TTP-k) is a well-known benchmark problem in tournament timetabling and has been extensively studied in the field of AI. In this problem, we are going to design a double round-robin schedule such that each pair of teams plays one game in each other's home venue, minimizing the total distance traveled by all n teams (n is even) under the constraint that each team can have at most k-consecutive home games or away games. The Linear Distance Traveling Tournament Problem (LDTTP-k), where all teams are located on a line, was introduced by Hoshino and Kawarabayashi (AAAI 2012). For LDTTP-3, they gave a 4/3-approximation algorithm for n≡4 (mod 6) teams. In this paper, we show that for any 3≤k=o(∛n), LDTTP-k allows an efficient polynomial-time approximation scheme (EPTAS)",
    "checked": true,
    "id": "cd49c8c0ea7633798017e2583b453b4a3a428f7f",
    "semantic_title": "the linear distance traveling tournament problem allows an eptas",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26434": {
    "title": "Learning Relational Causal Models with Cycles through Relational Acyclification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2785a607dd1a88722184b130c191f96bfd23caa0",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26435": {
    "title": "Causal Effect Identification in Cluster DAGs",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9aee5972617d44866993dd9bd07b6ff66452c62f",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26436": {
    "title": "A Simple Unified Approach to Testing High-Dimensional Conditional Independences for Categorical and Ordinal Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "24a2b64e8daa805c6de265a70ad7f0c2b45479d1",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26437": {
    "title": "Score-Based Learning of Graphical Event Models with Background Knowledge Augmentation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "14ec61dad1880ef08666c7e9f2bc0822e07df1c3",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26438": {
    "title": "Entropy Regularization for Population Estimation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "361f21c1ffd6c178ff9c5745dd15efb64dfb182c",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26439": {
    "title": "Principled and Efficient Motif Finding for Structure Learning of Lifted Graphical Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "374a455fac344c990b99c6c4369f7e1a4a7bcf57",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26440": {
    "title": "A Faster Practical Approximation Scheme for the Permanent",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d883d135250580201b091e1552128b0a29a7052c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26441": {
    "title": "Neural Diffeomorphic Non-uniform B-spline Flows",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fe61a832a45cab659d617c534d4d0da142d51821",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26442": {
    "title": "Identification and Estimation of the Probabilities of Potential Outcome Types Using Covariate Information in Studies with Non-compliance",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "176b6676bbce824f0d0d53ad5d99edf1565da491",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26443": {
    "title": "Computing Divergences between Discrete Decomposable Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9a03e29b3558f4d82e6f3216258c9469ed05d150",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26444": {
    "title": "Out-of-Distribution Generalization by Neural-Symbolic Joint Training",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b0ed6b805c99d98e5e684a1a5eab6717eef14529",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26445": {
    "title": "Novel Ordering-Based Approaches for Causal Structure Learning in the Presence of Unobserved Variables",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6b9b58dd484ea2c198b7fcc17c02ea9d91068ede",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26446": {
    "title": "Maximizing the Probability of Fixation in the Positional Voter Model",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6764b61918b9190914c1a11ccb4a75ab9f06ee2c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26447": {
    "title": "Certifying Fairness of Probabilistic Circuits",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0418f5b401ee793bf0e99a1523d5edcd0427d019",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26448": {
    "title": "Probabilities of Potential Outcome Types in Experimental Studies: Identification and Estimation Based on Proxy Covariate Information",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "489d2650ad53d430127b8577a2fbf4857f23387c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26449": {
    "title": "Lifted Inference with Linear Order Axiom",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1402cac2817e664ec9d76da8eab1431e4d27f7dd",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26450": {
    "title": "Vector Causal Inference between Two Groups of Variables",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6b0bd0bc26c656d333ed4a5b3b7eca2ba4dc8ccc",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26451": {
    "title": "Efficient Enumeration of Markov Equivalent DAGs",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ae3aad78395546c58df9b18287991508a0471873",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26452": {
    "title": "Differentially Private Nonlinear Causal Discovery from Numerical Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "23689c77d6ed841ce601eaa21db5eb946407fab6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26453": {
    "title": "Safe Interval Path Planning with Kinodynamic Constraints",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c9ed07ef6d320f32179db8acca96105265057c1c",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26454": {
    "title": "Diversity Maximization in the Presence of Outliers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d2079b3c8ea31d25512a59f63992dfdeddbb7e7e",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26455": {
    "title": "Fair Short Paths in Vertex-Colored Graphs",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0dad9f0caae52a64cc87e2663c4bb2bc0d30790b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26456": {
    "title": "AC-Band: A Combinatorial Bandit-Based Approach to Algorithm Configuration",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fe95085e0a6d19a60a524f85be5ccd63a5959735",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26457": {
    "title": "GRASMOS: Graph Signage Model Selection for Gene Regulatory Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "565a26ec9e8f548dcf20dea554d0e267a14c72f5",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26458": {
    "title": "Optimal Pathfinding on Weighted Grid Maps",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e74c87d6b9d9bbe26d2114ad38add7b5442453e8",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26459": {
    "title": "Warm-Starting Nested Rollout Policy Adaptation with Optimal Stopping",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "efe3b72a9da2bb9cf8ea1900d56ae0b3a422b851",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26460": {
    "title": "A Proof That Using Crossover Can Guarantee Exponential Speed-Ups in Evolutionary Multi-Objective Optimisation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "117c4cadc884afd10038fc60d1261c5db82c739c",
    "semantic_title": "",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26461": {
    "title": "Runtime Analysis for the NSGA-II: Provable Speed-Ups from Crossover",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bac992afcc9c0b508f589751f8b9f5fe2f55b05c",
    "semantic_title": "",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26462": {
    "title": "From Understanding the Population Dynamics of the NSGA-II to the First Proven Lower Bounds",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "787b61c525f2489bb42076ca5afae1b439b0a3a5",
    "semantic_title": "",
    "citation_count": 16
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26463": {
    "title": "Ultrafast Euclidean Shortest Path Computation Using Hub Labeling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d47dadbcc38fdbae6d27a6999b2bdc62a763d93a",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26464": {
    "title": "A Formal Metareasoning Model of Concurrent Planning and Execution",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a1e304ee403237b71740b837e934ab5153410356",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26465": {
    "title": "TransPath: Learning Heuristics for Grid-Based Pathfinding via Transformers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6ab30e5b9fcbb2203b99f8f1753ae23e0b735fa6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26466": {
    "title": "Large-State Reinforcement Learning for Hyper-Heuristics",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ab8ae110b84a98e2c9319bb2347e1d11d439f211",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26467": {
    "title": "Human Assisted Learning by Evolutionary Multi-Objective Optimization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a6295fe7ca3873ceead43df39f8f945462cffb54",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26468": {
    "title": "OPT-GAN: A Broad-Spectrum Global Optimizer for Black-Box Problems by Learning Distribution",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4d0fe6de29720fc358348ca9f5d48edf7ece6518",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26469": {
    "title": "Analyzing and Improving the Use of the FastMap Embedding in Pathfinding Tasks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6939432fb47a75db461f89b1b44e7012adda0006",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26470": {
    "title": "Fully Computer-Assisted Proofs in Extremal Combinatorics",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c0ed378a0b63c52f17a89c5dc92e2325dbb31e35",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26471": {
    "title": "Electrophysiological Brain Source Imaging via Combinatorial Search with Provable Optimality",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d45c401bc7b192c588f24c1ae9d0f6b875579d02",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26472": {
    "title": "Improved Algorithm for Regret Ratio Minimization in Multi-Objective Submodular Maximization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "71e07dd7f9327d0a030c40b8b15bd2ee8059d34a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26473": {
    "title": "Efficient Gradient Approximation Method for Constrained Bilevel Optimization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f23bff48ad4da52684fa983e8188343385be7a4b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26474": {
    "title": "A Generalized Scalarization Method for Evolutionary Multi-Objective Optimization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0239c9c90a81686ac386aee7d795096858bb8c8a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26475": {
    "title": "Generalized Category Discovery with Decoupled Prototypical Network",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9a6f06a7a03380a09e7bee185054d9bac9c32ac9",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26476": {
    "title": "Structured Case-Based Reasoning for Inference-Time Adaptation of Text-to-SQL Parsers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5435ed7c26f0c250493f244acffb69dd929d116b",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26477": {
    "title": "SegFormer: A Topic Segmentation Model with Controllable Range of Attention",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "cc40f72c6bbe6a78d41d34a82984b8bd107a5e4d",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26478": {
    "title": "Rich Event Modeling for Script Event Prediction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5c8191cf4cfe1abf70775ab95f01cacb3d26cdc1",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26479": {
    "title": "Avocodo: Generative Adversarial Network for Artifact-Free Vocoder",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d1a595d6f3248898a5b4f7611b24f3771eef3540",
    "semantic_title": "",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26480": {
    "title": "End-to-End Deep Reinforcement Learning for Conversation Disentanglement",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "61643a757de121303a2af817870e2792211e6ad6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26481": {
    "title": "Self-Supervised Logic Induction for Explainable Fuzzy Temporal Commonsense Reasoning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "031970e341dfd1385ce0ac9f4a21dc8d20e85645",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26482": {
    "title": "Zero-Shot Cross-Lingual Event Argument Extraction with Language-Oriented Prefix-Tuning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "183551cf1dbc4692e9567491a204b23140bb6cde",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26483": {
    "title": "RPA: Reasoning Path Augmentation in Iterative Retrieving for Multi-Hop QA",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "dfd1508ad2fd5c79fe7c17fa77f2bf660fc24999",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26484": {
    "title": "Leveraging Modality-Specific Representations for Audio-Visual Speech Recognition via Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "38d4c9de406c02239cc2e7c6e8797523d8093380",
    "semantic_title": "",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26485": {
    "title": "Converge to the Truth: Factual Error Correction via Iterative Constrained Editing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0657858dbda4546e4209505764e73b3f58964617",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26486": {
    "title": "Adversarial Word Dilution as Text Data Augmentation in Low-Resource Regime",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "51c2714ab5dbc330619a1a96db994c9905900160",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26487": {
    "title": "CP-Rec: Contextual Prompting for Conversational Recommender Systems",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ae67d9b533f257584ecb91647e9520dad8de931b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26488": {
    "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4b8d3ede673ddeab9dfb5184da6b748d7a526754",
    "semantic_title": "",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26489": {
    "title": "Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "72ae79c894cc5f4829cb190500fc143994545648",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26490": {
    "title": "Preference-Controlled Multi-Objective Reinforcement Learning for Conditional Text Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "33aa0aff496d182a8ea8bf11a5155c257dea40d4",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26491": {
    "title": "Learning towards Selective Data Augmentation for Dialogue Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "50ba225bcfd6bfd34f5c6957ee0b72a4f12f4e89",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26492": {
    "title": "Learn from Yesterday: A Semi-supervised Continual Learning Method for Supervision-Limited Text-to-SQL Task Streams",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "53d62f2e7bf20c442ce7197b2d39e2f6ea77a47b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26493": {
    "title": "A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "38ec6fc80ccbf1dd8a7e66111ff37473f9edbad8",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26494": {
    "title": "Unsupervised Explanation Generation via Correct Instantiations",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "655d34b590e8911e072013ad21582c0382447600",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26495": {
    "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-Shot In-Context Learners",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "06edda0310b4ec7c5012d012349252a3a77521b6",
    "semantic_title": "",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26496": {
    "title": "Neural Dynamic Focused Topic Model",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "215ca900b723a975b2fc0ad6eef4b650f522e96e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26497": {
    "title": "Improving Simultaneous Machine Translation with Monolingual Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bf4ccb8446c6f9fa79b799b407852c7a218a3c02",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26498": {
    "title": "Domain-Adapted Dependency Parsing for Cross-Domain Named Entity Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "82b2dd1bc2e7a7522d6f6abb77770be550e84ff7",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26499": {
    "title": "MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f377e2768aaab569db6c985a91b5eca4afbd4e1c",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26500": {
    "title": "Learning to Select from Multiple Options",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c9feb2cf67e17e22312b30f0eb4e8484fe3d5b11",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26501": {
    "title": "Real or Fake Text?: Investigating Human Ability to Detect Boundaries between Human-Written and Machine-Generated Text",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b118283afc5d8652de52cd13a5e287d76c5ec91f",
    "semantic_title": "",
    "citation_count": 10
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26502": {
    "title": "Diffuser: Efficient Transformers with Multi-Hop Attention Diffusion for Long Sequences",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b0c5c673c690c644a7d4af73adb783bd98486181",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26503": {
    "title": "Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7e70391224b5dd8c2b13a3165b322858da17bcdf",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26504": {
    "title": "MIGA: A Unified Multi-Task Generation Framework for Conversational Text-to-SQL",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "642e04ece55c5b532ff7e5408d8723c7d9c835db",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26505": {
    "title": "On the Effectiveness of Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "846bdc54563f9de2e8388078ea2467b81151f6d5",
    "semantic_title": "",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26506": {
    "title": "SumREN: Summarizing Reported Speech about Events in News",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "3a356a3a48b952837c79c229531ad93a72bc62df",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26507": {
    "title": "ProKD: An Unsupervised Prototypical Knowledge Distillation Network for Zero-Resource Cross-Lingual Named Entity Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5be413ff1171d43dd531de38d1c19325569ccda3",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26508": {
    "title": "Denoising Pre-training for Machine Translation Quality Estimation with Curriculum Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "056d3bd8c3b77643bdfa34adcf602cb538600e0c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26509": {
    "title": "Generating Coherent Narratives by Learning Dynamic and Discrete Entity States with a Contrastive Framework",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b87a519690516c47aac03b1d794e4ae199812990",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26510": {
    "title": "Learning to Imagine: Distillation-Based Interactive Context Exploitation for Dialogue State Tracking",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "03b78c85b72b973f1109dc605bc0030a55febde9",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26511": {
    "title": "RenewNAT: Renewing Potential Translation for Non-autoregressive Transformer",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "14897d84d004b37b38ddbafc178e518ab31f616e",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26512": {
    "title": "Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5349d7d21d59633553ff7ce2d57376b8e5ddd698",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26513": {
    "title": "Competition or Cooperation? Exploring Unlabeled Data via Challenging Minimax Game for Semi-supervised Relation Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "16bbe866a849e538dc4cd2b213ca06e50595a23b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26514": {
    "title": "Feature Normalization and Cartography-Based Demonstrations for Prompt-Based Fine-Tuning on Emotion-Related Tasks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "230137d02910e43a9d161e21af24b80fd94d351e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26515": {
    "title": "A Simple Yet Effective Subsequence-Enhanced Approach for Cross-Domain NER",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "593434904a5fe302aa876d7736a737c86e795724",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26516": {
    "title": "A Question-Answering Approach to Key Value Pair Extraction from Form-Like Document Images",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f5ddb5440ac6eb46da1b91b6bfe247c3a45eaaf1",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26517": {
    "title": "SEAT: Stable and Explainable Attention",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e2f4c22090073ddd63708bea61de1e5a5bc905f4",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26518": {
    "title": "Personalized Dialogue Generation with Persona-Adaptive Attention",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c66b4f762f94b8d3532f532a401c67c2d35e2546",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26519": {
    "title": "Question Decomposition Tree for Answering Complex Questions over Knowledge Bases",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9da75352f8d5dc289f830096edadae1849444f53",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26520": {
    "title": "Hierarchical Text Classification as Sub-hierarchy Sequence Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "22c963ff1488e009e119f6137657ddfe321affbe",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26521": {
    "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian Languages",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5b870ccad0746225c71cf1167d929d80e6db47f6",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26522": {
    "title": "SheetPT: Spreadsheet Pre-training Based on Hierarchical Attention Network",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7617b8476756479789a1a0964427b5a81024e123",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26523": {
    "title": "SeDepTTS: Enhancing the Naturalness via Semantic Dependency and Local Convolution for Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "08bdd31c50de29d7b69e2622ddfa6e7e842be075",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26524": {
    "title": "Prototypical Fine-Tuning: Towards Robust Performance under Varying Data Sizes",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5cbd6a61a072567511789646407ea16934b10baa",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26525": {
    "title": "Cross-Modal Distillation for Speaker Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "496f94f427a63ec0e6c1a1cbab9aea995ecc9c33",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26526": {
    "title": "Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "23cb0c2f27df7d92d073c99a2c101789a5454160",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26527": {
    "title": "COCA: COllaborative CAusal Regularization for Audio-Visual Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "14a208be5919ab52b8b2fc732f358295a7bc203f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26528": {
    "title": "Script, Language, and Labels: Overcoming Three Discrepancies for Low-Resource Language Specialization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6f763bd76f48c9f34fa0b9cd2ca56594566f47f4",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26529": {
    "title": "LIQUID: A Framework for List Question Answering Dataset Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "3b0424149731d10829015cb4ab6299d18162128e",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26530": {
    "title": "UniSyn: An End-to-End Unified Model for Text-to-Speech and Singing Voice Synthesis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "304a8668a44730e9d12df8a479c0257c2a194190",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26531": {
    "title": "SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "09299aa885d4cef2c391c41b3f3ee444f3c00414",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26532": {
    "title": "Sequence Generation with Label Augmentation for Relation Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "85ca3f50c60aa13237bfa2d46d9fc3c42ef5049c",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26533": {
    "title": "Reviewing Labels: Label Graph Network with Top-k Prediction Set for Relation Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e118becc25034e5ba287f11b2c72ebaf765f870b",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26534": {
    "title": "Online Noisy Continual Relation Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1406c1a0c9d0e855e5bf16b8303f8cde86e8fb2c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26535": {
    "title": "RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6d51f4b220cb2c8321dc5f9755b7d66f10f1cad6",
    "semantic_title": "",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26536": {
    "title": "Graphix-T5: Mixing Pre-trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "dcffef7c94546389c02c837e0e9290039938b4d2",
    "semantic_title": "",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26537": {
    "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8b150819910d9e05eebfa746b7648e4dc2c5b7c9",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26538": {
    "title": "TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "de688c6e73ccf6ed33ff1cc7919d24456a1f74e2",
    "semantic_title": "",
    "citation_count": 78
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26539": {
    "title": "Mitigating Negative Style Transfer in Hybrid Dialogue System",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "18220734faa6a355e949cf101aa1e2a8463c898c",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26540": {
    "title": "Low Resource Quantitative Information Extraction via Structure Searching and Prefix-Based Text Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c78b90b1462a7d3c50c5d2910c8e95243ea54da4",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26541": {
    "title": "SKIER: A Symbolic Knowledge Integrated Model for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d8088d6b45e3ae523b9472745971f698a3b1d7f5",
    "semantic_title": "",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26542": {
    "title": "PGSS: Pitch-Guided Speech Separation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1e3e6d16bf4278815ad26d4388c3edf46c1486df",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26543": {
    "title": "DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26544": {
    "title": "Heterogeneous-Branch Collaborative Learning for Dialogue Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "86dcd6268324b65960f0ac6287dabf155647c5f7",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26545": {
    "title": "Learning to Know Myself: A Coarse-to-Fine Persona-Aware Training Framework for Personalized Dialogue Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "775cfa30a26a723c8c4fd1dca63baed24067e17e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26546": {
    "title": "WIERT: Web Information Extraction via Render Tree",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4e5923251039ea4b118f29a3abd689b1ebfbedb0",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26547": {
    "title": "STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "80e01784da4c21687a67c0b68d03592d3d879ee5",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26548": {
    "title": "Generalizing Math Word Problem Solvers via Solution Diversification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6d686c2232d05b92b1549a36df24720c5d4bedd1",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26549": {
    "title": "On Grounded Planning for Embodied Tasks with Language Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
    "semantic_title": "",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26550": {
    "title": "DeAR: A Deep-Learning-Based Audio Re-recording Resilient Watermarking",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "74aa68c5ca4fef1ce13b14920427b5c229e25dd1",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26551": {
    "title": "Detecting and Grounding Important Characters in Visual Stories",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8f51efde4213a3970ed498ee8a29a0e2b6b4d9fc",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26552": {
    "title": "Boosting Few-Shot Text Classification via Distribution Estimation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0bf970a8a5a580259891cdb40ebf8914e612ebd5",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26553": {
    "title": "SSPAttack: A Simple and Sweet Paradigm for Black-Box Hard-Label Textual Adversarial Attack",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1500c256a84a4746a70d670cdad4bf697196d5a2",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26554": {
    "title": "LADA-Trans-NER: Adaptive Efficient Transformer for Chinese Named Entity Recognition Using Lexicon-Attention and Data-Augmentation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8dddf88fe5de9e01b106cfb8760c725f90ade56c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26555": {
    "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "097490170c4691eec8844bf6454ff580f1151eec",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26556": {
    "title": "A Disentangled-Attention Based Framework with Persona-Aware Prompt Learning for Dialogue Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6b36b4ef56d9ac2eb622c331610f23d6e3f6799d",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26557": {
    "title": "Towards Credible Human Evaluation of Open-Domain Dialog Systems Using Interactive Setup",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f1eeba530be0ac537bcdf633ee6c462045c97fb5",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26558": {
    "title": "Unsupervised Paraphrasing under Syntax Knowledge",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f40a60a240b6026266db73a432db2724693c14af",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26559": {
    "title": "Adjective Scale Probe: Can Language Models Encode Formal Semantics Information?",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "23073e785f52e6641a89ba15947910870da2b32a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26560": {
    "title": "Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "db28e98d530cb6bd771f71c40a18979bad1d4df1",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26561": {
    "title": "Learning Compositional Tasks from Language Instructions",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ec5c45b70b136c7b125fa48af6c1bae79327448d",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26562": {
    "title": "SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "13579f2d5d522779caeb4aa916fb9ab283f13670",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26563": {
    "title": "Universal Information Extraction as Unified Semantic Matching",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7ccfd5953e75f242536e99cdeda545a3c66ea98f",
    "semantic_title": "",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26564": {
    "title": "PUnifiedNER: A Prompting-Based Unified NER System for Diverse Datasets",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a1aba65b617ba2c9e82f09f20b04b70e5d60ce31",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26565": {
    "title": "KICE: A Knowledge Consolidation and Expansion Framework for Relation Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "31bd0e8883fe9d1a3d28482543d53f7b18077114",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26566": {
    "title": "Zero-Shot Slot Filling with Slot-Prefix Prompting and Attention Relationship Descriptor",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d590e0a539e1392dc4f17ae1327bf021e0c39f58",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26567": {
    "title": "Feature-Level Debiased Natural Language Understanding",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c205cea82ec5db88be8f466d5b3823e37cb8f341",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26568": {
    "title": "Graph Component Contrastive Learning for Concept Relatedness Estimation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2adc4f098c27c2a7a1cf43916a6122abcb2908f7",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26569": {
    "title": "HybridPrompt: Bridging Language Models and Human Priors in Prompt Tuning for Visual Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6470a35a46bbc8a844954af9fdf31e440d1aa289",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26570": {
    "title": "Inferential Knowledge-Enhanced Integrated Reasoning for Video Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9b35aca6a0c632bd9e773861b77b01111ea34518",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26571": {
    "title": "AUC Maximization for Low-Resource Named Entity Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "87778991ffea60f1d4e80d726335338270ecc2f6",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26572": {
    "title": "Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5c4bece777bbc7c52cdd4bbb6e222163f6a580dd",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26573": {
    "title": "Towards a Holistic Understanding of Mathematical Questions with Contrastive Pre-training",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bde8c53c106c79bb47858b8cff1982a897061cee",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26574": {
    "title": "Improving the Cross-Lingual Generalisation in Visual Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e2c167aeccf14d2bc6b8624f08ff432b000fdc25",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26575": {
    "title": "RWEN-TTS: Relation-Aware Word Encoding Network for Natural Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2fa74769118d2726c2ba2eabf1216d029d178960",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26576": {
    "title": "Hierarchical Event Grounding",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ff77105b2c345f54e1a87f4fbb3a701201f0c1a8",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26577": {
    "title": "RINK: Reader-Inherited Evidence Reranker for Table-and-Text Open Domain Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ee9d38a3f889c5030209c78b2378d8a17d2b716b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26578": {
    "title": "Relation-Aware Language-Graph Transformer for Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0136f2faaa902082d0e6ad15e5fd14f5842d2b14",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26579": {
    "title": "Multi-Mask Label Mapping for Prompt-Based Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "99dca83345b957ee3eae868570a2ce9af52f729a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26580": {
    "title": "SSMI: Semantic Similarity and Mutual Information Maximization Based Enhancement for Chinese NER",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d75126cf933f80dc46a9e2fe636199817e2d52c1",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26581": {
    "title": "Towards Complex Scenarios: Building End-to-End Task-Oriented Dialogue System across Multiple Knowledge Bases",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a4c1e39a7fff8f761a0682320d3e4cf5f3217a02",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26582": {
    "title": "BERT-ERC: Fine-Tuning BERT Is Enough for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "eecd7b3394fc68901db974a2ff8f950c3a7a7705",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26583": {
    "title": "Distantly-Supervised Named Entity Recognition with Adaptive Teacher Learning and Fine-Grained Student Ensemble",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5f305333006fb76fdf2875bd0f6b4a990d9a2028",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26584": {
    "title": "Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "11f734c937b74b4950d67f61ab822c81a3f86ac9",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26585": {
    "title": "Prompting Neural Machine Translation with Translation Memories",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "90e0457049d9ff589985312c539b22a97aa04f73",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26586": {
    "title": "Improving Interpretability via Explicit Word Interaction Graph Layer",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f64745927d4a6f048dd0cba8cce7ea8c0055d26c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26587": {
    "title": "Rephrasing the Reference for Non-autoregressive Machine Translation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8671a125e0aa2c1a97ff7940008e47759ec94967",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26588": {
    "title": "Drop Clause: Enhancing Performance, Robustness and Pattern Recognition Capabilities of the Tsetlin Machine",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "558e5ffc1fd2af35d1052e70dbfdb4efab7db5a1",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26589": {
    "title": "CoP: Factual Inconsistency Detection by Controlling the Preference",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "34f300edbed4e4cf57d3d9a0499578ce97c5cf00",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26590": {
    "title": "Which Shortcut Solution Do Question Answering Models Prefer to Learn?",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ad7c788824aba70deb141617410dd70b390c4ae8",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26591": {
    "title": "Exploring Faithful Rationale for Multi-Hop Fact Verification via Salience-Aware Graph Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "622ea8252a0c04e91870ed895d34fdc93ac52949",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26592": {
    "title": "A Speaker Turn-Aware Multi-Task Adversarial Network for Joint User Satisfaction Estimation and Sentiment Analysis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "cf3b0c1331f4aeb15218c5d55a46655a237afc78",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26593": {
    "title": "A Latent-Variable Model for Intrinsic Probing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4adba678d51bfa5f96b18b7c1fedb7372cf83068",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26594": {
    "title": "Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5f157aa29b4363d4ecba0a0afbdf03977a9f62d5",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26595": {
    "title": "ConvNTM: Conversational Neural Topic Model",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c16cfc3fd993d15d702186c66561a6afbdab170a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26596": {
    "title": "Contrastive Learning Reduces Hallucination in Conversations",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d7ea898cc97754e06d209df0fd55ab60250601f2",
    "semantic_title": "",
    "citation_count": 11
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26597": {
    "title": "Revisiting Denoising Diffusion Probabilistic Models for Speech Enhancement: Condition Collapse, Efficiency and Refinement",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "86acceb40f6fa368eb1c7f765f37f93f6947b9d6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26598": {
    "title": "SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1b4c0a28b0c2a30cc3b84a3222e795c90357bc8a",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26599": {
    "title": "Reducing Sentiment Bias in Pre-trained Sentiment Classification via Adaptive Gumbel Attack",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "45e175a1bde8553b02d1675a3467112c2dcb535f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26600": {
    "title": "Latent Constraints on Unsupervised Text-Graph Alignment with Information Asymmetry",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "322e76e2492dd7e136cf7f101a8992567cec12b3",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26601": {
    "title": "M-sense: Modeling Narrative Structure in Short Personal Narratives Using Protagonist's Mental Representations",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1d9bcccadf1cfa727125f9e745384311165cf4c9",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26602": {
    "title": "Taming Continuous Posteriors for Latent Variational Dialogue Policies",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d5ff8d416aa1f7027f494124380e084ecf876e36",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26603": {
    "title": "Uncertainty-Aware Self-Training for Low-Resource Neural Sequence Labeling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "317948521f3e8fc5542af8cebdb28df00cc5c8ff",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26604": {
    "title": "Disentangled CVAEs with Contrastive Learning for Explainable Recommendation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b826c9318686f6e51a2f25484ea995b709283639",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26605": {
    "title": "fmLRE: A Low-Resource Relation Extraction Model Based on Feature Mapping Similarity Calculation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4eee7b8ccb5ed06a644611a91ba43a0e876086be",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26606": {
    "title": "Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c557ac809df2bf55f3ebc7701ad06af0f6ce15e6",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26607": {
    "title": "Zero-Shot Face-Based Voice Conversion: Bottleneck-Free Speech Disentanglement in the Real-World Scenario",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "dd923be883e8899eacb92b8b3b9bbb94ec26078a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26608": {
    "title": "Adversarial Self-Attention for Language Understanding",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "924462895c1a380acddbde3c281c8849b458f995",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26609": {
    "title": "See How You Read? Multi-Reading Habits Fusion Reasoning for Multi-Modal Fake News Detection",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6911062acc602db1cc9f318b9407adb4778141d0",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26610": {
    "title": "Identify Event Causality with Knowledge and Analogy",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1cd83fa10592855f2e6b25f0b9c3fe226546ec0d",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26611": {
    "title": "Continual Graph Convolutional Network for Text Classification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a637239dc0c2d105c8c3d28ed07579a02aef2a3a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26612": {
    "title": "InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "36cfbdd71ca244c27e0be757f53e1696ab35f32e",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26613": {
    "title": "VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5a61cdc2fe6c9f6887c4bfc13cce10f8e6a855af",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26614": {
    "title": "Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "17ef526352ef379f8c9365d50e141ef153a49e7d",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26615": {
    "title": "AMOM: Adaptive Masking over Masking for Conditional Masked Language Model",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a8767413d9b969b04c9c440a30f6b7c6a9c4b1bc",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26616": {
    "title": "Global Mixup: Eliminating Ambiguity with Clustering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "38ea7063cf571bf59ab2bceae5865dd3a1499a44",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26617": {
    "title": "MoEC: Mixture of Expert Clusters",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1c1eaea99e0cbbd4616125049bae8c6787bd368c",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26618": {
    "title": "Factual and Informative Review Generation for Explainable Recommendation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "64c4d622b88a88b9f5ab5fdc6cb01f1ce9ccb883",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26619": {
    "title": "Dialogue Rewriting via Skeleton-Guided Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "35e6a386c486f2c57b7f0580b19d03f93c74413b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26620": {
    "title": "Dialogue State Distillation Network with Inter-slot Contrastive Learning for Dialogue State Tracking",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c4abda222d53e19b381f99e27afce74195d6a6cb",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26621": {
    "title": "Balanced Meta Learning and Diverse Sampling for Lifelong Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "037f360a564d81015ad2d404a6bb0cd3a1468088",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26622": {
    "title": "Selector-Enhancer: Learning Dynamic Selection of Local and Non-local Attention Operation for Speech Enhancement",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1829c937872bd509b1e2aceaa6c0bb92a9caad6e",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26623": {
    "title": "A Graph Fusion Approach for Cross-Lingual Machine Reading Comprehension",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8c36246da412610435d605063885646b279958de",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26624": {
    "title": "Improving Biomedical Entity Linking with Cross-Entity Interaction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "600a811935bedaf25d26b908fa6a79b4c55f1191",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26625": {
    "title": "Nested Named Entity Recognition as Building Local Hypergraphs",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d8b8d0109f9b02442b366b10517e99f15e08a277",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26626": {
    "title": "A Domain-Transfer Meta Task Design Paradigm for Few-Shot Slot Tagging",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b4481a16cfac0008e5b12a4e6d375df81d5a9fce",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26627": {
    "title": "Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5d1d01fd199aff94240e7452e25e915cfa7b77e7",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26628": {
    "title": "What Does Your Face Sound Like? 3D Face Shape towards Voice",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "16f703e017b3e39c2aa52577a5a1882f68b2c0ef",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26629": {
    "title": "FiTs: Fine-Grained Two-Stage Training for Knowledge-Aware Question Answering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "114655441607fbf58f5b174f2905a006b3853d91",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26630": {
    "title": "On the Calibration and Uncertainty with Pólya-Gamma Augmentation for Dialog Retrieval Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "14b47cc6ce6bb154dbe0cf0416cf4e0272c7cd38",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26631": {
    "title": "Preserve Context Information for Extract-Generate Long-Input Summarization Framework",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "51f8bdac8ab7ebeb99106f5e57aa047e82e0e847",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26632": {
    "title": "Transferable Post-hoc Calibration on Pretrained Transformers in Noisy Text Classification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "688baadc8b1902f524101a2231c6d4b58ee4bfd0",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26633": {
    "title": "Quantum-Inspired Representation for Long-Tail Senses of Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d704aefcf01d67af12e4281909b71a28f96c0829",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26634": {
    "title": "MPMQA: Multimodal Question Answering on Product Manuals",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5f93d785983a51efe1debf7793b02e27757d1bbd",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26635": {
    "title": "Exploring Self-Distillation Based Relational Reasoning Training for Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "77afb7c2279a4a6d162810e863fdddff4c1555a8",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26636": {
    "title": "Multi-Action Dialog Policy Learning from Logged User Feedback",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c9a37fb45b73fb167f2b598776b9ba51e1aaea79",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26637": {
    "title": "Improving End-to-End Speech Translation by Leveraging Auxiliary Speech and Text Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7b16564559db7b03051fa7f80ad5669d30e72471",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26638": {
    "title": "A Neural Span-Based Continual Named Entity Recognition Model",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "078de8a6e93c7679fd82bd4527681b2ff1963e25",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26639": {
    "title": "Language Model Pre-training on True Negatives",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e8033d571c2120f59f21b253e54805b5a2c96c63",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26640": {
    "title": "MCL: Multi-Granularity Contrastive Learning Framework for Chinese NER",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "01881c011e7babc706bea37bfaaa1294dbdd77fa",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26641": {
    "title": "Knowledge-Bridged Causal Interaction Network for Causal Emotion Entailment",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "97ba54394f69d5e2157d5c3e2b058cfe7999b3c9",
    "semantic_title": "",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26642": {
    "title": "Query Your Model with Definitions in FrameNet: An Effective Method for Frame Semantic Role Labeling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0ef797cae9ae6eb1e49b10cab8ccabd2b09d276c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26643": {
    "title": "Event Process Typing via Hierarchical Optimal Transport",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "77b8299c9455829db7f37116933e448255bd72d7",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26644": {
    "title": "Improving Distantly Supervised Relation Extraction by Natural Language Inference",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "dc7a97da9d46e3f4a087ce404a19b8af03109bde",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26645": {
    "title": "A Generative Approach for Script Event Prediction via Contrastive Fine-Tuning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2a34d4e279e8da66a5600b1943de07ac5ddee402",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26646": {
    "title": "KPT: Keyword-Guided Pre-training for Grounded Dialog Generation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ef52ba73fbf41eed320a479f5736e127d3a06049",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26647": {
    "title": "An Ensemble Distillation Framework for Sentence Embeddings with Multilingual Round-Trip Translation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "32eca437ef020e692d93cdf3727e7158277ec395",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26648": {
    "title": "COSMOS: Catching Out-of-Context Image Misuse Using Self-Supervised Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4110d8119a2f38c610cc8e608f7e8a7fe51ae8eb",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26649": {
    "title": "Med-EASi: Finely Annotated Dataset and Models for Controllable Simplification of Medical Texts",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "5584cfacf5f88efe5237b0478cb38f5b37f14458",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26650": {
    "title": "On the Challenges of Using Reinforcement Learning in Precision Drug Dosing: Delay and Prolongedness of Action Effects",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c2bd228ae10af1091cd8411ed121d723f8993a18",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26651": {
    "title": "On the Cost of Demographic Parity in Influence Maximization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "48b300c70b9f384b38044b52cf03ca587d21c0a7",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26652": {
    "title": "Improving Fairness in Information Exposure by Adding Links",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "487c75c2af91361706231b156baeca844643cdc3",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26653": {
    "title": "A Fair Incentive Scheme for Community Health Workers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4eeea5d32ce847d0e36e1ccb4de7dce6036128e3",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26654": {
    "title": "Rehabilitating Homeless: Dataset and Key Insights",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0ef9eca54f39083bec6ca92f6d4b42db8f122fb3",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26655": {
    "title": "Counterfactuals for the Future",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7657eb913f77e3d95f2ebcd5f8e55d77f856064a",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26656": {
    "title": "Towards Learning to Discover Money Laundering Sub-network in Massive Transaction Network",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d19060b433e298d126ec287b372d4bb6a14043da",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26657": {
    "title": "Estimating Geographic Spillover Effects of COVID-19 Policies from Large-Scale Mobility Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f4586e3dbd70218af8ef421fa9d4ebbf9aa0bd04",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26658": {
    "title": "Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a0f6868dd37a44a952708f8171f9aee1d9be4c84",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26659": {
    "title": "Leveraging Old Knowledge to Continually Learn New Classes in Medical Images",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e2b1cd2aab7a939b90292f442bb1d0b15eeef357",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26660": {
    "title": "SARAS-Net: Scale and Relation Aware Siamese Network for Change Detection",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "62a2cf6baa5a19a618bfd7f1be9b15c6319167ff",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26661": {
    "title": "Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "31373d8d0718050fc6f589070ba083726759da8d",
    "semantic_title": "",
    "citation_count": 6
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26662": {
    "title": "Critical Firms Prediction for Stemming Contagion Risk in Networked-Loans through Graph-Based Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "98feed9365d4449711c71130cc1e1549b3a5aa12",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26663": {
    "title": "GAN-Based Domain Inference Attack",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d9875feb6292f3c38a94b45c7037fc76ed8a11f6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26664": {
    "title": "Physics Guided Neural Networks for Time-Aware Fairness: An Application in Crop Yield Prediction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2e698a17e4f40caa6558adb8d55c06f738acab95",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26665": {
    "title": "Nothing Abnormal\": Disambiguating Medical Reports via Contrastive Knowledge Infusion",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "be641c0c6c8f83c84963e1254b2eda74e6a29d3f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26666": {
    "title": "MTDiag: An Effective Multi-Task Framework for Automatic Diagnosis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1082d010b8011fde2156945861ca9e454d2c7a7b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26667": {
    "title": "Walkability Optimization: Formulations, Algorithms, and a Case Study of Toronto",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "93472ae4c136f940e6cb9910e67782f88ad4b057",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26668": {
    "title": "Low Emission Building Control with Zero-Shot Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "141fa4c44a663705a483c0ee877e52d5158bdc93",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26669": {
    "title": "Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1aa02bc5585a2c52e2643bf8be714e98a2e7a852",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26670": {
    "title": "Taxonomizing and Measuring Representational Harms: A Look at Image Tagging",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "56dc4105b113a48405604bb63e4ee42080e50bdd",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26671": {
    "title": "Winning the CityLearn Challenge: Adaptive Optimization with Evolutionary Search under Trajectory-Based Guidance",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "20f4c75e9d2bd0e6c31a374b2224f175f115eb59",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26672": {
    "title": "Robust Planning over Restless Groups: Engagement Interventions for a Large-Scale Maternal Telehealth Program",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d3c81daa02b979f13e61249bdbf41f0607ed61fa",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26673": {
    "title": "Equivariant Message Passing Neural Network for Crystal Material Discovery",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1c8fc598455b8e4f7017849e4c7c7741f9b91df9",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26674": {
    "title": "Accurate Fairness: Improving Individual Fairness without Trading Accuracy",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b1221518c79c1f04c15393e3591639485e8a5852",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26675": {
    "title": "Point-to-Region Co-learning for Poverty Mapping at High Resolution Using Satellite Imagery",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "520a77e2dd848f56c7a7c40789c8d84a760ca1bd",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26676": {
    "title": "AirFormer: Predicting Nationwide Air Quality in China with Transformers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "3253a37dd7b6ebfbe22f22ad4bedc07b4b5b0f29",
    "semantic_title": "",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26677": {
    "title": "SimFair: A Unified Framework for Fairness-Aware Multi-Label Classification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7ccc297b2741090f30626e42a5d692029a764be6",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26678": {
    "title": "Human Mobility Modeling during the COVID-19 Pandemic via Deep Graph Diffusion Infomax",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "c56100ffa07b5a6a4ef58d796a6f915149821de5",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26679": {
    "title": "Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "580f78af4bc37d28cbe4faaf4fc25d83b8bc8286",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26680": {
    "title": "Task-Adaptive Meta-Learning Framework for Advancing Spatial Generalizability",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1140878a116edb1cacc844e97853f4431203541e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26681": {
    "title": "A Composite Multi-Attention Framework for Intraoperative Hypotension Early Warning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7e270239bda02a2355367df7f02deafb96920c54",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26682": {
    "title": "Bugs in the Data: How ImageNet Misrepresents Biodiversity",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "cc6d0b1d0a11071034b2ab8b9a778f41c51a2e89",
    "semantic_title": "",
    "citation_count": 12
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26683": {
    "title": "LUCID: Exposing Algorithmic Bias through Inverse Design",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ac886d6ccddcd3846f2e8ba59b9fc128013156ac",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26684": {
    "title": "Neighbor Auto-Grouping Graph Neural Networks for Handover Parameter Configuration in Cellular Network",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f680a111ecba951a08a03fa1ba3a46ab5c23c99e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26685": {
    "title": "Help Me Heal: A Reinforced Polite and Empathetic Mental Health and Legal Counseling Dialogue System for Crime Victims",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bc79ac3d5e1c05fbf0a4f165b8841defe6592417",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26686": {
    "title": "Carburacy: Summarization Models Tuning and Comparison in Eco-Sustainable Regimes with a Novel Carbon-Aware Accuracy",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4a8c10a38eb92267a30c9472a17409a57ba050ec",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26687": {
    "title": "Joint Self-Supervised Image-Volume Representation Learning with Intra-inter Contrastive Clustering",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "37ad559f8a5cdd982b7b7c196e82b02f7f51501a",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26688": {
    "title": "For the Underrepresented in Gender Bias Research: Chinese Name Gender Prediction with Heterogeneous Graph Attention Network",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0d4dea3b2540623a55a24201fe22738b852b42fe",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26689": {
    "title": "FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News Detection on Short Video Platforms",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "569d56db5a41bec03e810e71baa7f1c6c9daaebc",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26690": {
    "title": "EINNs: Epidemiologically-Informed Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "46c08e7571af2e9c160d76e9140510706dbc2589",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26691": {
    "title": "Counterfactual Fairness Is Basically Demographic Parity",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "564c0f5dda77fd8a6a9cd41a0ec88989d92181ad",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26692": {
    "title": "Detecting Anomalous Networks of Opioid Prescribers and Dispensers in Prescription Drug Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4cc0e3c82690b276f39826fbd6cf436d04e2c5ad",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26693": {
    "title": "Practical Disruption of Image Translation Deepfake Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8a570041fb80b01d0684e84c39c04434dc25a506",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26694": {
    "title": "Daycare Matching in Japan: Transfers and Siblings",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9272afcfbccd18c4ffca522ccee41fd691cf49ba",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26695": {
    "title": "City-Scale Pollution Aware Traffic Routing by Sampling Max Flows Using MCMC",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2800e4820ee12255799c77f1b0bbc957e79e10c9",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26696": {
    "title": "Weather2vec: Representation Learning for Causal Inference with Non-local Confounding in Air Pollution and Climate Studies",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "4a76c4b5591dcee0a2170ac12e0d5ad813beb713",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26697": {
    "title": "Evaluating Digital Agriculture Recommendations with Causal Inference",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fc28e7b6122f1ffec1078fec12734bbcefac1b19",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26698": {
    "title": "Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "dd9f31e29b2ec771f4b73bf3e09e4ddefb450559",
    "semantic_title": "",
    "citation_count": 7
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26699": {
    "title": "MixFairFace: Towards Ultimate Fairness via MixFair Adapter in Face Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7915a4fafae62df0b3d2a84d2b0fe6357c10a996",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26700": {
    "title": "PateGail: A Privacy-Preserving Mobility Trajectory Generator with Imitation Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "78aadaf3c191bbc63f97c238853b0d878b32a99f",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26701": {
    "title": "Noise Based Deepfake Detection via Multi-Head Relative-Interaction",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b2c2d4b4f0ca78f58247944b1e808ba0d4197e5d",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26702": {
    "title": "Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "05b151d453012b0e2af34d36e0c6b6c57589a01f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26703": {
    "title": "Privacy-Preserved Evolutionary Graph Modeling via Gromov-Wasserstein Autoregression",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7918cfcee1fdca473d2b3e5bb47ead4ab3d10b00",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26704": {
    "title": "Auto-CM: Unsupervised Deep Learning for Satellite Imagery Composition and Cloud Masking Using Spatio-Temporal Dynamics",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "ce11f46a1d3262628578525f7597687afc115d32",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26705": {
    "title": "ERASER: AdvERsArial Sensitive Element Remover for Image Privacy Preservation",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "14bce1bc4fa59c0584a4ca10d10ce9df337426fb",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26706": {
    "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a45ff2a8b18abc850b267cf0ec6e391dba9138a5",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26707": {
    "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "aa394cd41651027edd70bdd1f7f909e648bc5a2e",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26708": {
    "title": "Censored Fairness through Awareness",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "1c8f9a06fd998a582e95d3ff2d237b9c322c1851",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26709": {
    "title": "A Continual Pre-training Approach to Tele-Triaging Pregnant Women in Kenya",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "42aa79b825616927348b03fab5eb33a38750ab44",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26710": {
    "title": "Future Aware Pricing and Matching for Sustainable On-Demand Ride Pooling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "131b6d02fa5080e322e83023c1d772738e82e5cf",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26711": {
    "title": "A Crowd-AI Collaborative Duo Relational Graph Learning Framework towards Social Impact Aware Photo Classification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fa8762d3e3811563a948096967fccbf3e7392a87",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26712": {
    "title": "People Taking Photos That Faces Never Share: Privacy Protection and Fairness Enhancement from Camera to User",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "57ff9d8e827c726659750509157f220478030e86",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26713": {
    "title": "OpenMapFlow: A Library for Rapid Map Creation with Machine Learning and Remote Sensing Data",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b6b328d3ba12aea981cea10c059ae92c0cba936e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26714": {
    "title": "Formally Verified SAT-Based AI Planning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "3e95e6ef0aa1ce98c38715ddbe7b8a3d838b844d",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26715": {
    "title": "Shielding in Resource-Constrained Goal POMDPs",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6745cb3cf6b32aae0ab906c15677726988fc03c3",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26716": {
    "title": "Implicit Bilevel Optimization: Differentiating through Bilevel Optimization Programming",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "482b6ff532ba5466fbc42faea4a2840b4c425539",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26717": {
    "title": "Query-Based Hard-Image Retrieval for Object Detection at Test Time",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2a51f94fe7750a010f2e86e2936eca3cbde51796",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26718": {
    "title": "Probabilities Are Not Enough: Formal Controller Synthesis for Stochastic Dynamical Models with Epistemic Uncertainty",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9342c9886030283ca507c7a34a03ae5b57537994",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26719": {
    "title": "Accelerating Inverse Learning via Intelligent Localization with Exploratory Sampling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "123c93e88781f2602e8558b87e80c88c4b3ab529",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26720": {
    "title": "Attention-Conditioned Augmentations for Self-Supervised Anomaly Detection and Localization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fd6ca61726bc0598d25e3edd4ac98c013deebcb7",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26721": {
    "title": "Robust-by-Design Classification via Unitary-Gradient Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "416be43f270134a37dc2d42ccc469737e5780eb1",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26722": {
    "title": "Ensemble-in-One: Ensemble Learning within Random Gated Networks for Enhanced Adversarial Robustness",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fe531d5918d3cc1b3ef950d154017b8c8f3f408f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26723": {
    "title": "Safe Reinforcement Learning via Shielding under Partial Observability",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2eced32191850ddfbdbc4ef9e2a8059e99e7f84a",
    "semantic_title": "",
    "citation_count": 9
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26724": {
    "title": "PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9eca0adbd644755d07ee1f503f7324c29d0ec907",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26725": {
    "title": "Two Wrongs Don't Make a Right: Combating Confirmation Bias in Learning with Label Noise",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0db4dba94d9412d3202afe2dbd76b74b50303831",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26726": {
    "title": "Testing the Channels of Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "163e075f1a0ab9545269e5d5b8f4a1e6183303b3",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26727": {
    "title": "Feature-Space Bayesian Adversarial Learning Improved Malware Detector Robustness",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "6eaa3bbf5e36e2ac3ff1ba3dc564653c8d066744",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26728": {
    "title": "Correct-by-Construction Reinforcement Learning of Cardiac Pacemakers from Duration Calculus Requirements",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "aa4a1a758e4e15eef46c5009c75092b1ba7a39b2",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26729": {
    "title": "SafeLight: A Reinforcement Learning Method toward Collision-Free Traffic Signal Control",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9683ac196f269461cd9a1018dfbf613ca336ca5a",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26730": {
    "title": "PatchNAS: Repairing DNNs in Deployment with Patched Network Architecture Search",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9a5247c8fb2d55d202489e38fe85abcceb433153",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26731": {
    "title": "Similarity Distribution Based Membership Inference Attack on Person Re-identification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "748e1978a9a9d940ae30a108f5824ab25071b664",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26732": {
    "title": "Out-of-Distribution Detection Is Not All You Need",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "2fdf63af2c04ac613b2acaf628d4a5dc60e84c8e",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26733": {
    "title": "Contrastive Self-Supervised Learning Leads to Higher Adversarial Susceptibility",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "91fa90034eb88bc31c46d4e4fa242c3b3c16b2f1",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26734": {
    "title": "AutoCost: Evolving Intrinsic Cost for Zero-Violation Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b65e9e7f500437a01848ac6fde6692e6b241de85",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26735": {
    "title": "Test Time Augmentation Meets Post-hoc Calibration: Uncertainty Quantification under Real-World Conditions",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "85d4f8c0e580f7cdacae5419eb58d4679787f830",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26736": {
    "title": "Robust Training of Neural Networks against Bias Field Perturbations",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b9f72ff47451132613e719ef7f08e414c786be79",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26737": {
    "title": "Redactor: A Data-Centric and Individualized Defense against Inference Attacks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8ef83ee73085e3db4914912fe8d5c483634cc063",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26738": {
    "title": "Improving Adversarial Robustness with Self-Paced Hard-Class Pair Reweighting",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "cdff55a9ffadc668dd5abdfbdb35f0280e1447e9",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26739": {
    "title": "CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
    "semantic_title": "",
    "citation_count": 8
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26740": {
    "title": "Formalising the Robustness of Counterfactual Explanations for Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a7e57f485f2dfe7d1a599cf976b2a7caaa424f2f",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26741": {
    "title": "READ: Aggregating Reconstruction Error into Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9669a87028cef17a9e5d30991502eae5f8179291",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26742": {
    "title": "Sample-Dependent Adaptive Temperature Scaling for Improved Calibration",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "63916bb5363d37b9a3adfd1a56dee3710190fce1",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26743": {
    "title": "Heuristic Search in Dual Space for Constrained Fixed-Horizon POMDPs with Durative Actions",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "faec5b3e8650bdd2f4dbc5b2ad7385ae3907ec3f",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26744": {
    "title": "Iteratively Enhanced Semidefinite Relaxations for Efficient Neural Network Verification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "3c906f453942ece03d97c598d6b2d312a9e7ff2e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26745": {
    "title": "A Semidefinite Relaxation Based Branch-and-Bound Method for Tight Neural Network Verification",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "621bba60ab56347038696b287b21c2bd8132b16b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26746": {
    "title": "Robust Image Steganography: Hiding Messages in Frequency Coefficients",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a5d92ccf035048dd8260f6d1dea216c9115eb841",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26747": {
    "title": "Quantization-Aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "54957e64705af01b3490ea0c84383fa2500e0f2b",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26748": {
    "title": "Revisiting the Importance of Amplifying Bias for Debiasing",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "76a4272b3008ebeb66631681da2a6b9f77d0b6c8",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26749": {
    "title": "WAT: Improve the Worst-Class Robustness in Adversarial Training",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a9a0b9be86acb372d78f349b84a7f21fa2f53919",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26750": {
    "title": "PLMmark: A Secure and Robust Black-Box Watermarking Framework for Pre-trained Language Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "33a555467eda19fa2a8661c268b0bbab0eb69831",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26751": {
    "title": "Rethinking Label Refurbishment: Model Robustness under Label Noise",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "35b2f215847a2773f1dc1f232d2fd91f63d8354c",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26752": {
    "title": "A Holistic Approach to Undesired Content Detection in the Real World",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b69aa32ee52f5efe8e3196114581ac610da8a2b2",
    "semantic_title": "",
    "citation_count": 26
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26753": {
    "title": "A Risk-Sensitive Approach to Policy Optimization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d37ca9aa15d6f34d942180752552132c51fe27e5",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26754": {
    "title": "Anonymization for Skeleton Action Recognition",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "8035ac46f303e67b47aeb4a44ef52ad049481f7e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26755": {
    "title": "Monitoring Model Deterioration with Explainable Uncertainty Estimation via Non-parametric Bootstrap",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "42de5c885c5774839b3f08e0b0c528bc186f880f",
    "semantic_title": "",
    "citation_count": 5
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26756": {
    "title": "Certified Policy Smoothing for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "11ed9f34b8f733ab17bf6fd8aa3686253a1d35be",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26757": {
    "title": "Constrained Reinforcement Learning in Hard Exploration Problems",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bc5bd7449e6a8c03e10c433dd65dfeee08262041",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26758": {
    "title": "Defending from Physically-Realizable Adversarial Attacks through Internal Over-Activation Analysis",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "b2c47f408cd89f1d97f57164e62ae43f245ec9ac",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26759": {
    "title": "Formally Verified Solution Methods for Markov Decision Processes",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "9fa16e38d704423feb31b35e4cd63f82d8806153",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26760": {
    "title": "Improving Training and Inference of Face Recognition Models via Random Temperature Scaling",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "98548a4f6f443cf2d7b13608941963caaaa0c352",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26761": {
    "title": "Task and Model Agnostic Adversarial Attack on Graph Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "a0755a1dc1fd8314fa3c669e54edbf1adc2f1b68",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26762": {
    "title": "Robust Sequence Networked Submodular Maximization",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0273024ab212b2fe1538b300f6ba1a424a58b52e",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26763": {
    "title": "Safe Policy Improvement for POMDPs via Finite-State Controllers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "f738aff510123771161ac0f38b218f51ac9eb7a0",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26764": {
    "title": "STL-Based Synthesis of Feedback Controllers Using Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "0aec8ed21d4b34112679a39d7070f4503d61b9d8",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26765": {
    "title": "Understanding and Enhancing Robustness of Concept-Based Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "e63b3f2ac5eee86c10e2265b483957dc27d970ee",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26766": {
    "title": "Misspecification in Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "aae382fe95b061018f214b1f101c55b6f4ae176b",
    "semantic_title": "",
    "citation_count": 4
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26767": {
    "title": "Planning and Learning for Non-markovian Negative Side Effects Using Finite State Controllers",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "95bbfe4bf3d83dd364ee07e6f5d65262107436c6",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26768": {
    "title": "Toward Robust Uncertainty Estimation with Random Activation Functions",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "bb1d563d52722cd148c91e24bc218bccd65ef13a",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26769": {
    "title": "Improving Robust Fariness via Balance Adversarial Training",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "a1b66604fc3464ad4041480ac93a3a40f3fff71e",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26770": {
    "title": "DPAUC: Differentially Private AUC Computation in Federated Learning",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "7db4c1b880da98466ea118af5c81d6c3d68e4986",
    "semantic_title": "",
    "citation_count": 3
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26771": {
    "title": "Conflicting Interactions among Protection Mechanisms for Machine Learning Models",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": false,
    "id": "0e5eabc075d569ba56661efcaa913ed0023daba8",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26772": {
    "title": "Neural Policy Safety Verification via Predicate Abstraction: CEGAR",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "d5f5a9f535d80c57e75539720750faab6c779f37",
    "semantic_title": "",
    "citation_count": 1
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26773": {
    "title": "Towards Verifying the Geometric Robustness of Large-Scale Neural Networks",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "49063a59bf80d92ecc4b58f0e44e824affe4d734",
    "semantic_title": "",
    "citation_count": 2
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26774": {
    "title": "Revisiting Item Promotion in GNN-Based Collaborative Filtering: A Masked Targeted Topological Attack Perspective",
    "volume": "main",
    "abstract": "Reconstructing a High Dynamic Range (HDR) image from several Low Dynamic Range (LDR) images with different exposures is a challenging task, especially in the presence of camera and object motion. Though existing models using convolutional neural networks (CNNs) have made great progress, challenges still exist, e.g., ghosting artifacts. Transformers, originating from the field of natural language processing, have shown success in computer vision tasks, due to their ability to address a large receptive field even within a single layer. In this paper, we propose a transformer model for HDR imaging. Our pipeline includes three steps: alignment, fusion, and reconstruction. The key component is the HDR transformer module. Through experiments and ablation studies, we demonstrate that our model outperforms the state-of-the-art by large margins on several popular public datasets",
    "checked": true,
    "id": "fe9af9c40ff93ffbc9536964e209bf2080d241b1",
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26775": {
    "title": "Robust Average-Reward Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26776": {
    "title": "Robust Graph Meta-Learning via Manifold Calibration with Proxy Subgraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26777": {
    "title": "HOTCOLD Block: Fooling Thermal Infrared Detectors with a Novel Wearable Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26778": {
    "title": "Beyond NaN: Resiliency of Optimization Layers in the Face of Infeasibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26779": {
    "title": "DeepGemini: Verifying Dependency Fairness for Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26780": {
    "title": "Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26781": {
    "title": "User-Oriented Robust Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26782": {
    "title": "Safety Verification of Nonlinear Systems with Bayesian Neural Network Controllers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26783": {
    "title": "Reachability Analysis of Neural Network Control Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26784": {
    "title": "BIFRNet: A Brain-Inspired Feature Restoration DNN for Partially Occluded Image Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26785": {
    "title": "Robustness to Spurious Correlations Improves Semantic Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26786": {
    "title": "Evaluating Model-Free Reinforcement Learning toward Safety-Critical Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26787": {
    "title": "Video-Audio Domain Generalization via Confounder Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26788": {
    "title": "Rethinking Safe Control in the Presence of Self-Seeking Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26789": {
    "title": "Towards Safe AI: Sandboxing DNNs-Based Controllers in Stochastic Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26790": {
    "title": "Probabilistic Programs as an Action Description Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26791": {
    "title": "Foundations of Cooperative AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26792": {
    "title": "Multimodal Propaganda Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26793": {
    "title": "Foundation Model for Material Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26794": {
    "title": "QA Is the New KR: Question-Answer Pairs as Knowledge Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26795": {
    "title": "Customer Service Combining Human Operators and Virtual Agents: A Call for Multidisciplinary AI Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26796": {
    "title": "The Many Faces of Adversarial Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26797": {
    "title": "Holistic Adversarial Robustness of Deep Learning Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26798": {
    "title": "Can We Trust Fair-AI?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26799": {
    "title": "Safety Validation of Learning-Based Autonomous Systems: A Multi-Fidelity Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26800": {
    "title": "Probabilistic Reasoning and Learning for Trustworthy AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26801": {
    "title": "The Automatic Computer Scientist",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26802": {
    "title": "Perception for General-purpose Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26803": {
    "title": "Cooperative Multi-Agent Learning in a Complex World: Challenges and Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26804": {
    "title": "Distributed Stochastic Nested Optimization for Emerging Machine Learning Models: Algorithm and Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26805": {
    "title": "Targeted Knowledge Infusion To Make Conversational AI Explainable and Safe",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26806": {
    "title": "Accountability Layers: Explaining Complex System Failures by Parts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26807": {
    "title": "Generative Decision Making Under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26808": {
    "title": "Food Information Engineering: A Systematic Literature Review",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26809": {
    "title": "Better Environments for Better AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26810": {
    "title": "Recent Developments in Data-Driven Algorithms for Discrete Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26811": {
    "title": "Advances in AI for Safety, Equity, and Well-Being on Web and Social Media: Detection, Robustness, Attribution, and Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26812": {
    "title": "Intelligent Planning for Large-Scale Multi-Robot Coordination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26813": {
    "title": "Robust and Adaptive Deep Learning via Bayesian Principles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26814": {
    "title": "AAAI New Faculty Highlights: General and Scalable Optimization for Robust AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26815": {
    "title": "Combining Runtime Monitoring and Machine Learning with Human Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26816": {
    "title": "Towards Safe and Resilient Autonomy in Multi-Robot Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26817": {
    "title": "Monitoring and Intervening on Large Populations of Weakly Coupled Processes with Social Impact Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26818": {
    "title": "Internal Robust Representations for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26819": {
    "title": "Planning and Learning for Reliable Autonomy in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26820": {
    "title": "Dynamics of Cooperation and Conflict in Multiagent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26821": {
    "title": "Combating Disinformation on Social Media and Its Challenges: A Computational Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26822": {
    "title": "Human-Aware AI – A Foundational Framework for Human-AI Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26823": {
    "title": "Towards Unified, Explainable, and Robust Multisensory Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26824": {
    "title": "Reshaping State-Space Search: From Dominance to Contrastive Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26825": {
    "title": "Artificial Intelligence at the Service of Society to Analyse Human Arguments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26826": {
    "title": "AI for Equitable, Data-Driven Decisions in Public Health",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26827": {
    "title": "Learning to See the Physical World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26828": {
    "title": "Enhance Robustness of Machine Learning with Improved Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26829": {
    "title": "The Analysis of Deep Neural Networks by Information Theory: From Explainability to Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26830": {
    "title": "Towards Societal Impact of AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26831": {
    "title": "Information Transfer in Multitask Learning, Data Augmentation, and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26832": {
    "title": "A New Challenge in Policy Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26833": {
    "title": "Building Compositional Robot Autonomy with Modularity and Abstraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26834": {
    "title": "Accurate Detection of Weld Seams for Laser Welding in Real-World Manufacturing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26835": {
    "title": "Blending Advertising with Organic Content in E-commerce via Virtual Bids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26836": {
    "title": "Efficient Training of Large-Scale Industrial Fault Diagnostic Models through Federated Opportunistic Block Dropout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26837": {
    "title": "AmnioML: Amniotic Fluid Segmentation and Volume Prediction with Uncertainty Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26838": {
    "title": "A Robust and Scalable Stacked Ensemble for Day-Ahead Forecasting of Distribution Network Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26839": {
    "title": "Developing the Wheel Image Similarity Application with Deep Metric Learning: Hyundai Motor Company Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26841": {
    "title": "NewsPanda: Media Monitoring for Timely Conservation Action",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26842": {
    "title": "Trustworthy Residual Vehicle Value Prediction for Auto Finance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26843": {
    "title": "A Dataset and Baseline Approach for Identifying Usage States from Non-intrusive Power Sensing with MiDAS IoT-Based Sensors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26844": {
    "title": "Real-Time Detection of Robotic Traffic in Online Advertising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26845": {
    "title": "Dynamic Pricing with Volume Discounts in Online Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26846": {
    "title": "An Explainable Forecasting System for Humanitarian Needs Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26847": {
    "title": "Industry-Scale Orchestrated Federated Learning for Drug Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26848": {
    "title": "THMA: Tencent HD Map AI System for Creating HD Map Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26849": {
    "title": "Increasing Impact of Mobile Health Programs: SAHELI for Maternal and Child Care",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26850": {
    "title": "MuMIC – Multimodal Embedding for Multi-Label Image Classification with Tempered Sigmoid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26852": {
    "title": "AHPA: Adaptive Horizontal Pod Autoscaling Systems on Alibaba Cloud Container Service for Kubernetes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26853": {
    "title": "eForecaster: Unifying Electricity Forecasting with Robust, Flexible, and Explainable Machine Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26854": {
    "title": "Cosmic Microwave Background Recovery: A Graph-Based Bayesian Convolutional Network Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26855": {
    "title": "Phase-Informed Bayesian Ensemble Models Improve Performance of COVID-19 Forecasts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26856": {
    "title": "Towards Hybrid Automation by Bootstrapping Conversational Interfaces for IT Operation Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26857": {
    "title": "Compressing Cross-Lingual Multi-Task Models at Qualtrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26858": {
    "title": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics Manufacturing Using Explainable Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26859": {
    "title": "MobilePTX: Sparse Coding for Pneumothorax Detection Given Limited Training Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26860": {
    "title": "Vessel-to-Vessel Motion Compensation with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26840": {
    "title": "Detecting VoIP Data Streams: Approaches Using Hidden Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26861": {
    "title": "Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26862": {
    "title": "Towards Safe Mechanical Ventilation Treatment Using Deep Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26863": {
    "title": "Data-Driven Machine Learning Models for a Multi-Objective Flapping Fin Unmanned Underwater Vehicle Control System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26864": {
    "title": "AnimateSVG: Autonomous Creation and Aesthetics Evaluation of Scalable Vector Graphics Animations for the Case of Brand Logos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26865": {
    "title": "Grape Cold Hardiness Prediction via Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26866": {
    "title": "Reward Design for an Online Reinforcement Learning Algorithm Supporting Oral Self-Care",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26867": {
    "title": "Embedding a Long Short-Term Memory Network in a Constraint Programming Framework for Tomato Greenhouse Optimisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26868": {
    "title": "Fault Injection Based Interventional Causal Learning for Distributed Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26869": {
    "title": "High-Throughput, High-Performance Deep Learning-Driven Light Guide Plate Surface Visual Quality Inspection Tailored for Real-World Manufacturing Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26870": {
    "title": "End-to-End Pipeline for Trigger Detection on Hit and Track Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26851": {
    "title": "OPRADI: Applying Security Game to Fight Drive under the Influence in Real-World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26871": {
    "title": "Xaitk-Saliency: An Open Source Explainable AI Toolkit for Saliency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26872": {
    "title": "DetAIL: A Tool to Automatically Detect and Analyze Drift in Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26873": {
    "title": "PARCS: A Deployment-Oriented AI System for Robust Parcel-Level Cropland Segmentation of Satellite Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26874": {
    "title": "Adaptive Temporal Planning for Multi-Robot Systems in Operations and Maintenance of Offshore Wind Farms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26875": {
    "title": "A Study of Students' Learning of Computing through an LP-Based Integrated Curriculum for Middle Schools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26876": {
    "title": "AI and Parallelism in CS1: Experiences and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26877": {
    "title": "Shared Tasks as Tutorials: A Methodical Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26878": {
    "title": "Maestro: A Gamified Platform for Teaching AI Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26879": {
    "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26880": {
    "title": "An Analysis of Engineering Students' Responses to an AI Ethics Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26881": {
    "title": "Autonomous Agents: An Advanced Course on AI Integration and Deployment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26882": {
    "title": "AI Made by Youth: A Conversational AI Curriculum for Middle School Summer Camps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26883": {
    "title": "Learning Affects Trust: Design Recommendations and Concepts for Teaching Children—and Nearly Anyone—about Conversational Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26884": {
    "title": "FOLL-E: Teaching First Order Logic to Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26885": {
    "title": "Responsible Robotics: A Socio-Ethical Addition to Robotics Courses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26886": {
    "title": "Data Labeling for Machine Learning Engineers: Project-Based Curriculum and Data-Centric Competitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26887": {
    "title": "Does Knowing When Help Is Needed Improve Subgoal Hint Performance in an Intelligent Data-Driven Logic Tutor?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26888": {
    "title": "Ripple: Concept-Based Interpretation for Raw Time Series Models in Education",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26889": {
    "title": "Exploring Tradeoffs in Automated School Redistricting: Computational and Ethical Perspectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27091": {
    "title": "A Dataset for Learning University STEM Courses at Scale and Generating Questions at a Human Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26891": {
    "title": "Learning Logical Reasoning Using an Intelligent Tutoring System: A Hybrid Approach to Student Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26892": {
    "title": "Context-Aware Analysis of Group Submissions for Group Anomaly Detection and Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26893": {
    "title": "CLGT: A Graph Transformer for Student Performance Prediction in Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26894": {
    "title": "H-AES: Towards Automated Essay Scoring for Hindi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26895": {
    "title": "Detecting Exclusive Language during Pair Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26896": {
    "title": "Solving Math Word Problems concerning Systems of Equations with GPT-3",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26897": {
    "title": "AI Audit: A Card Game to Reflect on Everyday AI Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26898": {
    "title": "Beyond Black-Boxes: Teaching Complex Machine Learning Ideas through Scaffolded Interactive Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26899": {
    "title": "Exploring Artificial Intelligence in English Language Arts with StoryQ",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26900": {
    "title": "An Introduction to Rule-Based Feature and Object Perception for Middle School Students",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26901": {
    "title": "Scratch for Sports: Athletic Drills as a Platform for Experiencing, Understanding, and Developing AI-Driven Apps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26902": {
    "title": "How Can I Code A.I. Responsibly?\": The Effect of Computational Action on K-12 Students Learning and Creating Socially Responsible A.I",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26903": {
    "title": "Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26904": {
    "title": "Develop AI Teaching and Learning Resources for Compulsory Education in China",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26905": {
    "title": "Guiding Students to Investigate What Google Speech Recognition Knows about Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26906": {
    "title": "Literacy and STEM Teachers Adapt AI Ethics Curriculum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26907": {
    "title": "MoMusic: A Motion-Driven Human-AI Collaborative Music Composition and Performing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26908": {
    "title": "A Multi-User Virtual World with Music Recommendations and Mood-Based Virtual Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26909": {
    "title": "Learning Adaptive Game Soundtrack Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26910": {
    "title": "Predicting Perceived Music Emotions with Respect to Instrument Combinations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26911": {
    "title": "Emotion-Aware Music Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26912": {
    "title": "Music-to-Facial Expressions: Emotion-Based Music Visualization for the Hearing Impaired",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26913": {
    "title": "Model AI Assignments 2023",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26914": {
    "title": "Probabilistic Shape Models of Anatomy Directly from Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26915": {
    "title": "Modeling Strategies as Programs: How to Study Strategy Differences in Intelligent Systems with Program Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26916": {
    "title": "Non-exponential Reward Discounting in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26917": {
    "title": "Enhancing Smart, Sustainable Mobility with Game Theory and Multi-Agent Reinforcement Learning With Applications to Ridesharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26918": {
    "title": "Assessing Learned Representations under Open-World Novelty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26919": {
    "title": "Efficient Non-parametric Neural Density Estimation and Its Application to Outlier and Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26920": {
    "title": "Explaining the Uncertainty in AI-Assisted Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26921": {
    "title": "Poisoning-Based Backdoor Attacks in Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26922": {
    "title": "Safe Interactive Autonomy for Multi-Agent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26923": {
    "title": "Theory of Mind: A Familiar Aspect of Humanity to Give Machines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26924": {
    "title": "Multimodal Deep Generative Models for Remote Medical Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26925": {
    "title": "Topics in Selective Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26926": {
    "title": "Knowledge-Embedded Narrative Construction from Open Source Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26927": {
    "title": "Learning Better Representations Using Auxiliary Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26928": {
    "title": "Embodied, Intelligent Communication for Multi-Agent Cooperation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26929": {
    "title": "Meta Learning in Decentralized Neural Networks: Towards More General AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26930": {
    "title": "Learning and Planning under Uncertainty for Conservation Decisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26931": {
    "title": "Failure-Resistant Intelligent Interaction for Reliable Human-AI Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26932": {
    "title": "Privacy-Preserving Representation Learning for Text-Attributed Networks with Simplicial Complexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26933": {
    "title": "Deep Learning for Medical Prediction in Electronic Health Records",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26934": {
    "title": "Efficient Algorithms for Regret Minimization in Billboard Advertisement (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26935": {
    "title": "Multi-Horizon Learning in Procedurally-Generated Environments for Off-Policy Reinforcement Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26936": {
    "title": "Modeling Metacognitive and Cognitive Processes in Data Science Problem Solving (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26937": {
    "title": "Hey, Siri! Why Are You Biased against Women? (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26938": {
    "title": "FV-Train: Quantum Convolutional Neural Network Training with a Finite Number of Qubits by Extracting Diverse Features (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26939": {
    "title": "PanTop: Pandemic Topic Detection and Monitoring System (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26940": {
    "title": "Social Intelligence towards Human-AI Teambuilding (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26941": {
    "title": "Robust Training for AC-OPF (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26942": {
    "title": "IdProv: Identity-Based Provenance for Synthetic Image Generation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26943": {
    "title": "Latent Space Evolution under Incremental Learning with Concept Drift (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26944": {
    "title": "Model Selection of Graph Signage Models Using Maximum Likelihood (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26945": {
    "title": "Optimal Execution via Multi-Objective Multi-Armed Bandits (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26946": {
    "title": "Lightweight Transformer for Multi-Modal Object Detection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26947": {
    "title": "Reconsidering Deception in Social Robotics: The Role of Human Vulnerability (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26948": {
    "title": "Know Your Enemy: Identifying Adversarial Behaviours in Deep Reinforcement Learning Agents (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26949": {
    "title": "An Emotion-Guided Approach to Domain Adaptive Fake News Detection Using Adversarial Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26950": {
    "title": "Deep Anomaly Detection and Search via Reinforcement Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26951": {
    "title": "Towards Deployment-Efficient and Collision-Free Multi-Agent Path Finding (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26952": {
    "title": "SkateboardAI: The Coolest Video Action Recognition for Skateboarding (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26953": {
    "title": "AsT: An Asymmetric-Sensitive Transformer for Osteonecrosis of the Femoral Head Detection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26954": {
    "title": "Self-Paced Learning Based Graph Convolutional Neural Network for Mixed Integer Programming (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26955": {
    "title": "Multi-Modal Protein Knowledge Graph Construction and Applications (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26956": {
    "title": "CasODE: Modeling Irregular Information Cascade via Neural Ordinary Differential Equations (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26957": {
    "title": "SR-AnoGAN: You Never Detect Alone. Super Resolution in Anomaly Detection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26958": {
    "title": "Transformer-Based Named Entity Recognition for French Using Adversarial Adaptation to Similar Domain Corpora (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26959": {
    "title": "Disentangling the Benefits of Self-Supervised Learning to Deployment-Driven Downstream Tasks of Satellite Images (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26960": {
    "title": "Performance Disparities between Accents in Automatic Speech Recognition (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26961": {
    "title": "Demystify the Gravity Well in the Optimization Landscape (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26962": {
    "title": "AlphaSnake: Policy Iteration on a Nondeterministic NP-Hard Markov Decision Process (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26963": {
    "title": "Transformer-Based Multi-Hop Question Generation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26964": {
    "title": "eCDANs: Efficient Temporal Causal Discovery from Autocorrelated and Non-stationary Data (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26965": {
    "title": "LEAN-DMKDE: Quantum Latent Density Estimation for Anomaly Detection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26966": {
    "title": "Safety Aware Neural Pruning for Deep Reinforcement Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26967": {
    "title": "Towards Fair and Selectively Privacy-Preserving Models Using Negative Multi-Task Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26968": {
    "title": "Towards Safe Reinforcement Learning via OOD Dynamics Detection in Autonomous Driving System (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26969": {
    "title": "Neural Implicit Surface Reconstruction from Noisy Camera Observations (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26970": {
    "title": "Expert Data Augmentation in Imitation Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26971": {
    "title": "Unsupervised Contrastive Representation Learning for 3D Mesh Segmentation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26972": {
    "title": "Invertible Conditional GAN Revisited: Photo-to-Manga Face Translation with Modern Architectures (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26973": {
    "title": "Exploring Hypergraph of Earnings Call for Risk Prediction (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26974": {
    "title": "An Analysis of the Deliberation and Task Performance of an Active Logic Based Agent (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26975": {
    "title": "Mobility Prediction via Sequential Trajectory Disentanglement (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26976": {
    "title": "A Reinforcement Learning Badminton Environment for Simulating Player Tactics (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26977": {
    "title": "Less Is More: Volatility Forecasting with Contrastive Representation Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26978": {
    "title": "Understand Restart of SAT Solver Using Search Similarity Index (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26979": {
    "title": "In-Game Toxic Language Detection: Shared Task and Attention Residuals (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26980": {
    "title": "CKS: A Community-Based K-shell Decomposition Approach Using Community Bridge Nodes for Influence Maximization (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26981": {
    "title": "Incremental Density-Based Clustering with Grid Partitioning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26982": {
    "title": "Sequential Graph Attention Learning for Predicting Dynamic Stock Trends (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26983": {
    "title": "Mitigating Negative Transfer in Multi-Task Learning with Exponential Moving Average Loss Weighting Strategies (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26984": {
    "title": "A Federated Learning Monitoring Tool for Self-Driving Car Simulation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26985": {
    "title": "Summarization Attack via Paraphrasing (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26986": {
    "title": "Evaluating Robustness of Vision Transformers on Imbalanced Datasets (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26987": {
    "title": "On Analyzing the Role of Image for Visual-Enhanced Relation Extraction (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26988": {
    "title": "Double Policy Network for Aspect Sentiment Triplet Extraction (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26989": {
    "title": "Learning Generalizable Batch Active Learning Strategies via Deep Q-networks (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26990": {
    "title": "Cross-Regional Fraud Detection via Continual Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26991": {
    "title": "Category-Guided Visual Question Generation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26992": {
    "title": "Can Graph Neural Networks Learn to Solve the MaxSAT Problem? (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26993": {
    "title": "Flaky Performances When Pretraining on Relational Databases (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26994": {
    "title": "A Highly Efficient Marine Mammals Classifier Based on a Cross-Covariance Attended Compact Feed-Forward Sequential Memory Network (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26995": {
    "title": "MGIA: Mutual Gradient Inversion Attack in Multi-Modal Federated Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26996": {
    "title": "Semi-supervised Review-Aware Rating Regression (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26997": {
    "title": "Toplogical Data Analysis Detects and Classifies Sunspots (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26998": {
    "title": "Risk-Aware Decentralized Safe Control via Dynamic Responsibility Allocation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/26999": {
    "title": "A Mutually Enhanced Bidirectional Approach for Jointly Mining User Demand and Sentiment (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27000": {
    "title": "Debiasing Intrinsic Bias and Application Bias Jointly via Invariant Risk Minimization (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27001": {
    "title": "Label Smoothing for Emotion Detection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27002": {
    "title": "Counting Knot Mosaics with ALLSAT (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27003": {
    "title": "Novel Intent Detection and Active Learning Based Classification (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27004": {
    "title": "Pre-training with Scientific Text Improves Educational Question Generation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27005": {
    "title": "Fraud's Bargain Attacks to Textual Classifiers via Metropolis-Hasting Sampling (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27006": {
    "title": "Improving Adversarial Robustness to Sensitivity and Invariance Attacks with Deep Metric Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27007": {
    "title": "LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27008": {
    "title": "Hardness of Learning AES Key (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27009": {
    "title": "Generative Pipeline for Data Augmentation of Unconstrained Document Images with Structural and Textural Degradation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27010": {
    "title": "Neural Language Model Based Attentive Term Dependence Model for Verbose Query (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27011": {
    "title": "Evaluating Factors Influencing COVID-19 Outcomes across Countries Using Decision Trees (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27012": {
    "title": "Ordinal Programmatic Weak Supervision and Crowdsourcing for Estimating Cognitive States (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27013": {
    "title": "A Probabilistic Graph Diffusion Model for Source Localization (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27014": {
    "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27015": {
    "title": "Fuzzy C-means: Differences on Clustering Behavior between High Dimensional and Functional Data (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27016": {
    "title": "Photogrammetry and VR for Comparing 2D and Immersive Linguistic Data Collection (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27017": {
    "title": "RFC-Net: Learning High Resolution Global Features for Medical Image Segmentation on a Computational Budget (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27018": {
    "title": "Maximizing Influence Spread through a Dynamic Social Network (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27019": {
    "title": "Can You Answer This? – Exploring Zero-Shot QA Generalization Capabilities in Large Language Models (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27020": {
    "title": "FakeKG: A Knowledge Graph of Fake Claims for Improving Automated Fact-Checking (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27021": {
    "title": "Can Adversarial Networks Make Uninformative Colonoscopy Video Frames Clinically Informative? (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27022": {
    "title": "Bayesian Models for Targeted Cyber Deception Strategies (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27023": {
    "title": "Scalable Negotiating Agent Strategy via Multi-Issue Policy Network (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27024": {
    "title": "Efficient Dynamic Batch Adaptation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27025": {
    "title": "Development of a Human-Agent Interaction System including Norm and Emotion in an Evacuation Situation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27026": {
    "title": "Persistent Homology through Image Segmentation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27027": {
    "title": "TA-DA: Topic-Aware Domain Adaptation for Scientific Keyphrase Identification and Classification (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27028": {
    "title": "Exploring the Relative Value of Collaborative Optimisation Pathways (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27029": {
    "title": "Backforward Propagation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27030": {
    "title": "Two-Streams: Dark and Light Networks with Graph Convolution for Action Recognition from Dark Videos (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27031": {
    "title": "ES-Mask: Evolutionary Strip Mask for Explaining Time Series Prediction (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27032": {
    "title": "Exploration on Physics-Informed Neural Networks on Partial Differential Equations (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27033": {
    "title": "Parallel Index-Based Search Algorithm for Coalition Structure Generation (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27034": {
    "title": "The Naughtyformer: A Transformer Understands and Moderates Adult Humor (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27035": {
    "title": "Exploring the Effectiveness of Mask-Guided Feature Modulation as a Mechanism for Localized Style Editing of Real Images (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27036": {
    "title": "Global Explanations for Image Classifiers (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27037": {
    "title": "Quantify the Political Bias in News Edits: Experiments with Few-Shot Learners (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27038": {
    "title": "Anti-drifting Feature Selection via Deep Reinforcement Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27039": {
    "title": "Learning Dynamic Temporal Relations with Continuous Graph for Multivariate Time Series Forecasting (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27040": {
    "title": "Enhancing Dynamic GCN for Node Attribute Forecasting with Meta Spatial-Temporal Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27041": {
    "title": "Tackling Safe and Efficient Multi-Agent Reinforcement Learning via Dynamic Shielding (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27042": {
    "title": "Long Legal Article Question Answering via Cascaded Key Segment Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27043": {
    "title": "Improving Dialogue Intent Classification with a Knowledge-Enhanced Multifactor Graph Model (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27044": {
    "title": "Class Incremental Learning for Task-Oriented Dialogue System with Contrastive Distillation on Internal Representations (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27045": {
    "title": "ACCD: An Adaptive Clustering-Based Collusion Detector in Crowdsourcing (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27046": {
    "title": "Logic Error Localization and Correction with Machine Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27047": {
    "title": "Mask-Net: Learning Context Aware Invariant Features Using Adversarial Forgetting (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27048": {
    "title": "Adaptive Constraint Partition Based Optimization Framework for Large-Scale Integer Linear Programming (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27049": {
    "title": "Clustered Federated Learning for Heterogeneous Data (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27050": {
    "title": "Measuring the Privacy Leakage via Graph Reconstruction Attacks on Simplicial Neural Networks (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27051": {
    "title": "DyCVAE: Learning Dynamic Causal Factors for Non-stationary Series Domain Generalization (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27052": {
    "title": "HaPPy: Harnessing the Wisdom from Multi-Perspective Graphs for Protein-Ligand Binding Affinity Prediction (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27053": {
    "title": "Graph of Graphs: A New Knowledge Representation Mechanism for Graph Learning (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27054": {
    "title": "Exploiting High-Order Interaction Relations to Explore User Intent (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27055": {
    "title": "Feature Decomposition for Reducing Negative Transfer: A Novel Multi-Task Learning Method for Recommender System (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27056": {
    "title": "Model-Based Offline Weighted Policy Optimization (Student Abstract)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27057": {
    "title": "ConceptX: A Framework for Latent Concept Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27058": {
    "title": "SOREO: A System for Safe and Autonomous Drones Fleet Navigation with Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27059": {
    "title": "A Tool for Generating Controllable Variations of Musical Themes Using Variational Autoencoders with Latent Space Regularisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27060": {
    "title": "Dagster: Parallel Structured Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27061": {
    "title": "AI-SNIPS: A Platform for Network Intelligence-Based Pharmaceutical Security",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27062": {
    "title": "TgrApp: Anomaly Detection and Visualization of Large-Scale Call Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27063": {
    "title": "TUTORING: Instruction-Grounded Conversational Agent for Language Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27064": {
    "title": "HAPI Explorer: Comprehension, Discovery, and Explanation on History of ML APIs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27065": {
    "title": "EasyRec: An Easy-to-Use, Extendable and Efficient Framework for Building Industrial Recommendation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27066": {
    "title": "edBB-Demo: Biometrics and Behavior Analysis for Online Educational Platforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27067": {
    "title": "DUCK: A Drone-Urban Cyber-Defense Framework Based on Pareto-Optimal Deontic Logic Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27068": {
    "title": "NL2LTL – a Python Package for Converting Natural Language (NL) Instructions to Linear Temporal Logic (LTL) Formulas",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27069": {
    "title": "DISPUTool 2.0: A Modular Architecture for Multi-Layer Argumentative Analysis of Political Debates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27070": {
    "title": "Generating Reflective Questions for Engaging Gallery Visitors in ArtMuse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27071": {
    "title": "Augmenting Flight Training with AI to Efficiently Train Pilots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27072": {
    "title": "Sudoku Assistant – an AI-Powered App to Help Solve Pen-and-Paper Sudokus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27073": {
    "title": "DFEE: Interactive DataFlow Execution and Evaluation Kit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27074": {
    "title": "NCTV: Neural Clamping Toolkit and Visualization for Neural Network Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27075": {
    "title": "Kajibuntan: A House Chore Division App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27076": {
    "title": "MARCOL: A Maritime Collision Avoidance Decision-Making Testbed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27077": {
    "title": "FC-TrackNet: Fast Convergence Net for 6D Pose Tracking in Synthetic Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27078": {
    "title": "Robust-MSA: Understanding the Impact of Modality Noise on Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27079": {
    "title": "GAAMA 2.0: An Integrated System That Answers Boolean and Extractive Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27080": {
    "title": "BiRDy: Bullying Role Detection in Multi-Party Chats",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27081": {
    "title": "AI Model Factory: Scaling AI for Industry 4.0 Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27082": {
    "title": "nBIIG: A Neural BI Insights Generation System for Table Reporting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27083": {
    "title": "Prototyping Logic-Based AI Services with LogicUS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27084": {
    "title": "KnowGL: Knowledge Generation and Linking from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27085": {
    "title": "Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27086": {
    "title": "Task2KB: A Public Task-Oriented Knowledge Base",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27087": {
    "title": "CodeStylist: A System for Performing Code Style Transfer Using Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27088": {
    "title": "AnoViz: A Visual Inspection Tool of Anomalies in Multivariate Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27089": {
    "title": "CLUE-AD: A Context-Based Method for Labeling Unobserved Entities in Autonomous Driving Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27090": {
    "title": "An Online Presentation Slide Assessment System Using Visual and Semantic Segmentation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}